,number,title,labels,body,assignees,milestone,comments,created_at,closed_at,author_association,body.1,address_date,Closing_time,Author_Priority,Closetime_class,hotness
0,17069,tf.cast() can't cast string to number,stat:awaiting response,"OS: Win10 64bit
Tensorflow version: 1.5.0

It looks like tf.cast() can't cast string to number,  but the documents doesn't explain it.
I found this problem when I migrated older versions of tensorflow code to a new version.

When I used tf.cast() to cast string to float, I got error ""Unimplemented: Cast string to float is not supported"". 
And when I used tf.string_to_number () instead of tf.cast (), the problem was solved.

",0,,1,2018-02-16T15:52:53Z,2018-02-17T03:29:54Z,NONE,"OS: Win10 64bit
Tensorflow version: 1.5.0

It looks like tf.cast() can't cast string to number,  but the documents doesn't explain it.
I found this problem when I migrated older versions of tensorflow code to a new version.

When I used tf.cast() to cast string to float, I got error ""Unimplemented: Cast string to float is not supported"". 
And when I used tf.string_to_number () instead of tf.cast (), the problem was solved.

",2018-02-17T01:38:59Z,1,1,0,1.88987124525128
1,17068,Get variable mapping (dictionary) after checkpoint restore,"stat:awaiting response,type:support","It is a feature request.

Please provide a way to get the list of Tensorflow variables restored from a checkpoint and corresponding Python variables.

More info:

Consider that you have this code:

```
    matrix_1 = tf.Variable([[1, 2], [3, 4]], name=""matrix1"")
    matrix_2 = tf.Variable([[5, 6], [7, 8]], name=""matrix2"")
```

When a tensorflow checkpoint is created it has matrix1 and matrix2 variables saved. It is easy to understand that matrix1 (in TF namespace) corresponds to matrix_1 (in Python namespace).

Now consider that the code is:

```
    matrix_1 = tf.Variable([[1, 2], [3, 4]])
    matrix_2 = tf.Variable([[5, 6], [7, 8]])
```

A tensorflow checkpoint will have Variable_1:0 and Variable_2:0 variables saved. And it is not obvious at all that Variable_1:0 (in TF namespace) corresponds to matrix_1 (in Python namespace). Especially if there are dozens of variables.

There should be a way to get a list of corresponding variables. For example:
```
Variable_1 = matrix_1
Variable_2 = matrix_2
...
Variable_100 = relu_1_weights
```


",0,,4,2018-02-16T12:50:26Z,2018-02-16T22:37:39Z,NONE,"It is a feature request.

Please provide a way to get the list of Tensorflow variables restored from a checkpoint and corresponding Python variables.

More info:

Consider that you have this code:

```
    matrix_1 = tf.Variable([[1, 2], [3, 4]], name=""matrix1"")
    matrix_2 = tf.Variable([[5, 6], [7, 8]], name=""matrix2"")
```

When a tensorflow checkpoint is created it has matrix1 and matrix2 variables saved. It is easy to understand that matrix1 (in TF namespace) corresponds to matrix_1 (in Python namespace).

Now consider that the code is:

```
    matrix_1 = tf.Variable([[1, 2], [3, 4]])
    matrix_2 = tf.Variable([[5, 6], [7, 8]])
```

A tensorflow checkpoint will have Variable_1:0 and Variable_2:0 variables saved. And it is not obvious at all that Variable_1:0 (in TF namespace) corresponds to matrix_1 (in Python namespace). Especially if there are dozens of variables.

There should be a way to get a list of corresponding variables. For example:
```
Variable_1 = matrix_1
Variable_2 = matrix_2
...
Variable_100 = relu_1_weights
```


",2018-02-16T19:19:47Z,0,1,0,3.38987124525128
2,17066,Update guide.md,cla: yes,Fixing grammatical errors in the Installation instructions,0,,3,2018-02-16T12:12:57Z,2018-02-16T18:06:39Z,CONTRIBUTOR,Fixing grammatical errors in the Installation instructions,2018-02-16T12:15:36Z,0,2,0,3.88987124525128
3,17060,OSError: [Errno 13] Permission denied.. TensorFlow couldn't install,stat:awaiting response,"Exception:
Traceback (most recent call last):
  File ""/Library/Python/2.7/site-packages/pip-9.0.1-py2.7.egg/pip/basecommand.py"", line 215, in main
    status = self.run(options, args)
  File ""/Library/Python/2.7/site-packages/pip-9.0.1-py2.7.egg/pip/commands/install.py"", line 342, in run
    prefix=options.prefix_path,
  File ""/Library/Python/2.7/site-packages/pip-9.0.1-py2.7.egg/pip/req/req_set.py"", line 784, in install
    **kwargs
  File ""/Library/Python/2.7/site-packages/pip-9.0.1-py2.7.egg/pip/req/req_install.py"", line 851, in install
    self.move_wheel_files(self.source_dir, root=root, prefix=prefix)
  File ""/Library/Python/2.7/site-packages/pip-9.0.1-py2.7.egg/pip/req/req_install.py"", line 1064, in move_wheel_files
    isolated=self.isolated,
  File ""/Library/Python/2.7/site-packages/pip-9.0.1-py2.7.egg/pip/wheel.py"", line 345, in move_wheel_files
    clobber(source, lib_dir, True)
  File ""/Library/Python/2.7/site-packages/pip-9.0.1-py2.7.egg/pip/wheel.py"", line 316, in clobber
    ensure_dir(destdir)
  File ""/Library/Python/2.7/site-packages/pip-9.0.1-py2.7.egg/pip/utils/__init__.py"", line 83, in ensure_dir
    os.makedirs(path)
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/os.py"", line 157, in makedirs
    mkdir(name, mode)
OSError: [Errno 13] Permission denied: '/Library/Python/2.7/site-packages/pbr-3.1.1.dist-info'",0,,3,2018-02-16T09:35:49Z,2018-02-16T18:29:47Z,NONE,"Exception:
Traceback (most recent call last):
  File ""/Library/Python/2.7/site-packages/pip-9.0.1-py2.7.egg/pip/basecommand.py"", line 215, in main
    status = self.run(options, args)
  File ""/Library/Python/2.7/site-packages/pip-9.0.1-py2.7.egg/pip/commands/install.py"", line 342, in run
    prefix=options.prefix_path,
  File ""/Library/Python/2.7/site-packages/pip-9.0.1-py2.7.egg/pip/req/req_set.py"", line 784, in install
    **kwargs
  File ""/Library/Python/2.7/site-packages/pip-9.0.1-py2.7.egg/pip/req/req_install.py"", line 851, in install
    self.move_wheel_files(self.source_dir, root=root, prefix=prefix)
  File ""/Library/Python/2.7/site-packages/pip-9.0.1-py2.7.egg/pip/req/req_install.py"", line 1064, in move_wheel_files
    isolated=self.isolated,
  File ""/Library/Python/2.7/site-packages/pip-9.0.1-py2.7.egg/pip/wheel.py"", line 345, in move_wheel_files
    clobber(source, lib_dir, True)
  File ""/Library/Python/2.7/site-packages/pip-9.0.1-py2.7.egg/pip/wheel.py"", line 316, in clobber
    ensure_dir(destdir)
  File ""/Library/Python/2.7/site-packages/pip-9.0.1-py2.7.egg/pip/utils/__init__.py"", line 83, in ensure_dir
    os.makedirs(path)
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/os.py"", line 157, in makedirs
    mkdir(name, mode)
OSError: [Errno 13] Permission denied: '/Library/Python/2.7/site-packages/pbr-3.1.1.dist-info'",2018-02-16T19:38:04Z,0,1,0,2.88987124525128
4,17059,Dataset API does not pass dimensionality information for its output tensor,,"SYSTEM INFO
python version: Python 2.7.12
tensorflow version: 1.4.0
CUDA version: 8.0
Ubuntu version: Ubuntu 16.04 LTS

----------------------------------------------------------------UPDATE-------------------------------------------------------------------

Please directly go to my third post which reproduces my issue with the minimum amount of code

----------------------------------------------------------UPDATE FINISHED----------------------------------------------------------

[https://github.com/tensorflow/tensorflow/issues/13348](url)

My problem is very similar to the issue above but I did not find solution in that post.

```
def image_parser(image_name):
    im_file = os.path.join(cfg.DATA_DIR, 'demo', image_name)
    im = cv2.imread(im_file)
    blobs, im_scales = _get_blobs(im)
    assert len(im_scales) == 1, ""Only single-image batch implemented""
    im_blob = blobs['data']
    blobs['im_info'] = np.array([im_blob.shape[1], im_blob.shape[2], im_scales[0]], dtype=np.float32)
    return blobs['data'], blobs['im_info'], im_scales, im

def _get_blobs(im):
  """"""Convert an image and RoIs within that image into network inputs.""""""
  blobs = {}
  blobs['data'], im_scale_factors = _get_image_blob(im)

  return blobs, im_scale_factors

def _get_image_blob(im):
  """"""Converts an image into a network input.
  Arguments:
    im (ndarray): a color image in BGR order
  Returns:
    blob (ndarray): a data blob holding an image pyramid
    im_scale_factors (list): list of image scales (relative to im) used
      in the image pyramid
  """"""
  im_orig = im.astype(np.float32, copy=True)
  im_orig -= cfg.PIXEL_MEANS

  im_shape = im_orig.shape
  im_size_min = np.min(im_shape[0:2])
  im_size_max = np.max(im_shape[0:2])

  processed_ims = []
  im_scale_factors = []

  for target_size in cfg.TEST.SCALES:
    im_scale = float(target_size) / float(im_size_min)
    # Prevent the biggest axis from being more than MAX_SIZE
    if np.round(im_scale * im_size_max) > cfg.TEST.MAX_SIZE:
      im_scale = float(cfg.TEST.MAX_SIZE) / float(im_size_max)
    im = cv2.resize(im_orig, None, None, fx=im_scale, fy=im_scale,
            interpolation=cv2.INTER_LINEAR)
    im_scale_factors.append(im_scale)
    processed_ims.append(im)

  # Create a blob to hold the input images
  blob = im_list_to_blob(processed_ims)
  return blob, np.array(im_scale_factors)


def im_list_to_blob(ims):
  """"""Convert a list of images into a network input.

  Assumes images are already prepared (means subtracted, BGR order, ...).
  """"""
  max_shape = np.array([im.shape for im in ims]).max(axis=0)
  num_images = len(ims)
  blob = np.zeros((num_images, max_shape[0], max_shape[1], 3),
                  dtype=np.float32)
  for i in range(num_images):
    im = ims[i]
    blob[i, 0:im.shape[0], 0:im.shape[1], :] = im

  return blob
```

I defined a function `image_parser` which will parse image file name into 4 numpy arrays (it has three subsequent function calls,  ` _get_blobs`, ` _get_image_blob`,  `im_list_to_blob`)

Then I construct a dataset with `py_func` mapping to form the input pipeline:

   ```
    images = []
    for root, dirs, files in os.walk('./data/demo/'):
        for file in files:
            if file.endswith('jpg'):
                images.append(file)

    # dataset construction
    im_dataset = tf.data.Dataset.from_tensor_slices(images)
    im_dataset = im_dataset.map(lambda image:
                                tuple(tf.py_func(image_parser, [image], [tf.float32, tf.float32, tf.float64, tf.uint8])),
                                num_parallel_calls = 2)
    im_dataset = im_dataset.prefetch(4)
    print(""output data type is "", im_dataset.output_types)
    print(""output data shape is "", im_dataset.output_shapes)
    iterator = im_dataset.make_initializable_iterator()
    with tf.Session() as sess:
        sess.run(iterator.initializer)
        a = sess.run(iterator.get_next())
    print(""shape of the run results are: "")
    print(a[0].shape)
    print(a[1].shape)
    print(a[2].shape)
    print(a[3].shape)
```

When I print the shape of my output tensors from dataset right after my dataset construction, the print is:
```
output data type is  (tf.float32, tf.float32, tf.float64, tf.uint8)
output data shape is  (TensorShape(None), TensorShape(None), TensorShape(None), TensorShape(None))
```
the shape of tensors is None.

However I could print out the output tensor shape after my sess.run. 
```
shape of the run results are: 
(1, 600, 800, 3)
(3,)
(1,)
(375, 500, 3)
```

I need the shape of the output tensor from dataset to be defined to feed into my graph. Thanks!",0,,4,2018-02-16T07:09:31Z,2018-02-17T12:03:43Z,NONE,"SYSTEM INFO
python version: Python 2.7.12
tensorflow version: 1.4.0
CUDA version: 8.0
Ubuntu version: Ubuntu 16.04 LTS

----------------------------------------------------------------UPDATE-------------------------------------------------------------------

Please directly go to my third post which reproduces my issue with the minimum amount of code

----------------------------------------------------------UPDATE FINISHED----------------------------------------------------------

[https://github.com/tensorflow/tensorflow/issues/13348](url)

My problem is very similar to the issue above but I did not find solution in that post.

```
def image_parser(image_name):
    im_file = os.path.join(cfg.DATA_DIR, 'demo', image_name)
    im = cv2.imread(im_file)
    blobs, im_scales = _get_blobs(im)
    assert len(im_scales) == 1, ""Only single-image batch implemented""
    im_blob = blobs['data']
    blobs['im_info'] = np.array([im_blob.shape[1], im_blob.shape[2], im_scales[0]], dtype=np.float32)
    return blobs['data'], blobs['im_info'], im_scales, im

def _get_blobs(im):
  """"""Convert an image and RoIs within that image into network inputs.""""""
  blobs = {}
  blobs['data'], im_scale_factors = _get_image_blob(im)

  return blobs, im_scale_factors

def _get_image_blob(im):
  """"""Converts an image into a network input.
  Arguments:
    im (ndarray): a color image in BGR order
  Returns:
    blob (ndarray): a data blob holding an image pyramid
    im_scale_factors (list): list of image scales (relative to im) used
      in the image pyramid
  """"""
  im_orig = im.astype(np.float32, copy=True)
  im_orig -= cfg.PIXEL_MEANS

  im_shape = im_orig.shape
  im_size_min = np.min(im_shape[0:2])
  im_size_max = np.max(im_shape[0:2])

  processed_ims = []
  im_scale_factors = []

  for target_size in cfg.TEST.SCALES:
    im_scale = float(target_size) / float(im_size_min)
    # Prevent the biggest axis from being more than MAX_SIZE
    if np.round(im_scale * im_size_max) > cfg.TEST.MAX_SIZE:
      im_scale = float(cfg.TEST.MAX_SIZE) / float(im_size_max)
    im = cv2.resize(im_orig, None, None, fx=im_scale, fy=im_scale,
            interpolation=cv2.INTER_LINEAR)
    im_scale_factors.append(im_scale)
    processed_ims.append(im)

  # Create a blob to hold the input images
  blob = im_list_to_blob(processed_ims)
  return blob, np.array(im_scale_factors)


def im_list_to_blob(ims):
  """"""Convert a list of images into a network input.

  Assumes images are already prepared (means subtracted, BGR order, ...).
  """"""
  max_shape = np.array([im.shape for im in ims]).max(axis=0)
  num_images = len(ims)
  blob = np.zeros((num_images, max_shape[0], max_shape[1], 3),
                  dtype=np.float32)
  for i in range(num_images):
    im = ims[i]
    blob[i, 0:im.shape[0], 0:im.shape[1], :] = im

  return blob
```

I defined a function `image_parser` which will parse image file name into 4 numpy arrays (it has three subsequent function calls,  ` _get_blobs`, ` _get_image_blob`,  `im_list_to_blob`)

Then I construct a dataset with `py_func` mapping to form the input pipeline:

   ```
    images = []
    for root, dirs, files in os.walk('./data/demo/'):
        for file in files:
            if file.endswith('jpg'):
                images.append(file)

    # dataset construction
    im_dataset = tf.data.Dataset.from_tensor_slices(images)
    im_dataset = im_dataset.map(lambda image:
                                tuple(tf.py_func(image_parser, [image], [tf.float32, tf.float32, tf.float64, tf.uint8])),
                                num_parallel_calls = 2)
    im_dataset = im_dataset.prefetch(4)
    print(""output data type is "", im_dataset.output_types)
    print(""output data shape is "", im_dataset.output_shapes)
    iterator = im_dataset.make_initializable_iterator()
    with tf.Session() as sess:
        sess.run(iterator.initializer)
        a = sess.run(iterator.get_next())
    print(""shape of the run results are: "")
    print(a[0].shape)
    print(a[1].shape)
    print(a[2].shape)
    print(a[3].shape)
```

When I print the shape of my output tensors from dataset right after my dataset construction, the print is:
```
output data type is  (tf.float32, tf.float32, tf.float64, tf.uint8)
output data shape is  (TensorShape(None), TensorShape(None), TensorShape(None), TensorShape(None))
```
the shape of tensors is None.

However I could print out the output tensor shape after my sess.run. 
```
shape of the run results are: 
(1, 600, 800, 3)
(3,)
(1,)
(375, 500, 3)
```

I need the shape of the output tensor from dataset to be defined to feed into my graph. Thanks!",2018-02-17T09:21:38Z,1,1,0,3.38987124525128
5,17040,conv1d doc string misnames first argument,cla: yes,The docs say `conv2d()`'s first argument is `input` which is maybe how the docstring for `conv1d()` ended up saying `input` instead of `value` when describing the `filters` argument.,0,,3,2018-02-15T13:11:14Z,2018-02-16T01:39:23Z,CONTRIBUTOR,The docs say `conv2d()`'s first argument is `input` which is maybe how the docstring for `conv1d()` ended up saying `input` instead of `value` when describing the `filters` argument.,2018-02-15T13:13:20Z,1,2,0,3.8789231816899514
6,17022,Is python 3.7.x supported with Tensorflow,,"Ive been trying to install Tensorflow on my computer which currently runs python 3.7, however I keep running into some common issues... And each time i try to use the solutions provided, nothing works. 

Im not sure, but Im guessing python 3.7 might not be supported considering the official Tensorflow page has no link to python 3.7, that maybe this is the reason I havent been able to correctly install Tensorflow.",0,,1,2018-02-15T00:00:58Z,2018-02-15T01:37:46Z,NONE,"Ive been trying to install Tensorflow on my computer which currently runs python 3.7, however I keep running into some common issues... And each time i try to use the solutions provided, nothing works. 

Im not sure, but Im guessing python 3.7 might not be supported considering the official Tensorflow page has no link to python 3.7, that maybe this is the reason I havent been able to correctly install Tensorflow.",2018-02-15T06:53:07Z,0,1,0,1.8789231816899512
7,17020,"""DeprecationWarning: The binary mode of fromstring is deprecated"" warning appears in some cases",,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: Binary, tf-nightly
- **TensorFlow version (use command below)**: 1.7.0.dev20180214, git version b2a0f1c
- **Python version**: 2.7.12
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**:

```python
import tensorflow as tf
from tensorflow.contrib.image.ops import gen_distort_image_ops
from tensorflow.python.framework import tensor_util
tensor_util.constant_value(tf.convert_to_tensor([1., 1.]))
```

### Describe the problem
When I run the program above, I get the warning
```
/home/reedwm/venvs/tfnightlycpu/local/lib/python2.7/site-packages/tensorflow/python/framework/tensor_util.py:560: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead
  return np.fromstring(tensor.tensor_content, dtype=dtype).reshape(shape)
```

This may not seem so bad, but when running [`tf_cnn_benchmarks`](https://github.com/tensorflow/benchmarks/tree/master/scripts/tf_cnn_benchmarks), I get hundreds of such warnings.

What's very strange is that if I comment the line `from tensorflow.contrib.image.ops import gen_distort_image_ops`, I don't get the warning.

This is the same issue as ppwwyyxx/tensorpack#641. @yaroslavvb, did you file a TensorFlow bug for this? If so, this can be marked as a duplicate.

Not really sure who to triage this to. /CC @mrry, can you address this or retriage?
",1,,2,2018-02-14T23:34:27Z,2018-02-16T21:00:25Z,MEMBER,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: Binary, tf-nightly
- **TensorFlow version (use command below)**: 1.7.0.dev20180214, git version b2a0f1c
- **Python version**: 2.7.12
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**:

```python
import tensorflow as tf
from tensorflow.contrib.image.ops import gen_distort_image_ops
from tensorflow.python.framework import tensor_util
tensor_util.constant_value(tf.convert_to_tensor([1., 1.]))
```

### Describe the problem
When I run the program above, I get the warning
```
/home/reedwm/venvs/tfnightlycpu/local/lib/python2.7/site-packages/tensorflow/python/framework/tensor_util.py:560: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead
  return np.fromstring(tensor.tensor_content, dtype=dtype).reshape(shape)
```

This may not seem so bad, but when running [`tf_cnn_benchmarks`](https://github.com/tensorflow/benchmarks/tree/master/scripts/tf_cnn_benchmarks), I get hundreds of such warnings.

What's very strange is that if I comment the line `from tensorflow.contrib.image.ops import gen_distort_image_ops`, I don't get the warning.

This is the same issue as ppwwyyxx/tensorpack#641. @yaroslavvb, did you file a TensorFlow bug for this? If so, this can be marked as a duplicate.

Not really sure who to triage this to. /CC @mrry, can you address this or retriage?
",2018-02-14T23:43:17Z,2,3,0,5.369269373068855
8,17010,Fixes #16976,cla: yes,,1,,7,2018-02-14T13:30:17Z,2018-02-17T08:01:22Z,CONTRIBUTOR,,2018-02-14T13:49:36Z,3,2,1,6.869269373068855
9,17000,MKL: Fixing MklCPUAllocator error introduced by commit #1baac78,"awaiting testing (then merge),cla: yes",PR #16987 also fixes this problem. Commit 1baac78 also broke http://ci.tensorflow.org/view/Nightly/job/nightly-mkl/127,0,,1,2018-02-14T05:24:45Z,2018-02-16T00:39:32Z,CONTRIBUTOR,PR #16987 also fixes this problem. Commit 1baac78 also broke http://ci.tensorflow.org/view/Nightly/job/nightly-mkl/127,2018-02-14T16:44:17Z,2,2,0,2.869269373068855
10,16996,Relu bn fix,cla: no,"1. Fix cifar 10 divergence issue due to MKL layout mismatch.
2. Fix BatchNorm unit test failures",0,,2,2018-02-14T00:42:55Z,2018-02-14T02:02:23Z,CONTRIBUTOR,"1. Fix cifar 10 divergence issue due to MKL layout mismatch.
2. Fix BatchNorm unit test failures",2018-02-14T02:02:15Z,0,2,0,3.369269373068855
11,16987,fixup 1baac78627: Underlying allocator must be a VisitableAllocator too,cla: no,Signed-off-by: Sylvain Gault <sylvain.gault@road-b-score.com>,0,,4,2018-02-13T19:22:01Z,2018-02-16T11:50:41Z,NONE,Signed-off-by: Sylvain Gault <sylvain.gault@road-b-score.com>,2018-02-16T00:43:44Z,3,1,1,3.360673760222241
12,16986,ParseSingleExample op is missing from op_def_registry.get_registered_ops(),,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: Pip install
- **TensorFlow version (use command below)**: cpu 1.5.0
- **Python version**: 3.0

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem

**ParseSingleSample** is available as tf.parse_single_example() method call, but it is not listed in the op_def_registry.get_registered_ops()

Which cause freeze_graph.py call to fail with following error:
ValueError: No op named ParseSingleExample in defined operations.

When converting mobilenet model with training information.

",0,,1,2018-02-13T19:19:42Z,2018-02-13T20:55:36Z,NONE,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: Pip install
- **TensorFlow version (use command below)**: cpu 1.5.0
- **Python version**: 3.0

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem

**ParseSingleSample** is available as tf.parse_single_example() method call, but it is not listed in the op_def_registry.get_registered_ops()

Which cause freeze_graph.py call to fail with following error:
ValueError: No op named ParseSingleExample in defined operations.

When converting mobilenet model with training information.

",2018-02-13T20:55:36Z,0,1,0,1.860673760222241
13,16984,New roadmap,cla: no,Revised Roadmap document with draft changes from Feb 2018.,0,,3,2018-02-13T16:04:56Z,2018-02-15T00:23:57Z,MEMBER,Revised Roadmap document with draft changes from Feb 2018.,2018-02-15T12:51:10Z,2,3,0,4.860673760222241
14,16983,Importing graph with tf.contrib.resampler.resampler fails,stat:awaiting response,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
OSx High Sierra
- **TensorFlow installed from (source or binary)**:
pip install
- **TensorFlow version (use command below)**:
1.5.0
- **Python version**: 
3.5.4
- **CUDA/cuDNN version**:
CPU
- **Bazel version (if compiling from source)**:
N/A
- **GPU model and memory**:
N/A
- **Exact command to reproduce**:
See below


### Describe the problem
Importing a graph def with a  `tf.contrib.resampler.resampler` op fails iff `tf.contrib` is not imported first.

Execute:
```
import tensorflow as tf

def export_model(filename, sess, output_node_names):
    from tensorflow.python.framework import graph_util
    output_graph_def = graph_util.convert_variables_to_constants(sess,
                                                                 sess.graph.as_graph_def(add_shapes=True),
                                                                 output_node_names)
    with tf.gfile.GFile(filename, ""wb"") as f:
        f.write(output_graph_def.SerializeToString())
        
def read_frozen_protobuf(path):
    with tf.gfile.FastGFile(str(path), 'rb') as f:
        graph_def = tf.GraphDef()
        graph_def.ParseFromString(f.read())
        return graph_def

    
def export(filename):
    tf.reset_default_graph()
    g = tf.Graph()
    with tf.Session(graph=g, config=tf.ConfigProto(allow_soft_placement=True)) as sess:
        images = tf.placeholder(dtype=tf.float64, shape=[32, 32], name='images')
        points = tf.placeholder(dtype=tf.float64, shape=[32, 2], name='points')
        resampled = tf.contrib.resampler.resampler(images, points, name='resampled')
        output_node_names = ['resampled/Resampler']
        export_model(filename, sess, output_node_names)
        
def load(filename):
    import numpy as np
    tf.reset_default_graph()
    g = tf.Graph()
    with tf.Session(graph=g, config=tf.ConfigProto(allow_soft_placement=True)) as sess:
        images = np.zeros((32, 32), dtype=np.float64)
        points = np.zeros((32, 2), dtype=np.float64)
        graph_def = read_frozen_protobuf(filename)
        tf.import_graph_def(graph_def, 
                            input_map={'images': images,
                                       'points': points},
                            return_elements=['resampled/Resampler:0'])
        
######################################################
frozen_graph_def = '/tmp/test.frozen'
export(frozen_graph_def)
load(frozen_graph_def)
```

Then, in a new interpreter (where the load(..) function is defined), execute:
```
frozen_graph_def = '/tmp/test.frozen'
load(frozen_graph_def)
```

This give the error message:
```
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-1-64251e160f7d> in <module>()
     42 frozen_graph_def = '/tmp/test.frozen'
     43 # export(frozen_graph_def)
---> 44 load(frozen_graph_def)

<ipython-input-1-64251e160f7d> in load(filename)
     37                             input_map={'images': images,
     38                                        'points': points},
---> 39                             return_elements=['resampled/Resampler:0'])
     40 
     41 ######################################################################

/lib/python3.5/site-packages/tensorflow/python/util/deprecation.py in new_func(*args, **kwargs)
    314                 'in a future version' if date is None else ('after %s' % date),
    315                 instructions)
--> 316       return func(*args, **kwargs)
    317     return tf_decorator.make_decorator(func, new_func, 'deprecated',
    318                                        _add_deprecated_arg_notice_to_docstring(

/lib/python3.5/site-packages/tensorflow/python/framework/importer.py in import_graph_def(graph_def, input_map, return_elements, name, op_dict, producer_op_list)
    539         # Set any default attr values that aren't present.
    540         if node.op not in op_dict:
--> 541           raise ValueError('No op named %s in defined operations.' % node.op)
    542         op_def = op_dict[node.op]
    543         for attr_def in op_def.attr:

ValueError: No op named Resampler in defined operations.
```",0,,2,2018-02-13T15:16:08Z,2018-02-15T09:53:59Z,NONE,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
OSx High Sierra
- **TensorFlow installed from (source or binary)**:
pip install
- **TensorFlow version (use command below)**:
1.5.0
- **Python version**: 
3.5.4
- **CUDA/cuDNN version**:
CPU
- **Bazel version (if compiling from source)**:
N/A
- **GPU model and memory**:
N/A
- **Exact command to reproduce**:
See below


### Describe the problem
Importing a graph def with a  `tf.contrib.resampler.resampler` op fails iff `tf.contrib` is not imported first.

Execute:
```
import tensorflow as tf

def export_model(filename, sess, output_node_names):
    from tensorflow.python.framework import graph_util
    output_graph_def = graph_util.convert_variables_to_constants(sess,
                                                                 sess.graph.as_graph_def(add_shapes=True),
                                                                 output_node_names)
    with tf.gfile.GFile(filename, ""wb"") as f:
        f.write(output_graph_def.SerializeToString())
        
def read_frozen_protobuf(path):
    with tf.gfile.FastGFile(str(path), 'rb') as f:
        graph_def = tf.GraphDef()
        graph_def.ParseFromString(f.read())
        return graph_def

    
def export(filename):
    tf.reset_default_graph()
    g = tf.Graph()
    with tf.Session(graph=g, config=tf.ConfigProto(allow_soft_placement=True)) as sess:
        images = tf.placeholder(dtype=tf.float64, shape=[32, 32], name='images')
        points = tf.placeholder(dtype=tf.float64, shape=[32, 2], name='points')
        resampled = tf.contrib.resampler.resampler(images, points, name='resampled')
        output_node_names = ['resampled/Resampler']
        export_model(filename, sess, output_node_names)
        
def load(filename):
    import numpy as np
    tf.reset_default_graph()
    g = tf.Graph()
    with tf.Session(graph=g, config=tf.ConfigProto(allow_soft_placement=True)) as sess:
        images = np.zeros((32, 32), dtype=np.float64)
        points = np.zeros((32, 2), dtype=np.float64)
        graph_def = read_frozen_protobuf(filename)
        tf.import_graph_def(graph_def, 
                            input_map={'images': images,
                                       'points': points},
                            return_elements=['resampled/Resampler:0'])
        
######################################################
frozen_graph_def = '/tmp/test.frozen'
export(frozen_graph_def)
load(frozen_graph_def)
```

Then, in a new interpreter (where the load(..) function is defined), execute:
```
frozen_graph_def = '/tmp/test.frozen'
load(frozen_graph_def)
```

This give the error message:
```
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-1-64251e160f7d> in <module>()
     42 frozen_graph_def = '/tmp/test.frozen'
     43 # export(frozen_graph_def)
---> 44 load(frozen_graph_def)

<ipython-input-1-64251e160f7d> in load(filename)
     37                             input_map={'images': images,
     38                                        'points': points},
---> 39                             return_elements=['resampled/Resampler:0'])
     40 
     41 ######################################################################

/lib/python3.5/site-packages/tensorflow/python/util/deprecation.py in new_func(*args, **kwargs)
    314                 'in a future version' if date is None else ('after %s' % date),
    315                 instructions)
--> 316       return func(*args, **kwargs)
    317     return tf_decorator.make_decorator(func, new_func, 'deprecated',
    318                                        _add_deprecated_arg_notice_to_docstring(

/lib/python3.5/site-packages/tensorflow/python/framework/importer.py in import_graph_def(graph_def, input_map, return_elements, name, op_dict, producer_op_list)
    539         # Set any default attr values that aren't present.
    540         if node.op not in op_dict:
--> 541           raise ValueError('No op named %s in defined operations.' % node.op)
    542         op_def = op_dict[node.op]
    543         for attr_def in op_def.attr:

ValueError: No op named Resampler in defined operations.
```",2018-02-14T01:55:01Z,2,1,0,2.360673760222241
15,16981,module 'tensorflow.python._pywrap_tensorflow_internal' has no attribute 'TFE_NewContextOptions',stat:awaiting response,"I have followed the TensorFlow tutorial, ""[Simple Audio Recognition](https://www.tensorflow.org/versions/master/tutorials/audio_recognition)""

When I was running **train.py**, I got this error message:

```
AttributeError                            Traceback (most recent call last)
<ipython-input-4-61266873bc5f> in <module>()
     77 import numpy as np
     78 from six.moves import xrange  # pylint: disable=redefined-builtin
---> 79 import tensorflow as tf
     80 
     81 import input_data

c:\users\jinsu\appdata\local\programs\python\python36\lib\site-packages\tensorflow\__init__.py in <module>()
     22 
     23 # pylint: disable=wildcard-import
---> 24 from tensorflow.python import *
     25 # pylint: enable=wildcard-import
     26 

c:\users\jinsu\appdata\local\programs\python\python36\lib\site-packages\tensorflow\python\__init__.py in <module>()
     47 import numpy as np
     48 
---> 49 from tensorflow.python import pywrap_tensorflow
     50 
     51 # Protocol buffers

c:\users\jinsu\appdata\local\programs\python\python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py in <module>()
     56     sys.setdlopenflags(_default_dlopen_flags | ctypes.RTLD_LOCAL)
     57 
---> 58   from tensorflow.python.pywrap_tensorflow_internal import *
     59   from tensorflow.python.pywrap_tensorflow_internal import __version__
     60   from tensorflow.python.pywrap_tensorflow_internal import __git_version__

c:\users\jinsu\appdata\local\programs\python\python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py in <module>()
    102 def TFE_NewContextOptions():
    103     return _pywrap_tensorflow_internal.TFE_NewContextOptions()
--> 104 TFE_NewContextOptions = _pywrap_tensorflow_internal.TFE_NewContextOptions
    105 
    106 def TFE_ContextOptionsSetConfig(options, proto, proto_len, status):

AttributeError: module 'tensorflow.python._pywrap_tensorflow_internal' has no attribute 'TFE_NewContextOptions'
```
So, I checked out **pywrap_tensorflow_internal.py** in the pyhton directory. 
This is the part of **pywrap_tensorflow_internal.py** that defines TFE_NewContextOptions.

```
def TFE_ContextOptionsSetConfig(options, proto, proto_len, status):
    return _pywrap_tensorflow_internal.TFE_ContextOptionsSetConfig(options, proto, proto_len, status)
TFE_ContextOptionsSetConfig = _pywrap_tensorflow_internal.TFE_ContextOptionsSetConfig
TFE_DEVICE_PLACEMENT_EXPLICIT = _pywrap_tensorflow_internal.TFE_DEVICE_PLACEMENT_EXPLICIT
TFE_DEVICE_PLACEMENT_WARN = _pywrap_tensorflow_internal.TFE_DEVICE_PLACEMENT_WARN
TFE_DEVICE_PLACEMENT_SILENT = _pywrap_tensorflow_internal.TFE_DEVICE_PLACEMENT_SILENT
TFE_DEVICE_PLACEMENT_SILENT_FOR_INT32 = _pywrap_tensorflow_internal.TFE_DEVICE_PLACEMENT_SILENT_FOR_INT32
```",0,,2,2018-02-13T11:48:22Z,2018-02-14T01:41:21Z,NONE,"I have followed the TensorFlow tutorial, ""[Simple Audio Recognition](https://www.tensorflow.org/versions/master/tutorials/audio_recognition)""

When I was running **train.py**, I got this error message:

```
AttributeError                            Traceback (most recent call last)
<ipython-input-4-61266873bc5f> in <module>()
     77 import numpy as np
     78 from six.moves import xrange  # pylint: disable=redefined-builtin
---> 79 import tensorflow as tf
     80 
     81 import input_data

c:\users\jinsu\appdata\local\programs\python\python36\lib\site-packages\tensorflow\__init__.py in <module>()
     22 
     23 # pylint: disable=wildcard-import
---> 24 from tensorflow.python import *
     25 # pylint: enable=wildcard-import
     26 

c:\users\jinsu\appdata\local\programs\python\python36\lib\site-packages\tensorflow\python\__init__.py in <module>()
     47 import numpy as np
     48 
---> 49 from tensorflow.python import pywrap_tensorflow
     50 
     51 # Protocol buffers

c:\users\jinsu\appdata\local\programs\python\python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py in <module>()
     56     sys.setdlopenflags(_default_dlopen_flags | ctypes.RTLD_LOCAL)
     57 
---> 58   from tensorflow.python.pywrap_tensorflow_internal import *
     59   from tensorflow.python.pywrap_tensorflow_internal import __version__
     60   from tensorflow.python.pywrap_tensorflow_internal import __git_version__

c:\users\jinsu\appdata\local\programs\python\python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py in <module>()
    102 def TFE_NewContextOptions():
    103     return _pywrap_tensorflow_internal.TFE_NewContextOptions()
--> 104 TFE_NewContextOptions = _pywrap_tensorflow_internal.TFE_NewContextOptions
    105 
    106 def TFE_ContextOptionsSetConfig(options, proto, proto_len, status):

AttributeError: module 'tensorflow.python._pywrap_tensorflow_internal' has no attribute 'TFE_NewContextOptions'
```
So, I checked out **pywrap_tensorflow_internal.py** in the pyhton directory. 
This is the part of **pywrap_tensorflow_internal.py** that defines TFE_NewContextOptions.

```
def TFE_ContextOptionsSetConfig(options, proto, proto_len, status):
    return _pywrap_tensorflow_internal.TFE_ContextOptionsSetConfig(options, proto, proto_len, status)
TFE_ContextOptionsSetConfig = _pywrap_tensorflow_internal.TFE_ContextOptionsSetConfig
TFE_DEVICE_PLACEMENT_EXPLICIT = _pywrap_tensorflow_internal.TFE_DEVICE_PLACEMENT_EXPLICIT
TFE_DEVICE_PLACEMENT_WARN = _pywrap_tensorflow_internal.TFE_DEVICE_PLACEMENT_WARN
TFE_DEVICE_PLACEMENT_SILENT = _pywrap_tensorflow_internal.TFE_DEVICE_PLACEMENT_SILENT
TFE_DEVICE_PLACEMENT_SILENT_FOR_INT32 = _pywrap_tensorflow_internal.TFE_DEVICE_PLACEMENT_SILENT_FOR_INT32
```",2018-02-13T14:22:02Z,1,1,0,2.360673760222241
16,16980,"Tensorboard Error 404, path /[[_dataImageSrc]] not found",stat:awaiting response,"I am using the object detection API
and training a new ssd_mobilenet from scratch with my dataset.
I already did a successful retraining, but now i want to try a complete new training without checkpoint.

The training and evaluation job seem to run normal, but when i start tensorboard and open port 6006 in my browser,  the terminal where its running shows following error:
`
W0213 11:04:23.869396 Thread-2 application.py:273] path /[[_dataImageSrc]] not found, sending 404
`
and no scalars, except of the learning rate, are visualized in tensorboard.
In contrast to that Evaluation images, the graph, distributions and histogram are all shown correctly.",0,,2,2018-02-13T10:14:45Z,2018-02-14T08:12:05Z,NONE,"I am using the object detection API
and training a new ssd_mobilenet from scratch with my dataset.
I already did a successful retraining, but now i want to try a complete new training without checkpoint.

The training and evaluation job seem to run normal, but when i start tensorboard and open port 6006 in my browser,  the terminal where its running shows following error:
`
W0213 11:04:23.869396 Thread-2 application.py:273] path /[[_dataImageSrc]] not found, sending 404
`
and no scalars, except of the learning rate, are visualized in tensorboard.
In contrast to that Evaluation images, the graph, distributions and histogram are all shown correctly.",2018-02-13T14:22:07Z,1,1,0,2.360673760222241
17,16976,df982b8de - Split gpu_id.h and GpuIdManager out from build target breaks build for verbs and GDR,,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: TF not compiling. (master)
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: 0.5.4
- **GCC/Compiler version (if compiling from source)**: 5.4
- **CUDA/cuDNN version**: 9.1
- **GPU model and memory**: Any
- **Exact command to reproduce**: 

1. ./configure ... with GDR (and/or verbs)
2. bazel build -c opt --config=cuda  //tensorflow/tools/pip_package:build_pip_package

### Describe the problem
Commit df982b8de breaks the build for GDR and verbs. 
> ERROR: /home/eladw/google/tensorflow/tensorflow/contrib/gdr/BUILD:52:1: undeclared inclusion(s) in rule '//tensorflow/contrib/gdr:gdr_memory_manager':
this rule is missing dependency declarations for the following files included by 'tensorflow/contrib/gdr/gdr_memory_manager.cc':
  '/home/eladw/google/tensorflow/tensorflow/core/common_runtime/gpu/gpu_id.h'.
",1,,4,2018-02-13T09:10:01Z,2018-02-17T08:26:41Z,CONTRIBUTOR,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: TF not compiling. (master)
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: 0.5.4
- **GCC/Compiler version (if compiling from source)**: 5.4
- **CUDA/cuDNN version**: 9.1
- **GPU model and memory**: Any
- **Exact command to reproduce**: 

1. ./configure ... with GDR (and/or verbs)
2. bazel build -c opt --config=cuda  //tensorflow/tools/pip_package:build_pip_package

### Describe the problem
Commit df982b8de breaks the build for GDR and verbs. 
> ERROR: /home/eladw/google/tensorflow/tensorflow/contrib/gdr/BUILD:52:1: undeclared inclusion(s) in rule '//tensorflow/contrib/gdr:gdr_memory_manager':
this rule is missing dependency declarations for the following files included by 'tensorflow/contrib/gdr/gdr_memory_manager.cc':
  '/home/eladw/google/tensorflow/tensorflow/core/common_runtime/gpu/gpu_id.h'.
",2018-02-14T00:40:28Z,4,2,1,5.360673760222241
18,16971,Go API - graph.get_tensor_by_name,,"
What is the equivalent of the Python graph.get_tensor_by_name in Go?

thanks
",0,,5,2018-02-13T08:12:55Z,2018-02-14T09:05:05Z,NONE,"
What is the equivalent of the Python graph.get_tensor_by_name in Go?

thanks
",2018-02-13T14:22:22Z,1,1,0,3.860673760222241
19,16968,Fix typo in build_and_run_inception_hexagon.sh,cla: yes,Signed-off-by: MyungSung Kwak <yesmung@gmail.com>,0,,2,2018-02-13T06:48:46Z,2018-02-13T07:17:37Z,CONTRIBUTOR,Signed-off-by: MyungSung Kwak <yesmung@gmail.com>,2018-02-13T06:53:31Z,0,2,0,3.360673760222241
20,16961,Add instructions for building CUDA-enabled Android TensorFlow,cla: yes,,0,,2,2018-02-12T23:29:27Z,2018-02-13T23:20:07Z,MEMBER,,2018-02-12T23:29:47Z,1,3,0,4.352956123864761
21,16960,Build error introduced by 1baac78627. Can't build sources (master).,,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: NO
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Debian Jessie
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: Master
- **Python version**: 2.7.14
- **Bazel version (if compiling from source)**: 0.10.0
- **GCC/Compiler version (if compiling from source)**: GCC 4.9.2-10
- **CUDA/cuDNN version**: Cuda8 / Cudnn7
- **GPU model and memory**: 1080Ti
- **Exact command to reproduce**:
```
git clone https://github.com/tensorflow/tensorflow .
export PYTHON_BIN_PATH=/path/to/python ## python 2.7.14
export USE_DEFAULT_PYTHON_LIB_PATH=1
export TF_NEED_JEMALLOC=1
export TF_NEED_GCP=0
export TF_NEED_HDFS=1
export TF_ENABLE_XLA=1
export TF_NEED_OPENCL=0
export TF_NEED_S3=0
export TF_NEED_GDR=0
export TF_NEED_VERBS=0
export TF_NEED_OPENCL_SYCL=0
export TF_NEED_CUDA=1
export TF_CUDA_VERSION=8.0
export CUDA_TOOLKIT_PATH=/path/to/cuda
export TF_CUDNN_VERSION=7
export CUDNN_INSTALL_PATH=/path/to/cudnn
export TF_CUDA_COMPUTE_CAPABILITIES=""3.5,5.2,6.0,6.1""
export TF_CUDA_CLANG=0
export GCC_HOST_COMPILER_PATH=/path/to/gcc
export TF_NEED_MPI=1
export MPI_HOME=/path/to/openmpi
export CC_OPT_FLAGS=""-march=native""
export TF_SET_ANDROID_WORKSPACE=0
./configure
bazel build --config=mkl --config=opt --config=cuda \
          //tensorflow/tools/pip_package:build_pip_package
```

### Describe the problem
Failure to build sources:
```
ERROR: /opt/tensorflow/tensorflow/core/BUILD:2077:1: C++ compilation of rule '//tensorflow/core:core_cpu_impl' failed (Exit 1)
In file included from tensorflow/core/common_runtime/threadpool_device.cc:32:0:
./tensorflow/core/common_runtime/mkl_cpu_allocator.h: In member function 'virtual void tensorflow::MklCPUAllocator::AddAllocVisitor(tensorflow::VisitableAllocator::Visitor)':
./tensorflow/core/common_runtime/mkl_cpu_allocator.h:123:17: error: 'class tensorflow::Allocator' has no member named 'AddAllocVisitor'
     allocator_->AddAllocVisitor(visitor);
                 ^
./tensorflow/core/common_runtime/mkl_cpu_allocator.h: In member function 'virtual void tensorflow::MklCPUAllocator::AddFreeVisitor(tensorflow::VisitableAllocator::Visitor)':
./tensorflow/core/common_runtime/mkl_cpu_allocator.h:127:17: error: 'class tensorflow::Allocator' has no member named 'AddFreeVisitor'
     allocator_->AddFreeVisitor(visitor);
                 ^
```
Class `MklCPUAllocator` has a member `allocator_` which is of type `tensorflow::Allocator` which does not have the member functions accessed. ",0,,6,2018-02-12T23:17:52Z,2018-02-14T23:40:33Z,CONTRIBUTOR,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: NO
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Debian Jessie
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: Master
- **Python version**: 2.7.14
- **Bazel version (if compiling from source)**: 0.10.0
- **GCC/Compiler version (if compiling from source)**: GCC 4.9.2-10
- **CUDA/cuDNN version**: Cuda8 / Cudnn7
- **GPU model and memory**: 1080Ti
- **Exact command to reproduce**:
```
git clone https://github.com/tensorflow/tensorflow .
export PYTHON_BIN_PATH=/path/to/python ## python 2.7.14
export USE_DEFAULT_PYTHON_LIB_PATH=1
export TF_NEED_JEMALLOC=1
export TF_NEED_GCP=0
export TF_NEED_HDFS=1
export TF_ENABLE_XLA=1
export TF_NEED_OPENCL=0
export TF_NEED_S3=0
export TF_NEED_GDR=0
export TF_NEED_VERBS=0
export TF_NEED_OPENCL_SYCL=0
export TF_NEED_CUDA=1
export TF_CUDA_VERSION=8.0
export CUDA_TOOLKIT_PATH=/path/to/cuda
export TF_CUDNN_VERSION=7
export CUDNN_INSTALL_PATH=/path/to/cudnn
export TF_CUDA_COMPUTE_CAPABILITIES=""3.5,5.2,6.0,6.1""
export TF_CUDA_CLANG=0
export GCC_HOST_COMPILER_PATH=/path/to/gcc
export TF_NEED_MPI=1
export MPI_HOME=/path/to/openmpi
export CC_OPT_FLAGS=""-march=native""
export TF_SET_ANDROID_WORKSPACE=0
./configure
bazel build --config=mkl --config=opt --config=cuda \
          //tensorflow/tools/pip_package:build_pip_package
```

### Describe the problem
Failure to build sources:
```
ERROR: /opt/tensorflow/tensorflow/core/BUILD:2077:1: C++ compilation of rule '//tensorflow/core:core_cpu_impl' failed (Exit 1)
In file included from tensorflow/core/common_runtime/threadpool_device.cc:32:0:
./tensorflow/core/common_runtime/mkl_cpu_allocator.h: In member function 'virtual void tensorflow::MklCPUAllocator::AddAllocVisitor(tensorflow::VisitableAllocator::Visitor)':
./tensorflow/core/common_runtime/mkl_cpu_allocator.h:123:17: error: 'class tensorflow::Allocator' has no member named 'AddAllocVisitor'
     allocator_->AddAllocVisitor(visitor);
                 ^
./tensorflow/core/common_runtime/mkl_cpu_allocator.h: In member function 'virtual void tensorflow::MklCPUAllocator::AddFreeVisitor(tensorflow::VisitableAllocator::Visitor)':
./tensorflow/core/common_runtime/mkl_cpu_allocator.h:127:17: error: 'class tensorflow::Allocator' has no member named 'AddFreeVisitor'
     allocator_->AddFreeVisitor(visitor);
                 ^
```
Class `MklCPUAllocator` has a member `allocator_` which is of type `tensorflow::Allocator` which does not have the member functions accessed. ",2018-02-12T23:18:53Z,2,2,0,5.352956123864761
22,16948,tf.QueueBase.dequeue_many returns list instead of tuple,"stat:contributions welcome,type:docs","The dequeue_many operation returns a **list** of Tensors, while the documentation states that it should be a **tuple** (which is more sensible).

> Returns:
> The tuple of concatenated tensors that was dequeued.
(https://www.tensorflow.org/api_docs/python/tf/QueueBase)

Version: 1.5.0 from PIP, Python3, on OS/X",0,,3,2018-02-12T12:55:49Z,2018-02-15T23:43:39Z,CONTRIBUTOR,"The dequeue_many operation returns a **list** of Tensors, while the documentation states that it should be a **tuple** (which is more sensible).

> Returns:
> The tuple of concatenated tensors that was dequeued.
(https://www.tensorflow.org/api_docs/python/tf/QueueBase)

Version: 1.5.0 from PIP, Python3, on OS/X",2018-02-12T21:28:25Z,3,2,1,3.852956123864761
23,16940,"TypeError: ('Keyword argument not understood:', 'adjustment') when passing slim.batch_norm as normalizer_fn to slim.conv2d",,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes
- **TensorFlow installed from (source or binary)**:
Binary
- **TensorFlow version**:
1.4.0 CPU
- **Python version**: 
3.6
- **Bazel version (if compiling from source)**:
N/A
- **GCC/Compiler version (if compiling from source)**:
N/A
- **CUDA/cuDNN version**:
N/A
- **GPU model and memory**:
N/A


### Describe the problem
I encountered a problem using batch norm in `slim.conv2d`. Whenever I pass `slim.batch_norm` as the `normalizer_fn` for `slim.conv2d`, I encounter this error:

```
  File ""...\Anaconda3\lib\site-packages\tensorflow\contrib\framework\python\ops\arg_scope.py"", line 181, in func_with_args
    return func(*args, **current_args)
  File ""...\Anaconda3\lib\site-packages\tensorflow\contrib\layers\python\layers\layers.py"", line 1059, in convolution
    outputs = normalizer_fn(outputs, **normalizer_params)
  File ""...\Anaconda3\lib\site-packages\tensorflow\contrib\framework\python\ops\arg_scope.py"", line 181, in func_with_args
    return func(*args, **current_args)
  File ""...\Anaconda3\lib\site-packages\tensorflow\contrib\layers\python\layers\layers.py"", line 650, in batch_norm
    fused=fused)
  File ""...\Anaconda3\lib\site-packages\tensorflow\python\layers\normalization.py"", line 118, in __init__
    name=name, trainable=trainable, **kwargs)
  File ""...\Anaconda3\lib\site-packages\tensorflow\python\layers\base.py"", line 98, in __init__
    raise TypeError('Keyword argument not understood:', kwarg)
TypeError: ('Keyword argument not understood:', 'adjustment')
```",0,,2,2018-02-12T02:56:25Z,2018-02-12T03:16:33Z,CONTRIBUTOR,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes
- **TensorFlow installed from (source or binary)**:
Binary
- **TensorFlow version**:
1.4.0 CPU
- **Python version**: 
3.6
- **Bazel version (if compiling from source)**:
N/A
- **GCC/Compiler version (if compiling from source)**:
N/A
- **CUDA/cuDNN version**:
N/A
- **GPU model and memory**:
N/A


### Describe the problem
I encountered a problem using batch norm in `slim.conv2d`. Whenever I pass `slim.batch_norm` as the `normalizer_fn` for `slim.conv2d`, I encounter this error:

```
  File ""...\Anaconda3\lib\site-packages\tensorflow\contrib\framework\python\ops\arg_scope.py"", line 181, in func_with_args
    return func(*args, **current_args)
  File ""...\Anaconda3\lib\site-packages\tensorflow\contrib\layers\python\layers\layers.py"", line 1059, in convolution
    outputs = normalizer_fn(outputs, **normalizer_params)
  File ""...\Anaconda3\lib\site-packages\tensorflow\contrib\framework\python\ops\arg_scope.py"", line 181, in func_with_args
    return func(*args, **current_args)
  File ""...\Anaconda3\lib\site-packages\tensorflow\contrib\layers\python\layers\layers.py"", line 650, in batch_norm
    fused=fused)
  File ""...\Anaconda3\lib\site-packages\tensorflow\python\layers\normalization.py"", line 118, in __init__
    name=name, trainable=trainable, **kwargs)
  File ""...\Anaconda3\lib\site-packages\tensorflow\python\layers\base.py"", line 98, in __init__
    raise TypeError('Keyword argument not understood:', kwarg)
TypeError: ('Keyword argument not understood:', 'adjustment')
```",2018-02-12T02:59:27Z,0,2,0,3.352956123864761
24,16939,"tensorflow 1.3,1.4,1.5,1.6 DLL load failed with CUDA 9.1, CUDnn-7.05, Windows 10",stat:awaiting response,"import tensorflow strack trace

```
> py lib\_learn\tensorflow\versions\versiontest.py
Traceback (most recent call last):
  File ""lib/tensorflow_gpu_130\tensorflow\python\pywrap_tensorflow_internal.py"", line 18, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\g\TEST\lib\python\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 986, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 969, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 958, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 666, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 577, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 914, in create_module
  File ""<frozen importlib._bootstrap>"", line 222, in _call_with_frames_removed
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""lib/tensorflow_gpu_130\tensorflow\python\pywrap_tensorflow.py"", line 41, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""lib/tensorflow_gpu_130\tensorflow\python\pywrap_tensorflow_internal.py"", line 21, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""lib/tensorflow_gpu_130\tensorflow\python\pywrap_tensorflow_internal.py"", line 20, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""C:\g\TEST\lib\python\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ImportError: No module named '_pywrap_tensorflow_internal'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""lib\_learn\tensorflow\versions\versiontest.py"", line 4, in <module>
    import tensorflow as tf
  File ""lib/tensorflow_gpu_130\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""lib/tensorflow_gpu_130\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""lib/tensorflow_gpu_130\tensorflow\python\pywrap_tensorflow.py"", line 52, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""lib/tensorflow_gpu_130\tensorflow\python\pywrap_tensorflow_internal.py"", line 18, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\g\TEST\lib\python\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 986, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 969, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 958, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 666, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 577, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 914, in create_module
  File ""<frozen importlib._bootstrap>"", line 222, in _call_with_frames_removed
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""lib/tensorflow_gpu_130\tensorflow\python\pywrap_tensorflow.py"", line 41, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""lib/tensorflow_gpu_130\tensorflow\python\pywrap_tensorflow_internal.py"", line 21, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""lib/tensorflow_gpu_130\tensorflow\python\pywrap_tensorflow_internal.py"", line 20, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""C:\g\TEST\lib\python\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ImportError: No module named '_pywrap_tensorflow_internal'


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
```",0,,9,2018-02-11T20:23:48Z,2018-02-12T22:38:52Z,NONE,"import tensorflow strack trace

```
> py lib\_learn\tensorflow\versions\versiontest.py
Traceback (most recent call last):
  File ""lib/tensorflow_gpu_130\tensorflow\python\pywrap_tensorflow_internal.py"", line 18, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\g\TEST\lib\python\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 986, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 969, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 958, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 666, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 577, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 914, in create_module
  File ""<frozen importlib._bootstrap>"", line 222, in _call_with_frames_removed
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""lib/tensorflow_gpu_130\tensorflow\python\pywrap_tensorflow.py"", line 41, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""lib/tensorflow_gpu_130\tensorflow\python\pywrap_tensorflow_internal.py"", line 21, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""lib/tensorflow_gpu_130\tensorflow\python\pywrap_tensorflow_internal.py"", line 20, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""C:\g\TEST\lib\python\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ImportError: No module named '_pywrap_tensorflow_internal'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""lib\_learn\tensorflow\versions\versiontest.py"", line 4, in <module>
    import tensorflow as tf
  File ""lib/tensorflow_gpu_130\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""lib/tensorflow_gpu_130\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""lib/tensorflow_gpu_130\tensorflow\python\pywrap_tensorflow.py"", line 52, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""lib/tensorflow_gpu_130\tensorflow\python\pywrap_tensorflow_internal.py"", line 18, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\g\TEST\lib\python\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 986, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 969, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 958, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 666, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 577, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 914, in create_module
  File ""<frozen importlib._bootstrap>"", line 222, in _call_with_frames_removed
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""lib/tensorflow_gpu_130\tensorflow\python\pywrap_tensorflow.py"", line 41, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""lib/tensorflow_gpu_130\tensorflow\python\pywrap_tensorflow_internal.py"", line 21, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""lib/tensorflow_gpu_130\tensorflow\python\pywrap_tensorflow_internal.py"", line 20, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""C:\g\TEST\lib\python\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ImportError: No module named '_pywrap_tensorflow_internal'


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
```",2018-02-12T07:29:30Z,1,1,0,5.845976256261194
25,16924,typeid broken across shared boundary makes a271c36b5ead4686b72d972b193bf1f534a92ffd not work ,,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Custom shared object linking to 
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04)
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: a271c36b5ead4686b72d972b193bf1f534a92ffd
- **Python version**: 3.5
- **Bazel version (if compiling from source)**: 0.5.4
- **GCC/Compiler version (if compiling from source)**: 5.4
- **CUDA/cuDNN version**: 9.0/7.0.5
- **GPU model and memory**: P4000 (8 GB)
- **Exact command to reproduce**: This is a little involved.

### Describe the problem

@mrry First of all, thanks for quickly responding to my issue #16682, and for following up with the fixes in a271c36b5ead4686b72d972b193bf1f534a92ffd without my even mentioning the problem to you. Very impressive.

I know that in that commit, you mention ""A subsequent change will move `tf.contrib.data` kernel implementations to a custom op library."" When you say custom op library, I assume you mean a  distinct shared object file.

Unfortunately, unless you use --config=monolithic to build, this will not work because the typeid of DatasetVariantWrapper will be different between libtensorflow_framework.so and the custom op library shared object, because they are loaded with RTLD_LOCAL. --config=monolithic avoids the problem because _python_framework_internal.so is loaded with RTLD_GLOBAL in that case. This will override the custom op library's ""weak"" (I am talking about weak linkage of symbols here) typeid of DatasetVariantWrapper. Otherwise, `variant::get<DatasetVariantWrapper>()` will fail in dataset.cc's GetDatasetFromVariantTensor, because the two typeids that get compared have two separate pointers.

I first found this problem documented [here](https://svn.boost.org/trac10/ticket/754). [This](https://stackoverflow.com/questions/23383102/dynamic-cast-troubles-over-shared-libraries) stack overflow answer was also helpful.

I'm not sure what the right solution is to this yet. It seems it may be possible to change from pointer equality for type info to checking the equality of mangled strings with strcmp, based on my reading of libstdc++'s typeinfo header file.

Happy to answer any questions,s ince this is rather involved.

### Source code / logs

```
2018-02-10 13:53:14.169478: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2018-02-10 13:53:14.422106: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Found device 0 with properties:
name: Quadro P4000 major: 6 minor: 1 memoryClockRate(GHz): 1.48
pciBusID: 0000:02:00.0
totalMemory: 7.91GiB freeMemory: 6.98GiB
2018-02-10 13:53:14.422134: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1405] Adding visible gpu devices: 0
2018-02-10 13:54:11.579203: I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-02-10 13:54:11.579230: I tensorflow/core/common_runtime/gpu/gpu_device.cc:912]      0
2018-02-10 13:54:11.579254: I tensorflow/core/common_runtime/gpu/gpu_device.cc:925] 0:   N
2018-02-10 13:54:11.579420: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1016] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6740 MB memory) -> physical GPU (device: 0, name: Quadro P4000, pci bus id: 0000:02:00.0, compute capability: 6.1)
2018-02-10 13:54:11.619971: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at iterator_ops.cc:827 : Invalid argument: Tensor must be a Dataset object.
```",1,,6,2018-02-10T22:09:34Z,2018-02-11T18:39:30Z,NONE,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Custom shared object linking to 
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04)
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: a271c36b5ead4686b72d972b193bf1f534a92ffd
- **Python version**: 3.5
- **Bazel version (if compiling from source)**: 0.5.4
- **GCC/Compiler version (if compiling from source)**: 5.4
- **CUDA/cuDNN version**: 9.0/7.0.5
- **GPU model and memory**: P4000 (8 GB)
- **Exact command to reproduce**: This is a little involved.

### Describe the problem

@mrry First of all, thanks for quickly responding to my issue #16682, and for following up with the fixes in a271c36b5ead4686b72d972b193bf1f534a92ffd without my even mentioning the problem to you. Very impressive.

I know that in that commit, you mention ""A subsequent change will move `tf.contrib.data` kernel implementations to a custom op library."" When you say custom op library, I assume you mean a  distinct shared object file.

Unfortunately, unless you use --config=monolithic to build, this will not work because the typeid of DatasetVariantWrapper will be different between libtensorflow_framework.so and the custom op library shared object, because they are loaded with RTLD_LOCAL. --config=monolithic avoids the problem because _python_framework_internal.so is loaded with RTLD_GLOBAL in that case. This will override the custom op library's ""weak"" (I am talking about weak linkage of symbols here) typeid of DatasetVariantWrapper. Otherwise, `variant::get<DatasetVariantWrapper>()` will fail in dataset.cc's GetDatasetFromVariantTensor, because the two typeids that get compared have two separate pointers.

I first found this problem documented [here](https://svn.boost.org/trac10/ticket/754). [This](https://stackoverflow.com/questions/23383102/dynamic-cast-troubles-over-shared-libraries) stack overflow answer was also helpful.

I'm not sure what the right solution is to this yet. It seems it may be possible to change from pointer equality for type info to checking the equality of mangled strings with strcmp, based on my reading of libstdc++'s typeinfo header file.

Happy to answer any questions,s ince this is rather involved.

### Source code / logs

```
2018-02-10 13:53:14.169478: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2018-02-10 13:53:14.422106: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Found device 0 with properties:
name: Quadro P4000 major: 6 minor: 1 memoryClockRate(GHz): 1.48
pciBusID: 0000:02:00.0
totalMemory: 7.91GiB freeMemory: 6.98GiB
2018-02-10 13:53:14.422134: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1405] Adding visible gpu devices: 0
2018-02-10 13:54:11.579203: I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-02-10 13:54:11.579230: I tensorflow/core/common_runtime/gpu/gpu_device.cc:912]      0
2018-02-10 13:54:11.579254: I tensorflow/core/common_runtime/gpu/gpu_device.cc:925] 0:   N
2018-02-10 13:54:11.579420: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1016] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6740 MB memory) -> physical GPU (device: 0, name: Quadro P4000, pci bus id: 0000:02:00.0, compute capability: 6.1)
2018-02-10 13:54:11.619971: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at iterator_ops.cc:827 : Invalid argument: Tensor must be a Dataset object.
```",2018-02-11T00:29:41Z,1,1,0,5.339623271895109
26,16923,Add documentation for s3 usage with TensorFlow,cla: yes,"This fix adds a very preliminary documentation for s3 usage with TensorFlow, as an attempt to address the comment https://github.com/tensorflow/tensorflow/pull/11089#issuecomment-364682030.

Ideally I think it would be good if usage with GCS could be added to the doc page as well.",1,,1,2018-02-10T21:36:34Z,2018-02-12T23:29:08Z,MEMBER,"This fix adds a very preliminary documentation for s3 usage with TensorFlow, as an attempt to address the comment https://github.com/tensorflow/tensorflow/pull/11089#issuecomment-364682030.

Ideally I think it would be good if usage with GCS could be added to the doc page as well.",2018-02-11T19:35:56Z,2,3,0,4.839623271895109
27,16915,Update version string to 1.6.0-rc1,cla: yes,,0,,1,2018-02-10T17:51:54Z,2018-02-12T17:35:44Z,OWNER,,2018-02-12T17:35:37Z,2,4,0,4.839623271895109
28,16907,Multi-GPU could not provide performance improve with dataset API,,"I just wrote a small piece of code in tensorflow to test its multi-gpu performance, with dataset API.

```
import tensorflow as tf
import numpy as np
import time
import os

#dataset with 1000 vectors
dataset = tf.data.Dataset.from_tensor_slices(tf.random_uniform([1000,4], maxval=4, dtype=tf.int32))
          
print(dataset.output_types)
print(dataset.output_shapes)

iterator = dataset.make_initializable_iterator()
#next_element = iterator.get_next()

tensor_results = []


for i in range(500):
    for j in range(2):
        with tf.device(""/gpu:%d"" % j):
            with tf.name_scope(""Tower_%d"" % j) as scope:
                operand = iterator.get_next()
                tensor_result = tf.matmul(tf.reshape(operand,shape=[1,4]), tf.reshape(operand,shape=[4,1]))
                tensor_results.append(tensor_result)


tfconfig = tf.ConfigProto(allow_soft_placement=True, log_device_placement=True)
tfconfig.gpu_options.allow_growth=True

sess = tf.Session(config=tfconfig)

sess.run(iterator.initializer)
t0 = time.time()

results = sess.run(tensor_results)

t1 = time.time()

elapsed_time = t1 - t0
print(elapsed_time)
results
```
I have 2 GPUs and this program takes 0.68 seconds to finish.
When I change to a single GPU execution mode:

```
import tensorflow as tf
import numpy as np
import time
import os

os.environ[""CUDA_VISIBLE_DEVICES""]=""0""

dataset = tf.data.Dataset.from_tensor_slices(tf.random_uniform([1000,4], maxval=4, dtype=tf.int32))
          
print(dataset.output_types)
print(dataset.output_shapes)

iterator = dataset.make_initializable_iterator()
#next_element = iterator.get_next()

tensor_results = []

with tf.device(""/gpu:0""):
    for i in range(1000):
        operand = iterator.get_next()
        tensor_result = tf.matmul(tf.reshape(operand,shape=[1,4]), tf.reshape(operand,shape=[4,1]))
        tensor_results.append(tensor_result)


tfconfig = tf.ConfigProto(allow_soft_placement=True, log_device_placement=True)
tfconfig.gpu_options.allow_growth=True

sess = tf.Session(config=tfconfig)

sess.run(iterator.initializer)
t0 = time.time()

results = sess.run(tensor_results)

t1 = time.time()

elapsed_time = t1 - t0
print(elapsed_time)
results
```

It takes the same time to finish (actually single GPU is even faster perhaps due to overhead reasons). Do anyone know where does the problem come from?

python version: Python 2.7.12
tensorflow version: 1.4.0
CUDA version: 8.0
Ubuntu version: Ubuntu 16.04 LTS

Thanks!
",0,,8,2018-02-10T06:59:27Z,2018-02-10T22:21:33Z,NONE,"I just wrote a small piece of code in tensorflow to test its multi-gpu performance, with dataset API.

```
import tensorflow as tf
import numpy as np
import time
import os

#dataset with 1000 vectors
dataset = tf.data.Dataset.from_tensor_slices(tf.random_uniform([1000,4], maxval=4, dtype=tf.int32))
          
print(dataset.output_types)
print(dataset.output_shapes)

iterator = dataset.make_initializable_iterator()
#next_element = iterator.get_next()

tensor_results = []


for i in range(500):
    for j in range(2):
        with tf.device(""/gpu:%d"" % j):
            with tf.name_scope(""Tower_%d"" % j) as scope:
                operand = iterator.get_next()
                tensor_result = tf.matmul(tf.reshape(operand,shape=[1,4]), tf.reshape(operand,shape=[4,1]))
                tensor_results.append(tensor_result)


tfconfig = tf.ConfigProto(allow_soft_placement=True, log_device_placement=True)
tfconfig.gpu_options.allow_growth=True

sess = tf.Session(config=tfconfig)

sess.run(iterator.initializer)
t0 = time.time()

results = sess.run(tensor_results)

t1 = time.time()

elapsed_time = t1 - t0
print(elapsed_time)
results
```
I have 2 GPUs and this program takes 0.68 seconds to finish.
When I change to a single GPU execution mode:

```
import tensorflow as tf
import numpy as np
import time
import os

os.environ[""CUDA_VISIBLE_DEVICES""]=""0""

dataset = tf.data.Dataset.from_tensor_slices(tf.random_uniform([1000,4], maxval=4, dtype=tf.int32))
          
print(dataset.output_types)
print(dataset.output_shapes)

iterator = dataset.make_initializable_iterator()
#next_element = iterator.get_next()

tensor_results = []

with tf.device(""/gpu:0""):
    for i in range(1000):
        operand = iterator.get_next()
        tensor_result = tf.matmul(tf.reshape(operand,shape=[1,4]), tf.reshape(operand,shape=[4,1]))
        tensor_results.append(tensor_result)


tfconfig = tf.ConfigProto(allow_soft_placement=True, log_device_placement=True)
tfconfig.gpu_options.allow_growth=True

sess = tf.Session(config=tfconfig)

sess.run(iterator.initializer)
t0 = time.time()

results = sess.run(tensor_results)

t1 = time.time()

elapsed_time = t1 - t0
print(elapsed_time)
results
```

It takes the same time to finish (actually single GPU is even faster perhaps due to overhead reasons). Do anyone know where does the problem come from?

python version: Python 2.7.12
tensorflow version: 1.4.0
CUDA version: 8.0
Ubuntu version: Ubuntu 16.04 LTS

Thanks!
",2018-02-10T22:21:33Z,0,1,0,5.339623271895109
29,16906,"python 3.6 + latest tf = infinity of ""msgpack_numpy.py:142 PendingDeprecationWarning: encoding is deprecated."" ",,"Symptom, trying to train resnet-50 imagenet @ppwwyyxx 's [imagenet-resnet.py](https://github.com/ppwwyyxx/tensorpack/blob/master/examples/ResNet/imagenet-resnet.py) script fails to start after 5 minutes, because of some thread being blocked trying to flush a gazillion of warning messages above

Since this problem is solved by downgrading from ""tf-nightly-gpu"" (`1.7.0-dev20180208`) to tensorflow-gpu 1.5, TensorFlow must be blame for introducing a new usage of this deprecated method. Unfortunately the warning messages are quite uninformative, with no indication of who is calling this :/

Note that Python 3.6 is the only Python 3 version supported on Amazon Deep Learning conda AMI images.

Warning messages look like this:

```
/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/msgpack_numpy.py:142: PendingDeprecationWarning: encoding is deprecated.
  use_bin_type=use_bin_type)
/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/msgpack_numpy.py:142: PendingDeprecationWarning: encoding is deprecated.
  use_bin_type=use_bin_type)
/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/msgpack_numpy.py:142: PendingDeprecationWarning: encoding is deprecated.
  use_bin_type=use_bin_type)

```",0,,2,2018-02-10T04:21:46Z,2018-02-10T05:08:48Z,CONTRIBUTOR,"Symptom, trying to train resnet-50 imagenet @ppwwyyxx 's [imagenet-resnet.py](https://github.com/ppwwyyxx/tensorpack/blob/master/examples/ResNet/imagenet-resnet.py) script fails to start after 5 minutes, because of some thread being blocked trying to flush a gazillion of warning messages above

Since this problem is solved by downgrading from ""tf-nightly-gpu"" (`1.7.0-dev20180208`) to tensorflow-gpu 1.5, TensorFlow must be blame for introducing a new usage of this deprecated method. Unfortunately the warning messages are quite uninformative, with no indication of who is calling this :/

Note that Python 3.6 is the only Python 3 version supported on Amazon Deep Learning conda AMI images.

Warning messages look like this:

```
/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/msgpack_numpy.py:142: PendingDeprecationWarning: encoding is deprecated.
  use_bin_type=use_bin_type)
/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/msgpack_numpy.py:142: PendingDeprecationWarning: encoding is deprecated.
  use_bin_type=use_bin_type)
/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/msgpack_numpy.py:142: PendingDeprecationWarning: encoding is deprecated.
  use_bin_type=use_bin_type)

```",2018-02-10T05:05:29Z,0,2,0,3.3396232718951087
30,16900,Restoring a model trained with tf.estimator and feeding input through feed_dict,,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Windows 10
- **TensorFlow installed from (source or binary)**:
binary
- **TensorFlow version (use command below)**:
1.5
- **Python version**: 
3.6.2
- **CUDA/cuDNN version**:
9.0/7.0
- **GPU model and memory**:
NVIDIA 1050 4 GB

I trained a resnet with tf.estimator, the model was saved during the training process. The saved files consist of `.data`, `.index` and `.meta`. I'd like to load this model back and get predictions for new images. The data was fed to the model during training using `tf.data.Dataset`.  I have closely followed the resnet implementation given [here][1].

I would like to restore the model and feed inputs to the nodes using a feed_dict. 
**My attempt**
      #rebuild input pipeline
      images, labels = input_fn(data_dir, batch_size=32, num_epochs=1)
        
      #rebuild graph
      prediction= imagenet_model_fn(images,labels,{'batch_size':32,'data_format':'channels_first','resnet_size':18},mode = tf.estimator.ModeKeys.EVAL).predictions 
    
      saver  = tf.train.Saver()
      with tf.Session() as sess:
        ckpt = tf.train.get_checkpoint_state(r'./model')
        saver.restore(sess, ckpt.model_checkpoint_path)
        while True:
        try:
            pred,im= sess.run([prediction,images])
            print(pred)
        except tf.errors.OutOfRangeError:
          break

I fed a dataset which was evaluated on the same model using `classifier.evaluate`, but the above method gives wrong predictions. The model gives same class and probability, 1.0, for all images.

The code I used for training and building the model is as below:

Specification for parsing the dataset:

    def parse_record(raw_record, is_training):
      keys_to_features = {
          'image/encoded':
              tf.FixedLenFeature((), tf.string, default_value=''),
          'image/class/label':
              tf.FixedLenFeature([], dtype=tf.int64, default_value=-1),
      }
      parsed = tf.parse_single_example(raw_record, keys_to_features)
      image = tf.image.decode_image(
          tf.reshape(parsed['image/encoded'], shape=[]),3)
      image = tf.image.convert_image_dtype(image, dtype=tf.float32)
      label = tf.cast(
          tf.reshape(parsed['image/class/label'], shape=[]),
          dtype=tf.int32)
      return image, tf.one_hot(label,2)

The following function parses the data and creates batches for training

    def input_fn(is_training, data_dir, batch_size, num_epochs=1):
      dataset = tf.data.Dataset.from_tensor_slices(
          filenames(is_training, data_dir))
      if is_training:
         dataset = dataset.shuffle(buffer_size=_FILE_SHUFFLE_BUFFER)
      dataset = dataset.flat_map(tf.data.TFRecordDataset)
      dataset = dataset.map(lambda value: parse_record(value, is_training),
                            num_parallel_calls=5)
      dataset = dataset.prefetch(batch_size)
      if is_training:
          dataset = dataset.shuffle(buffer_size=_SHUFFLE_BUFFER)
      dataset = dataset.repeat(num_epochs)
      dataset = dataset.batch(batch_size)
    
      iterator = dataset.make_one_shot_iterator()
      images, labels = iterator.get_next()
      return images, labels

A classifier is created as below for training on train set and evaluation on validation set

    classifier = tf.estimator.Estimator(
          model_fn=model_function, model_dir=flags.model_dir, config=run_config,
          params={
              'resnet_size': flags.resnet_size,
              'data_format': flags.data_format,
              'batch_size': flags.batch_size,
          })
    
        #Training cycle
         classifier.train(
             input_fn=lambda: input_function(
                 training_phase=True, flags.data_dir, flags.batch_size, flags.epochs_per_eval),
             hooks=[logging_hook])
        # Evaluate the model 
        eval_results = classifier.evaluate(input_fn=lambda: input_function(
            training_phase=False, flags.data_dir, flags.batch_size))

This is how I tried to load and get predictions from the model.
I'd like to feed images through a `feed_dict` so that I can see the model's performance on individual images.
What is the right way to restore a saved model and perform inference on it. I want to feed images directly without using `tf.data.Dataset`.  I had opened a [question ](https://stackoverflow.com/questions/48679622/restoring-a-model-trained-with-tf-estimator-and-feeding-input-through-feed-dict)on stackoverflow, but didn't get any response. I'm wondering if there really is a feature that can help with restoring a model trained with `tf.estimator` and feeding images through a `feed_dict`. 


  [1]: https://github.com/deepaksuresh/models/blob/master/official/resnet/resnet.py

",0,,2,2018-02-09T21:13:31Z,2018-02-09T23:08:42Z,NONE,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Windows 10
- **TensorFlow installed from (source or binary)**:
binary
- **TensorFlow version (use command below)**:
1.5
- **Python version**: 
3.6.2
- **CUDA/cuDNN version**:
9.0/7.0
- **GPU model and memory**:
NVIDIA 1050 4 GB

I trained a resnet with tf.estimator, the model was saved during the training process. The saved files consist of `.data`, `.index` and `.meta`. I'd like to load this model back and get predictions for new images. The data was fed to the model during training using `tf.data.Dataset`.  I have closely followed the resnet implementation given [here][1].

I would like to restore the model and feed inputs to the nodes using a feed_dict. 
**My attempt**
      #rebuild input pipeline
      images, labels = input_fn(data_dir, batch_size=32, num_epochs=1)
        
      #rebuild graph
      prediction= imagenet_model_fn(images,labels,{'batch_size':32,'data_format':'channels_first','resnet_size':18},mode = tf.estimator.ModeKeys.EVAL).predictions 
    
      saver  = tf.train.Saver()
      with tf.Session() as sess:
        ckpt = tf.train.get_checkpoint_state(r'./model')
        saver.restore(sess, ckpt.model_checkpoint_path)
        while True:
        try:
            pred,im= sess.run([prediction,images])
            print(pred)
        except tf.errors.OutOfRangeError:
          break

I fed a dataset which was evaluated on the same model using `classifier.evaluate`, but the above method gives wrong predictions. The model gives same class and probability, 1.0, for all images.

The code I used for training and building the model is as below:

Specification for parsing the dataset:

    def parse_record(raw_record, is_training):
      keys_to_features = {
          'image/encoded':
              tf.FixedLenFeature((), tf.string, default_value=''),
          'image/class/label':
              tf.FixedLenFeature([], dtype=tf.int64, default_value=-1),
      }
      parsed = tf.parse_single_example(raw_record, keys_to_features)
      image = tf.image.decode_image(
          tf.reshape(parsed['image/encoded'], shape=[]),3)
      image = tf.image.convert_image_dtype(image, dtype=tf.float32)
      label = tf.cast(
          tf.reshape(parsed['image/class/label'], shape=[]),
          dtype=tf.int32)
      return image, tf.one_hot(label,2)

The following function parses the data and creates batches for training

    def input_fn(is_training, data_dir, batch_size, num_epochs=1):
      dataset = tf.data.Dataset.from_tensor_slices(
          filenames(is_training, data_dir))
      if is_training:
         dataset = dataset.shuffle(buffer_size=_FILE_SHUFFLE_BUFFER)
      dataset = dataset.flat_map(tf.data.TFRecordDataset)
      dataset = dataset.map(lambda value: parse_record(value, is_training),
                            num_parallel_calls=5)
      dataset = dataset.prefetch(batch_size)
      if is_training:
          dataset = dataset.shuffle(buffer_size=_SHUFFLE_BUFFER)
      dataset = dataset.repeat(num_epochs)
      dataset = dataset.batch(batch_size)
    
      iterator = dataset.make_one_shot_iterator()
      images, labels = iterator.get_next()
      return images, labels

A classifier is created as below for training on train set and evaluation on validation set

    classifier = tf.estimator.Estimator(
          model_fn=model_function, model_dir=flags.model_dir, config=run_config,
          params={
              'resnet_size': flags.resnet_size,
              'data_format': flags.data_format,
              'batch_size': flags.batch_size,
          })
    
        #Training cycle
         classifier.train(
             input_fn=lambda: input_function(
                 training_phase=True, flags.data_dir, flags.batch_size, flags.epochs_per_eval),
             hooks=[logging_hook])
        # Evaluate the model 
        eval_results = classifier.evaluate(input_fn=lambda: input_function(
            training_phase=False, flags.data_dir, flags.batch_size))

This is how I tried to load and get predictions from the model.
I'd like to feed images through a `feed_dict` so that I can see the model's performance on individual images.
What is the right way to restore a saved model and perform inference on it. I want to feed images directly without using `tf.data.Dataset`.  I had opened a [question ](https://stackoverflow.com/questions/48679622/restoring-a-model-trained-with-tf-estimator-and-feeding-input-through-feed-dict)on stackoverflow, but didn't get any response. I'm wondering if there really is a feature that can help with restoring a model trained with `tf.estimator` and feeding images through a `feed_dict`. 


  [1]: https://github.com/deepaksuresh/models/blob/master/official/resnet/resnet.py

",2018-02-09T23:08:42Z,0,1,0,2.333808200695334
31,16897,Improve formatting of Tensor shapes in tf.losses,cla: yes,"Updating the documentation of Tensor shapes in `tf.losses` to match the documentation guide at 
https://www.tensorflow.org/community/documentation",0,,5,2018-02-09T19:31:38Z,2018-02-10T11:55:26Z,CONTRIBUTOR,"Updating the documentation of Tensor shapes in `tf.losses` to match the documentation guide at 
https://www.tensorflow.org/community/documentation",2018-02-09T19:32:26Z,1,2,0,4.833808200695334
32,16896,dataset_ops.py batch() checks type immediately,cla: yes,"Currently if you inadvertently pass a non integer value for batch_size you get a strangely cryptic error with a long trace:
```
  File ""/home/ahundt/.local/lib/python2.7/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 734, in batch
    return BatchDataset(self, batch_size)
  File ""/home/ahundt/.local/lib/python2.7/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 1392, in __init__
    batch_size, dtype=dtypes.int64, name=""batch_size"")
  File ""/home/ahundt/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 932, in convert_to_tensor
    as_ref=False)
  File ""/home/ahundt/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1022, in internal_convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""/home/ahundt/.local/lib/python2.7/site-packages/tensorflow/python/framework/constant_op.py"", line 233, in _constant_tensor_conversion_function
    return constant(v, dtype=dtype, name=name)
  File ""/home/ahundt/.local/lib/python2.7/site-packages/tensorflow/python/framework/constant_op.py"", line 212, in constant
    value, dtype=dtype, shape=shape, verify_shape=verify_shape))
  File ""/home/ahundt/.local/lib/python2.7/site-packages/tensorflow/python/framework/tensor_util.py"", line 413, in make_tensor_proto
    _AssertCompatible(values, dtype)
  File ""/home/ahundt/.local/lib/python2.7/site-packages/tensorflow/python/framework/tensor_util.py"", line 328, in _AssertCompatible
    (dtype.name, repr(mismatch), type(mismatch).__name__))
TypeError: Expected int64, got 'dataset_reader.py' of type 'str' instead.
```

This PR hopes to clear that up by checking the type right at the call site.

If I made a mistake in the preferred approach to such a check I'd appreciate advice, I did some searching and I couldn't find obvious examples that can accept both integers and scalar tensors, there are always slight variations and acceptable imports depend on the code location. ",1,,1,2018-02-09T19:11:54Z,2018-02-10T22:06:37Z,NONE,"Currently if you inadvertently pass a non integer value for batch_size you get a strangely cryptic error with a long trace:
```
  File ""/home/ahundt/.local/lib/python2.7/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 734, in batch
    return BatchDataset(self, batch_size)
  File ""/home/ahundt/.local/lib/python2.7/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 1392, in __init__
    batch_size, dtype=dtypes.int64, name=""batch_size"")
  File ""/home/ahundt/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 932, in convert_to_tensor
    as_ref=False)
  File ""/home/ahundt/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1022, in internal_convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""/home/ahundt/.local/lib/python2.7/site-packages/tensorflow/python/framework/constant_op.py"", line 233, in _constant_tensor_conversion_function
    return constant(v, dtype=dtype, name=name)
  File ""/home/ahundt/.local/lib/python2.7/site-packages/tensorflow/python/framework/constant_op.py"", line 212, in constant
    value, dtype=dtype, shape=shape, verify_shape=verify_shape))
  File ""/home/ahundt/.local/lib/python2.7/site-packages/tensorflow/python/framework/tensor_util.py"", line 413, in make_tensor_proto
    _AssertCompatible(values, dtype)
  File ""/home/ahundt/.local/lib/python2.7/site-packages/tensorflow/python/framework/tensor_util.py"", line 328, in _AssertCompatible
    (dtype.name, repr(mismatch), type(mismatch).__name__))
TypeError: Expected int64, got 'dataset_reader.py' of type 'str' instead.
```

This PR hopes to clear that up by checking the type right at the call site.

If I made a mistake in the preferred approach to such a check I'd appreciate advice, I did some searching and I couldn't find obvious examples that can accept both integers and scalar tensors, there are always slight variations and acceptable imports depend on the code location. ",2018-02-10T22:06:37Z,1,1,0,2.833808200695334
33,16894,`control_dependencies` unexpected behavior with tensorflow>=1.3,stat:awaiting response,"------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 14.04
- **TensorFlow installed from (source or binary)**: from binary
- **TensorFlow version (use command below)**: 1.1.0, 1.3.0, 1.5.0
- **Python version**:  2.7.6
- **Bazel version (if compiling from source)**: None
- **GCC/Compiler version (if compiling from source)**:
gcc version 4.9.4 (Ubuntu 4.9.4-2ubuntu1~14.04.1) for compiling custom ops.
- **CUDA/cuDNN version**: do not matter
- **GPU model and memory**: do not matter 
- **Exact command to reproduce**: As following

### Describe the problem

For the code below: (the complete sample follows later)
```python
first_op = custom_ops_module.custom_first(features_v)
with tf.control_dependencies([first_op]):
    second_op = tf.reshape(custom_ops_module.custom_second(), [480, 220, 220, 1])

for i in range(5):
    sess.run(second_op)
```
According to [the doc of control dependencies](https://www.tensorflow.org/api_docs/python/tf/Graph#control_dependencies), `second_op` will only run after `first_op` has executed.
However, this is not true for tensorflow 1.3+ when the ops are customized cpp codes.

* tensorflow 1.1
The code runs exactly what I want: a `first_op`-`second_op` loop.
* tensorflow 1.3
`second_op` **runs first**, followed the `first_op`-`second_op` loop.
* tensorflow 1.5
`second_op` runs **first and only once globally**, then `first_op` runs again and again...

### Source code / logs

To reproduce, a customized op is need (C++ source and compile script attached). The Python part loads the compiled `custom_ops.so` and behaves as described above, for TensorFlow 1.1, 1.3 and 1.5, each with log provided.

```python
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

from __future__ import print_function

import os
os.environ['TF_CPP_MIN_LOG_LEVEL']='2'
import tensorflow as tf
custom_ops_module = tf.load_op_library('./custom_ops.so')

def run():
    features_v = tf.Variable(
            tf.zeros(shape=[480, 256]), 
            name='features_v',
            trainable=False,
            collections=[tf.GraphKeys.LOCAL_VARIABLES])
    first_op = custom_ops_module.custom_first(features_v)
    with tf.control_dependencies([first_op]):
        second_op = tf.reshape(custom_ops_module.custom_second(), [480, 220, 220, 1])

    sess_config = tf.ConfigProto()
    sess_config.allow_soft_placement = True 

    sess = tf.Session(config=sess_config)

    sess.run(tf.global_variables_initializer())
    sess.run(tf.local_variables_initializer())
    print('init vars done')

    print('start loop')
    for i in range(5):
        print('loop: ', i)
        sess.run(second_op)

if __name__ == '__main__':
    print(""tensorflow"", tf.__version__)
    run()

```

The custom ops
```cpp
#include ""tensorflow/core/framework/op.h""
#include ""tensorflow/core/framework/shape_inference.h""
#include ""tensorflow/core/framework/op_kernel.h""

#include <glog/logging.h>

///////////////////////////////////////// NameSpace ////////////////////////////////////////////

using tensorflow::DEVICE_CPU;
using tensorflow::Tensor;

///////////////////////////////////////// CustomFirstOp ////////////////////////////////////////
REGISTER_OP(""CustomFirst"")
    .Input(""features: float32"")
    ;
class CustomFirstOp : public tensorflow::OpKernel {
 public:
  explicit CustomFirstOp(tensorflow::OpKernelConstruction* context) : tensorflow::OpKernel(context) {}

  void Compute(tensorflow::OpKernelContext* context) override {
      const Tensor& input_tensor = context->input(0);
      LOG(INFO) << ""FirstOp enter with "" << input_tensor.DebugString(); 
      LOG(INFO) << ""FirstOp leave"";
  }
};
REGISTER_KERNEL_BUILDER(Name(""CustomFirst"").Device(DEVICE_CPU), CustomFirstOp);
///////////////////////////////////////// CustomSecondOp ///////////////////////////////////////
REGISTER_OP(""CustomSecond"")
    .Output(""images: uint8"")
    ;
class CustomSecondOp : public tensorflow::OpKernel {
 public:
  explicit CustomSecondOp(tensorflow::OpKernelConstruction* context) : tensorflow::OpKernel(context) {}

  void Compute(tensorflow::OpKernelContext* context) override {
      LOG(INFO) << ""SecondOp enter"";
      tensorflow::Tensor* output_tensor = nullptr;
      OP_REQUIRES_OK(context, context->allocate_output(0, tensorflow::TensorShape({480, 220, 220, 1}), &output_tensor));
      auto output_tensor_buffer = output_tensor->shaped<tensorflow::uint8, 4>({480, 220, 220, 1});
      LOG(INFO) << ""SecondOp leave with "" << output_tensor->DebugString();
  }
};
REGISTER_KERNEL_BUILDER(Name(""CustomSecond"").Device(DEVICE_CPU), CustomSecondOp);
//////////////////////////////////////// Op End //////////////////////////////////////////////////////

```
Compile scripts for tensorflow 1.1 and 1.3
```
#!/bin/bash
TF_INC=( $(python -c 'import tensorflow as tf; print(tf.sysconfig.get_include())') )
g++ -std=c++11 -shared custom_ops.cc -o custom_ops.so -fPIC -I $TF_INC -lglog
```
Compile scripts for tensorflow 1.5
```
#!/bin/bash
TF_CFLAGS=( $(python -c 'import tensorflow as tf; print("" "".join(tf.sysconfig.get_compile_flags()))') )
TF_LFLAGS=( $(python -c 'import tensorflow as tf; print("" "".join(tf.sysconfig.get_link_flags()))') )
g++ -std=c++11 -shared custom_ops.cc -o custom_ops.so -fPIC ${TF_CFLAGS[@]} ${TF_LFLAGS[@]} -lglog
```
Log with tensorflow 1.1
```
tensorflow 1.1.0
init vars done
start loop
loop:  0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0210 00:04:19.464854 31270 custom_ops.cc:22] FirstOp enter with Tensor<type: float shape: [480,256] values: [0 0 0]...>
I0210 00:04:19.464900 31270 custom_ops.cc:23] FirstOp leave
I0210 00:04:19.464946 31268 custom_ops.cc:36] SecondOp enter
I0210 00:04:19.486101 31268 custom_ops.cc:40] SecondOp leave with Tensor<type: uint8 shape: [480,220,220,1] values: [[[0][0][0]]]...>
loop:  1
I0210 00:04:19.507359 31266 custom_ops.cc:22] FirstOp enter with Tensor<type: float shape: [480,256] values: [0 0 0]...>
I0210 00:04:19.507411 31266 custom_ops.cc:23] FirstOp leave
I0210 00:04:19.507489 31269 custom_ops.cc:36] SecondOp enter
I0210 00:04:19.507637 31269 custom_ops.cc:40] SecondOp leave with Tensor<type: uint8 shape: [480,220,220,1] values: [[[0][0][0]]]...>
loop:  2
I0210 00:04:19.547608 31269 custom_ops.cc:22] FirstOp enter with Tensor<type: float shape: [480,256] values: [0 0 0]...>
I0210 00:04:19.547644 31269 custom_ops.cc:23] FirstOp leave
I0210 00:04:19.547683 31268 custom_ops.cc:36] SecondOp enter
I0210 00:04:19.547745 31268 custom_ops.cc:40] SecondOp leave with Tensor<type: uint8 shape: [480,220,220,1] values: [[[0][0][0]]]...>
loop:  3
I0210 00:04:19.579887 31267 custom_ops.cc:22] FirstOp enter with Tensor<type: float shape: [480,256] values: [0 0 0]...>
I0210 00:04:19.579923 31267 custom_ops.cc:23] FirstOp leave
I0210 00:04:19.579952 31270 custom_ops.cc:36] SecondOp enter
I0210 00:04:19.580021 31270 custom_ops.cc:40] SecondOp leave with Tensor<type: uint8 shape: [480,220,220,1] values: [[[0][0][0]]]...>
loop:  4
I0210 00:04:19.874367 31270 custom_ops.cc:22] FirstOp enter with Tensor<type: float shape: [480,256] values: [0 0 0]...>
I0210 00:04:19.874406 31270 custom_ops.cc:23] FirstOp leave
I0210 00:04:19.874454 31267 custom_ops.cc:36] SecondOp enter
I0210 00:04:19.874563 31267 custom_ops.cc:40] SecondOp leave with Tensor<type: uint8 shape: [480,220,220,1] values: [[[0][0][0]]]...>
```
Log with tensorflow 1.3
```
tensorflow 1.3.0
init vars done
start loop
loop:  0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0210 00:03:34.893167 28884 custom_ops.cc:36] SecondOp enter
I0210 00:03:34.893406 28884 custom_ops.cc:40] SecondOp leave with Tensor<type: uint8 shape: [480,220,220,1] values: [[[0][0][0]]]...>
I0210 00:03:35.004695 29283 custom_ops.cc:22] FirstOp enter with Tensor<type: float shape: [480,256] values: [0 0 0]...>
I0210 00:03:35.004729 29283 custom_ops.cc:23] FirstOp leave
I0210 00:03:35.004763 29283 custom_ops.cc:36] SecondOp enter
I0210 00:03:35.025548 29283 custom_ops.cc:40] SecondOp leave with Tensor<type: uint8 shape: [480,220,220,1] values: [[[0][0][0]]]...>
loop:  1
I0210 00:03:35.031692 29281 custom_ops.cc:22] FirstOp enter with Tensor<type: float shape: [480,256] values: [0 0 0]...>
I0210 00:03:35.031735 29281 custom_ops.cc:23] FirstOp leave
I0210 00:03:35.031818 29280 custom_ops.cc:36] SecondOp enter
I0210 00:03:35.051908 29280 custom_ops.cc:40] SecondOp leave with Tensor<type: uint8 shape: [480,220,220,1] values: [[[0][0][0]]]...>
loop:  2
I0210 00:03:35.056428 29281 custom_ops.cc:22] FirstOp enter with Tensor<type: float shape: [480,256] values: [0 0 0]...>
I0210 00:03:35.056473 29281 custom_ops.cc:23] FirstOp leave
I0210 00:03:35.056516 29280 custom_ops.cc:36] SecondOp enter
I0210 00:03:35.056560 29280 custom_ops.cc:40] SecondOp leave with Tensor<type: uint8 shape: [480,220,220,1] values: [[[0][0][0]]]...>
loop:  3
I0210 00:03:35.061014 29281 custom_ops.cc:22] FirstOp enter with Tensor<type: float shape: [480,256] values: [0 0 0]...>
I0210 00:03:35.061050 29281 custom_ops.cc:23] FirstOp leave
I0210 00:03:35.061085 29283 custom_ops.cc:36] SecondOp enter
I0210 00:03:35.061139 29283 custom_ops.cc:40] SecondOp leave with Tensor<type: uint8 shape: [480,220,220,1] values: [[[0][0][0]]]...>
loop:  4
I0210 00:03:35.065387 29282 custom_ops.cc:22] FirstOp enter with Tensor<type: float shape: [480,256] values: [0 0 0]...>
I0210 00:03:35.065429 29282 custom_ops.cc:23] FirstOp leave
I0210 00:03:35.065500 29281 custom_ops.cc:36] SecondOp enter
I0210 00:03:35.065654 29281 custom_ops.cc:40] SecondOp leave with Tensor<type: uint8 shape: [480,220,220,1] values: [[[0][0][0]]]...>
```
Log with tensorflow 1.5
```
tensorflow 1.5.0
init vars done
start loop
loop:  0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0210 00:39:03.166337 27046 custom_ops.cc:36] SecondOp enter
I0210 00:39:03.166424 27046 custom_ops.cc:40] SecondOp leave with Tensor<type: uint8 shape: [480,220,220,1] values: [[[0][0][0]]]...>
I0210 00:39:03.868782 27965 custom_ops.cc:22] FirstOp enter with Tensor<type: float shape: [480,256] values: [0 0 0]...>
I0210 00:39:03.868820 27965 custom_ops.cc:23] FirstOp leave
loop:  1
I0210 00:39:03.879077 27964 custom_ops.cc:22] FirstOp enter with Tensor<type: float shape: [480,256] values: [0 0 0]...>
I0210 00:39:03.879094 27964 custom_ops.cc:23] FirstOp leave
loop:  2
I0210 00:39:03.893448 27962 custom_ops.cc:22] FirstOp enter with Tensor<type: float shape: [480,256] values: [0 0 0]...>
I0210 00:39:03.893474 27962 custom_ops.cc:23] FirstOp leave
loop:  3
I0210 00:39:03.906409 27963 custom_ops.cc:22] FirstOp enter with Tensor<type: float shape: [480,256] values: [0 0 0]...>
I0210 00:39:03.906440 27963 custom_ops.cc:23] FirstOp leave
loop:  4
I0210 00:39:03.917815 27963 custom_ops.cc:22] FirstOp enter with Tensor<type: float shape: [480,256] values: [0 0 0]...>
I0210 00:39:03.917836 27963 custom_ops.cc:23] FirstOp leave
```",1,,2,2018-02-09T17:09:15Z,2018-02-10T09:34:43Z,NONE,"------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 14.04
- **TensorFlow installed from (source or binary)**: from binary
- **TensorFlow version (use command below)**: 1.1.0, 1.3.0, 1.5.0
- **Python version**:  2.7.6
- **Bazel version (if compiling from source)**: None
- **GCC/Compiler version (if compiling from source)**:
gcc version 4.9.4 (Ubuntu 4.9.4-2ubuntu1~14.04.1) for compiling custom ops.
- **CUDA/cuDNN version**: do not matter
- **GPU model and memory**: do not matter 
- **Exact command to reproduce**: As following

### Describe the problem

For the code below: (the complete sample follows later)
```python
first_op = custom_ops_module.custom_first(features_v)
with tf.control_dependencies([first_op]):
    second_op = tf.reshape(custom_ops_module.custom_second(), [480, 220, 220, 1])

for i in range(5):
    sess.run(second_op)
```
According to [the doc of control dependencies](https://www.tensorflow.org/api_docs/python/tf/Graph#control_dependencies), `second_op` will only run after `first_op` has executed.
However, this is not true for tensorflow 1.3+ when the ops are customized cpp codes.

* tensorflow 1.1
The code runs exactly what I want: a `first_op`-`second_op` loop.
* tensorflow 1.3
`second_op` **runs first**, followed the `first_op`-`second_op` loop.
* tensorflow 1.5
`second_op` runs **first and only once globally**, then `first_op` runs again and again...

### Source code / logs

To reproduce, a customized op is need (C++ source and compile script attached). The Python part loads the compiled `custom_ops.so` and behaves as described above, for TensorFlow 1.1, 1.3 and 1.5, each with log provided.

```python
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

from __future__ import print_function

import os
os.environ['TF_CPP_MIN_LOG_LEVEL']='2'
import tensorflow as tf
custom_ops_module = tf.load_op_library('./custom_ops.so')

def run():
    features_v = tf.Variable(
            tf.zeros(shape=[480, 256]), 
            name='features_v',
            trainable=False,
            collections=[tf.GraphKeys.LOCAL_VARIABLES])
    first_op = custom_ops_module.custom_first(features_v)
    with tf.control_dependencies([first_op]):
        second_op = tf.reshape(custom_ops_module.custom_second(), [480, 220, 220, 1])

    sess_config = tf.ConfigProto()
    sess_config.allow_soft_placement = True 

    sess = tf.Session(config=sess_config)

    sess.run(tf.global_variables_initializer())
    sess.run(tf.local_variables_initializer())
    print('init vars done')

    print('start loop')
    for i in range(5):
        print('loop: ', i)
        sess.run(second_op)

if __name__ == '__main__':
    print(""tensorflow"", tf.__version__)
    run()

```

The custom ops
```cpp
#include ""tensorflow/core/framework/op.h""
#include ""tensorflow/core/framework/shape_inference.h""
#include ""tensorflow/core/framework/op_kernel.h""

#include <glog/logging.h>

///////////////////////////////////////// NameSpace ////////////////////////////////////////////

using tensorflow::DEVICE_CPU;
using tensorflow::Tensor;

///////////////////////////////////////// CustomFirstOp ////////////////////////////////////////
REGISTER_OP(""CustomFirst"")
    .Input(""features: float32"")
    ;
class CustomFirstOp : public tensorflow::OpKernel {
 public:
  explicit CustomFirstOp(tensorflow::OpKernelConstruction* context) : tensorflow::OpKernel(context) {}

  void Compute(tensorflow::OpKernelContext* context) override {
      const Tensor& input_tensor = context->input(0);
      LOG(INFO) << ""FirstOp enter with "" << input_tensor.DebugString(); 
      LOG(INFO) << ""FirstOp leave"";
  }
};
REGISTER_KERNEL_BUILDER(Name(""CustomFirst"").Device(DEVICE_CPU), CustomFirstOp);
///////////////////////////////////////// CustomSecondOp ///////////////////////////////////////
REGISTER_OP(""CustomSecond"")
    .Output(""images: uint8"")
    ;
class CustomSecondOp : public tensorflow::OpKernel {
 public:
  explicit CustomSecondOp(tensorflow::OpKernelConstruction* context) : tensorflow::OpKernel(context) {}

  void Compute(tensorflow::OpKernelContext* context) override {
      LOG(INFO) << ""SecondOp enter"";
      tensorflow::Tensor* output_tensor = nullptr;
      OP_REQUIRES_OK(context, context->allocate_output(0, tensorflow::TensorShape({480, 220, 220, 1}), &output_tensor));
      auto output_tensor_buffer = output_tensor->shaped<tensorflow::uint8, 4>({480, 220, 220, 1});
      LOG(INFO) << ""SecondOp leave with "" << output_tensor->DebugString();
  }
};
REGISTER_KERNEL_BUILDER(Name(""CustomSecond"").Device(DEVICE_CPU), CustomSecondOp);
//////////////////////////////////////// Op End //////////////////////////////////////////////////////

```
Compile scripts for tensorflow 1.1 and 1.3
```
#!/bin/bash
TF_INC=( $(python -c 'import tensorflow as tf; print(tf.sysconfig.get_include())') )
g++ -std=c++11 -shared custom_ops.cc -o custom_ops.so -fPIC -I $TF_INC -lglog
```
Compile scripts for tensorflow 1.5
```
#!/bin/bash
TF_CFLAGS=( $(python -c 'import tensorflow as tf; print("" "".join(tf.sysconfig.get_compile_flags()))') )
TF_LFLAGS=( $(python -c 'import tensorflow as tf; print("" "".join(tf.sysconfig.get_link_flags()))') )
g++ -std=c++11 -shared custom_ops.cc -o custom_ops.so -fPIC ${TF_CFLAGS[@]} ${TF_LFLAGS[@]} -lglog
```
Log with tensorflow 1.1
```
tensorflow 1.1.0
init vars done
start loop
loop:  0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0210 00:04:19.464854 31270 custom_ops.cc:22] FirstOp enter with Tensor<type: float shape: [480,256] values: [0 0 0]...>
I0210 00:04:19.464900 31270 custom_ops.cc:23] FirstOp leave
I0210 00:04:19.464946 31268 custom_ops.cc:36] SecondOp enter
I0210 00:04:19.486101 31268 custom_ops.cc:40] SecondOp leave with Tensor<type: uint8 shape: [480,220,220,1] values: [[[0][0][0]]]...>
loop:  1
I0210 00:04:19.507359 31266 custom_ops.cc:22] FirstOp enter with Tensor<type: float shape: [480,256] values: [0 0 0]...>
I0210 00:04:19.507411 31266 custom_ops.cc:23] FirstOp leave
I0210 00:04:19.507489 31269 custom_ops.cc:36] SecondOp enter
I0210 00:04:19.507637 31269 custom_ops.cc:40] SecondOp leave with Tensor<type: uint8 shape: [480,220,220,1] values: [[[0][0][0]]]...>
loop:  2
I0210 00:04:19.547608 31269 custom_ops.cc:22] FirstOp enter with Tensor<type: float shape: [480,256] values: [0 0 0]...>
I0210 00:04:19.547644 31269 custom_ops.cc:23] FirstOp leave
I0210 00:04:19.547683 31268 custom_ops.cc:36] SecondOp enter
I0210 00:04:19.547745 31268 custom_ops.cc:40] SecondOp leave with Tensor<type: uint8 shape: [480,220,220,1] values: [[[0][0][0]]]...>
loop:  3
I0210 00:04:19.579887 31267 custom_ops.cc:22] FirstOp enter with Tensor<type: float shape: [480,256] values: [0 0 0]...>
I0210 00:04:19.579923 31267 custom_ops.cc:23] FirstOp leave
I0210 00:04:19.579952 31270 custom_ops.cc:36] SecondOp enter
I0210 00:04:19.580021 31270 custom_ops.cc:40] SecondOp leave with Tensor<type: uint8 shape: [480,220,220,1] values: [[[0][0][0]]]...>
loop:  4
I0210 00:04:19.874367 31270 custom_ops.cc:22] FirstOp enter with Tensor<type: float shape: [480,256] values: [0 0 0]...>
I0210 00:04:19.874406 31270 custom_ops.cc:23] FirstOp leave
I0210 00:04:19.874454 31267 custom_ops.cc:36] SecondOp enter
I0210 00:04:19.874563 31267 custom_ops.cc:40] SecondOp leave with Tensor<type: uint8 shape: [480,220,220,1] values: [[[0][0][0]]]...>
```
Log with tensorflow 1.3
```
tensorflow 1.3.0
init vars done
start loop
loop:  0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0210 00:03:34.893167 28884 custom_ops.cc:36] SecondOp enter
I0210 00:03:34.893406 28884 custom_ops.cc:40] SecondOp leave with Tensor<type: uint8 shape: [480,220,220,1] values: [[[0][0][0]]]...>
I0210 00:03:35.004695 29283 custom_ops.cc:22] FirstOp enter with Tensor<type: float shape: [480,256] values: [0 0 0]...>
I0210 00:03:35.004729 29283 custom_ops.cc:23] FirstOp leave
I0210 00:03:35.004763 29283 custom_ops.cc:36] SecondOp enter
I0210 00:03:35.025548 29283 custom_ops.cc:40] SecondOp leave with Tensor<type: uint8 shape: [480,220,220,1] values: [[[0][0][0]]]...>
loop:  1
I0210 00:03:35.031692 29281 custom_ops.cc:22] FirstOp enter with Tensor<type: float shape: [480,256] values: [0 0 0]...>
I0210 00:03:35.031735 29281 custom_ops.cc:23] FirstOp leave
I0210 00:03:35.031818 29280 custom_ops.cc:36] SecondOp enter
I0210 00:03:35.051908 29280 custom_ops.cc:40] SecondOp leave with Tensor<type: uint8 shape: [480,220,220,1] values: [[[0][0][0]]]...>
loop:  2
I0210 00:03:35.056428 29281 custom_ops.cc:22] FirstOp enter with Tensor<type: float shape: [480,256] values: [0 0 0]...>
I0210 00:03:35.056473 29281 custom_ops.cc:23] FirstOp leave
I0210 00:03:35.056516 29280 custom_ops.cc:36] SecondOp enter
I0210 00:03:35.056560 29280 custom_ops.cc:40] SecondOp leave with Tensor<type: uint8 shape: [480,220,220,1] values: [[[0][0][0]]]...>
loop:  3
I0210 00:03:35.061014 29281 custom_ops.cc:22] FirstOp enter with Tensor<type: float shape: [480,256] values: [0 0 0]...>
I0210 00:03:35.061050 29281 custom_ops.cc:23] FirstOp leave
I0210 00:03:35.061085 29283 custom_ops.cc:36] SecondOp enter
I0210 00:03:35.061139 29283 custom_ops.cc:40] SecondOp leave with Tensor<type: uint8 shape: [480,220,220,1] values: [[[0][0][0]]]...>
loop:  4
I0210 00:03:35.065387 29282 custom_ops.cc:22] FirstOp enter with Tensor<type: float shape: [480,256] values: [0 0 0]...>
I0210 00:03:35.065429 29282 custom_ops.cc:23] FirstOp leave
I0210 00:03:35.065500 29281 custom_ops.cc:36] SecondOp enter
I0210 00:03:35.065654 29281 custom_ops.cc:40] SecondOp leave with Tensor<type: uint8 shape: [480,220,220,1] values: [[[0][0][0]]]...>
```
Log with tensorflow 1.5
```
tensorflow 1.5.0
init vars done
start loop
loop:  0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0210 00:39:03.166337 27046 custom_ops.cc:36] SecondOp enter
I0210 00:39:03.166424 27046 custom_ops.cc:40] SecondOp leave with Tensor<type: uint8 shape: [480,220,220,1] values: [[[0][0][0]]]...>
I0210 00:39:03.868782 27965 custom_ops.cc:22] FirstOp enter with Tensor<type: float shape: [480,256] values: [0 0 0]...>
I0210 00:39:03.868820 27965 custom_ops.cc:23] FirstOp leave
loop:  1
I0210 00:39:03.879077 27964 custom_ops.cc:22] FirstOp enter with Tensor<type: float shape: [480,256] values: [0 0 0]...>
I0210 00:39:03.879094 27964 custom_ops.cc:23] FirstOp leave
loop:  2
I0210 00:39:03.893448 27962 custom_ops.cc:22] FirstOp enter with Tensor<type: float shape: [480,256] values: [0 0 0]...>
I0210 00:39:03.893474 27962 custom_ops.cc:23] FirstOp leave
loop:  3
I0210 00:39:03.906409 27963 custom_ops.cc:22] FirstOp enter with Tensor<type: float shape: [480,256] values: [0 0 0]...>
I0210 00:39:03.906440 27963 custom_ops.cc:23] FirstOp leave
loop:  4
I0210 00:39:03.917815 27963 custom_ops.cc:22] FirstOp enter with Tensor<type: float shape: [480,256] values: [0 0 0]...>
I0210 00:39:03.917836 27963 custom_ops.cc:23] FirstOp leave
```",2018-02-09T22:07:54Z,1,1,0,3.333808200695334
34,16891,How show detecting image.,stat:awaiting response,"When I start a object_detection_tutorial script via ipython, it's didn't show me any image.  My console:
![image](https://user-images.githubusercontent.com/17855733/36030750-0f96f458-0db1-11e8-867c-7d545c6e3170.png)
But something is starting in process because starting some python program, but before it's  open, the program is closed.
![image](https://user-images.githubusercontent.com/17855733/36030857-713182fa-0db1-11e8-9b25-c208d0979d12.png)
code is here: https://pastebin.com/eh0SWjyU",0,,2,2018-02-09T13:57:41Z,2018-02-13T09:32:42Z,NONE,"When I start a object_detection_tutorial script via ipython, it's didn't show me any image.  My console:
![image](https://user-images.githubusercontent.com/17855733/36030750-0f96f458-0db1-11e8-867c-7d545c6e3170.png)
But something is starting in process because starting some python program, but before it's  open, the program is closed.
![image](https://user-images.githubusercontent.com/17855733/36030857-713182fa-0db1-11e8-9b25-c208d0979d12.png)
code is here: https://pastebin.com/eh0SWjyU",2018-02-10T01:28:29Z,4,1,1,2.333808200695334
35,16886,Please support Cuda 9.1,stat:awaiting response,"I tried cloning the alpha zero chess from https://github.com/Zeta36/chess-alpha-zero and almost got everything to work.  However when I try ""python src/chess_zero/run.py self"" I get the message 

ImportError: Could not find 'cudart64_90.dll'. TensorFlow requires that this DLL be installed in a directory that is named in your %PATH% environment variable. Download and install CUDA 9.0 from this URL: https://developer.nvidia.com/cuda-toolkit 

The problem is that I have CUDA 9.1 and cannot seem to get CUDA 9.0 on the website.  Would it be hard to make a version of TensorFlow-gpu that works with CUDA 9.1 ?

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
NO

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Windows 10  
 
- **TensorFlow installed from (source or binary)**:
Installed from https://www.tensorflow.org/  

- **TensorFlow version (use command below)**:
1.5

- **Python version**:
 3.6.3

- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
9.1
- **GPU model and memory**:
NVIDIA GeForce GTX 1050 Ti  8GB Ram

- **Exact command to reproduce**:
python src/chess_zero/run.py self

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

e:\eDownloads>python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
Traceback (most recent call last):
  File ""C:\Users\User\Anaconda3\lib\site-packages\tensorflow\python\platform\self_check.py"", line 75, in preload_check
    ctypes.WinDLL(build_info.cudart_dll_name)
  File ""C:\Users\User\Anaconda3\lib\ctypes\__init__.py"", line 348, in __init__
    self._handle = _dlopen(self._name, mode)
OSError: [WinError 126] The specified module could not be found

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""C:\Users\User\Anaconda3\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""C:\Users\User\Anaconda3\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\User\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 30, in <module>
    self_check.preload_check()
  File ""C:\Users\User\Anaconda3\lib\site-packages\tensorflow\python\platform\self_check.py"", line 82, in preload_check
    % (build_info.cudart_dll_name, build_info.cuda_version_number))
ImportError: Could not find 'cudart64_90.dll'. TensorFlow requires that this DLL be installed in a directory that is named in your %PATH% environment variable. Download and install CUDA 9.0 from this URL: https://developer.nvidia.com/cuda-toolkit

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.  
I am requesting an update because I have CUDA 9.1 and can't get CUDA 9.0

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",1,,4,2018-02-09T06:15:20Z,2018-02-13T00:21:04Z,NONE,"I tried cloning the alpha zero chess from https://github.com/Zeta36/chess-alpha-zero and almost got everything to work.  However when I try ""python src/chess_zero/run.py self"" I get the message 

ImportError: Could not find 'cudart64_90.dll'. TensorFlow requires that this DLL be installed in a directory that is named in your %PATH% environment variable. Download and install CUDA 9.0 from this URL: https://developer.nvidia.com/cuda-toolkit 

The problem is that I have CUDA 9.1 and cannot seem to get CUDA 9.0 on the website.  Would it be hard to make a version of TensorFlow-gpu that works with CUDA 9.1 ?

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
NO

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Windows 10  
 
- **TensorFlow installed from (source or binary)**:
Installed from https://www.tensorflow.org/  

- **TensorFlow version (use command below)**:
1.5

- **Python version**:
 3.6.3

- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
9.1
- **GPU model and memory**:
NVIDIA GeForce GTX 1050 Ti  8GB Ram

- **Exact command to reproduce**:
python src/chess_zero/run.py self

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

e:\eDownloads>python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
Traceback (most recent call last):
  File ""C:\Users\User\Anaconda3\lib\site-packages\tensorflow\python\platform\self_check.py"", line 75, in preload_check
    ctypes.WinDLL(build_info.cudart_dll_name)
  File ""C:\Users\User\Anaconda3\lib\ctypes\__init__.py"", line 348, in __init__
    self._handle = _dlopen(self._name, mode)
OSError: [WinError 126] The specified module could not be found

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""C:\Users\User\Anaconda3\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""C:\Users\User\Anaconda3\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\User\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 30, in <module>
    self_check.preload_check()
  File ""C:\Users\User\Anaconda3\lib\site-packages\tensorflow\python\platform\self_check.py"", line 82, in preload_check
    % (build_info.cudart_dll_name, build_info.cuda_version_number))
ImportError: Could not find 'cudart64_90.dll'. TensorFlow requires that this DLL be installed in a directory that is named in your %PATH% environment variable. Download and install CUDA 9.0 from this URL: https://developer.nvidia.com/cuda-toolkit

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.  
I am requesting an update because I have CUDA 9.1 and can't get CUDA 9.0

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",2018-02-13T00:21:04Z,4,1,1,4.333808200695334
36,16878,Adding the CMAKE_GENERATOR  line to all external cmake files.,"cla: yes,stat:awaiting tensorflower",,0,,4,2018-02-08T22:09:24Z,2018-02-15T23:30:17Z,MEMBER,,2018-02-09T16:00:11Z,7,3,1,5.328458738753051
37,16876,Fix warning about keep_dims. keep_dims -> keepdims for tf.reduce_sum().,cla: yes,Fix warning about keep_dims. keep_dims -> keepdims for tf.reduce_sum().,1,,2,2018-02-08T20:36:39Z,2018-02-12T07:05:38Z,CONTRIBUTOR,Fix warning about keep_dims. keep_dims -> keepdims for tf.reduce_sum().,2018-02-11T18:44:32Z,4,2,1,4.328458738753051
38,16875,Update Dockerfile to install Python 3.6,stat:awaiting response,"The Python 3 containers at gcr.io/tensorflow/tensorflow generated by [tensorflow/tools/docker](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/docker) are currently built using Python 3.5.

It would be nice to update them to Python 3.6 instead, which I think would be as simple as adding `sudo apt-get install python3.6`.

- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N/A
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: N/A
- **TensorFlow installed from (source or binary)**: N/A
- **TensorFlow version (use command below)**: N/A
- **Python version**: N/A
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A
",0,,3,2018-02-08T20:36:22Z,2018-02-12T22:46:23Z,MEMBER,"The Python 3 containers at gcr.io/tensorflow/tensorflow generated by [tensorflow/tools/docker](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/docker) are currently built using Python 3.5.

It would be nice to update them to Python 3.6 instead, which I think would be as simple as adding `sudo apt-get install python3.6`.

- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N/A
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: N/A
- **TensorFlow installed from (source or binary)**: N/A
- **TensorFlow version (use command below)**: N/A
- **Python version**: N/A
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A
",2018-02-09T07:10:40Z,4,3,1,4.828458738753051
39,16871,Request For Tagalog Translation,stat:awaiting tensorflower,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
I want to help in the Translation for Tagalog version so that this project will be available in our country. Hope you'll grant my request.

### Source code / logs
",0,,3,2018-02-08T16:56:10Z,2018-02-09T20:16:32Z,NONE,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
I want to help in the Translation for Tagalog version so that this project will be available in our country. Hope you'll grant my request.

### Source code / logs
",2018-02-09T18:23:01Z,1,1,0,2.828458738753051
40,16868,Backpropagation/weight update issue with custom layer,,"
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Win 10
- **TensorFlow installed from (source or binary)**: From pip (binary)
- **TensorFlow version (use command below)**: 1.5
- **Python version**:  3.5
- **Bazel version (if compiling from source)**:---
- **GCC/Compiler version (if compiling from source)**:----
- **CUDA/cuDNN version**:None
- **GPU model and memory**:None
- **Exact command to reproduce**:See source code

### Describe the problem
I am attempting to implement a custom layer. The layer uses the Image to Patch function and simple tensorflow operator. The layer is implemented in keras to simplify the model building and training but the backend is in tensorflow.

I am using a simple cnn as a benchmark, whenever I implement my custom layer ( even only as the first layer to 'encode' the data) backpropagation seems to break as no weights get updated in the entirety of the model.

From my understanding the all the operations used (mult, div, add, minus) are differentiable and things such as reshape, transpose and extract_image_patches should not prevent backpropagation and weight updates.

I tried using the basic layer building method and inheriting from the convolution class (_Conv) and both cases prevent the weight update for the whole model, but such a thing shouldn't be the case.

### Source code / logs

Prototype layer: https://github.com/roya0045/cvar2/blob/master/tfvar.py
Model builder: https://github.com/roya0045/cvar2/blob/master/test2.py",0,,1,2018-02-08T14:27:12Z,2018-02-09T00:33:25Z,NONE,"
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Win 10
- **TensorFlow installed from (source or binary)**: From pip (binary)
- **TensorFlow version (use command below)**: 1.5
- **Python version**:  3.5
- **Bazel version (if compiling from source)**:---
- **GCC/Compiler version (if compiling from source)**:----
- **CUDA/cuDNN version**:None
- **GPU model and memory**:None
- **Exact command to reproduce**:See source code

### Describe the problem
I am attempting to implement a custom layer. The layer uses the Image to Patch function and simple tensorflow operator. The layer is implemented in keras to simplify the model building and training but the backend is in tensorflow.

I am using a simple cnn as a benchmark, whenever I implement my custom layer ( even only as the first layer to 'encode' the data) backpropagation seems to break as no weights get updated in the entirety of the model.

From my understanding the all the operations used (mult, div, add, minus) are differentiable and things such as reshape, transpose and extract_image_patches should not prevent backpropagation and weight updates.

I tried using the basic layer building method and inheriting from the convolution class (_Conv) and both cases prevent the weight update for the whole model, but such a thing shouldn't be the case.

### Source code / logs

Prototype layer: https://github.com/roya0045/cvar2/blob/master/tfvar.py
Model builder: https://github.com/roya0045/cvar2/blob/master/test2.py",2018-02-09T00:33:25Z,1,1,0,1.828458738753051
41,16867,How to redirect tfdbg dumping directory,,"By default, `tfdbg` dumps saved tensors to `/tmp`, but in my case, `/tmp` is mount in `/root `, `/root` has only several G's space, running the example debug is not a problem, but when debugging large network, for which in one run will generate tensors that exceeds 10 G's memory, it would prompts space not enough.",0,,1,2018-02-08T14:12:19Z,2018-02-08T16:21:03Z,NONE,"By default, `tfdbg` dumps saved tensors to `/tmp`, but in my case, `/tmp` is mount in `/root `, `/root` has only several G's space, running the example debug is not a problem, but when debugging large network, for which in one run will generate tensors that exceeds 10 G's memory, it would prompts space not enough.",2018-02-08T16:20:59Z,0,1,0,1.828458738753051
42,16860,Tensorflow-gpu 1.6 : failed call to cuInit: CUDA_ERROR_NO_DEVICE,stat:awaiting response,"
- CUDA 9.0
- Cudnn 7.0
- Tensorflow-gpu 1.6

nvida-smi is good
![nvidia-smi](https://user-images.githubusercontent.com/2728049/35970141-d827bac6-0cfb-11e8-886f-92b068c62c4e.png)

Error 
2018-02-08 18:14:24.768537: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\platform\cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
**2018-02-08 18:14:25.438352: E C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\stream_executor\cuda\cuda_driver.cc:406] failed call to cuInit: CUDA_ERROR_NO_DEVICE**
2018-02-08 18:14:25.441350: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\stream_executor\cuda\cuda_diagnostics.cc:158] retrieving CUDA diagnostic information for host: Vincent
2018-02-08 18:14:25.441633: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\stream_executor\cuda\cuda_diagnostics.cc:165] hostname: Vincent
2018-02-08 18:14:25.443152: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\direct_session.cc:297] Device mapping:

Device mapping: no known devices.
MatMul: (MatMul): /job:localhost/replica:0/task:0/device:CPU:0
b: (Const): /job:localhost/replica:0/task:0/device:CPU:0
a: (Const): /job:localhost/replica:0/task:0/device:CPU:0

Code : 
`import os
import tensorflow as tf

os.environ[""CUDA_DEVICE_ORDER""]=""PCI_BUS_ID""   # see issue #152
os.environ[""CUDA_VISIBLE_DEVICES""]=""1""

from tensorflow.python.client import device_lib
#print (device_lib.list_local_devices())

# Creates a graph.
a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')
b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')
c = tf.matmul(a, b)
# Creates a session with log_device_placement set to True.
sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))
# Runs the op.
print(sess.run(c))` ",0,,2,2018-02-08T11:17:04Z,2018-02-09T07:51:44Z,NONE,"
- CUDA 9.0
- Cudnn 7.0
- Tensorflow-gpu 1.6

nvida-smi is good
![nvidia-smi](https://user-images.githubusercontent.com/2728049/35970141-d827bac6-0cfb-11e8-886f-92b068c62c4e.png)

Error 
2018-02-08 18:14:24.768537: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\platform\cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
**2018-02-08 18:14:25.438352: E C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\stream_executor\cuda\cuda_driver.cc:406] failed call to cuInit: CUDA_ERROR_NO_DEVICE**
2018-02-08 18:14:25.441350: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\stream_executor\cuda\cuda_diagnostics.cc:158] retrieving CUDA diagnostic information for host: Vincent
2018-02-08 18:14:25.441633: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\stream_executor\cuda\cuda_diagnostics.cc:165] hostname: Vincent
2018-02-08 18:14:25.443152: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\direct_session.cc:297] Device mapping:

Device mapping: no known devices.
MatMul: (MatMul): /job:localhost/replica:0/task:0/device:CPU:0
b: (Const): /job:localhost/replica:0/task:0/device:CPU:0
a: (Const): /job:localhost/replica:0/task:0/device:CPU:0

Code : 
`import os
import tensorflow as tf

os.environ[""CUDA_DEVICE_ORDER""]=""PCI_BUS_ID""   # see issue #152
os.environ[""CUDA_VISIBLE_DEVICES""]=""1""

from tensorflow.python.client import device_lib
#print (device_lib.list_local_devices())

# Creates a graph.
a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')
b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')
c = tf.matmul(a, b)
# Creates a session with log_device_placement set to True.
sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))
# Runs the op.
print(sess.run(c))` ",2018-02-08T16:12:04Z,1,1,0,2.328458738753051
43,16858,Branch 184929151,cla: yes,Had probably 50-100 files with merge conflicts due to some files being formatted by internal tools during a recent GitHub pull. Pretty sure I fixed all of them correctly though.,0,,1,2018-02-08T09:00:52Z,2018-02-09T23:16:02Z,MEMBER,Had probably 50-100 files with merge conflicts due to some files being formatted by internal tools during a recent GitHub pull. Pretty sure I fixed all of them correctly though.,2018-02-09T19:03:33Z,1,3,0,3.828458738753051
44,16843,"Have _check_bazel_version_at_least compare versions as ints, not stri",cla: yes,"Fix bazel version check issue on r1.5 branch. Example breakage https://source.cloud.google.com/results/invocations/9388d7a7-a2ed-45ce-9fcd-cf69fbdebcf5/log

PiperOrigin-RevId: 182085505",0,,3,2018-02-07T19:56:55Z,2018-02-07T21:34:39Z,MEMBER,"Fix bazel version check issue on r1.5 branch. Example breakage https://source.cloud.google.com/results/invocations/9388d7a7-a2ed-45ce-9fcd-cf69fbdebcf5/log

PiperOrigin-RevId: 182085505",2018-02-07T20:05:01Z,0,3,0,4.823515453148673
45,16842,Bump JetPack default to 3.2 in Android build script,cla: yes,,0,,1,2018-02-07T19:34:12Z,2018-02-07T22:19:12Z,MEMBER,,2018-02-13T22:21:25Z,0,3,0,3.8235154531486724
46,16840,Point Tensorflow To My Local Protobuf Installation,,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: sorta
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 16
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.4.1
- **Python version**: 2.7 and 3.0
- **Bazel version (if compiling from source)**: 0.6.0
- **GCC/Compiler version (if compiling from source)**: 5
- **CUDA/cuDNN version**: 8 and 6
- **GPU model and memory**: GTX 1080
- **Exact command to reproduce**: make
-**PROTOBUF VERSION**: 3.4.1

Folks, I am using tensorflow C++, and I added it's path to my CMakeLists.txt.
Everything was working fine, but I had to change the protobuf installation to a different path because CAFFE needs a different version of protobuf other than 3.4.1
Now, Tensorflow libraries are complaining they cannot find common.h from protobuf:

/usr/local/include/google/tensorflow/tensorflow/core/framework/tensor.pb.h:9:42: fatal error: google/protobuf/stubs/common.h: No such file or directory

I installed protobuf 3.4.1 here:
/usr/local/include/google

How do I make Tensorflow realize that protobuf is installed in a different path path?",0,,1,2018-02-07T18:47:08Z,2018-02-07T22:41:35Z,NONE,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: sorta
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 16
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.4.1
- **Python version**: 2.7 and 3.0
- **Bazel version (if compiling from source)**: 0.6.0
- **GCC/Compiler version (if compiling from source)**: 5
- **CUDA/cuDNN version**: 8 and 6
- **GPU model and memory**: GTX 1080
- **Exact command to reproduce**: make
-**PROTOBUF VERSION**: 3.4.1

Folks, I am using tensorflow C++, and I added it's path to my CMakeLists.txt.
Everything was working fine, but I had to change the protobuf installation to a different path because CAFFE needs a different version of protobuf other than 3.4.1
Now, Tensorflow libraries are complaining they cannot find common.h from protobuf:

/usr/local/include/google/tensorflow/tensorflow/core/framework/tensor.pb.h:9:42: fatal error: google/protobuf/stubs/common.h: No such file or directory

I installed protobuf 3.4.1 here:
/usr/local/include/google

How do I make Tensorflow realize that protobuf is installed in a different path path?",2018-02-07T22:41:35Z,0,1,0,1.8235154531486724
47,16838,Fixes issue when linking of rule '//tensorflow/contrib/lite/toco:toco,"awaiting testing (then merge),cla: yes","Fixes issue when linking of rule '//tensorflow/contrib/lite/toco:toco' fails because LD_LIBRARY_PATH is not configured. The workaround is to add `--action_env=""LD_LIBRARY_PATH=${LD_LIBRARY_PATH}""` to `bazel build` as described here https://stackoverflow.com/questions/47080760/tensorflow-fails-to-compile/47295278#47295278.

Related to issue: https://github.com/tensorflow/tensorflow/issues/15142#issuecomment-352562394",1,,3,2018-02-07T17:21:21Z,2018-02-07T22:19:26Z,CONTRIBUTOR,"Fixes issue when linking of rule '//tensorflow/contrib/lite/toco:toco' fails because LD_LIBRARY_PATH is not configured. The workaround is to add `--action_env=""LD_LIBRARY_PATH=${LD_LIBRARY_PATH}""` to `bazel build` as described here https://stackoverflow.com/questions/47080760/tensorflow-fails-to-compile/47295278#47295278.

Related to issue: https://github.com/tensorflow/tensorflow/issues/15142#issuecomment-352562394",2018-02-07T17:22:25Z,0,2,0,4.823515453148673
48,16837,Remove warnings in tf.losses.softmax_cross_entropy,cla: yes,"This fix tries to address the issue raised in #16534 where tf.losses.softmax_cross_entropy causes warnings due to the calling of tf.nn.softmax_cross_entropy_with_logits.

This fix switches to tf.nn.softmax_cross_entropy_with_logits_v2 to remove the warning.

This fix fixes #16534.

Signed-off-by: Yong Tang <yong.tang.github@outlook.com>",1,,3,2018-02-07T16:27:50Z,2018-02-08T00:14:24Z,MEMBER,"This fix tries to address the issue raised in #16534 where tf.losses.softmax_cross_entropy causes warnings due to the calling of tf.nn.softmax_cross_entropy_with_logits.

This fix switches to tf.nn.softmax_cross_entropy_with_logits_v2 to remove the warning.

This fix fixes #16534.

Signed-off-by: Yong Tang <yong.tang.github@outlook.com>",2018-02-07T22:45:55Z,1,3,0,5.823515453148673
49,16833,Tensorflow Lite demo app for Android: add support for floating point models as Inception-v3,"awaiting testing (then merge),cla: yes,comp:lite","Although the new Lite interface does support float models as well, the current Android demo app does only support quantized models. Furthermore, it isn't obvious to transfer the code from the quantized version to the floating point model. [Based on this discussion](https://github.com/tensorflow/tensorflow/issues/14719) I integrated the [Inception-v3 slim model](https://storage.googleapis.com/download.tensorflow.org/models/tflite/inception_v3_slim_2016_android_2017_11_10.zip) as an alternative to the existing MobileNet.

_Remaining TODO_:
The confidence scores returned by the inception net are not in [0,1] yet. Besides that, the inference itself seems to work. So the correct results are listed on top, but the confidence score isn't normalized. Maybe the given model doesn't include a Softmax layer and ends with the logits? I'm not sure about this. Any help is appreciated.",1,,4,2018-02-07T14:23:50Z,2018-02-16T00:04:12Z,CONTRIBUTOR,"Although the new Lite interface does support float models as well, the current Android demo app does only support quantized models. Furthermore, it isn't obvious to transfer the code from the quantized version to the floating point model. [Based on this discussion](https://github.com/tensorflow/tensorflow/issues/14719) I integrated the [Inception-v3 slim model](https://storage.googleapis.com/download.tensorflow.org/models/tflite/inception_v3_slim_2016_android_2017_11_10.zip) as an alternative to the existing MobileNet.

_Remaining TODO_:
The confidence scores returned by the inception net are not in [0,1] yet. Besides that, the inference itself seems to work. So the correct results are listed on top, but the confidence score isn't normalized. Maybe the given model doesn't include a Softmax layer and ends with the logits? I'm not sure about this. Any help is appreciated.",2018-02-07T14:35:44Z,9,2,2,5.323515453148673
50,16830,[tflite] fixed label_image resize bilinear problems,cla: yes,"1. Interpreter does not need delete anymore, so cannot use `std::unique_ptr<>`, otherwise there will be double free
2. ResizeBilinear need the `align_corners` parameter after https://github.com/tensorflow/tensorflow/commit/1a0b637df8d082301118dd0f85ec63704f862aeb",1,,1,2018-02-07T12:48:06Z,2018-02-14T00:53:37Z,CONTRIBUTOR,"1. Interpreter does not need delete anymore, so cannot use `std::unique_ptr<>`, otherwise there will be double free
2. ResizeBilinear need the `align_corners` parameter after https://github.com/tensorflow/tensorflow/commit/1a0b637df8d082301118dd0f85ec63704f862aeb",2018-02-14T00:53:37Z,7,2,1,3.8235154531486724
51,16829,tf.contrib.estimator.replicate_model_fn fails when a trainable variable doesn't have gradient,type:bug/performance,"tf.contrib.estimator.replicate_model_fn fails when the gradient of a trainable variable is None. The error messages are:

```
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py"", line 302, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py"", line 711, in _train_model
    features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py"", line 694, in _call_model_fn
    model_fn_results = self._model_fn(features=features, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/estimator/python/estimator/replicate_model_fn.py"", line 235, in replicated_model_fn
    local_ps_devices=ps_devices)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/estimator/python/estimator/replicate_model_fn.py"", line 558, in _get_loss_towers
    **optional_params)
  File ""model-60m-1280-2gpus-16-32-64-128-bn50000/net.py"", line 38, in model_fn
    train_op = optimizer.minimize(model.total_loss, global_step)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py"", line 353, in minimize
    name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/estimator/python/estimator/replicate_model_fn.py"", line 317, in apply_gradients
    with ops_lib.control_dependencies(_extract_tensors(grads_and_vars)):
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 4304, in control_dependencies
    return get_default_graph().control_dependencies(control_inputs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 4017, in control_dependencies
    c = self.as_graph_element(c)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 3035, in as_graph_element
    return self._as_graph_element_locked(obj, allow_tensor, allow_operation)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 3124, in _as_graph_element_locked
    types_str))
TypeError: Can not convert a NoneType into a Tensor or Operation.
```",1,,3,2018-02-07T11:27:24Z,2018-02-13T22:48:05Z,NONE,"tf.contrib.estimator.replicate_model_fn fails when the gradient of a trainable variable is None. The error messages are:

```
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py"", line 302, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py"", line 711, in _train_model
    features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py"", line 694, in _call_model_fn
    model_fn_results = self._model_fn(features=features, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/estimator/python/estimator/replicate_model_fn.py"", line 235, in replicated_model_fn
    local_ps_devices=ps_devices)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/estimator/python/estimator/replicate_model_fn.py"", line 558, in _get_loss_towers
    **optional_params)
  File ""model-60m-1280-2gpus-16-32-64-128-bn50000/net.py"", line 38, in model_fn
    train_op = optimizer.minimize(model.total_loss, global_step)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py"", line 353, in minimize
    name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/estimator/python/estimator/replicate_model_fn.py"", line 317, in apply_gradients
    with ops_lib.control_dependencies(_extract_tensors(grads_and_vars)):
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 4304, in control_dependencies
    return get_default_graph().control_dependencies(control_inputs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 4017, in control_dependencies
    c = self.as_graph_element(c)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 3035, in as_graph_element
    return self._as_graph_element_locked(obj, allow_tensor, allow_operation)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 3124, in _as_graph_element_locked
    types_str))
TypeError: Can not convert a NoneType into a Tensor or Operation.
```",2018-02-07T19:36:32Z,6,1,1,3.8235154531486724
52,16828,Code size with XLA AOT,,"I currently research about xla using AOT compilation and use Cifar10 as benchmark

After using AOT compilation, I got a binary file that size is 5.3MB, but the original tensorflow graph's size is about 4.2MB. Does it means a using AOT compilation can't promise always reduce code size efficiently?
",0,,5,2018-02-07T11:05:27Z,2018-02-09T19:36:39Z,NONE,"I currently research about xla using AOT compilation and use Cifar10 as benchmark

After using AOT compilation, I got a binary file that size is 5.3MB, but the original tensorflow graph's size is about 4.2MB. Does it means a using AOT compilation can't promise always reduce code size efficiently?
",2018-02-07T19:36:27Z,2,1,0,3.8235154531486724
53,16827,how to build and install cplusplus library and header file to /usr/local?,,how to build and install cplusplus library and header file to /usr/local?,0,,3,2018-02-07T10:25:25Z,2018-02-09T18:09:02Z,NONE,how to build and install cplusplus library and header file to /usr/local?,2018-02-07T19:36:22Z,2,1,0,2.8235154531486724
54,16816,Move some ndlstm functions to contrib,cla: yes,Moved `images_to_sequence` and `sequence_to_images` to contrib.layers since the ndlstm module might be removed in the future tensorflow versions. Addresses this issue: https://github.com/tensorflow/tensorflow/issues/16794,0,,8,2018-02-07T01:19:02Z,2018-02-08T20:16:38Z,CONTRIBUTOR,Moved `images_to_sequence` and `sequence_to_images` to contrib.layers since the ndlstm module might be removed in the future tensorflow versions. Addresses this issue: https://github.com/tensorflow/tensorflow/issues/16794,2018-02-07T01:46:48Z,1,2,0,6.323515453148673
55,16815,Update tensorboard dependency to 1.6.0+ and new name,cla: yes,"TensorBoard for versions 1.6.0+ will use the `tensorboard` name on PyPI rather than the previous `tensorflow-tensorboard` name.  Unfortunately PyPI/pip have no notion of package ""renames"" so it will look like an unrelated dependency, i.e. users doing an upgrade will wind up with both names installed, although the new `tensorboard` package will overwrite the old one functionally speaking.

Let me know if you think it's worth mentioning this in the release notes.

Note that the new name currently just has [TB 1.6.0-rc0 on PyPI](https://pypi.python.org/pypi/tensorboard/1.6.0rc0) but we should have the 1.6.0 final release out in a couple days, before TF 1.6.0 is published.

cc @jhseu 
",0,,4,2018-02-07T01:02:33Z,2018-02-09T21:15:32Z,CONTRIBUTOR,"TensorBoard for versions 1.6.0+ will use the `tensorboard` name on PyPI rather than the previous `tensorflow-tensorboard` name.  Unfortunately PyPI/pip have no notion of package ""renames"" so it will look like an unrelated dependency, i.e. users doing an upgrade will wind up with both names installed, although the new `tensorboard` package will overwrite the old one functionally speaking.

Let me know if you think it's worth mentioning this in the release notes.

Note that the new name currently just has [TB 1.6.0-rc0 on PyPI](https://pypi.python.org/pypi/tensorboard/1.6.0rc0) but we should have the 1.6.0 final release out in a couple days, before TF 1.6.0 is published.

cc @jhseu 
",2018-02-07T21:09:49Z,2,2,0,4.323515453148673
56,16812,Improve TensorFlow Lite description,cla: yes,"I just started working with the new TensorFlow Lite interface and noticed a few errors in the [description](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/lite).
- First of all, there is a small typo / copy and paste error that is already fixed in this pr.
- Furthermore, the new file extension isn't described consistently. Sometimes it's _.lite_ and sometimes it's _.tflite_. This inconsistency may be improved at other places as well.
- Finally, the given [link](https://www.tensorflow.org/mobile/android_build) that shall describe the Android integration isn't up-to-date. Instead of the new `compile 'org.tensorflow:tensorflow-lite:+'` resource, the old one (`compile 'org.tensorflow:tensorflow-android:+'`) is given.",1,,1,2018-02-06T23:56:43Z,2018-02-08T20:16:53Z,CONTRIBUTOR,"I just started working with the new TensorFlow Lite interface and noticed a few errors in the [description](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/lite).
- First of all, there is a small typo / copy and paste error that is already fixed in this pr.
- Furthermore, the new file extension isn't described consistently. Sometimes it's _.lite_ and sometimes it's _.tflite_. This inconsistency may be improved at other places as well.
- Finally, the given [link](https://www.tensorflow.org/mobile/android_build) that shall describe the Android integration isn't up-to-date. Instead of the new `compile 'org.tensorflow:tensorflow-lite:+'` resource, the old one (`compile 'org.tensorflow:tensorflow-android:+'`) is given.",2018-02-08T01:01:29Z,2,2,0,3.818928988903801
57,16810,reshuffle_each_iteration args now default to True,cla: yes,"The [documentation for the tf.Dataset.data.shuffle function](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#shuffle) states the following:

> + **reshuffle_each_iteration**: (Optional.) A boolean, which if true indicates that the dataset should be pseudorandomly reshuffled each time it is iterated over. (Defaults to True.)

However, the default value in the function is None:

    def shuffle(self, buffer_size, seed=None, reshuffle_each_iteration=None):

The function calls the ShuffleDataset class, whose `__init__` function also sets the same argument to None by default, and uses the following logic to set the default value of the argument to True:

    if reshuffle_each_iteration is None:
      self._reshuffle_each_iteration = True
    else:
      self._reshuffle_each_iteration = reshuffle_each_iteration

This commit sets the argument to True by default in both the function and the class, making the above code block redundant and replacing it with only `self._reshuffle_each_iteration = reshuffle_each_iteration`.
",1,,2,2018-02-06T23:05:27Z,2018-02-07T21:34:04Z,NONE,"The [documentation for the tf.Dataset.data.shuffle function](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#shuffle) states the following:

> + **reshuffle_each_iteration**: (Optional.) A boolean, which if true indicates that the dataset should be pseudorandomly reshuffled each time it is iterated over. (Defaults to True.)

However, the default value in the function is None:

    def shuffle(self, buffer_size, seed=None, reshuffle_each_iteration=None):

The function calls the ShuffleDataset class, whose `__init__` function also sets the same argument to None by default, and uses the following logic to set the default value of the argument to True:

    if reshuffle_each_iteration is None:
      self._reshuffle_each_iteration = True
    else:
      self._reshuffle_each_iteration = reshuffle_each_iteration

This commit sets the argument to True by default in both the function and the class, making the above code block redundant and replacing it with only `self._reshuffle_each_iteration = reshuffle_each_iteration`.
",2018-02-07T21:34:04Z,1,1,0,3.318928988903801
58,16808,Fix error message in record_reader,cla: yes,Corrected error message logged if unsupported compression type.,1,,3,2018-02-06T20:40:25Z,2018-02-08T18:46:13Z,CONTRIBUTOR,Corrected error message logged if unsupported compression type.,2018-02-07T21:32:36Z,2,2,0,4.818928988903801
59,16807,TF 1.3 unable to create Session in the first time,stat:awaiting response,"I have install TF 1.3 GPU using anaconda. It is failed to create session as run TF in script file.

As run TF interactively or using spyder, same error messages were shown and it fail to create session in the first time. However, it able to create session if run ""sess = tf.Session()"" again. The TF will run either by input line by line script or using ""exec(compile(open(filename, ""rb"").read(), filename, 'exec'))"" 

The error message as create session:

$ source activiate tf13py36
(tf13py36)$ python
>>> import tensorflow as tf
>>> sess = tf.Session()
2018-02-05 17:44:25.343373: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-05 17:44:25.343398: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-05 17:44:25.540883: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:893] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-02-05 17:44:25.541399: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties:
name: GeForce GTX 980 Ti
major: 5 minor: 2 memoryClockRate (GHz) 1.228
pciBusID 0000:03:00.0
Total memory: 5.94GiB
Free memory: 5.83GiB
2018-02-05 17:44:25.600695: W tensorflow/stream_executor/cuda/cuda_driver.cc:523] A non-primary context 0x225e110 exists before initializing the StreamExecutor. We haven't verified StreamExecutor works with that.
2018-02-05 17:44:25.600988: E tensorflow/core/common_runtime/direct_session.cc:171] Internal: failed initializing StreamExecutor for CUDA device ordinal 1: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_INVALID_DEVICE
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/tchen/anaconda3/envs/tf13py36/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1486, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File ""/home/tchen/anaconda3/envs/tf13py36/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 621, in __init__
    self._session = tf_session.TF_NewDeprecatedSession(opts, status)
  File ""/home/tchen/anaconda3/envs/tf13py36/lib/python3.6/contextlib.py"", line 89, in __exit__
    next(self.gen)
  File ""/home/tchen/anaconda3/envs/tf13py36/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py"", line 466, in raise_exception_on_not_ok_status
    pywrap_tensorflow.TF_GetCode(status))
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
>>> sess = tf.Session()
2018-02-05 17:45:19.371509: W tensorflow/stream_executor/cuda/cuda_driver.cc:523] A non-primary context 0x225e110 exists before initializing the StreamExecutor. We haven't verified StreamExecutor works with that.
2018-02-05 17:45:19.371738: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 980 Ti, pci bus id: 0000:03:00.0)
2018-02-05 17:45:19.430913: W tensorflow/stream_executor/cuda/cuda_driver.cc:523] A non-primary context 0x225e110 exists before initializing the StreamExecutor. We haven't verified StreamExecutor works with that.
>>> x1 = tf.constant([1,2,3,4])
>>> x2 = tf.constant([5,6,7,8])
>>> result = tf.multiply(x1, x2)
>>> print(sess.run(result))
[ 5 12 21 32]

SYSTEM Infomation
ubuntu16.04
cuda V8.061
cudnn 6021
gtx1060
tensorflow1.3.0
python3.6.1
memory 30G ,used 5.5GB

$ nvidia-smi
Tue Feb  6 10:21:38 2018
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 384.111                Driver Version: 384.111                   |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX 980 Ti  Off  | 00000000:03:00.0 Off |                  N/A |
| 22%   31C    P8    19W / 250W |    110MiB /  6078MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   1  Quadro 600          Off  | 00000000:04:00.0  On |                  N/A |
| 36%   53C    P0    N/A /  N/A |    524MiB /   959MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|    0     19564      C   python                                        98MiB |
+-----------------------------------------------------------------------------+

I am grateful to anyone for helping me
Thank you very much!",0,,2,2018-02-06T19:27:01Z,2018-02-12T15:46:09Z,NONE,"I have install TF 1.3 GPU using anaconda. It is failed to create session as run TF in script file.

As run TF interactively or using spyder, same error messages were shown and it fail to create session in the first time. However, it able to create session if run ""sess = tf.Session()"" again. The TF will run either by input line by line script or using ""exec(compile(open(filename, ""rb"").read(), filename, 'exec'))"" 

The error message as create session:

$ source activiate tf13py36
(tf13py36)$ python
>>> import tensorflow as tf
>>> sess = tf.Session()
2018-02-05 17:44:25.343373: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-05 17:44:25.343398: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-05 17:44:25.540883: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:893] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-02-05 17:44:25.541399: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties:
name: GeForce GTX 980 Ti
major: 5 minor: 2 memoryClockRate (GHz) 1.228
pciBusID 0000:03:00.0
Total memory: 5.94GiB
Free memory: 5.83GiB
2018-02-05 17:44:25.600695: W tensorflow/stream_executor/cuda/cuda_driver.cc:523] A non-primary context 0x225e110 exists before initializing the StreamExecutor. We haven't verified StreamExecutor works with that.
2018-02-05 17:44:25.600988: E tensorflow/core/common_runtime/direct_session.cc:171] Internal: failed initializing StreamExecutor for CUDA device ordinal 1: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_INVALID_DEVICE
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/tchen/anaconda3/envs/tf13py36/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1486, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File ""/home/tchen/anaconda3/envs/tf13py36/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 621, in __init__
    self._session = tf_session.TF_NewDeprecatedSession(opts, status)
  File ""/home/tchen/anaconda3/envs/tf13py36/lib/python3.6/contextlib.py"", line 89, in __exit__
    next(self.gen)
  File ""/home/tchen/anaconda3/envs/tf13py36/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py"", line 466, in raise_exception_on_not_ok_status
    pywrap_tensorflow.TF_GetCode(status))
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
>>> sess = tf.Session()
2018-02-05 17:45:19.371509: W tensorflow/stream_executor/cuda/cuda_driver.cc:523] A non-primary context 0x225e110 exists before initializing the StreamExecutor. We haven't verified StreamExecutor works with that.
2018-02-05 17:45:19.371738: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 980 Ti, pci bus id: 0000:03:00.0)
2018-02-05 17:45:19.430913: W tensorflow/stream_executor/cuda/cuda_driver.cc:523] A non-primary context 0x225e110 exists before initializing the StreamExecutor. We haven't verified StreamExecutor works with that.
>>> x1 = tf.constant([1,2,3,4])
>>> x2 = tf.constant([5,6,7,8])
>>> result = tf.multiply(x1, x2)
>>> print(sess.run(result))
[ 5 12 21 32]

SYSTEM Infomation
ubuntu16.04
cuda V8.061
cudnn 6021
gtx1060
tensorflow1.3.0
python3.6.1
memory 30G ,used 5.5GB

$ nvidia-smi
Tue Feb  6 10:21:38 2018
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 384.111                Driver Version: 384.111                   |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX 980 Ti  Off  | 00000000:03:00.0 Off |                  N/A |
| 22%   31C    P8    19W / 250W |    110MiB /  6078MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   1  Quadro 600          Off  | 00000000:04:00.0  On |                  N/A |
| 36%   53C    P0    N/A /  N/A |    524MiB /   959MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|    0     19564      C   python                                        98MiB |
+-----------------------------------------------------------------------------+

I am grateful to anyone for helping me
Thank you very much!",2018-02-07T07:51:56Z,6,1,1,2.318928988903801
60,16806,Update CONTRIBUTING.md,cla: yes,Edited a few grammar issues.,1,,3,2018-02-06T17:50:47Z,2018-02-08T18:47:35Z,CONTRIBUTOR,Edited a few grammar issues.,2018-02-06T18:04:54Z,2,2,0,4.818928988903801
61,16802,[tflite] make calling NNAPI work again (resend),cla: yes,"for the previous one (https://github.com/tensorflow/tensorflow/pull/16256) somehow reverted/overwritten

calling PrepareOpsAndTensors() before using NN API looks
  1. unnecessary
  2. decrease next_execution_plan_index_to_prepare_ so that
     the logic check in the next line
     `next_execution_plan_index_to_prepare_ == execution_plan_.size`
     will fail",1,,2,2018-02-06T15:26:45Z,2018-02-09T23:50:45Z,CONTRIBUTOR,"for the previous one (https://github.com/tensorflow/tensorflow/pull/16256) somehow reverted/overwritten

calling PrepareOpsAndTensors() before using NN API looks
  1. unnecessary
  2. decrease next_execution_plan_index_to_prepare_ so that
     the logic check in the next line
     `next_execution_plan_index_to_prepare_ == execution_plan_.size`
     will fail",2018-02-09T01:42:03Z,3,2,1,4.318928988903801
62,16800,How to compile and use Opencv in tensorflow c++?,,"I want to implement a model inference in tensorflow c++ and have saved the model as .pb file. But I can't use opencv to process the image. I wonder how can I add the opencv lib to the bazel project? Are there any tricks to solve the problem? Thanks!

This is the code of the bazel BUILD file:

    package(
        default_visibility = [""//tensorflow:internal""],
    )

    licenses([""notice""])  # Apache 2.0

    exports_files([""LICENSE""])

    load(""//tensorflow:tensorflow.bzl"", ""tf_cc_binary"")

    tf_cc_binary(
        name = ""mask_rcnn"",
        srcs = [
            ""main.cc"",
        ],
        #prefix = ""flower"",
        linkopts = select({
            ""//tensorflow:android"": [
                ""-pie"",
                ""-landroid"",
                ""-ljnigraphics"",
                ""-llog"",
                ""-lm"",
                ""-z defs"",
                ""-s"",
                ""-Wl,--exclude-libs,ALL"",
            ],
            ""//conditions:default"": [""-lm""],
        }),
        deps = select({
            ""//tensorflow:android"": [
                # cc:cc_ops is used to include image ops (for label_image)
                # Jpg, gif, and png related code won't be included
                ""//tensorflow/cc:cc_ops"",
                ""//tensorflow/core:android_tensorflow_lib"",
                # cc:android_tensorflow_image_op is for including jpeg/gif/png
                # decoder to enable real-image evaluation on Android
                ""//tensorflow/core/kernels:android_tensorflow_image_op"",
            ],
            ""//conditions:default"": [
                ""//tensorflow/cc:cc_ops"",
                ""//tensorflow/core:core_cpu"",
                ""//tensorflow/core:framework"",
                ""//tensorflow/core:framework_internal"",
                ""//tensorflow/core:lib"",
                ""//tensorflow/core:protos_all_cc"",
                ""//tensorflow/core:tensorflow"",
            ],
        }),
    )

    filegroup(
        name = ""all_files"",
        srcs = glob(
            [""**/*""],
            exclude = [
                ""**/METADATA"",
                ""**/OWNERS"",
                ""bin/**"",
                ""gen/**"",
            ],
        ),
        visibility = [""//tensorflow:__subpackages__""],
    )


",0,,1,2018-02-06T12:33:00Z,2018-02-06T17:00:33Z,NONE,"I want to implement a model inference in tensorflow c++ and have saved the model as .pb file. But I can't use opencv to process the image. I wonder how can I add the opencv lib to the bazel project? Are there any tricks to solve the problem? Thanks!

This is the code of the bazel BUILD file:

    package(
        default_visibility = [""//tensorflow:internal""],
    )

    licenses([""notice""])  # Apache 2.0

    exports_files([""LICENSE""])

    load(""//tensorflow:tensorflow.bzl"", ""tf_cc_binary"")

    tf_cc_binary(
        name = ""mask_rcnn"",
        srcs = [
            ""main.cc"",
        ],
        #prefix = ""flower"",
        linkopts = select({
            ""//tensorflow:android"": [
                ""-pie"",
                ""-landroid"",
                ""-ljnigraphics"",
                ""-llog"",
                ""-lm"",
                ""-z defs"",
                ""-s"",
                ""-Wl,--exclude-libs,ALL"",
            ],
            ""//conditions:default"": [""-lm""],
        }),
        deps = select({
            ""//tensorflow:android"": [
                # cc:cc_ops is used to include image ops (for label_image)
                # Jpg, gif, and png related code won't be included
                ""//tensorflow/cc:cc_ops"",
                ""//tensorflow/core:android_tensorflow_lib"",
                # cc:android_tensorflow_image_op is for including jpeg/gif/png
                # decoder to enable real-image evaluation on Android
                ""//tensorflow/core/kernels:android_tensorflow_image_op"",
            ],
            ""//conditions:default"": [
                ""//tensorflow/cc:cc_ops"",
                ""//tensorflow/core:core_cpu"",
                ""//tensorflow/core:framework"",
                ""//tensorflow/core:framework_internal"",
                ""//tensorflow/core:lib"",
                ""//tensorflow/core:protos_all_cc"",
                ""//tensorflow/core:tensorflow"",
            ],
        }),
    )

    filegroup(
        name = ""all_files"",
        srcs = glob(
            [""**/*""],
            exclude = [
                ""**/METADATA"",
                ""**/OWNERS"",
                ""bin/**"",
                ""gen/**"",
            ],
        ),
        visibility = [""//tensorflow:__subpackages__""],
    )


",2018-02-06T17:00:33Z,0,1,0,1.818928988903801
63,16799,Debug prompts use_default_colors() returned ERR,,"I was running a example code from [tensorpack](http://tensorpack.readthedocs.io/en/latest/index.html) on Pycharm, which runs properly, then I changed the session to 
`sess = tf_debug.LocalCLIDebugWrapperSession(sess) ` in order to debug
as suggested by official example https://www.tensorflow.org/programmers_guide/debugger, but then I got:
```
Traceback (most recent call last):
  File ""/home/user/.pycharm_helpers/pydev/pydev_run_in_console.py"", line 53, in run_file
    pydev_imports.execfile(file, globals, locals)  # execute the script
  File ""/home/user/prj/shufflenet_v1/shufflenet.py"", line 231, in <module>
    launch_train_with_config(config, SyncMultiGPUTrainerParameterServer(nr_tower))
  File ""/home/user/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorpack/train/interface.py"", line 96, in launch_train_with_config
    config.steps_per_epoch, config.starting_epoch, config.max_epoch)
  File ""/home/user/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorpack/train/base.py"", line 288, in train
    self.main_loop(steps_per_epoch, starting_epoch, max_epoch)
  File ""/home/user/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorpack/utils/argtools.py"", line 171, in wrapper
    return func(*args, **kwargs)
  File ""/home/user/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorpack/train/base.py"", line 239, in main_loop
    self.loop.update_global_step()
  File ""/home/user/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorpack/train/base.py"", line 59, in update_global_step
    self._global_step = get_global_step_value()
  File ""/home/user/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorpack/tfutils/common.py"", line 77, in get_global_step_value
    get_global_step_var())
  File ""/home/user/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/training/training_util.py"", line 67, in global_step
    return int(sess.run(global_step_tensor))
  File ""/home/user/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/debug/wrappers/framework.py"", line 455, in run
    is_callable_runner=bool(callable_runner)))
  File ""/home/user/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/debug/wrappers/local_cli_wrapper.py"", line 253, in on_run_start
    self._prep_cli_for_run_start()
  File ""/home/user/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/debug/wrappers/local_cli_wrapper.py"", line 275, in _prep_cli_for_run_start
    self._run_cli = ui_factory.get_ui(self._ui_type)
  File ""/home/user/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/debug/cli/ui_factory.py"", line 56, in get_ui
    return curses_ui.CursesUI(on_ui_exit=on_ui_exit)
  File ""/home/user/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/debug/cli/curses_ui.py"", line 285, in __init__
    self._screen_init()
  File ""/home/user/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/debug/cli/curses_ui.py"", line 400, in _screen_init
    self._screen_color_init()
  File ""/home/user/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/debug/cli/curses_ui.py"", line 405, in _screen_color_init
    curses.use_default_colors()
error: use_default_colors() returned ERR
```
which I have no clue for the reason, can someone help?",0,,3,2018-02-06T12:23:08Z,2018-02-09T18:07:22Z,NONE,"I was running a example code from [tensorpack](http://tensorpack.readthedocs.io/en/latest/index.html) on Pycharm, which runs properly, then I changed the session to 
`sess = tf_debug.LocalCLIDebugWrapperSession(sess) ` in order to debug
as suggested by official example https://www.tensorflow.org/programmers_guide/debugger, but then I got:
```
Traceback (most recent call last):
  File ""/home/user/.pycharm_helpers/pydev/pydev_run_in_console.py"", line 53, in run_file
    pydev_imports.execfile(file, globals, locals)  # execute the script
  File ""/home/user/prj/shufflenet_v1/shufflenet.py"", line 231, in <module>
    launch_train_with_config(config, SyncMultiGPUTrainerParameterServer(nr_tower))
  File ""/home/user/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorpack/train/interface.py"", line 96, in launch_train_with_config
    config.steps_per_epoch, config.starting_epoch, config.max_epoch)
  File ""/home/user/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorpack/train/base.py"", line 288, in train
    self.main_loop(steps_per_epoch, starting_epoch, max_epoch)
  File ""/home/user/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorpack/utils/argtools.py"", line 171, in wrapper
    return func(*args, **kwargs)
  File ""/home/user/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorpack/train/base.py"", line 239, in main_loop
    self.loop.update_global_step()
  File ""/home/user/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorpack/train/base.py"", line 59, in update_global_step
    self._global_step = get_global_step_value()
  File ""/home/user/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorpack/tfutils/common.py"", line 77, in get_global_step_value
    get_global_step_var())
  File ""/home/user/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/training/training_util.py"", line 67, in global_step
    return int(sess.run(global_step_tensor))
  File ""/home/user/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/debug/wrappers/framework.py"", line 455, in run
    is_callable_runner=bool(callable_runner)))
  File ""/home/user/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/debug/wrappers/local_cli_wrapper.py"", line 253, in on_run_start
    self._prep_cli_for_run_start()
  File ""/home/user/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/debug/wrappers/local_cli_wrapper.py"", line 275, in _prep_cli_for_run_start
    self._run_cli = ui_factory.get_ui(self._ui_type)
  File ""/home/user/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/debug/cli/ui_factory.py"", line 56, in get_ui
    return curses_ui.CursesUI(on_ui_exit=on_ui_exit)
  File ""/home/user/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/debug/cli/curses_ui.py"", line 285, in __init__
    self._screen_init()
  File ""/home/user/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/debug/cli/curses_ui.py"", line 400, in _screen_init
    self._screen_color_init()
  File ""/home/user/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/debug/cli/curses_ui.py"", line 405, in _screen_color_init
    curses.use_default_colors()
error: use_default_colors() returned ERR
```
which I have no clue for the reason, can someone help?",2018-02-07T01:28:35Z,3,1,1,2.818928988903801
64,16797,Fixes variable name,cla: yes,,1,,1,2018-02-06T09:12:10Z,2018-02-08T18:48:19Z,CONTRIBUTOR,,2018-02-07T09:12:44Z,2,2,0,3.818928988903801
65,16796,Slim batch image prediciton?,,"I just finished training a model by following train_image_classifier.py

CUDA_VISIBLE_DEVICE=0,1 python train_image_classifier.py --train_dir=train_logs --dataset_dir=../train --num_samples=15500 --num_classes=4 --labels_to_names_path=../labels.txt --model_name=inception_resnet_v2 --checkpoint_path=../checkpoints/inception_resnet_v2_2016_08_30.ckpt --checkpoint_exclude_scopes=InceptionResnetV2/Logits,InceptionResnetV2/AuxLogits --num_clones=2 --num_preprocessing_threads=8 --max_number_of_steps=100000 --batch_size=32 --learning_rate=0.0001 --learning_rate_decay_type=fixed --save_interval_secs=60 --save_summaries_secs=60 --log_every_n_steps=10 --optimizer=rmsprop --weight_decay=0.00004

and evaluate the model by using eval_image_classifier.py. 

CUDA_VISIBLE_DEVICE=0,1 python eval_image_classifier.py --checkpoint_path=train_logs --eval_dir=eval_logs --dataset_dir=../val --num_samples=797 --num_classes=4 --model_name=inception_resnet_v2

Everything seems great. But, how to test image classifier unfortunately is not provided, and I cannot really find an good example of how to use this model to predict testing images. Is there a good example that I can use my trained model to predict multiple images in a batch way? Thank you for helping. 
",0,,1,2018-02-06T08:55:19Z,2018-02-06T16:58:18Z,NONE,"I just finished training a model by following train_image_classifier.py

CUDA_VISIBLE_DEVICE=0,1 python train_image_classifier.py --train_dir=train_logs --dataset_dir=../train --num_samples=15500 --num_classes=4 --labels_to_names_path=../labels.txt --model_name=inception_resnet_v2 --checkpoint_path=../checkpoints/inception_resnet_v2_2016_08_30.ckpt --checkpoint_exclude_scopes=InceptionResnetV2/Logits,InceptionResnetV2/AuxLogits --num_clones=2 --num_preprocessing_threads=8 --max_number_of_steps=100000 --batch_size=32 --learning_rate=0.0001 --learning_rate_decay_type=fixed --save_interval_secs=60 --save_summaries_secs=60 --log_every_n_steps=10 --optimizer=rmsprop --weight_decay=0.00004

and evaluate the model by using eval_image_classifier.py. 

CUDA_VISIBLE_DEVICE=0,1 python eval_image_classifier.py --checkpoint_path=train_logs --eval_dir=eval_logs --dataset_dir=../val --num_samples=797 --num_classes=4 --model_name=inception_resnet_v2

Everything seems great. But, how to test image classifier unfortunately is not provided, and I cannot really find an good example of how to use this model to predict testing images. Is there a good example that I can use my trained model to predict multiple images in a batch way? Thank you for helping. 
",2018-02-06T16:58:18Z,0,1,0,1.818928988903801
66,16794,Feature Request: Modification of lstm2d.horizontal_lstm implementation,,"I noticed something in the documentation of `lstm2d.horizontal_lstm`. It says:

> Run an LSTM bidirectionally over all the rows of each image.

Kinda looks like a bidirectional_lstm to me. I propose to change the implentation such that it will use bidirectional_lstm within the function replacing this:

```
with variable_scope.variable_scope(""lr""):
  hidden_sequence_lr = lstm1d.ndlstm_base(sequence, num_filters_out // 2)
with variable_scope.variable_scope(""rl""):
  hidden_sequence_rl = (lstm1d.ndlstm_base(
      sequence, num_filters_out - num_filters_out // 2, reverse=1))
output_sequence = array_ops.concat([hidden_sequence_lr, hidden_sequence_rl],
                                       2)
```

With this:

```
cell_fw = rnn_cell.BasicLSTMCell(num_filters_out // 2)
cell_bw = rnn_cell.BasicLSTMCell(num_filters_out // 2)
output_sequence = rnn.bidirectional_dynamic_rnn(cell_fw, cell_bw, sequence, time_major=True, 
                                                                                 dtype=sequence.dtype)
```
  ",1,,5,2018-02-06T08:28:55Z,2018-02-06T16:59:19Z,CONTRIBUTOR,"I noticed something in the documentation of `lstm2d.horizontal_lstm`. It says:

> Run an LSTM bidirectionally over all the rows of each image.

Kinda looks like a bidirectional_lstm to me. I propose to change the implentation such that it will use bidirectional_lstm within the function replacing this:

```
with variable_scope.variable_scope(""lr""):
  hidden_sequence_lr = lstm1d.ndlstm_base(sequence, num_filters_out // 2)
with variable_scope.variable_scope(""rl""):
  hidden_sequence_rl = (lstm1d.ndlstm_base(
      sequence, num_filters_out - num_filters_out // 2, reverse=1))
output_sequence = array_ops.concat([hidden_sequence_lr, hidden_sequence_rl],
                                       2)
```

With this:

```
cell_fw = rnn_cell.BasicLSTMCell(num_filters_out // 2)
cell_bw = rnn_cell.BasicLSTMCell(num_filters_out // 2)
output_sequence = rnn.bidirectional_dynamic_rnn(cell_fw, cell_bw, sequence, time_major=True, 
                                                                                 dtype=sequence.dtype)
```
  ",2018-02-06T16:59:19Z,0,2,0,5.818928988903801
67,16791,Tensorflow 1.5.0 Import Error on CUDA 9.0,"stat:awaiting tensorflower,type:docs","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No, I used the stock version.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **TensorFlow installed from (source or binary)**: pip binary for windows with GPU support
- **TensorFlow version (use command below)**: 1.5.0
- **Python version**: Python 3.6.1 from Anaconda
- **CUDA/cuDNN version**: 9.0/7.0.5
- **GPU model and memory**:
- **Exact command to reproduce**: import tensorflow as tf

### Describe the problem
Installed CUDA/CUDNN 8.0/6, 9.0/7, 9.1/7
Used pip to install tensorflow_gpu 1.5.0

The installation process finished normally.

However, when import the module by ""import tensorflow as tf"", error messages raises and it says ImportError: Could not find 'cudart64_90.dll'.

I double checked the CUDA_PATH and PATH environmental variables to make sure that CUDA/CUDNN 9.0/7 are being used. Later I removed the 8.0/6 and 9.1/7, and the problem still exists.

### Source code / logs
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\user\Anaconda3\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""C:\Users\user\Anaconda3\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\user\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 30, in <module>
    self_check.preload_check()
  File ""C:\Users\user\Anaconda3\lib\site-packages\tensorflow\python\platform\self_check.py"", line 82, in preload_check
    % (build_info.cudart_dll_name, build_info.cuda_version_number))
ImportError: Could not find 'cudart64_90.dll'. TensorFlow requires that this DLL be installed in a directory that is named in your %PATH% environment variable. Download and install CUDA 9.0 from this URL: https://developer.nvidia.com/cuda-toolkit
",0,,3,2018-02-06T06:25:59Z,2018-02-09T21:23:56Z,NONE,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No, I used the stock version.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **TensorFlow installed from (source or binary)**: pip binary for windows with GPU support
- **TensorFlow version (use command below)**: 1.5.0
- **Python version**: Python 3.6.1 from Anaconda
- **CUDA/cuDNN version**: 9.0/7.0.5
- **GPU model and memory**:
- **Exact command to reproduce**: import tensorflow as tf

### Describe the problem
Installed CUDA/CUDNN 8.0/6, 9.0/7, 9.1/7
Used pip to install tensorflow_gpu 1.5.0

The installation process finished normally.

However, when import the module by ""import tensorflow as tf"", error messages raises and it says ImportError: Could not find 'cudart64_90.dll'.

I double checked the CUDA_PATH and PATH environmental variables to make sure that CUDA/CUDNN 9.0/7 are being used. Later I removed the 8.0/6 and 9.1/7, and the problem still exists.

### Source code / logs
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\user\Anaconda3\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""C:\Users\user\Anaconda3\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\user\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 30, in <module>
    self_check.preload_check()
  File ""C:\Users\user\Anaconda3\lib\site-packages\tensorflow\python\platform\self_check.py"", line 82, in preload_check
    % (build_info.cudart_dll_name, build_info.cuda_version_number))
ImportError: Could not find 'cudart64_90.dll'. TensorFlow requires that this DLL be installed in a directory that is named in your %PATH% environment variable. Download and install CUDA 9.0 from this URL: https://developer.nvidia.com/cuda-toolkit
",2018-02-09T18:04:26Z,3,1,1,2.818928988903801
68,16790,Save a numpy params as tensorflow model!,,"Hi, I have trained a model, and i save the params in a numpy file in dict type.
Now i construct the network manually, and set the params as my trained model.
i want to save the network as tensorflow model, but the session is empty, 
so how could i save the model?",0,,1,2018-02-06T03:21:16Z,2018-02-06T05:15:59Z,NONE,"Hi, I have trained a model, and i save the params in a numpy file in dict type.
Now i construct the network manually, and set the params as my trained model.
i want to save the network as tensorflow model, but the session is empty, 
so how could i save the model?",2018-02-06T05:15:59Z,0,1,0,1.818928988903801
69,16788,Fix for GLSTMCell implementation,cla: yes,"* Fix issue described here: https://github.com/tensorflow/tensorflow/issues/16703
* Make it work correctly for input sizes!=num_units",1,,1,2018-02-06T00:23:01Z,2018-02-08T00:39:45Z,CONTRIBUTOR,"* Fix issue described here: https://github.com/tensorflow/tensorflow/issues/16703
* Make it work correctly for input sizes!=num_units",2018-02-08T00:39:45Z,2,2,0,3.818928988903801
70,16787,Bump the rtol in hmc_test,cla: yes,"This test is flaky due to the rtol:
https://source.cloud.google.com/results/invocations/e381cb7e-272e-41ac-b0da-931f6cf293bb/log",1,,2,2018-02-05T23:29:53Z,2018-02-06T00:05:26Z,OWNER,"This test is flaky due to the rtol:
https://source.cloud.google.com/results/invocations/e381cb7e-272e-41ac-b0da-931f6cf293bb/log",2018-02-06T21:36:13Z,1,4,0,6.3146579804441245
71,16782,Module missing,,"
",0,,1,2018-02-05T21:08:05Z,2018-02-06T05:15:41Z,NONE,"
",2018-02-06T05:15:41Z,1,1,0,1.814657980444125
72,16780,Trying to allocate large output tensor in custom op leads to multiple evaluation of Compute method,,"------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS X 10.13.2
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.5.0
- **Python version**: 2.7.14
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: CPU-version of Tensorflow
- **GPU model and memory**:
- **Exact command to reproduce**: make


### Describe the problem
I wrote a custom op. During debugging i've noticed then Compute method from my op fired multiple times during single op.eval() call.
My quest lead me to row:
```c++
OP_REQUIRES_OK(ctx, ctx->allocate_output(0, TensorShape({output_size}), &values_tensor));
```
If output_size is small (e.g. 1000), my op works well and executes one time.
If output_size is big (e.g. 15000000), my op executes multiple times.

### Source code / logs
There are 3 files in archive:
- custom op source code
- demo eval script
- makefile config to build op and run eval


[issue.zip](https://github.com/tensorflow/tensorflow/files/1696503/issue.zip)
",0,,4,2018-02-05T19:37:37Z,2018-02-09T23:03:52Z,NONE,"------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS X 10.13.2
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.5.0
- **Python version**: 2.7.14
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: CPU-version of Tensorflow
- **GPU model and memory**:
- **Exact command to reproduce**: make


### Describe the problem
I wrote a custom op. During debugging i've noticed then Compute method from my op fired multiple times during single op.eval() call.
My quest lead me to row:
```c++
OP_REQUIRES_OK(ctx, ctx->allocate_output(0, TensorShape({output_size}), &values_tensor));
```
If output_size is small (e.g. 1000), my op works well and executes one time.
If output_size is big (e.g. 15000000), my op executes multiple times.

### Source code / logs
There are 3 files in archive:
- custom op source code
- demo eval script
- makefile config to build op and run eval


[issue.zip](https://github.com/tensorflow/tensorflow/files/1696503/issue.zip)
",2018-02-06T05:13:53Z,4,1,1,3.314657980444125
73,16773,TensorBoard: Fit domain to data in 1.5 under Windows cuts off max values,stat:awaiting tensorflower,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Using TensorBoard in custom U-Net implementation
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 7
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: 1.5
- **Python version**: 3.5
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: CUDA 9.0, CuDNN 7
- **GPU model and memory**: GeForce GTX 1050 Ti (4 GB), 32 GB RAM
- **Exact command to reproduce**: tensorboard with tf.summary.scalar

### Describe the problem
After updating from 1.4 to 1.5 I have the problems that the y-scale for the scalar graphs in TensorBoard seems to be misscalculated. The maximum values are not included in the shown graph, they are cut off.

![image](https://user-images.githubusercontent.com/31246640/35806246-1efb92ee-0a7f-11e8-8441-17ebe97473b7.png)

",0,,4,2018-02-05T13:17:13Z,2018-02-07T19:38:54Z,NONE,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Using TensorBoard in custom U-Net implementation
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 7
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: 1.5
- **Python version**: 3.5
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: CUDA 9.0, CuDNN 7
- **GPU model and memory**: GeForce GTX 1050 Ti (4 GB), 32 GB RAM
- **Exact command to reproduce**: tensorboard with tf.summary.scalar

### Describe the problem
After updating from 1.4 to 1.5 I have the problems that the y-scale for the scalar graphs in TensorBoard seems to be misscalculated. The maximum values are not included in the shown graph, they are cut off.

![image](https://user-images.githubusercontent.com/31246640/35806246-1efb92ee-0a7f-11e8-8441-17ebe97473b7.png)

",2018-02-06T07:30:12Z,2,1,0,3.314657980444125
74,16768,OOM when allocating tensor with shape,"stat:awaiting response,type:support","I'm using tensorflow 1.3, tested on a linux machine with 2 NVIDIA Tesla K80 cards,
however, I keep getting OOM error on GPU, but it does not happen when using cpu for training:

-------------------------------------
---log below
-------------------------------------

Exception happened during training, message: OOM when allocating tensor with shape[1792,4096]
	 [[Node: projectx/trainig_gpu_0/gradients/projectx/trainig_gpu_0/decoder/projectxDecoderCell_1/lstm_0/lstm_0/lstm_cell/MatMul_36_grad/MatMul_1 = MatMul[T=DT_FLOAT, transpose_a=true, transpose_b=false, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](projectx/trainig_gpu_0/decoder/projectxDecoderCell_1/lstm_0/lstm_0/lstm_cell/concat_36, projectx/trainig_gpu_0/gradients/projectx/trainig_gpu_0/decoder/projectxDecoderCell_1/lstm_0/lstm_0/lstm_cell/BiasAdd_36_grad/tuple/control_dependency)]]
	 [[Node: projectx/trainig_gpu_0/gradients/concat/_2929 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_646323_projectx/trainig_gpu_0/gradients/concat"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]

Caused by op 'projectx/trainig_gpu_0/gradients/projectx/trainig_gpu_0/decoder/projectxDecoderCell_1/lstm_0/lstm_0/lstm_cell/MatMul_36_grad/MatMul_1', defined at:
  File ""sync_train.py"", line 383, in <module>
    main()
  File ""sync_train.py"", line 380, in main
    train(config)
  File ""sync_train.py"", line 64, in train
    train_model = projectx.projectx(n_gpu, config, is_training=True, reuse=False)
  File ""/kaldi/exp/tacotron/exp_2/projectx.py"", line 291, in __init__
    grads_and_vars = self.optimizer.compute_gradients(loss)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/optimizer.py"", line 414, in compute_gradients
    colocate_gradients_with_ops=colocate_gradients_with_ops)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gradients_impl.py"", line 581, in gradients
    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gradients_impl.py"", line 353, in _MaybeCompile
    return grad_fn()  # Exit early
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gradients_impl.py"", line 581, in <lambda>
    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_grad.py"", line 922, in _MatMulGrad
    grad_b = math_ops.matmul(a, grad, transpose_a=True)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py"", line 1891, in matmul
    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_math_ops.py"", line 2437, in _mat_mul
    name=name)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py"", line 2956, in create_op
    op_def=op_def)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py"", line 1470, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

...which was originally created as op 'projectx/trainig_gpu_0/decoder/projectxDecoderCell_1/lstm_0/lstm_0/lstm_cell/MatMul_36', defined at:
  File ""sync_train.py"", line 383, in <module>
    main()
[elided 1 identical lines from previous traceback]
  File ""sync_train.py"", line 64, in train
    train_model = projectx.projectx(n_gpu, config, is_training=True, reuse=False)
  File ""/kaldi/exp/tacotron/exp_2/projectx.py"", line 189, in __init__
    feed_previous=feed_previous)
  File ""/kaldi/exp/tacotron/exp_2/projectx.py"", line 427, in seq2seq
    pre_alignments)
  File ""/kaldi/exp/tacotron/exp_2/decoder.py"", line 99, in __call__
    attention_rnn_outputs, new_attention_rnn_state, context, alignments = self._attention_rnn_cell(prenet_output, state, pre_alignments)
  File ""/kaldi/exp/tacotron/exp_2/attention.py"", line 427, in __call__
    lstm_output, next_lstm_state = cell(lstm_inputs, states[i + 1])
  File ""/kaldi/exp/tacotron/exp_2/zoneout_lstm.py"", line 48, in __call__
    output, new_state = self._cell(inputs, state, scope)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/rnn_cell_impl.py"", line 183, in __call__
    return super(RNNCell, self).__call__(inputs, state)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/layers/base.py"", line 575, in __call__
    outputs = self.call(inputs, *args, **kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/rnn_cell_impl.py"", line 611, in call
    lstm_matrix = self._linear1([inputs, m_prev])
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/rnn_cell_impl.py"", line 1189, in __call__
    res = math_ops.matmul(array_ops.concat(args, 1), self._weights)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py"", line 1891, in matmul
    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_math_ops.py"", line 2437, in _mat_mul
    name=name)

ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[1792,4096]
	 [[Node: projectx/trainig_gpu_0/gradients/projectx/trainig_gpu_0/decoder/projectxDecoderCell_1/lstm_0/lstm_0/lstm_cell/MatMul_36_grad/MatMul_1 = MatMul[T=DT_FLOAT, transpose_a=true, transpose_b=false, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](projectx/trainig_gpu_0/decoder/projectxDecoderCell_1/lstm_0/lstm_0/lstm_cell/concat_36, projectx/trainig_gpu_0/gradients/projectx/trainig_gpu_0/decoder/projectxDecoderCell_1/lstm_0/lstm_0/lstm_cell/BiasAdd_36_grad/tuple/control_dependency)]]
	 [[Node: projectx/trainig_gpu_0/gradients/concat/_2929 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_646323_projectx/trainig_gpu_0/gradients/concat"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]",0,,5,2018-02-05T07:10:39Z,2018-02-06T08:30:56Z,NONE,"I'm using tensorflow 1.3, tested on a linux machine with 2 NVIDIA Tesla K80 cards,
however, I keep getting OOM error on GPU, but it does not happen when using cpu for training:

-------------------------------------
---log below
-------------------------------------

Exception happened during training, message: OOM when allocating tensor with shape[1792,4096]
	 [[Node: projectx/trainig_gpu_0/gradients/projectx/trainig_gpu_0/decoder/projectxDecoderCell_1/lstm_0/lstm_0/lstm_cell/MatMul_36_grad/MatMul_1 = MatMul[T=DT_FLOAT, transpose_a=true, transpose_b=false, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](projectx/trainig_gpu_0/decoder/projectxDecoderCell_1/lstm_0/lstm_0/lstm_cell/concat_36, projectx/trainig_gpu_0/gradients/projectx/trainig_gpu_0/decoder/projectxDecoderCell_1/lstm_0/lstm_0/lstm_cell/BiasAdd_36_grad/tuple/control_dependency)]]
	 [[Node: projectx/trainig_gpu_0/gradients/concat/_2929 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_646323_projectx/trainig_gpu_0/gradients/concat"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]

Caused by op 'projectx/trainig_gpu_0/gradients/projectx/trainig_gpu_0/decoder/projectxDecoderCell_1/lstm_0/lstm_0/lstm_cell/MatMul_36_grad/MatMul_1', defined at:
  File ""sync_train.py"", line 383, in <module>
    main()
  File ""sync_train.py"", line 380, in main
    train(config)
  File ""sync_train.py"", line 64, in train
    train_model = projectx.projectx(n_gpu, config, is_training=True, reuse=False)
  File ""/kaldi/exp/tacotron/exp_2/projectx.py"", line 291, in __init__
    grads_and_vars = self.optimizer.compute_gradients(loss)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/optimizer.py"", line 414, in compute_gradients
    colocate_gradients_with_ops=colocate_gradients_with_ops)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gradients_impl.py"", line 581, in gradients
    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gradients_impl.py"", line 353, in _MaybeCompile
    return grad_fn()  # Exit early
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gradients_impl.py"", line 581, in <lambda>
    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_grad.py"", line 922, in _MatMulGrad
    grad_b = math_ops.matmul(a, grad, transpose_a=True)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py"", line 1891, in matmul
    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_math_ops.py"", line 2437, in _mat_mul
    name=name)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py"", line 2956, in create_op
    op_def=op_def)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py"", line 1470, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

...which was originally created as op 'projectx/trainig_gpu_0/decoder/projectxDecoderCell_1/lstm_0/lstm_0/lstm_cell/MatMul_36', defined at:
  File ""sync_train.py"", line 383, in <module>
    main()
[elided 1 identical lines from previous traceback]
  File ""sync_train.py"", line 64, in train
    train_model = projectx.projectx(n_gpu, config, is_training=True, reuse=False)
  File ""/kaldi/exp/tacotron/exp_2/projectx.py"", line 189, in __init__
    feed_previous=feed_previous)
  File ""/kaldi/exp/tacotron/exp_2/projectx.py"", line 427, in seq2seq
    pre_alignments)
  File ""/kaldi/exp/tacotron/exp_2/decoder.py"", line 99, in __call__
    attention_rnn_outputs, new_attention_rnn_state, context, alignments = self._attention_rnn_cell(prenet_output, state, pre_alignments)
  File ""/kaldi/exp/tacotron/exp_2/attention.py"", line 427, in __call__
    lstm_output, next_lstm_state = cell(lstm_inputs, states[i + 1])
  File ""/kaldi/exp/tacotron/exp_2/zoneout_lstm.py"", line 48, in __call__
    output, new_state = self._cell(inputs, state, scope)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/rnn_cell_impl.py"", line 183, in __call__
    return super(RNNCell, self).__call__(inputs, state)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/layers/base.py"", line 575, in __call__
    outputs = self.call(inputs, *args, **kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/rnn_cell_impl.py"", line 611, in call
    lstm_matrix = self._linear1([inputs, m_prev])
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/rnn_cell_impl.py"", line 1189, in __call__
    res = math_ops.matmul(array_ops.concat(args, 1), self._weights)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py"", line 1891, in matmul
    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_math_ops.py"", line 2437, in _mat_mul
    name=name)

ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[1792,4096]
	 [[Node: projectx/trainig_gpu_0/gradients/projectx/trainig_gpu_0/decoder/projectxDecoderCell_1/lstm_0/lstm_0/lstm_cell/MatMul_36_grad/MatMul_1 = MatMul[T=DT_FLOAT, transpose_a=true, transpose_b=false, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](projectx/trainig_gpu_0/decoder/projectxDecoderCell_1/lstm_0/lstm_0/lstm_cell/concat_36, projectx/trainig_gpu_0/gradients/projectx/trainig_gpu_0/decoder/projectxDecoderCell_1/lstm_0/lstm_0/lstm_cell/BiasAdd_36_grad/tuple/control_dependency)]]
	 [[Node: projectx/trainig_gpu_0/gradients/concat/_2929 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_646323_projectx/trainig_gpu_0/gradients/concat"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]",2018-02-05T07:19:30Z,1,1,0,3.814657980444125
75,16765,Resolve Programmatic mistake.,cla: yes,SPECIES should have been either imported here or it should have referenced from iris_data. The corresponding code is correct but the conflict in the documentation.,1,,5,2018-02-05T04:03:04Z,2018-02-05T23:15:12Z,CONTRIBUTOR,SPECIES should have been either imported here or it should have referenced from iris_data. The corresponding code is correct but the conflict in the documentation.,2018-02-05T17:14:17Z,0,2,0,5.8146579804441245
76,16764,Does it makes sense to use AdamOptimizer with Dropout?,,"I was experimenting with Dropout and I tried to check the number of weights updated in every iteration.
My network has an input layer of size 100 and output layer of size 1, and I use dropout with keep_prob of 0.8.  With this configuration, I am expecting to update every time around 80 neurons. I tried to check this, and I got weird results. I asked in stackexchange and someone got the right answer: the optimizer was updating all the weights.

I was using Adam, and when I changed to GradientDescent, Adagrad and Adadelta it worked well. I haven't tried more optimizers thought.

Here is the code

```
import numpy as np
import tensorflow as tf

# As input, 100 random numbers.
input_size = 100
output_size = 1

x = tf.placeholder(tf.float32,[None, input_size],name=""input"")
y = tf.placeholder(tf.float32,[None, output_size],name=""labels"")

with tf.variable_scope(""dense1"") as scope:
    W = tf.get_variable(""W"",shape=[input_size,output_size],initializer=tf.keras.initializers.he_uniform())
    b = tf.get_variable(""b"",initializer=tf.zeros([output_size]))
    dropped = tf.nn.dropout(x,0.8)
    dense = tf.matmul(dropped,W)+b

eval_pred = tf.nn.sigmoid(dense,name=""prediction"")

cost = tf.reduce_mean(tf.losses.absolute_difference(eval_pred,y))
#train_step = tf.train.AdamOptimizer(learning_rate=0.01).minimize(cost)
train_step = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)
train_step = tf.train.AdadeltaOptimizer(learning_rate=0.01).minimize(cost)


# 20 epochs, batch size of 1
epochs = 20

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())

    allWeights = []
    for i in range(epochs):

        x_raw = np.random.random((1,input_size))
        y_raw = np.random.random((1,output_size))
        [_,c,d,w]=sess.run([train_step,cost,dropped,W], feed_dict={x: x_raw, y: y_raw})
        #print(""Epoch {0}/{1}. Loss: {2}"".format(i+1,epochs,c))

        # Numbers will be around 20% of input_size (17-22)
        print(np.sum(d==0))
        allWeights.append(w)

print(""Calculate the difference between W_i and W_{i-1}"")
for wi in range(1,len(allWeights)):
    difference = allWeights[wi]-allWeights[wi-1]
    # I expect that there will be around 20 weights that won't be updated
    # so the difference between the current weight and the previous one
    # should be zero.
    print(np.sum(difference==0))
```

Just in case is not clear enough in the code, I'm printing two sets of numbers:
The first set is the number of zeros in the masked dropout layer, and since I'm using 0.8 keep_prob, I should have around 20% of zeros (so, I should get a number around 20). This part works well.
The second set counts how many weights were NOT updated (difference between the previous weight and the current weight). Therefore, I am expecting these two sets to display the same numbers.

Again, with Adam doesn't work because it updates more weights whereas GradientDescend, Adadelta and Adagrad it works well.

Question:
Is this a bug or is it supposed to be like this?
In the latter case, does it make sense to use Adam with Dropout?
",0,,1,2018-02-05T03:26:57Z,2018-02-05T07:23:02Z,NONE,"I was experimenting with Dropout and I tried to check the number of weights updated in every iteration.
My network has an input layer of size 100 and output layer of size 1, and I use dropout with keep_prob of 0.8.  With this configuration, I am expecting to update every time around 80 neurons. I tried to check this, and I got weird results. I asked in stackexchange and someone got the right answer: the optimizer was updating all the weights.

I was using Adam, and when I changed to GradientDescent, Adagrad and Adadelta it worked well. I haven't tried more optimizers thought.

Here is the code

```
import numpy as np
import tensorflow as tf

# As input, 100 random numbers.
input_size = 100
output_size = 1

x = tf.placeholder(tf.float32,[None, input_size],name=""input"")
y = tf.placeholder(tf.float32,[None, output_size],name=""labels"")

with tf.variable_scope(""dense1"") as scope:
    W = tf.get_variable(""W"",shape=[input_size,output_size],initializer=tf.keras.initializers.he_uniform())
    b = tf.get_variable(""b"",initializer=tf.zeros([output_size]))
    dropped = tf.nn.dropout(x,0.8)
    dense = tf.matmul(dropped,W)+b

eval_pred = tf.nn.sigmoid(dense,name=""prediction"")

cost = tf.reduce_mean(tf.losses.absolute_difference(eval_pred,y))
#train_step = tf.train.AdamOptimizer(learning_rate=0.01).minimize(cost)
train_step = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)
train_step = tf.train.AdadeltaOptimizer(learning_rate=0.01).minimize(cost)


# 20 epochs, batch size of 1
epochs = 20

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())

    allWeights = []
    for i in range(epochs):

        x_raw = np.random.random((1,input_size))
        y_raw = np.random.random((1,output_size))
        [_,c,d,w]=sess.run([train_step,cost,dropped,W], feed_dict={x: x_raw, y: y_raw})
        #print(""Epoch {0}/{1}. Loss: {2}"".format(i+1,epochs,c))

        # Numbers will be around 20% of input_size (17-22)
        print(np.sum(d==0))
        allWeights.append(w)

print(""Calculate the difference between W_i and W_{i-1}"")
for wi in range(1,len(allWeights)):
    difference = allWeights[wi]-allWeights[wi-1]
    # I expect that there will be around 20 weights that won't be updated
    # so the difference between the current weight and the previous one
    # should be zero.
    print(np.sum(difference==0))
```

Just in case is not clear enough in the code, I'm printing two sets of numbers:
The first set is the number of zeros in the masked dropout layer, and since I'm using 0.8 keep_prob, I should have around 20% of zeros (so, I should get a number around 20). This part works well.
The second set counts how many weights were NOT updated (difference between the previous weight and the current weight). Therefore, I am expecting these two sets to display the same numbers.

Again, with Adam doesn't work because it updates more weights whereas GradientDescend, Adadelta and Adagrad it works well.

Question:
Is this a bug or is it supposed to be like this?
In the latter case, does it make sense to use Adam with Dropout?
",2018-02-05T07:23:02Z,0,1,0,1.814657980444125
77,16763,CMake (Windows): Added support for ninja build and some fixes/changes,cla: yes,"Most changes have comments stuck with them, but here's the summary of it.  

Changes made:
- Added EXACT flag on find_package CUDA to clarify it is building for exactly that version, the output ""minimum required"" can be misleading
- Tests if the compiler is compatible with CUDA before it starts building
- BYPRODUCTS/BUILD_BYPRODUCTS are added for Ninja to search for dependency, otherwise it will output the error shown below  
`ninja: error: 'zlib/install/lib/zlibstatic.lib', needed by 'proto_text.exe', missing and no known rule to make it`
- Visual Studio is the only generator with that will output under $(Configuration) directory, thus removed if it is not Visual Studio
- Fixed CONFIGURE_COMMAND causing some CMake versions to break (it resets to default generator, the [CMake](https://cmake.org/files/v3.6/cmake-3.6.3-win64-x64.zip) I tried uses NMake Makefiles as default, which causes the output below  
  ```
  ...
  Submodule path 'third_party/benchmark': checked out '360e66c1c4777c99402cf8cd535aa510fee16573'
  Performing update step for 'protobuf'
  No patch step for 'protobuf'
  Performing configure step for 'protobuf'
  CMake Error at CMakeLists.txt:12 (project):
    Generator

      NMake Makefiles

    does not support platform specification, but platform

      x64

    was specified.

  -- Building for: NMake Makefiles
  -- Configuring incomplete, errors occurred!
  ...
  ```

Additional Info: The current version of CMake has a problem with its ninja generator, as it may produce an error below  
`ninja: fatal: CreateProcess: The filename or extension is too long.`  
The CMake with its ninja generator improved can be found [here](https://gitlab.kitware.com/cmake/cmake/merge_requests/1604)
",1,,4,2018-02-05T03:18:32Z,2018-02-08T00:25:49Z,CONTRIBUTOR,"Most changes have comments stuck with them, but here's the summary of it.  

Changes made:
- Added EXACT flag on find_package CUDA to clarify it is building for exactly that version, the output ""minimum required"" can be misleading
- Tests if the compiler is compatible with CUDA before it starts building
- BYPRODUCTS/BUILD_BYPRODUCTS are added for Ninja to search for dependency, otherwise it will output the error shown below  
`ninja: error: 'zlib/install/lib/zlibstatic.lib', needed by 'proto_text.exe', missing and no known rule to make it`
- Visual Studio is the only generator with that will output under $(Configuration) directory, thus removed if it is not Visual Studio
- Fixed CONFIGURE_COMMAND causing some CMake versions to break (it resets to default generator, the [CMake](https://cmake.org/files/v3.6/cmake-3.6.3-win64-x64.zip) I tried uses NMake Makefiles as default, which causes the output below  
  ```
  ...
  Submodule path 'third_party/benchmark': checked out '360e66c1c4777c99402cf8cd535aa510fee16573'
  Performing update step for 'protobuf'
  No patch step for 'protobuf'
  Performing configure step for 'protobuf'
  CMake Error at CMakeLists.txt:12 (project):
    Generator

      NMake Makefiles

    does not support platform specification, but platform

      x64

    was specified.

  -- Building for: NMake Makefiles
  -- Configuring incomplete, errors occurred!
  ...
  ```

Additional Info: The current version of CMake has a problem with its ninja generator, as it may produce an error below  
`ninja: fatal: CreateProcess: The filename or extension is too long.`  
The CMake with its ninja generator improved can be found [here](https://gitlab.kitware.com/cmake/cmake/merge_requests/1604)
",2018-02-05T03:23:58Z,3,2,1,5.3146579804441245
78,16762,build android demo error,type:support,"ERROR: missing input file '@local_jdk//:jre/lib/resources.jar'
ERROR: /home/wangmeng/.cache/bazel/_bazel_wangmeng/b0a6578298b02ab8f9d039326e51a46f/external/bazel_tools/tools/android/BUILD:104:1: @bazel_tools//tools/android:gen_java_lang_extras_jar: missing input file '@local_jdk//:jre/lib/resources.jar'
Target //tensorflow/examples/android:tensorflow_demo failed to build
Use --verbose_failures to see the command lines of failed build steps.
ERROR: /home/wangmeng/.cache/bazel/_bazel_wangmeng/b0a6578298b02ab8f9d039326e51a46f/external/bazel_tools/tools/android/BUILD:104:1 1 input file(s) do not exist
INFO: Elapsed time: 22.408s, Critical Path: 0.03s
FAILED: Build did NOT complete successfully
",1,,3,2018-02-05T03:07:57Z,2018-02-16T19:23:34Z,NONE,"ERROR: missing input file '@local_jdk//:jre/lib/resources.jar'
ERROR: /home/wangmeng/.cache/bazel/_bazel_wangmeng/b0a6578298b02ab8f9d039326e51a46f/external/bazel_tools/tools/android/BUILD:104:1: @bazel_tools//tools/android:gen_java_lang_extras_jar: missing input file '@local_jdk//:jre/lib/resources.jar'
Target //tensorflow/examples/android:tensorflow_demo failed to build
Use --verbose_failures to see the command lines of failed build steps.
ERROR: /home/wangmeng/.cache/bazel/_bazel_wangmeng/b0a6578298b02ab8f9d039326e51a46f/external/bazel_tools/tools/android/BUILD:104:1 1 input file(s) do not exist
INFO: Elapsed time: 22.408s, Critical Path: 0.03s
FAILED: Build did NOT complete successfully
",2018-02-05T20:09:51Z,11,1,2,3.814657980444125
79,16760,Feature request: Adding data_format argument to lstm2d.separable_lstm,stat:contributions welcome,"Since ndlstm is used for 2D data such as images, I think it would be nice to include a `data_format` argument in lstm2d.separable_lstm in case one decides to use the channels first format (NCHW) for their images (i.e. Using CNN having NCHW data format followed by NDLSTM).",0,,2,2018-02-05T00:49:33Z,2018-02-13T08:29:32Z,CONTRIBUTOR,"Since ndlstm is used for 2D data such as images, I think it would be nice to include a `data_format` argument in lstm2d.separable_lstm in case one decides to use the channels first format (NCHW) for their images (i.e. Using CNN having NCHW data format followed by NDLSTM).",2018-02-13T08:29:32Z,8,2,2,3.314657980444125
80,16753,Virtual GPU config crashes TensorFlow after physical GPU loaded,,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Debian 9.3
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: ('v1.5.0-2132-gbdea071e68', '1.5.0')
- **Python version**: 2.7.13
- **Bazel version (if compiling from source)**:0.9.0
- **GCC/Compiler version (if compiling from source)**: (Debian 4.9.2-10) 4.9.2
- **CUDA/cuDNN version**: 9.1/7.0
- **GPU model and memory**: K40m, 11439 MB
- **Exact command to reproduce**: see the following script

### Describe the problem

Check failed when creating virtual GPU device after loading physical GPU information with `tensorflow.python.client.device_lib.list_local_devices`.

### Source code / logs

Source code:

```python
import tensorflow as tf
from tensorflow.core.protobuf import config_pb2
from tensorflow.python.client import device_lib

device_lib.list_local_devices()

virtual_device_gpu_options = config_pb2.GPUOptions(
    visible_device_list='0',
    experimental=config_pb2.GPUOptions.Experimental(virtual_devices=[
        config_pb2.GPUOptions.Experimental.VirtualDevices(
            memory_limit_mb=[200, 300])]))
config = config_pb2.ConfigProto(gpu_options=virtual_device_gpu_options)

with tf.Session(config=config) as sess:
    with tf.device('/gpu:1'):
        result = sess.run(tf.constant(42))
```

Logs:

```
2018-02-04 20:36:14.145943: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2018-02-04 20:36:23.613248: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1208] Found device 0 with properties:
name: Tesla K40m major: 3 minor: 5 memoryClockRate(GHz): 0.745
pciBusID: 0000:02:00.0
totalMemory: 11.17GiB freeMemory: 11.09GiB
2018-02-04 20:36:23.806370: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1208] Found device 1 with properties:
name: Tesla K40m major: 3 minor: 5 memoryClockRate(GHz): 0.745
pciBusID: 0000:03:00.0
totalMemory: 11.17GiB freeMemory: 11.09GiB
2018-02-04 20:36:24.019343: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1208] Found device 2 with properties:
name: Tesla K40m major: 3 minor: 5 memoryClockRate(GHz): 0.745
pciBusID: 0000:82:00.0
totalMemory: 11.17GiB freeMemory: 11.09GiB
2018-02-04 20:36:24.341878: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1208] Found device 3 with properties:
name: Tesla K40m major: 3 minor: 5 memoryClockRate(GHz): 0.745
pciBusID: 0000:83:00.0
totalMemory: 11.17GiB freeMemory: 11.09GiB
2018-02-04 20:36:24.342631: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1223] Device peer to peer matrix
2018-02-04 20:36:24.342769: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1229] DMA: 0 1 2 3
2018-02-04 20:36:24.342789: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1239] 0:   Y Y N N
2018-02-04 20:36:24.342803: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1239] 1:   Y Y N N
2018-02-04 20:36:24.342815: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1239] 2:   N N Y Y
2018-02-04 20:36:24.342827: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1239] 3:   N N Y Y
2018-02-04 20:36:24.342846: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1308] Adding visible gpu devices: 0, 1, 2, 3
2018-02-04 20:36:25.856113: I tensorflow/core/common_runtime/gpu/gpu_device.cc:989] Creating TensorFlow device (/device:GPU:0 with 10755 MB memory) -> physical GPU (device: 0, name: Tesla K40m, pci bus id: 0000:02:00.0, compute capability: 3.5)
2018-02-04 20:36:26.092252: I tensorflow/core/common_runtime/gpu/gpu_device.cc:989] Creating TensorFlow device (/device:GPU:1 with 10753 MB memory) -> physical GPU (device: 1, name: Tesla K40m, pci bus id: 0000:03:00.0, compute capability: 3.5)
2018-02-04 20:36:26.329914: I tensorflow/core/common_runtime/gpu/gpu_device.cc:989] Creating TensorFlow device (/device:GPU:2 with 10755 MB memory) -> physical GPU (device: 2, name: Tesla K40m, pci bus id: 0000:82:00.0, compute capability: 3.5)
2018-02-04 20:36:26.567499: I tensorflow/core/common_runtime/gpu/gpu_device.cc:989] Creating TensorFlow device (/device:GPU:3 with 10753 MB memory) -> physical GPU (device: 3, name: Tesla K40m, pci bus id: 0000:83:00.0, compute capability: 3.5)
2018-02-04 20:36:26.852624: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1308] Adding visible gpu devices: 0
2018-02-04 20:36:26.852709: I tensorflow/core/common_runtime/gpu/gpu_device.cc:989] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 200 MB memory) -> physical GPU (device: 0, name: Tesla K40m, pci bus id: 0000:02:00.0, compute capability: 3.5)
2018-02-04 20:36:26.852868: F tensorflow/core/common_runtime/gpu/gpu_id_utils.cc:42] Check failed: cuda_gpu_id.value() == result.first->second (0 vs. 1)Mapping the same TfGpuId to a different CUDA GPU id. TfGpuId: 1 Existing mapped CUDA GPU id: 1 CUDA GPU id being tried to map to: 0
```",1,,1,2018-02-04T12:41:27Z,2018-02-05T16:24:11Z,CONTRIBUTOR,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Debian 9.3
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: ('v1.5.0-2132-gbdea071e68', '1.5.0')
- **Python version**: 2.7.13
- **Bazel version (if compiling from source)**:0.9.0
- **GCC/Compiler version (if compiling from source)**: (Debian 4.9.2-10) 4.9.2
- **CUDA/cuDNN version**: 9.1/7.0
- **GPU model and memory**: K40m, 11439 MB
- **Exact command to reproduce**: see the following script

### Describe the problem

Check failed when creating virtual GPU device after loading physical GPU information with `tensorflow.python.client.device_lib.list_local_devices`.

### Source code / logs

Source code:

```python
import tensorflow as tf
from tensorflow.core.protobuf import config_pb2
from tensorflow.python.client import device_lib

device_lib.list_local_devices()

virtual_device_gpu_options = config_pb2.GPUOptions(
    visible_device_list='0',
    experimental=config_pb2.GPUOptions.Experimental(virtual_devices=[
        config_pb2.GPUOptions.Experimental.VirtualDevices(
            memory_limit_mb=[200, 300])]))
config = config_pb2.ConfigProto(gpu_options=virtual_device_gpu_options)

with tf.Session(config=config) as sess:
    with tf.device('/gpu:1'):
        result = sess.run(tf.constant(42))
```

Logs:

```
2018-02-04 20:36:14.145943: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2018-02-04 20:36:23.613248: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1208] Found device 0 with properties:
name: Tesla K40m major: 3 minor: 5 memoryClockRate(GHz): 0.745
pciBusID: 0000:02:00.0
totalMemory: 11.17GiB freeMemory: 11.09GiB
2018-02-04 20:36:23.806370: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1208] Found device 1 with properties:
name: Tesla K40m major: 3 minor: 5 memoryClockRate(GHz): 0.745
pciBusID: 0000:03:00.0
totalMemory: 11.17GiB freeMemory: 11.09GiB
2018-02-04 20:36:24.019343: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1208] Found device 2 with properties:
name: Tesla K40m major: 3 minor: 5 memoryClockRate(GHz): 0.745
pciBusID: 0000:82:00.0
totalMemory: 11.17GiB freeMemory: 11.09GiB
2018-02-04 20:36:24.341878: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1208] Found device 3 with properties:
name: Tesla K40m major: 3 minor: 5 memoryClockRate(GHz): 0.745
pciBusID: 0000:83:00.0
totalMemory: 11.17GiB freeMemory: 11.09GiB
2018-02-04 20:36:24.342631: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1223] Device peer to peer matrix
2018-02-04 20:36:24.342769: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1229] DMA: 0 1 2 3
2018-02-04 20:36:24.342789: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1239] 0:   Y Y N N
2018-02-04 20:36:24.342803: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1239] 1:   Y Y N N
2018-02-04 20:36:24.342815: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1239] 2:   N N Y Y
2018-02-04 20:36:24.342827: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1239] 3:   N N Y Y
2018-02-04 20:36:24.342846: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1308] Adding visible gpu devices: 0, 1, 2, 3
2018-02-04 20:36:25.856113: I tensorflow/core/common_runtime/gpu/gpu_device.cc:989] Creating TensorFlow device (/device:GPU:0 with 10755 MB memory) -> physical GPU (device: 0, name: Tesla K40m, pci bus id: 0000:02:00.0, compute capability: 3.5)
2018-02-04 20:36:26.092252: I tensorflow/core/common_runtime/gpu/gpu_device.cc:989] Creating TensorFlow device (/device:GPU:1 with 10753 MB memory) -> physical GPU (device: 1, name: Tesla K40m, pci bus id: 0000:03:00.0, compute capability: 3.5)
2018-02-04 20:36:26.329914: I tensorflow/core/common_runtime/gpu/gpu_device.cc:989] Creating TensorFlow device (/device:GPU:2 with 10755 MB memory) -> physical GPU (device: 2, name: Tesla K40m, pci bus id: 0000:82:00.0, compute capability: 3.5)
2018-02-04 20:36:26.567499: I tensorflow/core/common_runtime/gpu/gpu_device.cc:989] Creating TensorFlow device (/device:GPU:3 with 10753 MB memory) -> physical GPU (device: 3, name: Tesla K40m, pci bus id: 0000:83:00.0, compute capability: 3.5)
2018-02-04 20:36:26.852624: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1308] Adding visible gpu devices: 0
2018-02-04 20:36:26.852709: I tensorflow/core/common_runtime/gpu/gpu_device.cc:989] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 200 MB memory) -> physical GPU (device: 0, name: Tesla K40m, pci bus id: 0000:02:00.0, compute capability: 3.5)
2018-02-04 20:36:26.852868: F tensorflow/core/common_runtime/gpu/gpu_id_utils.cc:42] Check failed: cuda_gpu_id.value() == result.first->second (0 vs. 1)Mapping the same TfGpuId to a different CUDA GPU id. TfGpuId: 1 Existing mapped CUDA GPU id: 1 CUDA GPU id being tried to map to: 0
```",2018-02-05T16:24:07Z,1,2,0,3.810667467279806
81,16743,MonitoredSession after_run hook returning empty SessionRunValues results,,"### System information
- **Have I written custom code**: yes
- **OS Platform and Distribution**: Arch Linux 4.14.15-1
- **TensorFlow installed from**: source (master)
- **TensorFlow version**: v1.5.0-2123-g66105a6144
- **Python version**: 3.6.4
- **Bazel version (if compiling from source)**: 0.9.0
- **GCC/Compiler version (if compiling from source)**: 7.3.0
- **CUDA/cuDNN version**: 9.1.85/7.0.5
- **GPU model and memory**: Nvidia GTX 1080 8GB
- **Exact command to reproduce**: `python test.py`

### Describe the problem
When running a `MonitoredSession` with `after_run` hooks, the result passed to `run_values` is None, when there should be output.

### Source code / logs
`test.py`:
```python
import tensorflow as tf

one = tf.Variable(1)

class TestHook(tf.train.SessionRunHook):
    def __init__(self):
        super().__init__()
        self.result = None

    def after_run(self, run_context, run_values):
        # run_values.results should be 1 here
        self.result = run_values.results

hook = TestHook()
with tf.train.MonitoredSession(hooks=[hook]) as sess:
    print('Eval result: {}'.format(
        one.eval(session=sess)))
    print('Hook result: {}'.format(
        hook.result))
```

Expected output:
```
Eval result: 1
Hook result: 1
```

Actual output:
```
Eval result: 1
Hook result: None
```

I've changed this in my fork by replacing https://github.com/tensorflow/tensorflow/blob/3fb47614c4c3f29d59085c2eb6ad9a4f9adfa98e/tensorflow/python/training/monitored_session.py#L1176 with `results=outputs['caller'],`. However, this breaks training with an `Estimator` wrapping a `MonitoredSession`. If I'm misinterpreting the usage of the `after_run` hook please let me know!",0,,2,2018-02-04T03:33:56Z,2018-02-04T18:28:09Z,NONE,"### System information
- **Have I written custom code**: yes
- **OS Platform and Distribution**: Arch Linux 4.14.15-1
- **TensorFlow installed from**: source (master)
- **TensorFlow version**: v1.5.0-2123-g66105a6144
- **Python version**: 3.6.4
- **Bazel version (if compiling from source)**: 0.9.0
- **GCC/Compiler version (if compiling from source)**: 7.3.0
- **CUDA/cuDNN version**: 9.1.85/7.0.5
- **GPU model and memory**: Nvidia GTX 1080 8GB
- **Exact command to reproduce**: `python test.py`

### Describe the problem
When running a `MonitoredSession` with `after_run` hooks, the result passed to `run_values` is None, when there should be output.

### Source code / logs
`test.py`:
```python
import tensorflow as tf

one = tf.Variable(1)

class TestHook(tf.train.SessionRunHook):
    def __init__(self):
        super().__init__()
        self.result = None

    def after_run(self, run_context, run_values):
        # run_values.results should be 1 here
        self.result = run_values.results

hook = TestHook()
with tf.train.MonitoredSession(hooks=[hook]) as sess:
    print('Eval result: {}'.format(
        one.eval(session=sess)))
    print('Hook result: {}'.format(
        hook.result))
```

Expected output:
```
Eval result: 1
Hook result: 1
```

Actual output:
```
Eval result: 1
Hook result: None
```

I've changed this in my fork by replacing https://github.com/tensorflow/tensorflow/blob/3fb47614c4c3f29d59085c2eb6ad9a4f9adfa98e/tensorflow/python/training/monitored_session.py#L1176 with `results=outputs['caller'],`. However, this breaks training with an `Estimator` wrapping a `MonitoredSession`. If I'm misinterpreting the usage of the `after_run` hook please let me know!",2018-02-04T08:21:49Z,0,1,0,2.310667467279806
82,16740,error in code ,,"    W_0 = utils.weight_variable([FLAGS.z_dim, 64 * GEN_DIMENSION / 2 * IMAGE_SIZE / 16 * IMAGE_SIZE / 16],
NameError: name 'utils' is not defined
can you help me please ",0,,1,2018-02-04T00:58:17Z,2018-02-04T18:58:03Z,NONE,"    W_0 = utils.weight_variable([FLAGS.z_dim, 64 * GEN_DIMENSION / 2 * IMAGE_SIZE / 16 * IMAGE_SIZE / 16],
NameError: name 'utils' is not defined
can you help me please ",2018-02-04T18:58:03Z,0,1,0,1.810667467279806
83,16738,Fix logging format error in retrain.py,"awaiting testing (then merge),cla: yes","This fix fixes the logging format error in `tensorflow/examples/image_retraining/retrain.py`.

This fix fixes #16735.

Signed-off-by: Yong Tang <yong.tang.github@outlook.com>",0,,1,2018-02-03T23:50:53Z,2018-02-05T04:01:29Z,MEMBER,"This fix fixes the logging format error in `tensorflow/examples/image_retraining/retrain.py`.

This fix fixes #16735.

Signed-off-by: Yong Tang <yong.tang.github@outlook.com>",2018-02-04T19:23:55Z,2,3,0,3.806927676430135
84,16737,Discrepancies between GPU and CPU in floating-point operations,,"```
bs = 32
dim = 1024

tf.reset_default_graph()
with tf.device(""/cpu:0""):
  probs = tf.where(tf.greater(tf.random_normal((bs, dim)), 0.), tf.ones((bs, dim)), tf.zeros((bs, dim)))
  print(probs)
  logits = tf.log(probs / (1e-10 + 1 - probs))
  s = tf.Session(config=tf.ConfigProto(log_device_placement=True))
  print(s.run([logits]))

tf.reset_default_graph()
with tf.device(""/gpu:0""):
  probs = tf.where(tf.greater(tf.random_normal((bs, dim)), 0.), tf.ones((bs, dim)), tf.zeros((bs, dim)))
  print(probs)
  logits = tf.log(probs / (1e-10 + 1 - probs))
  s = tf.Session(config=tf.ConfigProto(log_device_placement=True))
  print(s.run([logits]))
```

Running this graph on GPU results in positive infinities, whereas on CPU these tensor entries evaluate to ~88.72284. I could not quite figure out which operation is responsible for the difference. In both cases TensorFlow reports `probs` as `float32`. The difference does not occur when replacing `probs` with a `tf.ones` tensor in `float32` format.",0,,1,2018-02-03T23:40:30Z,2018-02-05T11:40:28Z,NONE,"```
bs = 32
dim = 1024

tf.reset_default_graph()
with tf.device(""/cpu:0""):
  probs = tf.where(tf.greater(tf.random_normal((bs, dim)), 0.), tf.ones((bs, dim)), tf.zeros((bs, dim)))
  print(probs)
  logits = tf.log(probs / (1e-10 + 1 - probs))
  s = tf.Session(config=tf.ConfigProto(log_device_placement=True))
  print(s.run([logits]))

tf.reset_default_graph()
with tf.device(""/gpu:0""):
  probs = tf.where(tf.greater(tf.random_normal((bs, dim)), 0.), tf.ones((bs, dim)), tf.zeros((bs, dim)))
  print(probs)
  logits = tf.log(probs / (1e-10 + 1 - probs))
  s = tf.Session(config=tf.ConfigProto(log_device_placement=True))
  print(s.run([logits]))
```

Running this graph on GPU results in positive infinities, whereas on CPU these tensor entries evaluate to ~88.72284. I could not quite figure out which operation is responsible for the difference. In both cases TensorFlow reports `probs` as `float32`. The difference does not occur when replacing `probs` with a `tf.ones` tensor in `float32` format.",2018-02-05T01:38:12Z,2,1,0,1.806927676430135
85,16735,"incorrect logging formatting used in tensorflow / examples / image_retraining / retrain.py, causes error",,"In tensorflow -> examples -> image_retraining -> retrain.py, currently lines 347 / 348 look like this:

```
tf.logging.info('Successfully downloaded', filename, statinfo.st_size,
                    'bytes.')
```

This understandably causes an error since this function accepts strings and it is being fed an instance of statinfo.st_size which does not seem to be a string.  On my machine at least (TensorFlow 1.5, Windows 10) this causes the following error in function maybe_download_and_extract:

`TypeError: not all arguments converted during string formatting`

Here is a screenshot if that helps:

![error](https://user-images.githubusercontent.com/5672876/35772391-74f4433c-08f2-11e8-83d2-084605c14844.png)

The line numbers are slightly different in my screenshot because I moved a few lines around, but I can assure you the line above is causing the logging error.

I would suggest changing this line to the following, or similar:

`tf.logging.info('Successfully downloaded ' + str(filename) + ', statinfo.st_size = ' + str(statinfo.st_size) + ' bytes')`
",0,,1,2018-02-03T22:59:57Z,2018-02-05T04:01:29Z,NONE,"In tensorflow -> examples -> image_retraining -> retrain.py, currently lines 347 / 348 look like this:

```
tf.logging.info('Successfully downloaded', filename, statinfo.st_size,
                    'bytes.')
```

This understandably causes an error since this function accepts strings and it is being fed an instance of statinfo.st_size which does not seem to be a string.  On my machine at least (TensorFlow 1.5, Windows 10) this causes the following error in function maybe_download_and_extract:

`TypeError: not all arguments converted during string formatting`

Here is a screenshot if that helps:

![error](https://user-images.githubusercontent.com/5672876/35772391-74f4433c-08f2-11e8-83d2-084605c14844.png)

The line numbers are slightly different in my screenshot because I moved a few lines around, but I can assure you the line above is causing the logging error.

I would suggest changing this line to the following, or similar:

`tf.logging.info('Successfully downloaded ' + str(filename) + ', statinfo.st_size = ' + str(statinfo.st_size) + ' bytes')`
",2018-02-03T23:51:08Z,2,1,0,1.806927676430135
86,16734,Fix incorrect reference DOI number/link for GDR,cla: yes,"This fix fixes the incorrect reference DOI number/link for GDR:
`https://doi.org/10.1145/3123878.3123907` -> `https://doi.org/10.1145/3123878.3131975`.

The previous link (https://doi.org/10.1145/3123878.3123907) in the README.md does not work and returns 404. The new link (https://doi.org/10.1145/3123878.3131975) should be the correct one.


Signed-off-by: Yong Tang <yong.tang.github@outlook.com>",1,,1,2018-02-03T21:21:50Z,2018-02-05T22:01:26Z,MEMBER,"This fix fixes the incorrect reference DOI number/link for GDR:
`https://doi.org/10.1145/3123878.3123907` -> `https://doi.org/10.1145/3123878.3131975`.

The previous link (https://doi.org/10.1145/3123878.3123907) in the README.md does not work and returns 404. The new link (https://doi.org/10.1145/3123878.3131975) should be the correct one.


Signed-off-by: Yong Tang <yong.tang.github@outlook.com>",2018-02-08T00:29:57Z,2,3,0,4.806927676430135
87,16731,Fixed a couple of typos,cla: yes,Fixed a couple of typos,0,,3,2018-02-03T18:42:26Z,2018-02-03T23:22:23Z,CONTRIBUTOR,Fixed a couple of typos,2018-02-03T18:48:31Z,0,2,0,3.806927676430135
88,16723,"Bug: Compile Tensorflow 1.5.0 Java from source failed on NVIDIA Jetson TX2 with error ""'@bazel_tools//tools/jdk:singlejar' must produce a single file""","stat:awaiting response,type:support","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
NO
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 16.04 aarch64
- **TensorFlow installed from (source or binary)**:
source on branch r1.5
- **TensorFlow version (use command below)**:
v1.5.0-1934-g9e7ce91 1.5.0
- **Python version**:
Python 3.5
- **Bazel version (if compiling from source)**:
0.9.0 and 0.10.0 (both tried with clean installation)
- **GCC/Compiler version (if compiling from source)**:
gcc (Ubuntu/Linaro 5.4.0-6ubuntu1~16.04.6) 5.4.0 20160609
- **CUDA/cuDNN version**:
CUDA 9.0, cuDNN 7.0
- **GPU model and memory**:
NVIDIA Tegra X2 major (Pascal architecture) 8G
- **JDK Version**:

```
root@tegra-ubuntu:/usr/src# java -version
openjdk version ""1.8.0_151""
OpenJDK Runtime Environment (build 1.8.0_151-8u151-b12-0ubuntu0.16.04.2-b12)
OpenJDK 64-Bit Server VM (build 25.151-b12, mixed mode)

root@tegra-ubuntu:/usr/src# javac -version
javac 1.8.0_151
```
- **Exact command to reproduce**:

```shell
root@tegra-ubuntu:/usr/src/tensorflow# ./configure
Extracting Bazel installation...
You have bazel 0.9.0- (@non-git) installed.
Please specify the location of python. [Default is /usr/bin/python]: /usr/bin/python3


Found possible Python library paths:
  /usr/local/lib/python3.5/dist-packages
  /usr/lib/python3/dist-packages
Please input the desired Python library path to use.  Default is [/usr/local/lib/python3.5/dist-packages]

Do you wish to build TensorFlow with jemalloc as malloc support? [Y/n]: 
jemalloc as malloc support will be enabled for TensorFlow.

Do you wish to build TensorFlow with Google Cloud Platform support? [Y/n]: n
No Google Cloud Platform support will be enabled for TensorFlow.

Do you wish to build TensorFlow with Hadoop File System support? [Y/n]: 
Hadoop File System support will be enabled for TensorFlow.

Do you wish to build TensorFlow with Amazon S3 File System support? [Y/n]: n
No Amazon S3 File System support will be enabled for TensorFlow.

Do you wish to build TensorFlow with Apache Kafka Platform support? [y/N]: 
No Apache Kafka Platform support will be enabled for TensorFlow.

Do you wish to build TensorFlow with XLA JIT support? [y/N]: 
No XLA JIT support will be enabled for TensorFlow.

Do you wish to build TensorFlow with GDR support? [y/N]: 
No GDR support will be enabled for TensorFlow.

Do you wish to build TensorFlow with VERBS support? [y/N]: 
No VERBS support will be enabled for TensorFlow.

Do you wish to build TensorFlow with OpenCL SYCL support? [y/N]: 
No OpenCL SYCL support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: y
CUDA support will be enabled for TensorFlow.

Please specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to default to CUDA 9.0]: 


Please specify the location where CUDA 9.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: 


Please specify the cuDNN version you want to use. [Leave empty to default to cuDNN 7.0]: 


Please specify the location where cuDNN 7 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:


Do you wish to build TensorFlow with TensorRT support? [y/N]: 
No TensorRT support will be enabled for TensorFlow.

Please specify a list of comma-separated Cuda compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.
Please note that each additional compute capability significantly increases your build time and binary size. [Default is: 3.5,5.2]6.2


Do you want to use clang as CUDA compiler? [y/N]: 
nvcc will be used as CUDA compiler.

Please specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: 


Do you wish to build TensorFlow with MPI support? [y/N]: 
No MPI support will be enabled for TensorFlow.

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -march=native]: 


Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: 
Not configuring the WORKSPACE for Android builds.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See tools/bazel.rc for more details.
	--config=mkl         	# Build with MKL support.
	--config=monolithic  	# Config for mostly static monolithic build.
	--config=tensorrt    	# Build with TensorRT support.
Configuration finished
```
The output from compile procedure is

```shell
root@tegra-ubuntu:/usr/src/tensorflow# bazel build --config=opt --config=cuda //tensorflow/java:tensorflow //tensorflow/java:libtensorflow_jni
.........................
ERROR: /root/.cache/bazel/_bazel_root/cbe8b06b94787a6b39e59564d90f2497/external/bazel_tools/tools/jdk/BUILD:193:17: in singlejar attribute of java_toolchain rule @bazel_tools//tools/jdk:toolchain: '@bazel_tools//tools/jdk:singlejar' must produce a single file
ERROR: Analysis of target '//tensorflow/java:tensorflow' failed; build aborted: Analysis of target '@bazel_tools//tools/jdk:toolchain' failed; build aborted
INFO: Elapsed time: 57.138s
FAILED: Build did NOT complete successfully (7 packages loaded)
    currently loading: tensorflow
```
You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

```
root@tegra-ubuntu:/usr/src# tensorflow/tools/tf_env_collect.sh
Collecting system information...
2018-02-03 09:51:47.561112: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:865] ARM64 does not support NUMA - returning NUMA node zero
2018-02-03 09:51:47.561338: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1208] Found device 0 with properties: 
name: NVIDIA Tegra X2 major: 6 minor: 2 memoryClockRate(GHz): 1.3005
pciBusID: 0000:00:00.0
totalMemory: 7.66GiB freeMemory: 465.56MiB
2018-02-03 09:51:47.561450: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1308] Adding visible gpu devices: 0
2018-02-03 09:51:48.341988: I tensorflow/core/common_runtime/gpu/gpu_device.cc:989] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 52 MB memory) -> physical GPU (device: 0, name: NVIDIA Tegra X2, pci bus id: 0000:00:00.0, compute capability: 6.2)
Wrote environment to tf_env.txt. You can review the contents of that file.
and use it to populate the fields in the github issue template.

cat tf_env.txt
```

```
root@tegra-ubuntu:/usr/src# cat tf_env.txt

== cat /etc/issue ===============================================
Linux tegra-ubuntu 4.4.38-tegra #1 SMP PREEMPT Fri Dec 1 06:08:28 PST 2017 aarch64 aarch64 aarch64 GNU/Linux
VERSION=""16.04.3 LTS (Xenial Xerus)""
VERSION_ID=""16.04""
VERSION_CODENAME=xenial

== are we in docker =============================================
No

== compiler =====================================================
c++ (Ubuntu/Linaro 5.4.0-6ubuntu1~16.04.6) 5.4.0 20160609
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux tegra-ubuntu 4.4.38-tegra #1 SMP PREEMPT Fri Dec 1 06:08:28 PST 2017 aarch64 aarch64 aarch64 GNU/Linux

== check pips ===================================================
numpy (1.14.0)
protobuf (3.5.1)
tensorflow (1.5.0)
tensorflow-tensorboard (1.5.0)

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.5.0
tf.GIT_VERSION = v1.5.0-1934-g9e7ce91
tf.COMPILER_VERSION = v1.5.0-1934-g9e7ce91
Sanity check: array([1], dtype=int32)

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
tensorflow/tools/tf_env_collect.sh: line 105: nvidia-smi: command not found

== cuda libs  ===================================================
/usr/local/cuda-9.0/targets/aarch64-linux/lib/libcudart_static.a
/usr/local/cuda-9.0/targets/aarch64-linux/lib/libcudart.so.9.0.252
/usr/local/cuda-9.0/doc/man/man7/libcudart.7
/usr/local/cuda-9.0/doc/man/man7/libcudart.so.7
```

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

```
root@tegra-ubuntu:/usr/src# python3 -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
v1.5.0-1934-g9e7ce91 1.5.0
```

### Describe the problem

I have successfully compiled tensorflow python from source using same configure procedure as above with the following command:

```
root@tegra-ubuntu:/usr/src/tensorflow# bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package
``` 
There is no error in output and I can install the .whl file with pip.

After that, I tried to compile the Java native library without the configure step (because I already configured it when compiling python version) using the command:

```
root@tegra-ubuntu:/usr/src/tensorflow# bazel build --config=opt --config=cuda //tensorflow/java:tensorflow //tensorflow/java:libtensorflow_jni
```

It failed with errors:

```shell
root@tegra-ubuntu:/usr/src/tensorflow# bazel build --config=opt --config=cuda //tensorflow/java:tensorflow //tensorflow/java:libtensorflow_jni
.........................
ERROR: /root/.cache/bazel/_bazel_root/cbe8b06b94787a6b39e59564d90f2497/external/bazel_tools/tools/jdk/BUILD:193:17: in singlejar attribute of java_toolchain rule @bazel_tools//tools/jdk:toolchain: '@bazel_tools//tools/jdk:singlejar' must produce a single file
ERROR: Analysis of target '//tensorflow/java:tensorflow' failed; build aborted: Analysis of target '@bazel_tools//tools/jdk:toolchain' failed; build aborted
INFO: Elapsed time: 57.138s
FAILED: Build did NOT complete successfully (7 packages loaded)
    currently loading: tensorflow
```

### Here are some ways I tried but faild with same error:

1. Configure again (With same configure settings) and compile
2. Remove the directory ~/.cache and do the step 1
3. Remove the directory ~/.cache and tensorflow source directory, git clone tensorflow from r1.5 branch then do step 1
4. Remove ~/.cache and the bazel binary, compile and install bazel from latest source release (0.10.0) without error. Then I use bazel 0.10.0 to compile tensorflow java. This produced same error.
",0,,4,2018-02-03T10:22:31Z,2018-02-11T22:01:46Z,NONE,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
NO
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 16.04 aarch64
- **TensorFlow installed from (source or binary)**:
source on branch r1.5
- **TensorFlow version (use command below)**:
v1.5.0-1934-g9e7ce91 1.5.0
- **Python version**:
Python 3.5
- **Bazel version (if compiling from source)**:
0.9.0 and 0.10.0 (both tried with clean installation)
- **GCC/Compiler version (if compiling from source)**:
gcc (Ubuntu/Linaro 5.4.0-6ubuntu1~16.04.6) 5.4.0 20160609
- **CUDA/cuDNN version**:
CUDA 9.0, cuDNN 7.0
- **GPU model and memory**:
NVIDIA Tegra X2 major (Pascal architecture) 8G
- **JDK Version**:

```
root@tegra-ubuntu:/usr/src# java -version
openjdk version ""1.8.0_151""
OpenJDK Runtime Environment (build 1.8.0_151-8u151-b12-0ubuntu0.16.04.2-b12)
OpenJDK 64-Bit Server VM (build 25.151-b12, mixed mode)

root@tegra-ubuntu:/usr/src# javac -version
javac 1.8.0_151
```
- **Exact command to reproduce**:

```shell
root@tegra-ubuntu:/usr/src/tensorflow# ./configure
Extracting Bazel installation...
You have bazel 0.9.0- (@non-git) installed.
Please specify the location of python. [Default is /usr/bin/python]: /usr/bin/python3


Found possible Python library paths:
  /usr/local/lib/python3.5/dist-packages
  /usr/lib/python3/dist-packages
Please input the desired Python library path to use.  Default is [/usr/local/lib/python3.5/dist-packages]

Do you wish to build TensorFlow with jemalloc as malloc support? [Y/n]: 
jemalloc as malloc support will be enabled for TensorFlow.

Do you wish to build TensorFlow with Google Cloud Platform support? [Y/n]: n
No Google Cloud Platform support will be enabled for TensorFlow.

Do you wish to build TensorFlow with Hadoop File System support? [Y/n]: 
Hadoop File System support will be enabled for TensorFlow.

Do you wish to build TensorFlow with Amazon S3 File System support? [Y/n]: n
No Amazon S3 File System support will be enabled for TensorFlow.

Do you wish to build TensorFlow with Apache Kafka Platform support? [y/N]: 
No Apache Kafka Platform support will be enabled for TensorFlow.

Do you wish to build TensorFlow with XLA JIT support? [y/N]: 
No XLA JIT support will be enabled for TensorFlow.

Do you wish to build TensorFlow with GDR support? [y/N]: 
No GDR support will be enabled for TensorFlow.

Do you wish to build TensorFlow with VERBS support? [y/N]: 
No VERBS support will be enabled for TensorFlow.

Do you wish to build TensorFlow with OpenCL SYCL support? [y/N]: 
No OpenCL SYCL support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: y
CUDA support will be enabled for TensorFlow.

Please specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to default to CUDA 9.0]: 


Please specify the location where CUDA 9.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: 


Please specify the cuDNN version you want to use. [Leave empty to default to cuDNN 7.0]: 


Please specify the location where cuDNN 7 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:


Do you wish to build TensorFlow with TensorRT support? [y/N]: 
No TensorRT support will be enabled for TensorFlow.

Please specify a list of comma-separated Cuda compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.
Please note that each additional compute capability significantly increases your build time and binary size. [Default is: 3.5,5.2]6.2


Do you want to use clang as CUDA compiler? [y/N]: 
nvcc will be used as CUDA compiler.

Please specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: 


Do you wish to build TensorFlow with MPI support? [y/N]: 
No MPI support will be enabled for TensorFlow.

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -march=native]: 


Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: 
Not configuring the WORKSPACE for Android builds.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See tools/bazel.rc for more details.
	--config=mkl         	# Build with MKL support.
	--config=monolithic  	# Config for mostly static monolithic build.
	--config=tensorrt    	# Build with TensorRT support.
Configuration finished
```
The output from compile procedure is

```shell
root@tegra-ubuntu:/usr/src/tensorflow# bazel build --config=opt --config=cuda //tensorflow/java:tensorflow //tensorflow/java:libtensorflow_jni
.........................
ERROR: /root/.cache/bazel/_bazel_root/cbe8b06b94787a6b39e59564d90f2497/external/bazel_tools/tools/jdk/BUILD:193:17: in singlejar attribute of java_toolchain rule @bazel_tools//tools/jdk:toolchain: '@bazel_tools//tools/jdk:singlejar' must produce a single file
ERROR: Analysis of target '//tensorflow/java:tensorflow' failed; build aborted: Analysis of target '@bazel_tools//tools/jdk:toolchain' failed; build aborted
INFO: Elapsed time: 57.138s
FAILED: Build did NOT complete successfully (7 packages loaded)
    currently loading: tensorflow
```
You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

```
root@tegra-ubuntu:/usr/src# tensorflow/tools/tf_env_collect.sh
Collecting system information...
2018-02-03 09:51:47.561112: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:865] ARM64 does not support NUMA - returning NUMA node zero
2018-02-03 09:51:47.561338: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1208] Found device 0 with properties: 
name: NVIDIA Tegra X2 major: 6 minor: 2 memoryClockRate(GHz): 1.3005
pciBusID: 0000:00:00.0
totalMemory: 7.66GiB freeMemory: 465.56MiB
2018-02-03 09:51:47.561450: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1308] Adding visible gpu devices: 0
2018-02-03 09:51:48.341988: I tensorflow/core/common_runtime/gpu/gpu_device.cc:989] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 52 MB memory) -> physical GPU (device: 0, name: NVIDIA Tegra X2, pci bus id: 0000:00:00.0, compute capability: 6.2)
Wrote environment to tf_env.txt. You can review the contents of that file.
and use it to populate the fields in the github issue template.

cat tf_env.txt
```

```
root@tegra-ubuntu:/usr/src# cat tf_env.txt

== cat /etc/issue ===============================================
Linux tegra-ubuntu 4.4.38-tegra #1 SMP PREEMPT Fri Dec 1 06:08:28 PST 2017 aarch64 aarch64 aarch64 GNU/Linux
VERSION=""16.04.3 LTS (Xenial Xerus)""
VERSION_ID=""16.04""
VERSION_CODENAME=xenial

== are we in docker =============================================
No

== compiler =====================================================
c++ (Ubuntu/Linaro 5.4.0-6ubuntu1~16.04.6) 5.4.0 20160609
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux tegra-ubuntu 4.4.38-tegra #1 SMP PREEMPT Fri Dec 1 06:08:28 PST 2017 aarch64 aarch64 aarch64 GNU/Linux

== check pips ===================================================
numpy (1.14.0)
protobuf (3.5.1)
tensorflow (1.5.0)
tensorflow-tensorboard (1.5.0)

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.5.0
tf.GIT_VERSION = v1.5.0-1934-g9e7ce91
tf.COMPILER_VERSION = v1.5.0-1934-g9e7ce91
Sanity check: array([1], dtype=int32)

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
tensorflow/tools/tf_env_collect.sh: line 105: nvidia-smi: command not found

== cuda libs  ===================================================
/usr/local/cuda-9.0/targets/aarch64-linux/lib/libcudart_static.a
/usr/local/cuda-9.0/targets/aarch64-linux/lib/libcudart.so.9.0.252
/usr/local/cuda-9.0/doc/man/man7/libcudart.7
/usr/local/cuda-9.0/doc/man/man7/libcudart.so.7
```

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

```
root@tegra-ubuntu:/usr/src# python3 -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
v1.5.0-1934-g9e7ce91 1.5.0
```

### Describe the problem

I have successfully compiled tensorflow python from source using same configure procedure as above with the following command:

```
root@tegra-ubuntu:/usr/src/tensorflow# bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package
``` 
There is no error in output and I can install the .whl file with pip.

After that, I tried to compile the Java native library without the configure step (because I already configured it when compiling python version) using the command:

```
root@tegra-ubuntu:/usr/src/tensorflow# bazel build --config=opt --config=cuda //tensorflow/java:tensorflow //tensorflow/java:libtensorflow_jni
```

It failed with errors:

```shell
root@tegra-ubuntu:/usr/src/tensorflow# bazel build --config=opt --config=cuda //tensorflow/java:tensorflow //tensorflow/java:libtensorflow_jni
.........................
ERROR: /root/.cache/bazel/_bazel_root/cbe8b06b94787a6b39e59564d90f2497/external/bazel_tools/tools/jdk/BUILD:193:17: in singlejar attribute of java_toolchain rule @bazel_tools//tools/jdk:toolchain: '@bazel_tools//tools/jdk:singlejar' must produce a single file
ERROR: Analysis of target '//tensorflow/java:tensorflow' failed; build aborted: Analysis of target '@bazel_tools//tools/jdk:toolchain' failed; build aborted
INFO: Elapsed time: 57.138s
FAILED: Build did NOT complete successfully (7 packages loaded)
    currently loading: tensorflow
```

### Here are some ways I tried but faild with same error:

1. Configure again (With same configure settings) and compile
2. Remove the directory ~/.cache and do the step 1
3. Remove the directory ~/.cache and tensorflow source directory, git clone tensorflow from r1.5 branch then do step 1
4. Remove ~/.cache and the bazel binary, compile and install bazel from latest source release (0.10.0) without error. Then I use bazel 0.10.0 to compile tensorflow java. This produced same error.
",2018-02-07T00:26:51Z,8,1,2,3.306927676430135
89,16721,Customized loss in keras,,"Dear all,

I can run properly with the following code:
### System Information ####
Have I written custom code : As following
OS Platform and Distribution: Linux Ubuntu16.04
TensorFlow installed from : conda script
TensorFlow version : '1.4.0'
Bazel version : N/A
CUDA/cuDNN version : CUDA8.0, cudnn6.0
GPU model and memory : 1070/8G

```
import tensorflow as tf
from tensorflow.python.keras.layers import Dense
from tensorflow.python.keras.backend import categorical_crossentropy
from tensorflow.examples.tutorials.mnist import input_data
from tensorflow.python.keras.models import Model
from tensorflow.python.keras.layers import  Input

mnist_data = input_data.read_data_sets('MNIST_data', one_hot=True)
img_size_flat = 28*28
batch_size = 64

def gen(batch_size=32):
    while True:
        batch_data, batch_label = mnist_data.train.next_batch(batch_size)
        yield batch_data, batch_label   


inputs = Input(shape=(img_size_flat,))
x = Dense(128, activation='relu')(inputs)  # fully-connected layer with 128 units and ReLU activation
x = Dense(128, activation='relu')(x)
preds = Dense(10, activation='softmax')(x)  # output layer with 10 units and a softmax activation
model = Model(inputs=inputs, outputs=preds)

model.compile(optimizer='rmsprop',
               loss='categorical_crossentropy',
               metrics=['accuracy'])


model.fit_generator(gen(batch_size), steps_per_epoch=len(mnist_data.train.labels)//batch_size, epochs=2)
```

But if I want to write loss function with my own code like:
```
preds_softmax = tf.nn.softmax(preds)
step1 = tf.cast(y_true, tf.float32) * tf.log(preds_softmax)
step2 = -tf.reduce_sum(step1, reduction_indices=[1])
loss = tf.reduce_mean(step2)       # loss
```

Is something like the following code on tensorflow?
```
inputs = tf.placeholder(tf.float32, shape=(None, 784))
x = Dense(128, activation='relu')(inputs) # fully-connected layer with 128 units and ReLU activation
x = Dense(128, activation='relu')(x)
preds = Dense(10, activation='softmax')(x) # output layer with 10 units and a softmax activation

y_true = tf.placeholder(tf.float32, shape=(None, 10))
```

How can I do based on above code(part I)? Thanks for any help!!
",0,,4,2018-02-03T08:08:05Z,2018-02-07T00:25:52Z,NONE,"Dear all,

I can run properly with the following code:
### System Information ####
Have I written custom code : As following
OS Platform and Distribution: Linux Ubuntu16.04
TensorFlow installed from : conda script
TensorFlow version : '1.4.0'
Bazel version : N/A
CUDA/cuDNN version : CUDA8.0, cudnn6.0
GPU model and memory : 1070/8G

```
import tensorflow as tf
from tensorflow.python.keras.layers import Dense
from tensorflow.python.keras.backend import categorical_crossentropy
from tensorflow.examples.tutorials.mnist import input_data
from tensorflow.python.keras.models import Model
from tensorflow.python.keras.layers import  Input

mnist_data = input_data.read_data_sets('MNIST_data', one_hot=True)
img_size_flat = 28*28
batch_size = 64

def gen(batch_size=32):
    while True:
        batch_data, batch_label = mnist_data.train.next_batch(batch_size)
        yield batch_data, batch_label   


inputs = Input(shape=(img_size_flat,))
x = Dense(128, activation='relu')(inputs)  # fully-connected layer with 128 units and ReLU activation
x = Dense(128, activation='relu')(x)
preds = Dense(10, activation='softmax')(x)  # output layer with 10 units and a softmax activation
model = Model(inputs=inputs, outputs=preds)

model.compile(optimizer='rmsprop',
               loss='categorical_crossentropy',
               metrics=['accuracy'])


model.fit_generator(gen(batch_size), steps_per_epoch=len(mnist_data.train.labels)//batch_size, epochs=2)
```

But if I want to write loss function with my own code like:
```
preds_softmax = tf.nn.softmax(preds)
step1 = tf.cast(y_true, tf.float32) * tf.log(preds_softmax)
step2 = -tf.reduce_sum(step1, reduction_indices=[1])
loss = tf.reduce_mean(step2)       # loss
```

Is something like the following code on tensorflow?
```
inputs = tf.placeholder(tf.float32, shape=(None, 784))
x = Dense(128, activation='relu')(inputs) # fully-connected layer with 128 units and ReLU activation
x = Dense(128, activation='relu')(x)
preds = Dense(10, activation='softmax')(x) # output layer with 10 units and a softmax activation

y_true = tf.placeholder(tf.float32, shape=(None, 10))
```

How can I do based on above code(part I)? Thanks for any help!!
",2018-02-06T07:29:34Z,4,1,1,3.306927676430135
90,16718,add an interface to check if a variable is initialized,stat:awaiting response,"Currently, if we create an Adam optimizer and minimize some loss, Adam will create some new variables that need to be initialized. Howerver, this is only a part of variables and we donot want to use `tf.global_variables_initializer()`.

If there is an interface to check if a variable is initialized, then we can filter global variables and initialize only what needs to be initialized!

the interface should look like `Variable.is_initialized() -> bool`",0,,6,2018-02-03T02:19:58Z,2018-02-06T17:49:55Z,CONTRIBUTOR,"Currently, if we create an Adam optimizer and minimize some loss, Adam will create some new variables that need to be initialized. Howerver, this is only a part of variables and we donot want to use `tf.global_variables_initializer()`.

If there is an interface to check if a variable is initialized, then we can filter global variables and initialize only what needs to be initialized!

the interface should look like `Variable.is_initialized() -> bool`",2018-02-03T02:23:45Z,3,2,1,5.306927676430135
91,16716,Linker Tools Error encountered when use StepStats,,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Windows 10.0.16299
- **TensorFlow installed from (source or binary)**:
source
- **TensorFlow version (use command below)**:
1.5 release
- **Python version**: 
3.5.3
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: 



### Describe the problem

Encounter link error when build the program (source code attached). Build went through well with TF 1.4 release 

Error	LNK2001	unresolved external symbol ""class tensorflow::StepStatsDefaultTypeInternal tensorflow::_StepStats_default_instance_"" (?_StepStats_default_instance_@tensorflow@@3VStepStatsDefaultTypeInternal@1@A)	ReprBug	c:\Users\xx\documents\visual studio 2015\Projects\ReprBug\ReprBug\Source.obj


### Source code / logs

```cpp 
#include ""tensorflow/cc/saved_model/tag_constants.h""
#include ""tensorflow/core/public/session_options.h""
#include ""tensorflow/core/util/stat_summarizer.h""
#include ""tensorflow/contrib/session_bundle/bundle_shim.h""

class SynchronizedStatSummarizer
{
public:
	SynchronizedStatSummarizer(const tensorflow::StatSummarizerOptions& options)
		: m_statSummarizer{ options }, m_mutex{}
	{
	}

	void AddStepStats(const tensorflow::StepStats& stepStats)
	{
		std::lock_guard<std::mutex> guard{ m_mutex };
		m_statSummarizer.ProcessStepStats(stepStats);
	}

private:
	// The TF stat summarizer.
	tensorflow::StatSummarizer m_statSummarizer;

	// Synchronizes access to m_statSummarizer.
	mutable std::mutex m_mutex;
};

int main() {

	tensorflow::SessionOptions sessionOptions;
	tensorflow::RunOptions runOptions{};
	tensorflow::ConfigProto& config = sessionOptions.config;
	
	std::unique_ptr<tensorflow::SavedModelBundle> m_bundle (new tensorflow::SavedModelBundle());

	const std::string path = ""somepath"";
	tensorflow::Status status = tensorflow::serving::LoadSessionBundleOrSavedModelBundle(
		sessionOptions, runOptions, path, { tensorflow::kSavedModelTagServe }, m_bundle.get());

	std::vector<std::pair<std::string, tensorflow::Tensor>> modifiedInputs;
	std::vector<std::string> modifiedOutputNames;
	std::vector<tensorflow::Tensor> tensorOutputs;
	tensorflow::RunMetadata runMetadata{};

	tensorflow::Status run_status = m_bundle->session->Run(
		runOptions, modifiedInputs, modifiedOutputNames, {}, &tensorOutputs, &runMetadata);

	std::unique_ptr<SynchronizedStatSummarizer> m_runTracingStats;

	if (run_status.ok())
	{
		m_runTracingStats->AddStepStats(runMetadata.step_stats());
	}

}
```",0,,1,2018-02-03T01:07:58Z,2018-02-03T01:39:35Z,CONTRIBUTOR,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Windows 10.0.16299
- **TensorFlow installed from (source or binary)**:
source
- **TensorFlow version (use command below)**:
1.5 release
- **Python version**: 
3.5.3
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: 



### Describe the problem

Encounter link error when build the program (source code attached). Build went through well with TF 1.4 release 

Error	LNK2001	unresolved external symbol ""class tensorflow::StepStatsDefaultTypeInternal tensorflow::_StepStats_default_instance_"" (?_StepStats_default_instance_@tensorflow@@3VStepStatsDefaultTypeInternal@1@A)	ReprBug	c:\Users\xx\documents\visual studio 2015\Projects\ReprBug\ReprBug\Source.obj


### Source code / logs

```cpp 
#include ""tensorflow/cc/saved_model/tag_constants.h""
#include ""tensorflow/core/public/session_options.h""
#include ""tensorflow/core/util/stat_summarizer.h""
#include ""tensorflow/contrib/session_bundle/bundle_shim.h""

class SynchronizedStatSummarizer
{
public:
	SynchronizedStatSummarizer(const tensorflow::StatSummarizerOptions& options)
		: m_statSummarizer{ options }, m_mutex{}
	{
	}

	void AddStepStats(const tensorflow::StepStats& stepStats)
	{
		std::lock_guard<std::mutex> guard{ m_mutex };
		m_statSummarizer.ProcessStepStats(stepStats);
	}

private:
	// The TF stat summarizer.
	tensorflow::StatSummarizer m_statSummarizer;

	// Synchronizes access to m_statSummarizer.
	mutable std::mutex m_mutex;
};

int main() {

	tensorflow::SessionOptions sessionOptions;
	tensorflow::RunOptions runOptions{};
	tensorflow::ConfigProto& config = sessionOptions.config;
	
	std::unique_ptr<tensorflow::SavedModelBundle> m_bundle (new tensorflow::SavedModelBundle());

	const std::string path = ""somepath"";
	tensorflow::Status status = tensorflow::serving::LoadSessionBundleOrSavedModelBundle(
		sessionOptions, runOptions, path, { tensorflow::kSavedModelTagServe }, m_bundle.get());

	std::vector<std::pair<std::string, tensorflow::Tensor>> modifiedInputs;
	std::vector<std::string> modifiedOutputNames;
	std::vector<tensorflow::Tensor> tensorOutputs;
	tensorflow::RunMetadata runMetadata{};

	tensorflow::Status run_status = m_bundle->session->Run(
		runOptions, modifiedInputs, modifiedOutputNames, {}, &tensorOutputs, &runMetadata);

	std::unique_ptr<SynchronizedStatSummarizer> m_runTracingStats;

	if (run_status.ok())
	{
		m_runTracingStats->AddStepStats(runMetadata.step_stats());
	}

}
```",2018-02-03T01:39:35Z,0,2,0,2.806927676430135
92,16711,Fixed a typo in `group_by_window` documentation,cla: yes,Nothing else to add :),1,,3,2018-02-02T21:36:52Z,2018-02-08T00:26:37Z,CONTRIBUTOR,Nothing else to add :),2018-02-02T21:42:12Z,6,2,1,4.803413075542279
93,16707,Can't initialize an all zero SparseTensor,stat:awaiting tensorflower,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Kind of?
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac 10.12.6 (not relevant)
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: v1.5.0
- **Python version**: 3.6.3
- **Bazel version (if compiling from source)**: NA
- **GCC/Compiler version (if compiling from source)**: NA
- **CUDA/cuDNN version**: NA
- **GPU model and memory**: NA
- **Exact command to reproduce**: NA

### Describe the problem
It doesn't seem possible to initialize a `tf.SparseTensor` with all zero entries. 

A call doing this would look something like:

    tf.SparseTensor(indices=[], values=[], dense_shape=(10, 10))

However, attempting this initialization produces the error:

     ValueError: Shape (0,) must have rank 2


### Source code / logs
Current relevant section from `SparseTensor.__init__`:

    indices_shape = indices.get_shape().with_rank(2) # <--- .with_rank(2) is what causes the problem
    values_shape = values.get_shape().with_rank(1)
    dense_shape_shape = dense_shape.get_shape().with_rank(1)

    # Assert number of rows in indices match the number of elements in values.
    indices_shape[0].merge_with(values_shape[0])
    # Assert number of columns in indices matches the number of elements in
    # dense_shape.
    indices_shape[1].merge_with(dense_shape_shape[0])

Example solution:

    tf.cond(tf.equal(indices.get_shape()[0], 0),
            true_fn=lambda: None,
            false_fn=self._validate_input)

    def _validate_input(self):
        indices_shape = self._indices.get_shape().with_rank(2)
        values_shape = self._values.get_shape().with_rank(1)
        dense_shape_shape = self._dense_shape.get_shape().with_rank(1)

        # Assert number of rows in indices match the number of elements in values.
        indices_shape[0].merge_with(values_shape[0])
        # Assert number of columns in indices matches the number of elements in
        # dense_shape.
        indices_shape[1].merge_with(dense_shape_shape[0])

My only worry with the example solution is that `tf.cond` is too high level a function and there's some alternative that would be better. Is that the case? ",0,,3,2018-02-02T20:29:29Z,2018-02-03T00:14:37Z,NONE,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Kind of?
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac 10.12.6 (not relevant)
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: v1.5.0
- **Python version**: 3.6.3
- **Bazel version (if compiling from source)**: NA
- **GCC/Compiler version (if compiling from source)**: NA
- **CUDA/cuDNN version**: NA
- **GPU model and memory**: NA
- **Exact command to reproduce**: NA

### Describe the problem
It doesn't seem possible to initialize a `tf.SparseTensor` with all zero entries. 

A call doing this would look something like:

    tf.SparseTensor(indices=[], values=[], dense_shape=(10, 10))

However, attempting this initialization produces the error:

     ValueError: Shape (0,) must have rank 2


### Source code / logs
Current relevant section from `SparseTensor.__init__`:

    indices_shape = indices.get_shape().with_rank(2) # <--- .with_rank(2) is what causes the problem
    values_shape = values.get_shape().with_rank(1)
    dense_shape_shape = dense_shape.get_shape().with_rank(1)

    # Assert number of rows in indices match the number of elements in values.
    indices_shape[0].merge_with(values_shape[0])
    # Assert number of columns in indices matches the number of elements in
    # dense_shape.
    indices_shape[1].merge_with(dense_shape_shape[0])

Example solution:

    tf.cond(tf.equal(indices.get_shape()[0], 0),
            true_fn=lambda: None,
            false_fn=self._validate_input)

    def _validate_input(self):
        indices_shape = self._indices.get_shape().with_rank(2)
        values_shape = self._values.get_shape().with_rank(1)
        dense_shape_shape = self._dense_shape.get_shape().with_rank(1)

        # Assert number of rows in indices match the number of elements in values.
        indices_shape[0].merge_with(values_shape[0])
        # Assert number of columns in indices matches the number of elements in
        # dense_shape.
        indices_shape[1].merge_with(dense_shape_shape[0])

My only worry with the example solution is that `tf.cond` is too high level a function and there's some alternative that would be better. Is that the case? ",2018-02-02T23:12:50Z,1,1,0,2.803413075542279
94,16703,tf.contrib.rnn.GLSTMCell is hilariously broken,,"In 3f579020bab8f00e4621e9c7c740cbf13136a809 an ""if"" was added that caches linear transformation weights:
https://github.com/tensorflow/tensorflow/blob/r1.5/tensorflow/contrib/rnn/python/ops/rnn_cell.py#L2316

The problem is that this _linear is inside a loop. And so the change tied weights of all these linear transformations.

CC @okuchaiev 
",1,,6,2018-02-02T18:54:15Z,2018-02-06T18:29:02Z,NONE,"In 3f579020bab8f00e4621e9c7c740cbf13136a809 an ""if"" was added that caches linear transformation weights:
https://github.com/tensorflow/tensorflow/blob/r1.5/tensorflow/contrib/rnn/python/ops/rnn_cell.py#L2316

The problem is that this _linear is inside a loop. And so the change tied weights of all these linear transformations.

CC @okuchaiev 
",2018-02-02T23:14:32Z,4,1,1,5.303413075542279
95,16698,"We must also trim everything after TAB, in order to correctly parse version from TensorRT-3.0.2",cla: yes,"Note TABS after version numbers:

```
dmikushin@tesla-cmc:/opt/TensorRT-3.0.2/include$ cat NvInfer.h | grep SONAME
#define NV_TENSORRT_SONAME_MAJOR 4		//!< shared object library major version number
#define NV_TENSORRT_SONAME_MINOR 0		//!< shared object library minor version number
#define NV_TENSORRT_SONAME_PATCH 2		//!< shared object library patch version number
```

I'm not a Python expert, please feel free to rework this in a better way.

This pull requests fixes the following build error:

```
Cuda Configuration Error: TensorRT library version detected from /opt/TensorRT-3.0.2/include/NvInfer.h (4		//!<.0		//!<.2	//!<) does not match TF_TENSORRT_VERSION (4.0.2). To fix this rerun configure again.
WARNING: Target pattern parsing failed.
```",1,,6,2018-02-02T16:41:04Z,2018-02-06T15:37:04Z,NONE,"Note TABS after version numbers:

```
dmikushin@tesla-cmc:/opt/TensorRT-3.0.2/include$ cat NvInfer.h | grep SONAME
#define NV_TENSORRT_SONAME_MAJOR 4		//!< shared object library major version number
#define NV_TENSORRT_SONAME_MINOR 0		//!< shared object library minor version number
#define NV_TENSORRT_SONAME_PATCH 2		//!< shared object library patch version number
```

I'm not a Python expert, please feel free to rework this in a better way.

This pull requests fixes the following build error:

```
Cuda Configuration Error: TensorRT library version detected from /opt/TensorRT-3.0.2/include/NvInfer.h (4		//!<.0		//!<.2	//!<) does not match TF_TENSORRT_VERSION (4.0.2). To fix this rerun configure again.
WARNING: Target pattern parsing failed.
```",2018-02-06T03:36:43Z,4,1,1,5.303413075542279
96,16695,Padding algo is not working as doc says,,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linus centos 7
- **TensorFlow installed from (source or binary)**: pip
- **TensorFlow version (use command below)**: 1.4
- **Python version**: 2.7.5
- **Bazel version (if compiling from source)**: 0
- **GCC/Compiler version (if compiling from source)**:0
- **CUDA/cuDNN version**:0
- **GPU model and memory**:0
- **Exact command to reproduce**:

In the following situation, TF [doc](https://www.tensorflow.org/api_guides/python/nn#Convolution) is not correct.
- Input tensor shape : [1, 5, 2, 1]
- Kernel shape:           [1, 3, 1, 1]
- Stride :                      [1, 5, 5, 1]
- Padding =                  ""SAME""

According to the formula we can compute : 
out_h = 1
out_w = 1

```
if (in_height % strides[1] == 0):
  pad_along_height = max(filter_height - strides[1], 0)
else:
  pad_along_height = max(filter_height - (in_height % strides[1]), 0)
if (in_width % strides[2] == 0):
  pad_along_width = max(filter_width - strides[2], 0)
else:
  pad_along_width = max(filter_width - (in_width % strides[2]), 0)
```
gives :
pad_along_height = 0
pad_along_width = 1

then 
```
pad_top = pad_along_height // 2
pad_bottom = pad_along_height - pad_top
pad_left = pad_along_width // 2
pad_right = pad_along_width - pad_left
```

gives:

pad_top = 0
pad_bottom = 0
pad_left = 0
pad_right = 1

How tensorflow do a convolution with a kernel of height 1 on a image of height 5 and which gives output of height 1 (stride = 5) ??? How TF do this ? The doc can't explain the method used ... 

Doing retro engineering, I saw that TF apply the filter on the middle of the input tensor (pad_top = -2 and pad_bottom=-2).
I agree with this method, but the formulas of the Convolution doc is doing max(.., 0) so padding could never be negative (according to the doc).

Could someone explain me clearly what is the formula used in tensorflow ?
Could someone update the doc ?",0,,1,2018-02-02T15:19:12Z,2018-02-02T17:38:10Z,NONE,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linus centos 7
- **TensorFlow installed from (source or binary)**: pip
- **TensorFlow version (use command below)**: 1.4
- **Python version**: 2.7.5
- **Bazel version (if compiling from source)**: 0
- **GCC/Compiler version (if compiling from source)**:0
- **CUDA/cuDNN version**:0
- **GPU model and memory**:0
- **Exact command to reproduce**:

In the following situation, TF [doc](https://www.tensorflow.org/api_guides/python/nn#Convolution) is not correct.
- Input tensor shape : [1, 5, 2, 1]
- Kernel shape:           [1, 3, 1, 1]
- Stride :                      [1, 5, 5, 1]
- Padding =                  ""SAME""

According to the formula we can compute : 
out_h = 1
out_w = 1

```
if (in_height % strides[1] == 0):
  pad_along_height = max(filter_height - strides[1], 0)
else:
  pad_along_height = max(filter_height - (in_height % strides[1]), 0)
if (in_width % strides[2] == 0):
  pad_along_width = max(filter_width - strides[2], 0)
else:
  pad_along_width = max(filter_width - (in_width % strides[2]), 0)
```
gives :
pad_along_height = 0
pad_along_width = 1

then 
```
pad_top = pad_along_height // 2
pad_bottom = pad_along_height - pad_top
pad_left = pad_along_width // 2
pad_right = pad_along_width - pad_left
```

gives:

pad_top = 0
pad_bottom = 0
pad_left = 0
pad_right = 1

How tensorflow do a convolution with a kernel of height 1 on a image of height 5 and which gives output of height 1 (stride = 5) ??? How TF do this ? The doc can't explain the method used ... 

Doing retro engineering, I saw that TF apply the filter on the middle of the input tensor (pad_top = -2 and pad_bottom=-2).
I agree with this method, but the formulas of the Convolution doc is doing max(.., 0) so padding could never be negative (according to the doc).

Could someone explain me clearly what is the formula used in tensorflow ?
Could someone update the doc ?",2018-02-02T17:38:10Z,0,1,0,1.803413075542279
97,16693,Separate constant file for tpu to make reusable,cla: yes,"Using constants is more a way of defensive programming, also it improves performance optimization. Most importantly, it is for human reader.

Since I have made a nice cleanup to make a separate constant file for making global variable reusable for tpu that can be use anywhere.",1,,3,2018-02-02T12:51:11Z,2018-02-08T19:28:11Z,CONTRIBUTOR,"Using constants is more a way of defensive programming, also it improves performance optimization. Most importantly, it is for human reader.

Since I have made a nice cleanup to make a separate constant file for making global variable reusable for tpu that can be use anywhere.",2018-02-04T16:43:04Z,6,2,1,4.803413075542279
98,16690,change to anchor link,"awaiting testing (then merge),cla: yes",Fixing markdown typo,0,,4,2018-02-02T10:10:12Z,2018-02-03T23:22:49Z,CONTRIBUTOR,Fixing markdown typo,2018-02-02T10:12:07Z,1,2,0,4.303413075542279
99,16688,how to assign the GPU device using C++?,,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",0,,1,2018-02-02T09:14:57Z,2018-02-02T18:52:11Z,NONE,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",2018-02-02T18:52:08Z,0,1,0,1.803413075542279
100,16684,The link for the  tutorial on Google's Tensorflow SyntaxNet  page  gives 404 error,"stat:awaiting tensorflower,type:docs","Go tot the page 
https://www.tensorflow.org/versions/r0.12/tutorials/syntaxnet/

and click the ""tutorial"" link. It gets a 404 error.

The target of the link is
https://github.com/tensorflow/models/tree/master/syntaxnet#installation
",1,,4,2018-02-02T03:58:02Z,2018-02-03T04:49:04Z,NONE,"Go tot the page 
https://www.tensorflow.org/versions/r0.12/tutorials/syntaxnet/

and click the ""tutorial"" link. It gets a 404 error.

The target of the link is
https://github.com/tensorflow/models/tree/master/syntaxnet#installation
",2018-02-02T18:44:40Z,1,1,0,4.303413075542279
101,16683,Tensorflow 1.5: failed to use tf.keras.applications.MobileNet(),,"I updated my Tensorflow to 1.5, and I tried to run the codes as below:
`import tensorflow as tf`
`model = tf.keras.applications.MobileNet()`

But it raised an error  as below:
---------------------------------------------------------------------------
OSError                                   Traceback (most recent call last)
<ipython-input-14-2182e918e983> in <module>()
----> 1 model = tf.keras.applications.MobileNet()

/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/_impl/keras/applications/mobilenet.py in MobileNet(input_shape, alpha, depth_multiplier, dropout, include_top, weights, input_tensor, pooling, classes)
    538     K.set_image_data_format(old_data_format)
    539   elif weights is not None:
--> 540     model.load_weights(weights)
    541   return model
    542 

/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/_impl/keras/engine/topology.py in load_weights(self, filepath, by_name)
   1099     if h5py is None:
   1100       raise ImportError('`load_weights` requires h5py.')
-> 1101     f = h5py.File(filepath, mode='r')
   1102     if 'layer_names' not in f.attrs and 'model_weights' in f:
   1103       f = f['model_weights']

/usr/local/lib/python3.5/dist-packages/h5py/_hl/files.py in __init__(self, name, mode, driver, libver, userblock_size, swmr, **kwds)
    267             with phil:
    268                 fapl = make_fapl(driver, libver, **kwds)
--> 269                 fid = make_fid(name, mode, userblock_size, fapl, swmr=swmr)
    270 
    271                 if swmr_support:

/usr/local/lib/python3.5/dist-packages/h5py/_hl/files.py in make_fid(name, mode, userblock_size, fapl, fcpl, swmr)
     97         if swmr and swmr_support:
     98             flags |= h5f.ACC_SWMR_READ
---> 99         fid = h5f.open(name, flags, fapl=fapl)
    100     elif mode == 'r+':
    101         fid = h5f.open(name, h5f.ACC_RDWR, fapl=fapl)

h5py/_objects.pyx in h5py._objects.with_phil.wrapper()

h5py/_objects.pyx in h5py._objects.with_phil.wrapper()

h5py/h5f.pyx in h5py.h5f.open()

OSError: Unable to open file (unable to open file: name = 'imagenet', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)

It seems like that something wrong with h5py
I try to upgrade h5py but it's still invalid
This works well in Tensorflow 1.4
How should I resolve it with Tensorflow 1.5, thanks
",1,,5,2018-02-02T03:43:16Z,2018-02-07T01:11:21Z,NONE,"I updated my Tensorflow to 1.5, and I tried to run the codes as below:
`import tensorflow as tf`
`model = tf.keras.applications.MobileNet()`

But it raised an error  as below:
---------------------------------------------------------------------------
OSError                                   Traceback (most recent call last)
<ipython-input-14-2182e918e983> in <module>()
----> 1 model = tf.keras.applications.MobileNet()

/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/_impl/keras/applications/mobilenet.py in MobileNet(input_shape, alpha, depth_multiplier, dropout, include_top, weights, input_tensor, pooling, classes)
    538     K.set_image_data_format(old_data_format)
    539   elif weights is not None:
--> 540     model.load_weights(weights)
    541   return model
    542 

/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/_impl/keras/engine/topology.py in load_weights(self, filepath, by_name)
   1099     if h5py is None:
   1100       raise ImportError('`load_weights` requires h5py.')
-> 1101     f = h5py.File(filepath, mode='r')
   1102     if 'layer_names' not in f.attrs and 'model_weights' in f:
   1103       f = f['model_weights']

/usr/local/lib/python3.5/dist-packages/h5py/_hl/files.py in __init__(self, name, mode, driver, libver, userblock_size, swmr, **kwds)
    267             with phil:
    268                 fapl = make_fapl(driver, libver, **kwds)
--> 269                 fid = make_fid(name, mode, userblock_size, fapl, swmr=swmr)
    270 
    271                 if swmr_support:

/usr/local/lib/python3.5/dist-packages/h5py/_hl/files.py in make_fid(name, mode, userblock_size, fapl, fcpl, swmr)
     97         if swmr and swmr_support:
     98             flags |= h5f.ACC_SWMR_READ
---> 99         fid = h5f.open(name, flags, fapl=fapl)
    100     elif mode == 'r+':
    101         fid = h5f.open(name, h5f.ACC_RDWR, fapl=fapl)

h5py/_objects.pyx in h5py._objects.with_phil.wrapper()

h5py/_objects.pyx in h5py._objects.with_phil.wrapper()

h5py/h5f.pyx in h5py.h5f.open()

OSError: Unable to open file (unable to open file: name = 'imagenet', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)

It seems like that something wrong with h5py
I try to upgrade h5py but it's still invalid
This works well in Tensorflow 1.4
How should I resolve it with Tensorflow 1.5, thanks
",2018-02-02T17:31:01Z,5,1,1,4.803413075542279
102,16680,Branch 184220615,cla: yes,,0,,1,2018-02-02T01:59:52Z,2018-02-02T02:58:22Z,MEMBER,,2018-02-02T02:02:15Z,0,3,0,3.803413075542279
103,16678,Remove invalid exception in linear operator,cla: yes,Remove unreachable `NotImplementedError` exception from _assert_non_singular() in LinearOperator.,1,,1,2018-02-01T23:30:20Z,2018-02-06T00:15:22Z,CONTRIBUTOR,Remove unreachable `NotImplementedError` exception from _assert_non_singular() in LinearOperator.,2018-02-06T01:09:05Z,5,2,1,3.800101628500413
104,16674,Fix sanity build,cla: yes,"- [x] Fix build error
- [x] Update test
",0,,2,2018-02-01T19:00:57Z,2018-02-02T06:51:34Z,MEMBER,"- [x] Fix build error
- [x] Update test
",2018-02-02T00:54:42Z,1,3,0,4.300101628500413
105,16672,Updating the version to 1.6.0-rc0.,cla: yes,,0,,1,2018-02-01T18:30:59Z,2018-02-02T08:35:04Z,MEMBER,,2018-02-01T18:57:50Z,1,3,0,3.800101628500413
106,16670,Tensorflow 1.5.0 import error under CUDA 8.0,stat:awaiting response,"------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Windows 10
- **TensorFlow installed from (source or binary)**:
pip install tensorflow-gpu
- **TensorFlow version (use command below)**:
1.5.0
- **Python version**: 
3.5.4
- **CUDA/cuDNN version**:
8.0/6.0
- **GPU model and memory**:
GTX1080ti
- **Exact command to reproduce**:
pip install tensorflow-gpu
python
import tensorflow as tf

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Tensorflow 1.4 works fine with the same setup as described above. However, after upgrade tensorflow to 1.5 using pip install tensorflow-gpu, it fails to import tensorflow package in python.

### Source code / logs
(tensorflow_1_5) C:\WINDOWS\system32>python
Python 3.5.4 |Continuum Analytics, Inc.| (default, Aug 14 2017, 13:41:13) [MSC v.1900 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
Traceback (most recent call last):
  File ""C:\Program Files\Anaconda3\envs\tensorflow_1_5\lib\site-packages\tensorflow\python\platform\self_check.py"", line 75, in preload_check
    ctypes.WinDLL(build_info.cudart_dll_name)
  File ""C:\Program Files\Anaconda3\envs\tensorflow_1_5\lib\ctypes\__init__.py"", line 351, in __init__
    self._handle = _dlopen(self._name, mode)
OSError: [WinError 126] The specified module could not be found

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Program Files\Anaconda3\envs\tensorflow_1_5\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""C:\Program Files\Anaconda3\envs\tensorflow_1_5\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Program Files\Anaconda3\envs\tensorflow_1_5\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 30, in <module>
    self_check.preload_check()
  File ""C:\Program Files\Anaconda3\envs\tensorflow_1_5\lib\site-packages\tensorflow\python\platform\self_check.py"", line 82, in preload_check
    % (build_info.cudart_dll_name, build_info.cuda_version_number))
ImportError: Could not find 'cudart64_90.dll'. TensorFlow requires that this DLL be installed in a directory that is named in your %PATH% environment variable. Download and install CUDA 9.0 from this URL: https://developer.nvidia.com/cuda-toolkit",0,,19,2018-02-01T17:56:40Z,2018-02-05T07:30:03Z,NONE,"------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Windows 10
- **TensorFlow installed from (source or binary)**:
pip install tensorflow-gpu
- **TensorFlow version (use command below)**:
1.5.0
- **Python version**: 
3.5.4
- **CUDA/cuDNN version**:
8.0/6.0
- **GPU model and memory**:
GTX1080ti
- **Exact command to reproduce**:
pip install tensorflow-gpu
python
import tensorflow as tf

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Tensorflow 1.4 works fine with the same setup as described above. However, after upgrade tensorflow to 1.5 using pip install tensorflow-gpu, it fails to import tensorflow package in python.

### Source code / logs
(tensorflow_1_5) C:\WINDOWS\system32>python
Python 3.5.4 |Continuum Analytics, Inc.| (default, Aug 14 2017, 13:41:13) [MSC v.1900 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
Traceback (most recent call last):
  File ""C:\Program Files\Anaconda3\envs\tensorflow_1_5\lib\site-packages\tensorflow\python\platform\self_check.py"", line 75, in preload_check
    ctypes.WinDLL(build_info.cudart_dll_name)
  File ""C:\Program Files\Anaconda3\envs\tensorflow_1_5\lib\ctypes\__init__.py"", line 351, in __init__
    self._handle = _dlopen(self._name, mode)
OSError: [WinError 126] The specified module could not be found

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Program Files\Anaconda3\envs\tensorflow_1_5\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""C:\Program Files\Anaconda3\envs\tensorflow_1_5\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Program Files\Anaconda3\envs\tensorflow_1_5\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 30, in <module>
    self_check.preload_check()
  File ""C:\Program Files\Anaconda3\envs\tensorflow_1_5\lib\site-packages\tensorflow\python\platform\self_check.py"", line 82, in preload_check
    % (build_info.cudart_dll_name, build_info.cuda_version_number))
ImportError: Could not find 'cudart64_90.dll'. TensorFlow requires that this DLL be installed in a directory that is named in your %PATH% environment variable. Download and install CUDA 9.0 from this URL: https://developer.nvidia.com/cuda-toolkit",2018-02-01T19:00:34Z,4,1,1,10.800101628500412
107,16667,Adds parameter 'msg' to tf.TensorFlowTestCase.,cla: yes,"This commit adds a msg parameter that defaults to None to the following
functions:
- assertProtoEquals
- assertArrayNear
- assertNDArrayNear
- assertAllClose
- assertAllEqual
- assertShapeEqual
- assertDeviceEqual

Closes #15729.",1,,1,2018-02-01T17:20:58Z,2018-02-08T18:48:43Z,CONTRIBUTOR,"This commit adds a msg parameter that defaults to None to the following
functions:
- assertProtoEquals
- assertArrayNear
- assertNDArrayNear
- assertAllClose
- assertAllEqual
- assertShapeEqual
- assertDeviceEqual

Closes #15729.",2018-02-07T08:11:07Z,7,2,1,3.800101628500413
108,16662,"Current Bazel version is 0.10.0, expected at least 0.5.4",,"I get this error message when trying to build from source (r1.5) with the new bazel version published today.

Current Bazel version is 0.10.0, expected at least 0.5.4

I guess the version check is wrong. ",0,,10,2018-02-01T15:52:21Z,2018-02-01T21:40:12Z,NONE,"I get this error message when trying to build from source (r1.5) with the new bazel version published today.

Current Bazel version is 0.10.0, expected at least 0.5.4

I guess the version check is wrong. ",2018-02-01T16:51:44Z,0,1,0,6.300101628500413
109,16661,"Fix ""Define the model"" link.",cla: yes,"The link syntax was inverted, that is, round brackets were coming before square brackets, but Markdown doesn't like it.",0,,4,2018-02-01T15:38:23Z,2018-02-02T20:16:50Z,CONTRIBUTOR,"The link syntax was inverted, that is, round brackets were coming before square brackets, but Markdown doesn't like it.",2018-02-01T15:39:39Z,1,2,0,4.300101628500413
110,16658,Runtime Error with Qt GUI Application,"stat:awaiting response,type:bug/performance","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from source**:
- **TensorFlow version use master**:
- **Python version 2.7**: 
- **Bazel version 0.9.0**:
- **GCC/Compiler version 5.4.0**:
- **Without CUDA/cuDNN**:
- **Without GPU**:

### Describe the problem
When I used QtCreator to build GUI Application, if include ""tensorflow/core/lib/core/refcount.h"", it will throw The program has unexpectedly finished.

.pro like
####
    SOURCES += \
        main.cpp \
        mainwindow.cpp
    HEADERS += \
         mainwindow.h
    FORMS += \
         mainwindow.ui
    
    #tensorflow
    INCLUDEPATH += /home/face/Desktop/tensorflow/bazel-genfiles`
    INCLUDEPATH += /home/face/Desktop/tensorflow`
    INCLUDEPATH += /home/face/Desktop/tensorflow/tensorflow/contrib/makefile/gen/protobuf/include`
    INCLUDEPATH += /home/face/Desktop/tensorflow/tensorflow/contrib/makefile/downloads/nsync/public`
    INCLUDEPATH += /home/face/Desktop/eigen-eigen-5a0156e40feb`
    LIBS += -L/home/face/Desktop/tensorflow/bazel-bin/tensorflow -ltensorflow_cc -ltensorflow_framework

main.cpp
####
    #include ""mainwindow.h""
    #include <QApplication>
    #include <tensorflow/core/platform/env.h>
    #include <tensorflow/core/public/session.h>

    int main(int argc, char *argv[])
    {
        QApplication a(argc, argv);
        MainWindow w;
        w.show();
        return a.exec();
    }

then if ""tensorflow/core/lib/core/refcount.h"" line 79
####
    inline RefCounted::~RefCounted() {
        DCHECK_EQ(ref_.load(), 0); 
    }
to
####
    inline RefCounted::~RefCounted() {
        //DCHECK_EQ(ref_.load(), 0); 
    }
it will work.

### Source code / logs
debug log like:
####
    1  google::protobuf::internal::Mutex::Lock()                                    0x7fffde0c3516 
    2  google::protobuf::internal::OnShutdown(void ( *)())                          0x7fffde0c3833 
    3  call_init                                                     dl-init.c  72  0x7ffff7de76ba 
    4  call_init                                                     dl-init.c  30  0x7ffff7de77cb 
    5  _dl_init                                                      dl-init.c  120 0x7ffff7de77cb 
    6  dl_open_worker                                                dl-open.c  575 0x7ffff7dec8e2 
    7  _dl_catch_error                                               dl-error.c 187 0x7ffff7de7564 
    8  _dl_open                                                      dl-open.c  660 0x7ffff7debda9 
    9  dlopen_doit                                                   dlopen.c   66  0x7ffff18f0f09 
    10 _dl_catch_error                                               dl-error.c 187 0x7ffff7de7564 
    11 _dlerror_run                                                  dlerror.c  163 0x7ffff18f1571 
    12 __dlopen                                                      dlopen.c   87  0x7ffff18f0fa1 
    13 ??                                                                           0x7ffff33100e5 
    14 ??                                                                           0x7ffff3309975 
    15 QFactoryLoader::instance(int) const                                          0x7ffff32ff07e 
    16 QPlatformThemeFactory::create(QString const&, QString const&)                0x7ffff0b30231 
    17 QGuiApplicationPrivate::createPlatformIntegration()                          0x7ffff0b3aaf8 
    18 QGuiApplicationPrivate::createEventDispatcher()                              0x7ffff0b3b4bd 
    19 QCoreApplicationPrivate::init()                                              0x7ffff331ab3b 
    20 QGuiApplicationPrivate::init()                                               0x7ffff0b3cf7b 
    21 QApplicationPrivate::init()                                                  0x7ffff392d3b9 
    22 main                                                          main.cpp   103 0x402e3e  

",0,,3,2018-02-01T13:04:29Z,2018-02-02T12:56:35Z,NONE,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from source**:
- **TensorFlow version use master**:
- **Python version 2.7**: 
- **Bazel version 0.9.0**:
- **GCC/Compiler version 5.4.0**:
- **Without CUDA/cuDNN**:
- **Without GPU**:

### Describe the problem
When I used QtCreator to build GUI Application, if include ""tensorflow/core/lib/core/refcount.h"", it will throw The program has unexpectedly finished.

.pro like
####
    SOURCES += \
        main.cpp \
        mainwindow.cpp
    HEADERS += \
         mainwindow.h
    FORMS += \
         mainwindow.ui
    
    #tensorflow
    INCLUDEPATH += /home/face/Desktop/tensorflow/bazel-genfiles`
    INCLUDEPATH += /home/face/Desktop/tensorflow`
    INCLUDEPATH += /home/face/Desktop/tensorflow/tensorflow/contrib/makefile/gen/protobuf/include`
    INCLUDEPATH += /home/face/Desktop/tensorflow/tensorflow/contrib/makefile/downloads/nsync/public`
    INCLUDEPATH += /home/face/Desktop/eigen-eigen-5a0156e40feb`
    LIBS += -L/home/face/Desktop/tensorflow/bazel-bin/tensorflow -ltensorflow_cc -ltensorflow_framework

main.cpp
####
    #include ""mainwindow.h""
    #include <QApplication>
    #include <tensorflow/core/platform/env.h>
    #include <tensorflow/core/public/session.h>

    int main(int argc, char *argv[])
    {
        QApplication a(argc, argv);
        MainWindow w;
        w.show();
        return a.exec();
    }

then if ""tensorflow/core/lib/core/refcount.h"" line 79
####
    inline RefCounted::~RefCounted() {
        DCHECK_EQ(ref_.load(), 0); 
    }
to
####
    inline RefCounted::~RefCounted() {
        //DCHECK_EQ(ref_.load(), 0); 
    }
it will work.

### Source code / logs
debug log like:
####
    1  google::protobuf::internal::Mutex::Lock()                                    0x7fffde0c3516 
    2  google::protobuf::internal::OnShutdown(void ( *)())                          0x7fffde0c3833 
    3  call_init                                                     dl-init.c  72  0x7ffff7de76ba 
    4  call_init                                                     dl-init.c  30  0x7ffff7de77cb 
    5  _dl_init                                                      dl-init.c  120 0x7ffff7de77cb 
    6  dl_open_worker                                                dl-open.c  575 0x7ffff7dec8e2 
    7  _dl_catch_error                                               dl-error.c 187 0x7ffff7de7564 
    8  _dl_open                                                      dl-open.c  660 0x7ffff7debda9 
    9  dlopen_doit                                                   dlopen.c   66  0x7ffff18f0f09 
    10 _dl_catch_error                                               dl-error.c 187 0x7ffff7de7564 
    11 _dlerror_run                                                  dlerror.c  163 0x7ffff18f1571 
    12 __dlopen                                                      dlopen.c   87  0x7ffff18f0fa1 
    13 ??                                                                           0x7ffff33100e5 
    14 ??                                                                           0x7ffff3309975 
    15 QFactoryLoader::instance(int) const                                          0x7ffff32ff07e 
    16 QPlatformThemeFactory::create(QString const&, QString const&)                0x7ffff0b30231 
    17 QGuiApplicationPrivate::createPlatformIntegration()                          0x7ffff0b3aaf8 
    18 QGuiApplicationPrivate::createEventDispatcher()                              0x7ffff0b3b4bd 
    19 QCoreApplicationPrivate::init()                                              0x7ffff331ab3b 
    20 QGuiApplicationPrivate::init()                                               0x7ffff0b3cf7b 
    21 QApplicationPrivate::init()                                                  0x7ffff392d3b9 
    22 main                                                          main.cpp   103 0x402e3e  

",2018-02-02T03:49:17Z,1,1,0,2.800101628500413
111,16657,Tensorflow on banana-pi m64,,"How to install TF on banana m64? OS: Linux bpi-iot-ros-ai 3.10.105-BPI-M64-Kernel 

When i trying install i have an error

> tensorflow-1.5.0-cp34-none-any.whl is not a supported wheel on this platform.
",0,,1,2018-02-01T12:14:54Z,2018-02-01T18:01:59Z,NONE,"How to install TF on banana m64? OS: Linux bpi-iot-ros-ai 3.10.105-BPI-M64-Kernel 

When i trying install i have an error

> tensorflow-1.5.0-cp34-none-any.whl is not a supported wheel on this platform.
",2018-02-01T18:01:59Z,0,1,0,1.8001016285004132
112,16654,Bazel version comparison fails with bazel 0.10.0,,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No, just commented 6 lines in the bzl files out
- **OS Platform and Distribution**: 16.04 on Jetson TX2
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.5
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: 0.10.0
- **GCC/Compiler version (if compiling from source)**: 5.4.0
- **CUDA/cuDNN version**: 9.0
- **GPU model and memory**: TX2 GPU, 5GB (not sure)
- **Exact command to reproduce**: bazel build -c opt --local_resources 3072,4.0,1.0 --verbose_failures --config=cuda //tensorflow/tools/pip_package:build_pip_package

### Describe the problem
I cannot build tensorflow using bazel 0.10.0. It seems like the version checks in repositories.bzl and wokspace.bzl fail. Commenting them out solves the issue, even though I know that is no persistent solution. I think it is simply that bazel thinks that 0.10.0 is smaller 0.5.4 due to its string comparison, but I am no bazel expert.

### Source code / logs
ERROR: /home/nvidia/git/tensorflow/WORKSPACE:15:1: Traceback (most recent call last):
	File ""/home/nvidia/git/tensorflow/WORKSPACE"", line 15
		closure_repositories()
	File ""/home/nvidia/.cache/bazel/_bazel_nvidia/01c445c7b00bca0241913a79fcd99718/external/io_bazel_rules_closure/closure/repositories.bzl"", line 69, in closure_repositories
		_check_bazel_version(""Closure Rules"", ""0.4.5"")
	File ""/home/nvidia/.cache/bazel/_bazel_nvidia/01c445c7b00bca0241913a79fcd99718/external/io_bazel_rules_closure/closure/repositories.bzl"", line 172, in _check_bazel_version
		fail((""%s requires Bazel >=%s but was...)))
Closure Rules requires Bazel >=0.4.5 but was 0.10.0- (@non-git)
ERROR: Error evaluating WORKSPACE file
ERROR: /home/nvidia/git/tensorflow/WORKSPACE:41:1: Traceback (most recent call last):
	File ""/home/nvidia/git/tensorflow/WORKSPACE"", line 41
		tf_workspace()
	File ""/home/nvidia/git/tensorflow/tensorflow/workspace.bzl"", line 48, in tf_workspace
		check_version(""0.5.4"")
	File ""/home/nvidia/git/tensorflow/tensorflow/workspace.bzl"", line 38, in check_version
		fail(""\nCurrent Bazel version is {}, ...))

Current Bazel version is 0.10.0- (@non-git), expected at least 0.5.4
ERROR: Error evaluating WORKSPACE file
ERROR: Skipping '//tensorflow/tools/pip_package:build_pip_package': error loading package 'external': Package 'external' contains errors
WARNING: Target pattern parsing failed.
ERROR: error loading package 'external': Package 'external' contains errors
INFO: Elapsed time: 3.145s
FAILED: Build did NOT complete successfully (0 packages loaded)
",0,,14,2018-02-01T10:38:27Z,2018-02-01T13:36:45Z,NONE,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No, just commented 6 lines in the bzl files out
- **OS Platform and Distribution**: 16.04 on Jetson TX2
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.5
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: 0.10.0
- **GCC/Compiler version (if compiling from source)**: 5.4.0
- **CUDA/cuDNN version**: 9.0
- **GPU model and memory**: TX2 GPU, 5GB (not sure)
- **Exact command to reproduce**: bazel build -c opt --local_resources 3072,4.0,1.0 --verbose_failures --config=cuda //tensorflow/tools/pip_package:build_pip_package

### Describe the problem
I cannot build tensorflow using bazel 0.10.0. It seems like the version checks in repositories.bzl and wokspace.bzl fail. Commenting them out solves the issue, even though I know that is no persistent solution. I think it is simply that bazel thinks that 0.10.0 is smaller 0.5.4 due to its string comparison, but I am no bazel expert.

### Source code / logs
ERROR: /home/nvidia/git/tensorflow/WORKSPACE:15:1: Traceback (most recent call last):
	File ""/home/nvidia/git/tensorflow/WORKSPACE"", line 15
		closure_repositories()
	File ""/home/nvidia/.cache/bazel/_bazel_nvidia/01c445c7b00bca0241913a79fcd99718/external/io_bazel_rules_closure/closure/repositories.bzl"", line 69, in closure_repositories
		_check_bazel_version(""Closure Rules"", ""0.4.5"")
	File ""/home/nvidia/.cache/bazel/_bazel_nvidia/01c445c7b00bca0241913a79fcd99718/external/io_bazel_rules_closure/closure/repositories.bzl"", line 172, in _check_bazel_version
		fail((""%s requires Bazel >=%s but was...)))
Closure Rules requires Bazel >=0.4.5 but was 0.10.0- (@non-git)
ERROR: Error evaluating WORKSPACE file
ERROR: /home/nvidia/git/tensorflow/WORKSPACE:41:1: Traceback (most recent call last):
	File ""/home/nvidia/git/tensorflow/WORKSPACE"", line 41
		tf_workspace()
	File ""/home/nvidia/git/tensorflow/tensorflow/workspace.bzl"", line 48, in tf_workspace
		check_version(""0.5.4"")
	File ""/home/nvidia/git/tensorflow/tensorflow/workspace.bzl"", line 38, in check_version
		fail(""\nCurrent Bazel version is {}, ...))

Current Bazel version is 0.10.0- (@non-git), expected at least 0.5.4
ERROR: Error evaluating WORKSPACE file
ERROR: Skipping '//tensorflow/tools/pip_package:build_pip_package': error loading package 'external': Package 'external' contains errors
WARNING: Target pattern parsing failed.
ERROR: error loading package 'external': Package 'external' contains errors
INFO: Elapsed time: 3.145s
FAILED: Build did NOT complete successfully (0 packages loaded)
",2018-02-01T11:53:15Z,0,1,0,8.300101628500414
113,16653,Fix docs,cla: yes,* Fix xcode path error,0,,3,2018-02-01T10:24:43Z,2018-02-03T00:39:18Z,CONTRIBUTOR,* Fix xcode path error,2018-02-01T10:31:45Z,2,2,0,3.800101628500413
114,16652,v1.3 batch_norm layer,,"I use the batch norm layer like this:
`def batch_norm_layer(x,train_phase,scope_bn):

	bn_train = batch_norm(x, decay=0.999, center=True, scale=True,
	is_training=True,
	reuse=None, # is this right?
	trainable=True,
	scope=scope_bn)
	bn_inference = batch_norm(x, decay=0.999, center=True, scale=True,
	is_training=False,
	reuse=True, # is this right?
	trainable=True,
	scope=scope_bn)
	z = tf.cond(train_phase, lambda: bn_train, lambda: bn_inference)
	return z`
I don't know in v1.3.0 is the code worked?
I saw the [issue1122](https://github.com/tensorflow/tensorflow/issues/1122#issuecomment-232535426), someone said it would not work well.

thank you in advance.",0,,1,2018-02-01T10:02:01Z,2018-02-02T02:10:41Z,NONE,"I use the batch norm layer like this:
`def batch_norm_layer(x,train_phase,scope_bn):

	bn_train = batch_norm(x, decay=0.999, center=True, scale=True,
	is_training=True,
	reuse=None, # is this right?
	trainable=True,
	scope=scope_bn)
	bn_inference = batch_norm(x, decay=0.999, center=True, scale=True,
	is_training=False,
	reuse=True, # is this right?
	trainable=True,
	scope=scope_bn)
	z = tf.cond(train_phase, lambda: bn_train, lambda: bn_inference)
	return z`
I don't know in v1.3.0 is the code worked?
I saw the [issue1122](https://github.com/tensorflow/tensorflow/issues/1122#issuecomment-232535426), someone said it would not work well.

thank you in advance.",2018-02-02T02:10:41Z,1,1,0,1.8001016285004132
115,16651,Temporarily remove three linter checks for now.,cla: yes,"  # C0330 bad-continuation
  # C0301 line-too-long
  # C0326 bad-whitespace
Will fix the following 25 error and add them back:
tensorflow/contrib/session_bundle/bundle_shim.py:85: [C0301(line-too-long), ] Line too long (83/80)

tensorflow/contrib/session_bundle/bundle_shim.py:94: [C0301(line-too-long), ] Line too long (89/80)

tensorflow/contrib/session_bundle/bundle_shim.py:135: [C0301(line-too-long), ] Line too long (81/80)

tensorflow/contrib/session_bundle/bundle_shim.py:136: [C0301(line-too-long), ] Line too long (81/80)

tensorflow/contrib/kafka/python/ops/kafka_dataset_ops.py:33: [C0301(line-too-long), ] Line too long (85/80)

tensorflow/contrib/tpu/profiler/pip_package/cloud_tpu_profiler/main.py:29: [C0330(bad-continuation), ] Wrong continued indentation (remove 3 spaces).

tensorflow/contrib/tpu/profiler/pip_package/cloud_tpu_profiler/main.py:31: [C0330(bad-continuation), ] Wrong continued indentation (remove 3 spaces).

tensorflow/contrib/tpu/profiler/pip_package/cloud_tpu_profiler/main.py:35: [C0330(bad-continuation), ] Wrong continued indentation (remove 3 spaces).

tensorflow/contrib/learn/python/learn/datasets/synthetic_test.py:139: [E0102(function-redefined), SyntheticTest.test_spirals] method already defined line 92

tensorflow/contrib/layers/python/layers/layers.py:63: [C0330(bad-continuation), ] Wrong hanging indentation (remove 7 spaces).

tensorflow/contrib/layers/python/layers/layers.py:1421: [C0301(line-too-long), ] Line too long (104/80)

tensorflow/contrib/py2tf/impl/api.py:89: [C0301(line-too-long), ] Line too long (81/80)

tensorflow/contrib/rnn/python/kernel_tests/core_rnn_cell_test.py:160: [C0326(bad-whitespace), ] Exactly one space required after comma

tensorflow/contrib/ndlstm/python/lstm1d.py:91: [C0330(bad-continuation), ] Wrong hanging indentation (add 2 spaces).

tensorflow/contrib/rnn/python/kernel_tests/rnn_cell_test.py:1639: [E0102(function-redefined), WeightNormLSTMCellTest] class already defined line 1548

tensorflow/contrib/gan/python/eval/python/classifier_metrics_impl.py:209: [C0301(line-too-long), ] Line too long (98/80)

tensorflow/contrib/gan/python/eval/python/classifier_metrics_impl.py:209: [E1124(redundant-keyword-arg), get_graph_def_from_url_tarball] Argument 'filename' passed by position and keyword in function call

tensorflow/contrib/layers/python/layers/layers_test.py:1311: [C0301(line-too-long), ] Line too long (89/80)

tensorflow/python/kernel_tests/tensordot_op_test.py:108: [C0326(bad-whitespace), ] Exactly one space required after comma

tensorflow/python/data/util/nest.py:482: [C0301(line-too-long), ] Line too long (81/80)

tensorflow/python/data/ops/dataset_ops.py:909: [C0301(line-too-long), ] Line too long (88/80)

tensorflow/python/ops/image_ops_impl.py:1694: [C0301(line-too-long), ] Line too long (87/80)

tensorflow/python/ops/image_ops_impl.py:1720: [C0301(line-too-long), ] Line too long (87/80)

tensorflow/python/ops/image_ops_impl.py:1745: [C0301(line-too-long), ] Line too long (87/80)

tensorflow/python/ops/image_ops_impl.py:1771: [C0301(line-too-long), ] Line too long (87/80)",0,,1,2018-02-01T09:55:53Z,2018-02-01T18:03:32Z,MEMBER,"  # C0330 bad-continuation
  # C0301 line-too-long
  # C0326 bad-whitespace
Will fix the following 25 error and add them back:
tensorflow/contrib/session_bundle/bundle_shim.py:85: [C0301(line-too-long), ] Line too long (83/80)

tensorflow/contrib/session_bundle/bundle_shim.py:94: [C0301(line-too-long), ] Line too long (89/80)

tensorflow/contrib/session_bundle/bundle_shim.py:135: [C0301(line-too-long), ] Line too long (81/80)

tensorflow/contrib/session_bundle/bundle_shim.py:136: [C0301(line-too-long), ] Line too long (81/80)

tensorflow/contrib/kafka/python/ops/kafka_dataset_ops.py:33: [C0301(line-too-long), ] Line too long (85/80)

tensorflow/contrib/tpu/profiler/pip_package/cloud_tpu_profiler/main.py:29: [C0330(bad-continuation), ] Wrong continued indentation (remove 3 spaces).

tensorflow/contrib/tpu/profiler/pip_package/cloud_tpu_profiler/main.py:31: [C0330(bad-continuation), ] Wrong continued indentation (remove 3 spaces).

tensorflow/contrib/tpu/profiler/pip_package/cloud_tpu_profiler/main.py:35: [C0330(bad-continuation), ] Wrong continued indentation (remove 3 spaces).

tensorflow/contrib/learn/python/learn/datasets/synthetic_test.py:139: [E0102(function-redefined), SyntheticTest.test_spirals] method already defined line 92

tensorflow/contrib/layers/python/layers/layers.py:63: [C0330(bad-continuation), ] Wrong hanging indentation (remove 7 spaces).

tensorflow/contrib/layers/python/layers/layers.py:1421: [C0301(line-too-long), ] Line too long (104/80)

tensorflow/contrib/py2tf/impl/api.py:89: [C0301(line-too-long), ] Line too long (81/80)

tensorflow/contrib/rnn/python/kernel_tests/core_rnn_cell_test.py:160: [C0326(bad-whitespace), ] Exactly one space required after comma

tensorflow/contrib/ndlstm/python/lstm1d.py:91: [C0330(bad-continuation), ] Wrong hanging indentation (add 2 spaces).

tensorflow/contrib/rnn/python/kernel_tests/rnn_cell_test.py:1639: [E0102(function-redefined), WeightNormLSTMCellTest] class already defined line 1548

tensorflow/contrib/gan/python/eval/python/classifier_metrics_impl.py:209: [C0301(line-too-long), ] Line too long (98/80)

tensorflow/contrib/gan/python/eval/python/classifier_metrics_impl.py:209: [E1124(redundant-keyword-arg), get_graph_def_from_url_tarball] Argument 'filename' passed by position and keyword in function call

tensorflow/contrib/layers/python/layers/layers_test.py:1311: [C0301(line-too-long), ] Line too long (89/80)

tensorflow/python/kernel_tests/tensordot_op_test.py:108: [C0326(bad-whitespace), ] Exactly one space required after comma

tensorflow/python/data/util/nest.py:482: [C0301(line-too-long), ] Line too long (81/80)

tensorflow/python/data/ops/dataset_ops.py:909: [C0301(line-too-long), ] Line too long (88/80)

tensorflow/python/ops/image_ops_impl.py:1694: [C0301(line-too-long), ] Line too long (87/80)

tensorflow/python/ops/image_ops_impl.py:1720: [C0301(line-too-long), ] Line too long (87/80)

tensorflow/python/ops/image_ops_impl.py:1745: [C0301(line-too-long), ] Line too long (87/80)

tensorflow/python/ops/image_ops_impl.py:1771: [C0301(line-too-long), ] Line too long (87/80)",2018-02-01T10:22:31Z,0,3,0,3.800101628500413
116,16646,can tf.estimator.Estimator's  parameters be modified by hand? ,stat:awaiting response,"TF's  high level API  is very convenient to defined a new model. 
However, many DNN Machine Learning task has to reuse some old model's parameter to fill a new model and then  fine-tune it in new tasks. 
I have read the tf.estimator.Estimator'API  carefully, but cann't find any API to set It's parameters. Hope  TF developer  add this function to the high level API. 
Thank very much!",1,,7,2018-02-01T06:34:16Z,2018-02-02T19:11:42Z,NONE,"TF's  high level API  is very convenient to defined a new model. 
However, many DNN Machine Learning task has to reuse some old model's parameter to fill a new model and then  fine-tune it in new tasks. 
I have read the tf.estimator.Estimator'API  carefully, but cann't find any API to set It's parameters. Hope  TF developer  add this function to the high level API. 
Thank very much!",2018-02-02T02:08:28Z,1,1,0,5.800101628500413
117,16645,What's the difference between Univariate prediction and Multivariate prediction?,,My understanding is that paramenters of neurals are shared in Multivariate prediction and they can learn some correlations between series. There is less training time in Multivariate prediction. I wonder if that's right. Could you please explain any basic principles of Multivariate prediction with LSTM or recommend related papers to me? Thank you.,0,,1,2018-02-01T05:56:36Z,2018-02-01T18:02:23Z,NONE,My understanding is that paramenters of neurals are shared in Multivariate prediction and they can learn some correlations between series. There is less training time in Multivariate prediction. I wonder if that's right. Could you please explain any basic principles of Multivariate prediction with LSTM or recommend related papers to me? Thank you.,2018-02-01T18:02:23Z,0,1,0,1.8001016285004132
118,16643,Resolve pylint issues in image ops,cla: yes,There were 4 line too long errors reported in pylint check of image_ops_impl.py.  Changed the format to not exceed 80 characters based on indentation examples in [style guide](https://google.github.io/styleguide/pyguide.html#Indentation).  Reran pylint against the file and verified errors no longer listed for image_ops_impl.py.,0,,3,2018-02-01T04:09:42Z,2018-02-01T18:20:43Z,CONTRIBUTOR,There were 4 line too long errors reported in pylint check of image_ops_impl.py.  Changed the format to not exceed 80 characters based on indentation examples in [style guide](https://google.github.io/styleguide/pyguide.html#Indentation).  Reran pylint against the file and verified errors no longer listed for image_ops_impl.py.,2018-02-01T17:48:53Z,0,2,0,3.800101628500413
119,16636,Add relnote about bug in ptxas in CUDA 9 and 9.1.,cla: yes,,0,,3,2018-02-01T01:56:29Z,2018-02-01T03:15:45Z,MEMBER,,2018-02-01T01:57:15Z,0,3,0,4.800101628500413
120,16635,Update ISSUE_TEMPLATE.md,cla: yes,Fixes 16350,0,,1,2018-02-01T01:45:57Z,2018-02-01T02:58:24Z,CONTRIBUTOR,Fixes 16350,2018-02-08T01:03:07Z,0,2,0,2.800101628500413
121,16634,Fixes issues in tf.contrib.keras.utils.Progbar,cla: yes,"In version 1.5 of Tensorflow, if Progbar's target is set to None, internally it gets set to -1.
Changed code that referenced self.target is not None to self.target != -1, self.target is None to self.target == -1.
Also, ProgBar is printing twice as reported here https://github.com/tensorflow/tensorflow/issues/16538. Fixed duplicate text",1,,2,2018-02-01T00:35:02Z,2018-02-13T22:15:56Z,CONTRIBUTOR,"In version 1.5 of Tensorflow, if Progbar's target is set to None, internally it gets set to -1.
Changed code that referenced self.target is not None to self.target != -1, self.target is None to self.target == -1.
Also, ProgBar is printing twice as reported here https://github.com/tensorflow/tensorflow/issues/16538. Fixed duplicate text",2018-02-13T19:57:14Z,12,2,2,4.300101628500413
122,16631,Allow variable_overwrites on scope level,,"This is a request for allowing to pass a dict of `variable_overwrites` to variable scopes which to be returned when `tf.get_variable` is called instead of the usual procedure, if they are provided, otherwise do the usual procedure. A simple example of this beahviour is:
```
    import numpy as np
    with tf.variable_scope(""one""):
        a = tf.ones((5, 5), tf.float32)
        with tf.variable_scope(""two""):
            x1 = tf.get_variable(""x"", initializer=np.random.randn(5, 5).astype(""float32""))
            c1 = tf.sqrt(tf.abs(x1 + a))

    variables_overwrites = {x1._shared_name: c1}
    with tf.variable_scope(""one"", reuse=tf.AUTO_REUSE, variables_overwrites=variables_overwrites):
        a = tf.ones((5, 5), tf.float32)
        with tf.variable_scope(""two""):
           // x2 here is in fact the value of c1
            x2 = tf.get_variable(""x"", initializer=np.random.randn(5, 5).astype(""float32""))
            c2 = tf.sqrt(tf.abs(x2 + a))
```
This is particularly usefull for being able easily to bootstrap neural network parameters coming from inside the layers trough a standard function interface. My specific usage is for HMC for NN parameters. This is a question on whether you guys are interested in this so that I spend more time on doing this properly.",0,,5,2018-01-31T22:24:53Z,2018-02-01T00:27:35Z,NONE,"This is a request for allowing to pass a dict of `variable_overwrites` to variable scopes which to be returned when `tf.get_variable` is called instead of the usual procedure, if they are provided, otherwise do the usual procedure. A simple example of this beahviour is:
```
    import numpy as np
    with tf.variable_scope(""one""):
        a = tf.ones((5, 5), tf.float32)
        with tf.variable_scope(""two""):
            x1 = tf.get_variable(""x"", initializer=np.random.randn(5, 5).astype(""float32""))
            c1 = tf.sqrt(tf.abs(x1 + a))

    variables_overwrites = {x1._shared_name: c1}
    with tf.variable_scope(""one"", reuse=tf.AUTO_REUSE, variables_overwrites=variables_overwrites):
        a = tf.ones((5, 5), tf.float32)
        with tf.variable_scope(""two""):
           // x2 here is in fact the value of c1
            x2 = tf.get_variable(""x"", initializer=np.random.randn(5, 5).astype(""float32""))
            c2 = tf.sqrt(tf.abs(x2 + a))
```
This is particularly usefull for being able easily to bootstrap neural network parameters coming from inside the layers trough a standard function interface. My specific usage is for HMC for NN parameters. This is a question on whether you guys are interested in this so that I spend more time on doing this properly.",2018-01-31T23:00:57Z,0,1,0,3.800101628500413
123,16628,Tensorflow switches to CPU when using Variable.assign,,"### System information
- **OS**:Windows 10
- **TensorFlow installed from**: binary
- **TensorFlow version (use command below)**: 1.4.0
- **Python version**: 3.6.4
- **CUDA/cuDNN version**: 8.0 / 64
- **GPU model and memory**: GeForce GTX 1080 8 GB
- **Exact command to reproduce**: run the provided code below

### Describe the problem
I'm using a `tf.Variable` for the learning rate of a optimizer. If I change its value with `sess.run(var.assign(0.1))` the performance drops extremly and it seems tensorflow switches from GPU use to only CPU use (no workload on the GPU and more load on the CPU).

### Source code / logs

I did write a minimal working example (training a network with XOR). **To see the difference just comment the line `sess.run(learning_rate.assign(0.1))` out** and it will run much much faster using the GPU.

```
import tensorflow as tf


def XOR(x_, y_):
    Theta1 = tf.Variable(tf.random_uniform([2, 2], -1, 1), name=""Theta1"")
    Theta2 = tf.Variable(tf.random_uniform([2, 1], -1, 1), name=""Theta2"")

    Bias1 = tf.Variable(tf.zeros([2]), name=""Bias1"")
    Bias2 = tf.Variable(tf.zeros([1]), name=""Bias2"")

    with tf.name_scope(""layer2""):
        A2 = tf.sigmoid(tf.matmul(x_, Theta1) + Bias1)

    with tf.name_scope(""layer3""):
        Hypothesis = tf.sigmoid(tf.matmul(A2, Theta2) + Bias2)

    with tf.name_scope(""cost""):
        cost = tf.reduce_mean(((y_ * tf.log(Hypothesis)) +
                               ((1 - y_) * tf.log(1.0 - Hypothesis))) * -1)

    return cost


if __name__ == ""__main__"":
    x_ = tf.placeholder(dtype=tf.float32, shape=[4, 2], name='x-input')
    y_ = tf.placeholder(dtype=tf.float32, shape=[4, 1], name='y-input')
    xor_cost = XOR(x_, y_)

    learning_rate = tf.Variable(0.1, dtype=tf.float32)

    with tf.name_scope(""train""):
        train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(xor_cost)

    XOR_X = [[0, 0], [0, 1], [1, 0], [1, 1]]
    XOR_Y = [[0], [1], [1], [0]]

    sess = tf.Session()
    init = tf.global_variables_initializer()
    sess.run(init)

    for i in range(40001):
        sess.run(learning_rate.assign(0.1))

        _, loss = sess.run(fetches=[train_step, xor_cost], feed_dict={x_: XOR_X, y_: XOR_Y})

        if i % 100 == 0:
            print('iteration: {0:5}, loss: {1:15.10f}'.format(i, loss))


```",0,,2,2018-01-31T18:17:24Z,2018-01-31T19:20:50Z,NONE,"### System information
- **OS**:Windows 10
- **TensorFlow installed from**: binary
- **TensorFlow version (use command below)**: 1.4.0
- **Python version**: 3.6.4
- **CUDA/cuDNN version**: 8.0 / 64
- **GPU model and memory**: GeForce GTX 1080 8 GB
- **Exact command to reproduce**: run the provided code below

### Describe the problem
I'm using a `tf.Variable` for the learning rate of a optimizer. If I change its value with `sess.run(var.assign(0.1))` the performance drops extremly and it seems tensorflow switches from GPU use to only CPU use (no workload on the GPU and more load on the CPU).

### Source code / logs

I did write a minimal working example (training a network with XOR). **To see the difference just comment the line `sess.run(learning_rate.assign(0.1))` out** and it will run much much faster using the GPU.

```
import tensorflow as tf


def XOR(x_, y_):
    Theta1 = tf.Variable(tf.random_uniform([2, 2], -1, 1), name=""Theta1"")
    Theta2 = tf.Variable(tf.random_uniform([2, 1], -1, 1), name=""Theta2"")

    Bias1 = tf.Variable(tf.zeros([2]), name=""Bias1"")
    Bias2 = tf.Variable(tf.zeros([1]), name=""Bias2"")

    with tf.name_scope(""layer2""):
        A2 = tf.sigmoid(tf.matmul(x_, Theta1) + Bias1)

    with tf.name_scope(""layer3""):
        Hypothesis = tf.sigmoid(tf.matmul(A2, Theta2) + Bias2)

    with tf.name_scope(""cost""):
        cost = tf.reduce_mean(((y_ * tf.log(Hypothesis)) +
                               ((1 - y_) * tf.log(1.0 - Hypothesis))) * -1)

    return cost


if __name__ == ""__main__"":
    x_ = tf.placeholder(dtype=tf.float32, shape=[4, 2], name='x-input')
    y_ = tf.placeholder(dtype=tf.float32, shape=[4, 1], name='y-input')
    xor_cost = XOR(x_, y_)

    learning_rate = tf.Variable(0.1, dtype=tf.float32)

    with tf.name_scope(""train""):
        train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(xor_cost)

    XOR_X = [[0, 0], [0, 1], [1, 0], [1, 1]]
    XOR_Y = [[0], [1], [1], [0]]

    sess = tf.Session()
    init = tf.global_variables_initializer()
    sess.run(init)

    for i in range(40001):
        sess.run(learning_rate.assign(0.1))

        _, loss = sess.run(fetches=[train_step, xor_cost], feed_dict={x_: XOR_X, y_: XOR_Y})

        if i % 100 == 0:
            print('iteration: {0:5}, loss: {1:15.10f}'.format(i, loss))


```",2018-01-31T19:05:22Z,0,1,0,2.300101628500413
124,16627,"TF consumes all available RAM with a particular combination of conv2d, batch_norm and LSTM",,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: v1.5.0-0-g37aa430d84 1.5.0
- **Python version**: 3.6.4
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: 9.0.176 / 7.0.5
- **GPU model and memory**: Tesla V100 (AWS P3), 16GB
- **Exact command to reproduce**: python debug_tf.py

### Describe the problem
When the following code is run on a p3.2xlarge, the python process starts consuming RAM indefinitely, until the entire server RAM is used and the server dies. That's system RAM, not GPU memory. 

It doesn't look it, but the code below is the smallest I could find that produces the bad behavior.

1. Replacing residual_conv with a simple convolution makes the code work (i.e not hang).
2. Reducing the repetition number in `layers.repeat` (to e.g 2) makes the code work.
3. Removing the batch normalization from residual_conv makes the code work.
4. Removing the LSTM makes the code work.
5. The weirdest of all, if I set `is_training=True` in batch_norm instead of `is_training=is_training_var` whose value is set to `True` in the feed_dict, then the code works.

The same code runs successfully on a AWS P2 server with TensorFlow 1.3.

### Source code / logs

```python
import numpy as np
import tensorflow as tf
import tensorflow.contrib.layers as layers
from tensorflow.contrib.framework import arg_scope


def residual_conv(incoming, num_filters, scope, bn=True):
	with tf.variable_scope(scope):
	    input_filters = incoming.get_shape().as_list()[-1]
	    if input_filters != num_filters:
	        incoming = layers.conv2d(incoming, num_filters, scope='adjust_conv')

	    after_conv1 = layers.conv2d(incoming, num_filters)
	    after_conv2 = layers.conv2d(after_conv1, num_filters, normalizer_fn=None, activation_fn=None)

	    net = incoming + after_conv2

	    if bn:
	        net = layers.batch_norm(net)

	    return net

def tf_bilstm(incoming, n_units, name):
	net = incoming

	lstm_f = tf.contrib.rnn.LSTMCell(n_units)
	lstm_b = tf.contrib.rnn.LSTMCell(n_units)

	with tf.variable_scope(name):
	    results, _ = tf.nn.bidirectional_dynamic_rnn(lstm_f, lstm_b, net, 
	                                                 dtype=tf.float32, time_major=True)
	return tf.concat(results, axis=2)

def main():
	x_var = tf.placeholder(dtype=tf.float32, shape=(None, None, None, 512))
	is_training_var = tf.placeholder(dtype=bool)

	net = x_var

	with arg_scope([layers.batch_norm], is_training=is_training_var, decay=0.99, scale=True
	               ), \
	     arg_scope([layers.conv2d], padding='SAME', kernel_size=(3, 3)), \
	     arg_scope([layers.max_pool2d], padding='SAME', kernel_size=(2, 2), stride=(2, 2)):

	    net = layers.repeat(net, 20, residual_conv, 64, scope='block1')
	    net = layers.max_pool2d(net)

	    net = tf.squeeze(net, 2)
	    net = tf.transpose(net, [1, 0, 2])
	    net = tf_bilstm(net, 512, 'lstm1')

	sess = tf.Session()
	sess.run(tf.global_variables_initializer())
	
	X = np.zeros([10, 100, 2, 512])

	print('Right before TF call!')

	a = sess.run(net, {x_var: X, is_training_var: True})

	print(a)


if __name__ == '__main__':
	main()
```

Edited: Simplified the code.",0,,10,2018-01-31T17:26:23Z,2018-02-09T17:40:53Z,NONE,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: v1.5.0-0-g37aa430d84 1.5.0
- **Python version**: 3.6.4
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: 9.0.176 / 7.0.5
- **GPU model and memory**: Tesla V100 (AWS P3), 16GB
- **Exact command to reproduce**: python debug_tf.py

### Describe the problem
When the following code is run on a p3.2xlarge, the python process starts consuming RAM indefinitely, until the entire server RAM is used and the server dies. That's system RAM, not GPU memory. 

It doesn't look it, but the code below is the smallest I could find that produces the bad behavior.

1. Replacing residual_conv with a simple convolution makes the code work (i.e not hang).
2. Reducing the repetition number in `layers.repeat` (to e.g 2) makes the code work.
3. Removing the batch normalization from residual_conv makes the code work.
4. Removing the LSTM makes the code work.
5. The weirdest of all, if I set `is_training=True` in batch_norm instead of `is_training=is_training_var` whose value is set to `True` in the feed_dict, then the code works.

The same code runs successfully on a AWS P2 server with TensorFlow 1.3.

### Source code / logs

```python
import numpy as np
import tensorflow as tf
import tensorflow.contrib.layers as layers
from tensorflow.contrib.framework import arg_scope


def residual_conv(incoming, num_filters, scope, bn=True):
	with tf.variable_scope(scope):
	    input_filters = incoming.get_shape().as_list()[-1]
	    if input_filters != num_filters:
	        incoming = layers.conv2d(incoming, num_filters, scope='adjust_conv')

	    after_conv1 = layers.conv2d(incoming, num_filters)
	    after_conv2 = layers.conv2d(after_conv1, num_filters, normalizer_fn=None, activation_fn=None)

	    net = incoming + after_conv2

	    if bn:
	        net = layers.batch_norm(net)

	    return net

def tf_bilstm(incoming, n_units, name):
	net = incoming

	lstm_f = tf.contrib.rnn.LSTMCell(n_units)
	lstm_b = tf.contrib.rnn.LSTMCell(n_units)

	with tf.variable_scope(name):
	    results, _ = tf.nn.bidirectional_dynamic_rnn(lstm_f, lstm_b, net, 
	                                                 dtype=tf.float32, time_major=True)
	return tf.concat(results, axis=2)

def main():
	x_var = tf.placeholder(dtype=tf.float32, shape=(None, None, None, 512))
	is_training_var = tf.placeholder(dtype=bool)

	net = x_var

	with arg_scope([layers.batch_norm], is_training=is_training_var, decay=0.99, scale=True
	               ), \
	     arg_scope([layers.conv2d], padding='SAME', kernel_size=(3, 3)), \
	     arg_scope([layers.max_pool2d], padding='SAME', kernel_size=(2, 2), stride=(2, 2)):

	    net = layers.repeat(net, 20, residual_conv, 64, scope='block1')
	    net = layers.max_pool2d(net)

	    net = tf.squeeze(net, 2)
	    net = tf.transpose(net, [1, 0, 2])
	    net = tf_bilstm(net, 512, 'lstm1')

	sess = tf.Session()
	sess.run(tf.global_variables_initializer())
	
	X = np.zeros([10, 100, 2, 512])

	print('Right before TF call!')

	a = sess.run(net, {x_var: X, is_training_var: True})

	print(a)


if __name__ == '__main__':
	main()
```

Edited: Simplified the code.",2018-01-31T18:45:27Z,8,1,2,6.300101628500413
125,16626,"example script multivariate.py throws ""UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape.  This may consume a large amount of memory.""",,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
I am using the example script `tensorflow/contrib/timeseries/examples/multivariate.py`
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Mac OS High Sierra (darwin)
- **TensorFlow installed from (source or binary)**:
source
- **TensorFlow version (use command below)**:
v1.5.0-rc1-1781-g86c10063c8 1.5.0-rc1
- **Python version**: 
3.6.4
- **Bazel version (if compiling from source)**:
0.9.0-homebrew
- **GCC/Compiler version (if compiling from source)**:
Apple LLVM version 9.0.0 (clang-900.0.39.2)
- **CUDA/cuDNN version**:
n/a compiled without CUDA support
- **GPU model and memory**:
n/a GPU not supported on Mac with SIP
- **Exact command to reproduce**:
$ python $GOPATH/src/github.com/tensorflow/tensorflow/tensorflow/contrib/timeseries/examples/multivariate.py

### Describe the problem
The script throws warning:
`UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.`

### Source code / logs
Here is the traceback of the warning on my system (I have tensorflow installed in a virtual environment called ""tf3"".:
```
  File ""multivariate.py"", line 59, in multivariate_train_and_sample
    estimator.train(input_fn=train_input_fn, steps=training_steps)
  File ""~/.pyenv/versions/tf3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 352, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""~/.pyenv/versions/tf3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 809, in _train_model
    features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)
  File ""~/.pyenv/versions/tf3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 790, in _call_model_fn
    model_fn_results = self._model_fn(features=features, **kwargs)
  File ""~/.pyenv/versions/tf3/lib/python3.6/site-packages/tensorflow/contrib/timeseries/python/timeseries/head.py"", line 228, in create_estimator_spec
    return self._train_ops(features)
  File ""~/.pyenv/versions/tf3/lib/python3.6/site-packages/tensorflow/contrib/timeseries/python/timeseries/head.py"", line 85, in _train_ops
    learning_rate=None)
  File ""~/.pyenv/versions/tf3/lib/python3.6/site-packages/tensorflow/contrib/layers/python/layers/optimizers.py"", line 241, in optimize_loss
    colocate_gradients_with_ops=colocate_gradients_with_ops)
  File ""~/.pyenv/versions/tf3/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py"", line 458, in compute_gradients
    colocate_gradients_with_ops=colocate_gradients_with_ops)
  File ""~/.pyenv/versions/tf3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py"", line 610, in gradients
    lambda: grad_fn(op, *out_grads))
  File ""~/.pyenv/versions/tf3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py"", line 376, in _MaybeCompile
    return grad_fn()  # Exit early
  File ""~/.pyenv/versions/tf3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py"", line 610, in <lambda>
    lambda: grad_fn(op, *out_grads))
  File ""~/.pyenv/versions/tf3/lib/python3.6/site-packages/tensorflow/python/ops/array_grad.py"", line 589, in _PadGrad
    x_grad = array_ops.slice(grad, begin, sizes)
  File ""~/.pyenv/versions/tf3/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py"", line 640, in slice
    return gen_array_ops._slice(input_, begin, size, name=name)
  File ""~/.pyenv/versions/tf3/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 4591, in _slice
    ""Slice"", input=input, begin=begin, size=size, name=name)
  File ""~/.pyenv/versions/tf3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 510, in _apply_op_helper
    preferred_dtype=default_dtype)
  File ""~/.pyenv/versions/tf3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1036, in internal_convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""~/.pyenv/versions/tf3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py"", line 97, in _IndexedSlicesToTensor
    ""Converting sparse IndexedSlices to a dense Tensor of unknown shape. ""
  File ""~/.pyenv/versions/3.6.4/lib/python3.6/warnings.py"", line 99, in _showwarnmsg
    msg.file, msg.line)
~/.pyenv/versions/tf3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:97: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
```
I see discussion about this warning on SO:
https://stackoverflow.com/questions/35892412/tensorflow-dense-gradient-explanation/35893467
But the problem seems to happen in the estimator ""train"" method, not in any custom code I have written.",0,,1,2018-01-31T16:34:00Z,2018-01-31T19:24:53Z,NONE,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
I am using the example script `tensorflow/contrib/timeseries/examples/multivariate.py`
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Mac OS High Sierra (darwin)
- **TensorFlow installed from (source or binary)**:
source
- **TensorFlow version (use command below)**:
v1.5.0-rc1-1781-g86c10063c8 1.5.0-rc1
- **Python version**: 
3.6.4
- **Bazel version (if compiling from source)**:
0.9.0-homebrew
- **GCC/Compiler version (if compiling from source)**:
Apple LLVM version 9.0.0 (clang-900.0.39.2)
- **CUDA/cuDNN version**:
n/a compiled without CUDA support
- **GPU model and memory**:
n/a GPU not supported on Mac with SIP
- **Exact command to reproduce**:
$ python $GOPATH/src/github.com/tensorflow/tensorflow/tensorflow/contrib/timeseries/examples/multivariate.py

### Describe the problem
The script throws warning:
`UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.`

### Source code / logs
Here is the traceback of the warning on my system (I have tensorflow installed in a virtual environment called ""tf3"".:
```
  File ""multivariate.py"", line 59, in multivariate_train_and_sample
    estimator.train(input_fn=train_input_fn, steps=training_steps)
  File ""~/.pyenv/versions/tf3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 352, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""~/.pyenv/versions/tf3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 809, in _train_model
    features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)
  File ""~/.pyenv/versions/tf3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 790, in _call_model_fn
    model_fn_results = self._model_fn(features=features, **kwargs)
  File ""~/.pyenv/versions/tf3/lib/python3.6/site-packages/tensorflow/contrib/timeseries/python/timeseries/head.py"", line 228, in create_estimator_spec
    return self._train_ops(features)
  File ""~/.pyenv/versions/tf3/lib/python3.6/site-packages/tensorflow/contrib/timeseries/python/timeseries/head.py"", line 85, in _train_ops
    learning_rate=None)
  File ""~/.pyenv/versions/tf3/lib/python3.6/site-packages/tensorflow/contrib/layers/python/layers/optimizers.py"", line 241, in optimize_loss
    colocate_gradients_with_ops=colocate_gradients_with_ops)
  File ""~/.pyenv/versions/tf3/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py"", line 458, in compute_gradients
    colocate_gradients_with_ops=colocate_gradients_with_ops)
  File ""~/.pyenv/versions/tf3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py"", line 610, in gradients
    lambda: grad_fn(op, *out_grads))
  File ""~/.pyenv/versions/tf3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py"", line 376, in _MaybeCompile
    return grad_fn()  # Exit early
  File ""~/.pyenv/versions/tf3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py"", line 610, in <lambda>
    lambda: grad_fn(op, *out_grads))
  File ""~/.pyenv/versions/tf3/lib/python3.6/site-packages/tensorflow/python/ops/array_grad.py"", line 589, in _PadGrad
    x_grad = array_ops.slice(grad, begin, sizes)
  File ""~/.pyenv/versions/tf3/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py"", line 640, in slice
    return gen_array_ops._slice(input_, begin, size, name=name)
  File ""~/.pyenv/versions/tf3/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 4591, in _slice
    ""Slice"", input=input, begin=begin, size=size, name=name)
  File ""~/.pyenv/versions/tf3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 510, in _apply_op_helper
    preferred_dtype=default_dtype)
  File ""~/.pyenv/versions/tf3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1036, in internal_convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""~/.pyenv/versions/tf3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py"", line 97, in _IndexedSlicesToTensor
    ""Converting sparse IndexedSlices to a dense Tensor of unknown shape. ""
  File ""~/.pyenv/versions/3.6.4/lib/python3.6/warnings.py"", line 99, in _showwarnmsg
    msg.file, msg.line)
~/.pyenv/versions/tf3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:97: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
```
I see discussion about this warning on SO:
https://stackoverflow.com/questions/35892412/tensorflow-dense-gradient-explanation/35893467
But the problem seems to happen in the estimator ""train"" method, not in any custom code I have written.",2018-01-31T19:24:53Z,0,1,0,1.8001016285004132
126,16625,[For Test; DO NOT MERGE] Add grpcio as a pip dependency of tensorflow,cla: yes,,0,,1,2018-01-31T15:22:47Z,2018-02-01T01:41:14Z,CONTRIBUTOR,,2018-02-01T01:41:12Z,0,2,0,2.800101628500413
127,16622,update tensorflow to 1.5 ,,"I update tensorflow to 1.5,and reinstall cuda to 9.1,now  I run my program get the error:
Traceback (most recent call last):
  File ""/home/chris/tensorflowDemo/7_1_Word2Vec.py"", line 24, in <module>
    import tensorflow as tf
  File ""/home/chris/.local/lib/python3.5/site-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""/home/chris/.local/lib/python3.5/site-packages/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/home/chris/.local/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/home/chris/.local/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/home/chris/.local/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/home/chris/.local/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/usr/lib/python3.5/imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""/usr/lib/python3.5/imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory",0,,8,2018-01-31T12:33:00Z,2018-02-06T16:45:46Z,NONE,"I update tensorflow to 1.5,and reinstall cuda to 9.1,now  I run my program get the error:
Traceback (most recent call last):
  File ""/home/chris/tensorflowDemo/7_1_Word2Vec.py"", line 24, in <module>
    import tensorflow as tf
  File ""/home/chris/.local/lib/python3.5/site-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""/home/chris/.local/lib/python3.5/site-packages/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/home/chris/.local/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/home/chris/.local/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/home/chris/.local/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/home/chris/.local/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/usr/lib/python3.5/imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""/usr/lib/python3.5/imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory",2018-02-06T16:45:46Z,5,1,1,5.300101628500413
128,16621,Using tf.train.SyncReplicasOptimizer with multiple optimizers,stat:awaiting tensorflower,"I am trying to run the DeepLab Resnet (https://github.com/DrSleep/tensorflow-deeplab-resnet) in a distributed setup. I opted Synchronous Data parallel training approach similar to the one demonstrated in the Inception distributed training example.(https://github.com/tensorflow/models/tree/master/research/inception).

In the Inception example, a single RMS optimizer is used to reduce the loss. The tf.train.SyncReplicasOptimizer function wraps the optimizer and becomes responsible for synchronization, aggregation and application of gradients to various workers. Also, it takes care of updating the global_step variable. In my case, the DeepLab Resnet makes use of three optimizers each handling specific portions of the network. Following snippet explains the case:

    `#Three optimizers declared with different learning rates
     opt_conv = tf.train.MomentumOptimizer(learning_rate, args.momentum)
     opt_fc_w = tf.train.MomentumOptimizer(learning_rate * 10.0, args.momentum)
     opt_fc_b = tf.train.MomentumOptimizer(learning_rate * 20.0, args.momentum)`

    #Scope for every optimizer 
    grads = tf.gradients(reduced_loss, conv_trainable + fc_w_trainable + fc_b_trainable)
    grads_conv = grads[:len(conv_trainable)]
    grads_fc_w = grads[len(conv_trainable) : (len(conv_trainable) + len(fc_w_trainable))]
    grads_fc_b = grads[(len(conv_trainable) + len(fc_w_trainable)):]

    #Gradients applied to various portions of the network
    train_op_conv = opt_conv.apply_gradients(zip(grads_conv, conv_trainable))
    train_op_fc_w = opt_fc_w.apply_gradients(zip(grads_fc_w, fc_w_trainable))
    train_op_fc_b = opt_fc_b.apply_gradients(zip(grads_fc_b, fc_b_trainable))
 
    `#tf.group to combine all three operations
    train_op = tf.group(train_op_conv, train_op_fc_w, train_op_fc_b)`
   
I don't have any clue about using the tf.train.SyncReplicasOptimizer for multiple optimizers to achieve synchronous data parallel training. Also, I don't have an idea about updating the global_step variable and using the chief_queue_runner for this case. Please help me on this.


 


",0,,2,2018-01-31T11:59:09Z,2018-01-31T20:53:32Z,NONE,"I am trying to run the DeepLab Resnet (https://github.com/DrSleep/tensorflow-deeplab-resnet) in a distributed setup. I opted Synchronous Data parallel training approach similar to the one demonstrated in the Inception distributed training example.(https://github.com/tensorflow/models/tree/master/research/inception).

In the Inception example, a single RMS optimizer is used to reduce the loss. The tf.train.SyncReplicasOptimizer function wraps the optimizer and becomes responsible for synchronization, aggregation and application of gradients to various workers. Also, it takes care of updating the global_step variable. In my case, the DeepLab Resnet makes use of three optimizers each handling specific portions of the network. Following snippet explains the case:

    `#Three optimizers declared with different learning rates
     opt_conv = tf.train.MomentumOptimizer(learning_rate, args.momentum)
     opt_fc_w = tf.train.MomentumOptimizer(learning_rate * 10.0, args.momentum)
     opt_fc_b = tf.train.MomentumOptimizer(learning_rate * 20.0, args.momentum)`

    #Scope for every optimizer 
    grads = tf.gradients(reduced_loss, conv_trainable + fc_w_trainable + fc_b_trainable)
    grads_conv = grads[:len(conv_trainable)]
    grads_fc_w = grads[len(conv_trainable) : (len(conv_trainable) + len(fc_w_trainable))]
    grads_fc_b = grads[(len(conv_trainable) + len(fc_w_trainable)):]

    #Gradients applied to various portions of the network
    train_op_conv = opt_conv.apply_gradients(zip(grads_conv, conv_trainable))
    train_op_fc_w = opt_fc_w.apply_gradients(zip(grads_fc_w, fc_w_trainable))
    train_op_fc_b = opt_fc_b.apply_gradients(zip(grads_fc_b, fc_b_trainable))
 
    `#tf.group to combine all three operations
    train_op = tf.group(train_op_conv, train_op_fc_w, train_op_fc_b)`
   
I don't have any clue about using the tf.train.SyncReplicasOptimizer for multiple optimizers to achieve synchronous data parallel training. Also, I don't have an idea about updating the global_step variable and using the chief_queue_runner for this case. Please help me on this.


 


",2018-01-31T18:19:06Z,0,1,0,2.300101628500413
129,16620,How to use model.summary() when using placeholder instead of Input(keras),,"Dear all, 
I follow  post in ""https://blog.keras.io/keras-as-a-simplified-interface-to-tensorflow-tutorial.html""
The little modified code I use is:
----------------------------------------------------------------------------------------------------------------------------------
import tensorflow as tf
from tensorflow.python.keras.layers import Dense
from tensorflow.python.keras.backend import categorical_crossentropy
from tensorflow.examples.tutorials.mnist import input_data
from tensorflow.python.keras.models import Model


sess = tf.Session()
img = tf.placeholder(tf.float32, shape=(None, 784))
x = Dense(128, activation='relu')(img)  # fully-connected layer with 128 units and ReLU activation
x = Dense(128, activation='relu')(x)
preds = Dense(10, activation='softmax')(x)  # output layer with 10 units and a softmax activation

labels = tf.placeholder(tf.float32, shape=(None, 10))
loss = tf.reduce_mean(categorical_crossentropy(labels, preds))
mnist_data = input_data.read_data_sets('MNIST_data', one_hot=True)
train_step = tf.train.GradientDescentOptimizer(0.5).minimize(loss)

init_op = tf.global_variables_initializer()
sess.run(init_op)
with sess.as_default():
    for i in range(100):
        batch = mnist_data.train.next_batch(50)
        train_step.run(feed_dict={img: batch[0],
                                  labels: batch[1]})

----------------------------------------------------------------------------------------------------------------------------------
It work fine until I use model.summary :  

model = Model(inputs=img, outputs=preds)

The error message show"" Input tensors to a Model must come from `tf.layers.Input`""
I can use tf.layers.Input to solve this problem.
But I really want to use tf.placeholder so I can feed data as I like.
Can anyone help me?  Thanks!!",1,,7,2018-01-31T10:23:42Z,2018-02-02T19:15:06Z,NONE,"Dear all, 
I follow  post in ""https://blog.keras.io/keras-as-a-simplified-interface-to-tensorflow-tutorial.html""
The little modified code I use is:
----------------------------------------------------------------------------------------------------------------------------------
import tensorflow as tf
from tensorflow.python.keras.layers import Dense
from tensorflow.python.keras.backend import categorical_crossentropy
from tensorflow.examples.tutorials.mnist import input_data
from tensorflow.python.keras.models import Model


sess = tf.Session()
img = tf.placeholder(tf.float32, shape=(None, 784))
x = Dense(128, activation='relu')(img)  # fully-connected layer with 128 units and ReLU activation
x = Dense(128, activation='relu')(x)
preds = Dense(10, activation='softmax')(x)  # output layer with 10 units and a softmax activation

labels = tf.placeholder(tf.float32, shape=(None, 10))
loss = tf.reduce_mean(categorical_crossentropy(labels, preds))
mnist_data = input_data.read_data_sets('MNIST_data', one_hot=True)
train_step = tf.train.GradientDescentOptimizer(0.5).minimize(loss)

init_op = tf.global_variables_initializer()
sess.run(init_op)
with sess.as_default():
    for i in range(100):
        batch = mnist_data.train.next_batch(50)
        train_step.run(feed_dict={img: batch[0],
                                  labels: batch[1]})

----------------------------------------------------------------------------------------------------------------------------------
It work fine until I use model.summary :  

model = Model(inputs=img, outputs=preds)

The error message show"" Input tensors to a Model must come from `tf.layers.Input`""
I can use tf.layers.Input to solve this problem.
But I really want to use tf.placeholder so I can feed data as I like.
Can anyone help me?  Thanks!!",2018-02-02T05:15:49Z,1,1,0,5.800101628500413
130,16615,Refactoring by extracting duplicate code into methods,cla: yes,I extracted duplicate code into methods to improve maintainability.,0,,2,2018-01-31T07:59:04Z,2018-02-15T01:24:08Z,CONTRIBUTOR,I extracted duplicate code into methods to improve maintainability.,2018-02-13T19:33:34Z,14,2,2,3.300101628500413
131,16612,Simplify loader_impl.py logic around main Op Tensor.,"awaiting testing (then merge),cla: yes",,0,,1,2018-01-31T03:08:35Z,2018-01-31T21:08:14Z,CONTRIBUTOR,,2018-01-31T15:25:25Z,0,2,0,2.800101628500413
132,16607,MKL: Pooling and AddN bug fixes,"awaiting testing (then merge),cla: yes",,0,,1,2018-01-31T00:18:08Z,2018-02-01T08:37:48Z,CONTRIBUTOR,,2018-01-31T00:29:27Z,0,2,0,2.800101628500413
133,16605,Description in docs of one-hot vector for mnist deep example confusing and/or wrong,stat:awaiting tensorflower,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:n/a
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Windows
- **TensorFlow installed from (source or binary)**:source
- **TensorFlow version (use command below)**:1.4
- **Python version**: 3.6.4
- **Bazel version (if compiling from source)**:n/a
- **GCC/Compiler version (if compiling from source)**:n/a
- **CUDA/cuDNN version**:n/a
- **GPU model and memory**:n/a
- **Exact command to reproduce**:n/a

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Documentation found at https://www.tensorflow.org/versions/r1.4/get_started/mnist/pros describes one of the two parameters passed to the training process as a 2d tensor of ""one-hot"" 10-dimensional vectors specifying the classes of the samples in the other 2d tensor parameter. But clearly the placeholders define 2d and 1d tensors, not 2d and 2d. There is no ""one-hot"" representation used at all as far as I can tell by using print() statements - if the class if a sample is class 3, then the corresponding entry is simple 3, not the one-hot representation of it. If this class is subsequently converted into a one-hot representation, it does not happen in the mnist_deep.py source file. I'm too much of a beginner to say what the documentation should say, but it seems at best confusing, and at worst completely wrong.

### Source code / logs
n/a
",0,,4,2018-01-30T22:19:50Z,2018-01-30T23:11:10Z,NONE,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:n/a
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Windows
- **TensorFlow installed from (source or binary)**:source
- **TensorFlow version (use command below)**:1.4
- **Python version**: 3.6.4
- **Bazel version (if compiling from source)**:n/a
- **GCC/Compiler version (if compiling from source)**:n/a
- **CUDA/cuDNN version**:n/a
- **GPU model and memory**:n/a
- **Exact command to reproduce**:n/a

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Documentation found at https://www.tensorflow.org/versions/r1.4/get_started/mnist/pros describes one of the two parameters passed to the training process as a 2d tensor of ""one-hot"" 10-dimensional vectors specifying the classes of the samples in the other 2d tensor parameter. But clearly the placeholders define 2d and 1d tensors, not 2d and 2d. There is no ""one-hot"" representation used at all as far as I can tell by using print() statements - if the class if a sample is class 3, then the corresponding entry is simple 3, not the one-hot representation of it. If this class is subsequently converted into a one-hot representation, it does not happen in the mnist_deep.py source file. I'm too much of a beginner to say what the documentation should say, but it seems at best confusing, and at worst completely wrong.

### Source code / logs
n/a
",2018-01-30T23:11:10Z,0,1,0,3.2969742043733703
134,16604,Branch 183881907,cla: yes,,1,,2,2018-01-30T22:12:40Z,2018-01-30T23:33:14Z,MEMBER,,2018-01-30T22:57:24Z,0,3,0,5.29697420437337
135,16595,Branch 183846994,cla: yes,,0,,3,2018-01-30T18:37:36Z,2018-01-30T23:43:41Z,MEMBER,,2018-01-30T18:41:07Z,0,3,0,4.79697420437337
136,16593,"contrib.tfgan: batch_norm is_training=True for both training and inferencing, non-slim version",,"Hi, I am exploring contrib.tfgan, such a great work @joel-shor .

### batch_norm is_training=True for both training and inferencing
However, when I see the example in source code of both generator and discriminator of MNIST, as below.

https://github.com/tensorflow/models/blob/master/research/gan/tutorial.ipynb

`with slim.arg_scope(
        [layers.fully_connected, layers.conv2d_transpose],
        activation_fn=tf.nn.relu, normalizer_fn=layers.batch_norm,
        weights_regularizer=layers.l2_regularizer(weight_decay)):
        net = layers.fully_connected(noise, 1024)
        net = layers.fully_connected(net, 7 * 7 * 256)
        net = tf.reshape(net, [-1, 7, 7, 256])`

The default argument of layers.batch_norm is set to True, and this gen_fn and dis_fn are used for  both training phase and generating test images phase (inferencing).

Is it a bug or it is intended? If it is intended, can you explain why is that?

### non-slim implementation
In addition, I don't really like slim, and I believe some people don't either. Can I use other model construction libraries like tf.layers or keras to build the network. Is tfslim a must?

Thank you,

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS
- **TensorFlow installed from (source or binary)**:source
- **TensorFlow version (use command below)**:1.5 and 1.4.1
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: 0.7
- **GCC/Compiler version (if compiling from source)**: 4.2
- **CUDA/cuDNN version**:NA (CPU)
- **GPU model and memory**:NA
- **Exact command to reproduce**:",1,,4,2018-01-30T17:48:55Z,2018-02-12T19:03:42Z,NONE,"Hi, I am exploring contrib.tfgan, such a great work @joel-shor .

### batch_norm is_training=True for both training and inferencing
However, when I see the example in source code of both generator and discriminator of MNIST, as below.

https://github.com/tensorflow/models/blob/master/research/gan/tutorial.ipynb

`with slim.arg_scope(
        [layers.fully_connected, layers.conv2d_transpose],
        activation_fn=tf.nn.relu, normalizer_fn=layers.batch_norm,
        weights_regularizer=layers.l2_regularizer(weight_decay)):
        net = layers.fully_connected(noise, 1024)
        net = layers.fully_connected(net, 7 * 7 * 256)
        net = tf.reshape(net, [-1, 7, 7, 256])`

The default argument of layers.batch_norm is set to True, and this gen_fn and dis_fn are used for  both training phase and generating test images phase (inferencing).

Is it a bug or it is intended? If it is intended, can you explain why is that?

### non-slim implementation
In addition, I don't really like slim, and I believe some people don't either. Can I use other model construction libraries like tf.layers or keras to build the network. Is tfslim a must?

Thank you,

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS
- **TensorFlow installed from (source or binary)**:source
- **TensorFlow version (use command below)**:1.5 and 1.4.1
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: 0.7
- **GCC/Compiler version (if compiling from source)**: 4.2
- **CUDA/cuDNN version**:NA (CPU)
- **GPU model and memory**:NA
- **Exact command to reproduce**:",2018-01-30T18:34:49Z,12,1,2,4.29697420437337
137,16591,Fix FutureWarning on issubdtype from float to np.floating,"awaiting testing (then merge),cla: yes","This is try to fix [#16587](https://github.com/tensorflow/tensorflow/issues/16587). 
```
FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated
```

Before fix:
```
>>> np.issubdtype(np.integer, np.float)
__main__:1: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
False
```
After fix:
```
>>> np.issubdtype(np.integer, np.floating)
False
>>>
```",0,,1,2018-01-30T17:34:37Z,2018-02-01T03:03:19Z,CONTRIBUTOR,"This is try to fix [#16587](https://github.com/tensorflow/tensorflow/issues/16587). 
```
FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated
```

Before fix:
```
>>> np.issubdtype(np.integer, np.float)
__main__:1: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
False
```
After fix:
```
>>> np.issubdtype(np.integer, np.floating)
False
>>>
```",2018-01-31T13:19:05Z,1,2,0,2.7969742043733703
138,16590,how to save model for tensroflwo serving for lstm in tensorflow/contrib/timeseries/examples/lstm.py,,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
when I run lstm in tensorflow/contrib/timeseries/examples/lstm.py, I tried to add methods to save model into savedModel, but it gives back errors.

  File ""/Users/yang/.local/lib/python3.4/site-packages/tensorflow/python/estimator/estimator.py"", line 504, in export_savedmodel
    serving_input_receiver = serving_input_receiver_fn()
  File ""/Users/yang/.local/lib/python3.4/site-packages/tensorflow/contrib/timeseries/python/timeseries/estimators.py"", line 133, in _serving_input_receiver_fn
    self._model.initialize_graph()
TypeError: initialize_graph() missing 1 required positional argument: 'input_statistics'

The issue I guess is that, in self._model.initialize_graph(), no parameters are given, but in 

    def initialize_graph(self, input_statistics):
        """"""Save templates for components, which can then be used repeatedly.
        This method is called every time a new graph is created. It's safe to start
        adding ops to the current default graph here, but the graph should be
        constructed from scratch.
        Args:
          input_statistics: A math_utils.InputStatistics object.
        """"""
        super(_LSTMModel, self).initialize_graph(input_statistics=input_statistics)
        with tf.variable_scope("""", use_resource=True):
          # Use ResourceVariables to avoid race conditions.
          self._lstm_cell = tf.nn.rnn_cell.LSTMCell(num_units=self._num_units)
          # Create templates so we don't have to worry about variable reuse.
          self._lstm_cell_run = tf.make_template(
              name_=""lstm_cell"",
              func_=self._lstm_cell,
              create_scope_now_=True)
          # Transforms LSTM output into mean predictions.
          self._predict_from_lstm_output = tf.make_template(
              name_=""predict_from_lstm_output"",
              func_=functools.partial(tf.layers.dense, units=self.num_features),
              create_scope_now_=True)

one param input_statistics is asked. But how to fix this issue

### Source code / logs
    
    serving_input_receiver_fn = estimator.build_raw_serving_input_receiver_fn()
    estimator.export_savedmodel(
        ""../model"",
        serving_input_receiver_fn
    )
",1,,4,2018-01-30T16:46:48Z,2018-02-09T02:08:17Z,NONE,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
when I run lstm in tensorflow/contrib/timeseries/examples/lstm.py, I tried to add methods to save model into savedModel, but it gives back errors.

  File ""/Users/yang/.local/lib/python3.4/site-packages/tensorflow/python/estimator/estimator.py"", line 504, in export_savedmodel
    serving_input_receiver = serving_input_receiver_fn()
  File ""/Users/yang/.local/lib/python3.4/site-packages/tensorflow/contrib/timeseries/python/timeseries/estimators.py"", line 133, in _serving_input_receiver_fn
    self._model.initialize_graph()
TypeError: initialize_graph() missing 1 required positional argument: 'input_statistics'

The issue I guess is that, in self._model.initialize_graph(), no parameters are given, but in 

    def initialize_graph(self, input_statistics):
        """"""Save templates for components, which can then be used repeatedly.
        This method is called every time a new graph is created. It's safe to start
        adding ops to the current default graph here, but the graph should be
        constructed from scratch.
        Args:
          input_statistics: A math_utils.InputStatistics object.
        """"""
        super(_LSTMModel, self).initialize_graph(input_statistics=input_statistics)
        with tf.variable_scope("""", use_resource=True):
          # Use ResourceVariables to avoid race conditions.
          self._lstm_cell = tf.nn.rnn_cell.LSTMCell(num_units=self._num_units)
          # Create templates so we don't have to worry about variable reuse.
          self._lstm_cell_run = tf.make_template(
              name_=""lstm_cell"",
              func_=self._lstm_cell,
              create_scope_now_=True)
          # Transforms LSTM output into mean predictions.
          self._predict_from_lstm_output = tf.make_template(
              name_=""predict_from_lstm_output"",
              func_=functools.partial(tf.layers.dense, units=self.num_features),
              create_scope_now_=True)

one param input_statistics is asked. But how to fix this issue

### Source code / logs
    
    serving_input_receiver_fn = estimator.build_raw_serving_input_receiver_fn()
    estimator.export_savedmodel(
        ""../model"",
        serving_input_receiver_fn
    )
",2018-01-31T00:55:06Z,9,1,2,4.29697420437337
139,16588,IllegalArgumentException: Retval[0] does not have value,,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: iMac (27-inch, Late 2013) OS: 10.13.3 (17D47)
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**:  Using TensorFlow backend.
1.5.0-rc1
- **Python version**: Python 3.6.4
- **Bazel version (if compiling from source)**: 
Build label: 0.9.0-homebrew
Build target: bazel-out/darwin-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Sun Jul 12 12:24:01 +49936 (1513677414241)
Build timestamp: 1513677414241
Build timestamp as int: 1513677414241

- **GCC/Compiler version (if compiling from source)**: Xcode 9.2
Build version 9C40b
- **CUDA/cuDNN version**: No (CPU only)
- **GPU model and memory**: No
- **Exact command to reproduce**:

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

if I add GRU or LSTM to my code and try to export this model to Android I have got the exception:
```
Successfully loaded model from 'file:///android_asset/frozen_opt.pb'
01-30 07:45:35.424 25635-25635/ru.rimidalv.tensorflow I/TensorFlowImageClassif: Read 24 labels, output layer size is 32
01-30 07:45:35.424 25635-25635/ru.rimidalv.tensorflow I/tensorflow: ClassifierActivity: Camera orientation relative to screen canvas: 90
01-30 07:45:35.424 25635-25635/ru.rimidalv.tensorflow I/tensorflow: ClassifierActivity: Initializing at size 640x480
01-30 07:45:35.424 25635-25635/ru.rimidalv.tensorflow E/art: No implementation found for void ru.rimidalv.tensorflow.env.ImageUtils.convertYUV420SPToARGB8888(byte[], int[], int, int, boolean) (tried Java_ru_rimidalv_tensorflow_env_ImageUtils_convertYUV420SPToARGB8888 and Java_ru_rimidalv_tensorflow_env_ImageUtils_convertYUV420SPToARGB8888___3B_3IIIZ)
01-30 07:45:35.424 25635-25635/ru.rimidalv.tensorflow W/tensorflow: ImageUtils: Native YUV420SP -> RGB implementation not found, falling back to Java implementation
01-30 07:45:35.714 25635-25666/ru.rimidalv.tensorflow E/TensorFlowInferenceInterface: Failed to run TensorFlow inference with inputs:[the_input], outputs:[softmax/truediv]
01-30 07:45:35.714 25635-25666/ru.rimidalv.tensorflow E/AndroidRuntime: FATAL EXCEPTION: inference
                                                                        Process: ru.rimidalv.tensorflow, PID: 25635
                                                                        java.lang.IllegalArgumentException: Retval[0] does not have value
                                                                            at org.tensorflow.Session.run(Native Method)
                                                                            at org.tensorflow.Session.access$100(Session.java:48)
                                                                            at org.tensorflow.Session$Runner.runHelper(Session.java:298)
                                                                            at org.tensorflow.Session$Runner.run(Session.java:248)
                                                                            at org.tensorflow.contrib.android.TensorFlowInferenceInterface.run(TensorFlowInferenceInterface.java:230)
                                                                            at org.tensorflow.contrib.android.TensorFlowInferenceInterface.run(TensorFlowInferenceInterface.java:197)
                                                                            at ru.rimidalv.tensorflow.TensorFlowImageClassifier.recognizeImage(TensorFlowImageClassifier.java:171)
                                                                            at ru.rimidalv.tensorflow.ClassifierActivity$2.run(ClassifierActivity.java:175)
                                                                            at android.os.Handler.handleCallback(Handler.java:739)
                                                                            at android.os.Handler.dispatchMessage(Handler.java:95)
                                                                            at android.os.Looper.loop(Looper.java:158)
                                                                            at android.os.HandlerThread.run(HandlerThread.java:61)
```

code to export my model:

```
bazel build tensorflow/python/tools:freeze_graph
bazel-bin/tensorflow/python/tools/freeze_graph \
--input_graph=$PB_MAIN/protobuf_path.pbtxt \
--input_checkpoint=$PB_MAIN/checkpoint_path.ckpt \
--output_graph=$PB_MAIN/frozen_graph.pb \
--output_node_names=softmax/truediv \
```
and than

```
!bazel build tensorflow/tools/graph_transforms:transform_graph
!bazel-bin/tensorflow/tools/graph_transforms/transform_graph \
--in_graph=$PB_MAIN/frozen_graph.pb \
--out_graph=$PB_MAIN/frozen_opt.pb \
--inputs='the_input:0' \
--outputs='softmax/truediv:0' \
--transforms='add_default_attributes strip_unused_nodes(type=float, shape=""-1,128,64,1"") remove_nodes(op=Identity, op=CheckNumerics) fold_constants(ignore_errors=true) fold_batch_norms fold_old_batch_norms quantize_nodes round_weights strip_unused_nodes sort_by_execution_order'
```


### Source code / logs
My model:

```python
    input_shape = (128, 64, 1)
    latent_dim = 128
    decoder_inputs = Input(shape=(input_shape), name='the_input')
    conv_to_rnn_dims = (32, 128*2 )
    inner = Reshape(target_shape=conv_to_rnn_dims, name='reshape')(decoder_inputs)
    inner = GRU(latent_dim, return_sequences=True, kernel_initializer='he_normal')(inner)
    decoder_dense = Dense(10, activation='softmax', name=""softmax"")(inner)
    model = Model(inputs=[decoder_inputs], outputs=decoder_dense)
    model.compile(loss={'softmax': lambda y_true, y_pred: y_pred}, optimizer=""adam"")
```

Save the model:

```python

    K.set_learning_phase(0)
    model = load_model(os.path.join('model_data', 'model.h5'), compile=False)
    model.load_weights(os.path.join('model_data', 'weights00.h5'))
    sess = K.get_session()
   
    protobuf_path = os.path.join('tf-exports', 'protobuf_path.pbtxt')
    checkpoint_path = os.path.join('tf-exports', 'checkpoint_path.ckpt')

    tf.train.write_graph(sess.graph_def, '.', protobuf_path)

    saver = tf.train.Saver()
    saver.save(sess, save_path = checkpoint_path)

```
",1,,3,2018-01-30T16:00:53Z,2018-02-06T13:11:39Z,NONE,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: iMac (27-inch, Late 2013) OS: 10.13.3 (17D47)
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**:  Using TensorFlow backend.
1.5.0-rc1
- **Python version**: Python 3.6.4
- **Bazel version (if compiling from source)**: 
Build label: 0.9.0-homebrew
Build target: bazel-out/darwin-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Sun Jul 12 12:24:01 +49936 (1513677414241)
Build timestamp: 1513677414241
Build timestamp as int: 1513677414241

- **GCC/Compiler version (if compiling from source)**: Xcode 9.2
Build version 9C40b
- **CUDA/cuDNN version**: No (CPU only)
- **GPU model and memory**: No
- **Exact command to reproduce**:

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

if I add GRU or LSTM to my code and try to export this model to Android I have got the exception:
```
Successfully loaded model from 'file:///android_asset/frozen_opt.pb'
01-30 07:45:35.424 25635-25635/ru.rimidalv.tensorflow I/TensorFlowImageClassif: Read 24 labels, output layer size is 32
01-30 07:45:35.424 25635-25635/ru.rimidalv.tensorflow I/tensorflow: ClassifierActivity: Camera orientation relative to screen canvas: 90
01-30 07:45:35.424 25635-25635/ru.rimidalv.tensorflow I/tensorflow: ClassifierActivity: Initializing at size 640x480
01-30 07:45:35.424 25635-25635/ru.rimidalv.tensorflow E/art: No implementation found for void ru.rimidalv.tensorflow.env.ImageUtils.convertYUV420SPToARGB8888(byte[], int[], int, int, boolean) (tried Java_ru_rimidalv_tensorflow_env_ImageUtils_convertYUV420SPToARGB8888 and Java_ru_rimidalv_tensorflow_env_ImageUtils_convertYUV420SPToARGB8888___3B_3IIIZ)
01-30 07:45:35.424 25635-25635/ru.rimidalv.tensorflow W/tensorflow: ImageUtils: Native YUV420SP -> RGB implementation not found, falling back to Java implementation
01-30 07:45:35.714 25635-25666/ru.rimidalv.tensorflow E/TensorFlowInferenceInterface: Failed to run TensorFlow inference with inputs:[the_input], outputs:[softmax/truediv]
01-30 07:45:35.714 25635-25666/ru.rimidalv.tensorflow E/AndroidRuntime: FATAL EXCEPTION: inference
                                                                        Process: ru.rimidalv.tensorflow, PID: 25635
                                                                        java.lang.IllegalArgumentException: Retval[0] does not have value
                                                                            at org.tensorflow.Session.run(Native Method)
                                                                            at org.tensorflow.Session.access$100(Session.java:48)
                                                                            at org.tensorflow.Session$Runner.runHelper(Session.java:298)
                                                                            at org.tensorflow.Session$Runner.run(Session.java:248)
                                                                            at org.tensorflow.contrib.android.TensorFlowInferenceInterface.run(TensorFlowInferenceInterface.java:230)
                                                                            at org.tensorflow.contrib.android.TensorFlowInferenceInterface.run(TensorFlowInferenceInterface.java:197)
                                                                            at ru.rimidalv.tensorflow.TensorFlowImageClassifier.recognizeImage(TensorFlowImageClassifier.java:171)
                                                                            at ru.rimidalv.tensorflow.ClassifierActivity$2.run(ClassifierActivity.java:175)
                                                                            at android.os.Handler.handleCallback(Handler.java:739)
                                                                            at android.os.Handler.dispatchMessage(Handler.java:95)
                                                                            at android.os.Looper.loop(Looper.java:158)
                                                                            at android.os.HandlerThread.run(HandlerThread.java:61)
```

code to export my model:

```
bazel build tensorflow/python/tools:freeze_graph
bazel-bin/tensorflow/python/tools/freeze_graph \
--input_graph=$PB_MAIN/protobuf_path.pbtxt \
--input_checkpoint=$PB_MAIN/checkpoint_path.ckpt \
--output_graph=$PB_MAIN/frozen_graph.pb \
--output_node_names=softmax/truediv \
```
and than

```
!bazel build tensorflow/tools/graph_transforms:transform_graph
!bazel-bin/tensorflow/tools/graph_transforms/transform_graph \
--in_graph=$PB_MAIN/frozen_graph.pb \
--out_graph=$PB_MAIN/frozen_opt.pb \
--inputs='the_input:0' \
--outputs='softmax/truediv:0' \
--transforms='add_default_attributes strip_unused_nodes(type=float, shape=""-1,128,64,1"") remove_nodes(op=Identity, op=CheckNumerics) fold_constants(ignore_errors=true) fold_batch_norms fold_old_batch_norms quantize_nodes round_weights strip_unused_nodes sort_by_execution_order'
```


### Source code / logs
My model:

```python
    input_shape = (128, 64, 1)
    latent_dim = 128
    decoder_inputs = Input(shape=(input_shape), name='the_input')
    conv_to_rnn_dims = (32, 128*2 )
    inner = Reshape(target_shape=conv_to_rnn_dims, name='reshape')(decoder_inputs)
    inner = GRU(latent_dim, return_sequences=True, kernel_initializer='he_normal')(inner)
    decoder_dense = Dense(10, activation='softmax', name=""softmax"")(inner)
    model = Model(inputs=[decoder_inputs], outputs=decoder_dense)
    model.compile(loss={'softmax': lambda y_true, y_pred: y_pred}, optimizer=""adam"")
```

Save the model:

```python

    K.set_learning_phase(0)
    model = load_model(os.path.join('model_data', 'model.h5'), compile=False)
    model.load_weights(os.path.join('model_data', 'weights00.h5'))
    sess = K.get_session()
   
    protobuf_path = os.path.join('tf-exports', 'protobuf_path.pbtxt')
    checkpoint_path = os.path.join('tf-exports', 'checkpoint_path.ckpt')

    tf.train.write_graph(sess.graph_def, '.', protobuf_path)

    saver = tf.train.Saver()
    saver.save(sess, save_path = checkpoint_path)

```
",2018-01-30T23:52:33Z,6,1,1,3.7969742043733703
140,16587,Feature deprecated in h5py is used in TF1.5,stat:contributions welcome,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux, OS X
- **TensorFlow installed from (source or binary)**: source and binary
- **TensorFlow version (use command below)**: 1.5
- **Python version**: 3.6.4
- **Bazel version (if compiling from source)**: 0.9
- **GCC/Compiler version (if compiling from source)**: 
- **CUDA/cuDNN version**: 9.0 - 7.0
- **GPU model and memory**: GTX1060, GTX 1050Ti 
- **Exact command to reproduce**:
`sudo pip3 install h5py`
run python3, from there, type:
`import tensorflow as tf`


### Describe the problem
A feature of h5py used in TF 1.5 is deprecated, in particular: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated.

### Source code / logs
Warning message: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
",0,,2,2018-01-30T15:48:47Z,2018-02-01T03:03:19Z,NONE,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux, OS X
- **TensorFlow installed from (source or binary)**: source and binary
- **TensorFlow version (use command below)**: 1.5
- **Python version**: 3.6.4
- **Bazel version (if compiling from source)**: 0.9
- **GCC/Compiler version (if compiling from source)**: 
- **CUDA/cuDNN version**: 9.0 - 7.0
- **GPU model and memory**: GTX1060, GTX 1050Ti 
- **Exact command to reproduce**:
`sudo pip3 install h5py`
run python3, from there, type:
`import tensorflow as tf`


### Describe the problem
A feature of h5py used in TF 1.5 is deprecated, in particular: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated.

### Source code / logs
Warning message: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
",2018-01-30T17:35:55Z,1,1,0,2.2969742043733703
141,16585,TensorFlow with CUDA or Python might rebuilds more than necessary instead of re-using bazel cache,stat:contributions welcome,"Context: for DeepSpeech, we perform tensorflow builds and then keep the cache in a tar (capturing the whole of the home directory of the build user). We then untar it and the deepspeech build through `bazel build` picks the proper cached items so it does not rebuild anything.

Recently, we started to have increased (2.5x) build time on CUDA-enabled builds. Debugging with Bazel showed that it was rebuilding because the `actionKey` computed for `stream_executor_impl` was different. Instrumenting Bazel to get more informations, I could get down to the reason of the different actionKey: the ordering of the CUDA includes was different. The list itself contained the exact same content, just a different ordering.

Those includes are symlinks, and they are generated from a genrule. This is all taken care of by https://github.com/tensorflow/tensorflow/blob/ba64f5334d4bba31d22c30e09a96f806ea0e2f7e/third_party/gpus/cuda_configure.bzl#L915-L1035 which generated shell script for the genrules, that actually do perform the symlinks. Checking those shell scripts revealed the exact same and different ordering.

Checking more carefully, one will see that the headers are discovered by `_read_dir` function: https://github.com/tensorflow/tensorflow/blob/ba64f5334d4bba31d22c30e09a96f806ea0e2f7e/third_party/gpus/cuda_configure.bzl#L891-L894, it does directly get the output of `find`. This is dependant on the ordering provided by `readdir` syscall.

In our case, the ordering on the filesystem before making the tar archive, and after untarring it would be different.

One simple fix for that is to force ordering the list of headers, this way we are sure the order is always the same and we are not dependant on what `readdir` is going to get us.

In the past, Bazel would force the ordering of the elements considered to compute the actionKey. This was removed with 0.3.0 but it might have make the issue hidden https://github.com/bazelbuild/bazel/commit/9dc32111d5b6c1c7c5eaf39efad5fef75327ee75",0,,2,2018-01-30T14:16:49Z,2018-02-02T02:03:32Z,CONTRIBUTOR,"Context: for DeepSpeech, we perform tensorflow builds and then keep the cache in a tar (capturing the whole of the home directory of the build user). We then untar it and the deepspeech build through `bazel build` picks the proper cached items so it does not rebuild anything.

Recently, we started to have increased (2.5x) build time on CUDA-enabled builds. Debugging with Bazel showed that it was rebuilding because the `actionKey` computed for `stream_executor_impl` was different. Instrumenting Bazel to get more informations, I could get down to the reason of the different actionKey: the ordering of the CUDA includes was different. The list itself contained the exact same content, just a different ordering.

Those includes are symlinks, and they are generated from a genrule. This is all taken care of by https://github.com/tensorflow/tensorflow/blob/ba64f5334d4bba31d22c30e09a96f806ea0e2f7e/third_party/gpus/cuda_configure.bzl#L915-L1035 which generated shell script for the genrules, that actually do perform the symlinks. Checking those shell scripts revealed the exact same and different ordering.

Checking more carefully, one will see that the headers are discovered by `_read_dir` function: https://github.com/tensorflow/tensorflow/blob/ba64f5334d4bba31d22c30e09a96f806ea0e2f7e/third_party/gpus/cuda_configure.bzl#L891-L894, it does directly get the output of `find`. This is dependant on the ordering provided by `readdir` syscall.

In our case, the ordering on the filesystem before making the tar archive, and after untarring it would be different.

One simple fix for that is to force ordering the list of headers, this way we are sure the order is always the same and we are not dependant on what `readdir` is going to get us.

In the past, Bazel would force the ordering of the elements considered to compute the actionKey. This was removed with 0.3.0 but it might have make the issue hidden https://github.com/bazelbuild/bazel/commit/9dc32111d5b6c1c7c5eaf39efad5fef75327ee75",2018-01-30T18:31:46Z,2,2,0,3.2969742043733703
142,16584,TensorFlow op to copy weights of Keras model,,"I am doing a distributed calibration of an LSTM model (keras 2.0 + TensorFlow 1.0)

    with tf.device(tf.train.replica_device_setter(...):
          model = ##create model by keras
          clone_model = ## create the same model by keras but now a stateful one

after calibration, I want my chief worker to use the clone_model, copy the weights the calibration reached in model, and make predictions on some test set, but simply calling

     clone_model.set_weights(model.get_weights())

does not work.
I understand I need to define this weight copy as an op and then call session(run) of that op

Can you please help with a TensorFlow op copying weights of a keras model to another (identical architecture) Keras model?

",1,,3,2018-01-30T13:00:44Z,2018-02-01T12:38:21Z,NONE,"I am doing a distributed calibration of an LSTM model (keras 2.0 + TensorFlow 1.0)

    with tf.device(tf.train.replica_device_setter(...):
          model = ##create model by keras
          clone_model = ## create the same model by keras but now a stateful one

after calibration, I want my chief worker to use the clone_model, copy the weights the calibration reached in model, and make predictions on some test set, but simply calling

     clone_model.set_weights(model.get_weights())

does not work.
I understand I need to define this weight copy as an op and then call session(run) of that op

Can you please help with a TensorFlow op copying weights of a keras model to another (identical architecture) Keras model?

",2018-01-30T23:47:49Z,1,1,0,3.7969742043733703
143,16580,Ensure bash is invoked as a login shell on windows otherwise fixups fail.,cla: yes,"Hi,

This PR contains a small fix to `repo.bzl` to ensure that bash is invoked as a login shell on Windows. Otherwise, depending on the `tf_http_archive` execution, bash fails to find `rm` and/or `patch`. Note this issue only manifests itself when Bazel is invoked from a Windows command prompt.

**E.g.**
```
ERROR: Skipping '//tensorflow:libtensorflow.so': error loading package 'tensorflow': Encountered error while reading extension file 'protobuf.bzl': no such package '@protobuf_archive//': Traceback (most recent call last):
        File ""<redacted>/third_party/repo.bzl"", line 88
                _apply_patch(ctx, ctx.attr.patch_file)
        File ""<redacted>/third_party/repo.bzl"", line 59, in _apply_patch
                _execute_and_check_ret_code(ctx, cmd)
        File ""<redacted>/third_party/repo.bzl"", line 44, in _execute_and_check_ret_code
                fail(""Non-zero return code({1}) when ...))
Non-zero return code(127) when executing 'C:\tools\msys64\usr\bin\bash.exe -c patch -p1 -d <redacted>/4bwywwqm/external/protobuf_archive -i <redacted>/third_party/protobuf/add_noinlines.patch':
Stdout:
Stderr: /usr/bin/bash: patch: command not found
```
**Environment**

||Version|
|--|--|
|OS|Win10-Ent|
|Bazel|0.8.1|
|msys2|msys2-x86_64-20161025|

Cheers,

Andy",0,,4,2018-01-30T10:52:42Z,2018-02-05T23:33:51Z,CONTRIBUTOR,"Hi,

This PR contains a small fix to `repo.bzl` to ensure that bash is invoked as a login shell on Windows. Otherwise, depending on the `tf_http_archive` execution, bash fails to find `rm` and/or `patch`. Note this issue only manifests itself when Bazel is invoked from a Windows command prompt.

**E.g.**
```
ERROR: Skipping '//tensorflow:libtensorflow.so': error loading package 'tensorflow': Encountered error while reading extension file 'protobuf.bzl': no such package '@protobuf_archive//': Traceback (most recent call last):
        File ""<redacted>/third_party/repo.bzl"", line 88
                _apply_patch(ctx, ctx.attr.patch_file)
        File ""<redacted>/third_party/repo.bzl"", line 59, in _apply_patch
                _execute_and_check_ret_code(ctx, cmd)
        File ""<redacted>/third_party/repo.bzl"", line 44, in _execute_and_check_ret_code
                fail(""Non-zero return code({1}) when ...))
Non-zero return code(127) when executing 'C:\tools\msys64\usr\bin\bash.exe -c patch -p1 -d <redacted>/4bwywwqm/external/protobuf_archive -i <redacted>/third_party/protobuf/add_noinlines.patch':
Stdout:
Stderr: /usr/bin/bash: patch: command not found
```
**Environment**

||Version|
|--|--|
|OS|Win10-Ent|
|Bazel|0.8.1|
|msys2|msys2-x86_64-20161025|

Cheers,

Andy",2018-01-30T23:57:57Z,5,2,1,4.29697420437337
144,16570,Fix typos.,cla: yes,,0,,3,2018-01-30T03:59:02Z,2018-02-01T03:00:52Z,CONTRIBUTOR,,2018-01-30T04:06:40Z,1,2,0,3.7969742043733703
145,16568,Removing duplicate code block that raises exception,cla: yes,"For TensorFlow version 1.5.0-rc1, the code block below raises a `ValueError`. Simply remove the duplication (lines 274 - 277 are exactly the same) and the issue is resolved.

```
# Add returned summaries to writer in each step.
writer.add_summary(summary, step)
# Add metadata to visualize the graph for the last run.
if step == (num_steps - 1):
      writer.add_run_metadata(run_metadata, 'step%d' % step)
```",0,,3,2018-01-30T03:19:48Z,2018-02-05T23:35:47Z,CONTRIBUTOR,"For TensorFlow version 1.5.0-rc1, the code block below raises a `ValueError`. Simply remove the duplication (lines 274 - 277 are exactly the same) and the issue is resolved.

```
# Add returned summaries to writer in each step.
writer.add_summary(summary, step)
# Add metadata to visualize the graph for the last run.
if step == (num_steps - 1):
      writer.add_run_metadata(run_metadata, 'step%d' % step)
```",2018-01-30T03:21:43Z,5,2,1,3.7969742043733703
146,16566,add visibility to  //tensorflow/contrib/tensor_forest/proto:fertile_stats_proto,cla: yes,"Since the proto is a public-visible and TreePath depending on the proto 

Let's make it public...",1,,4,2018-01-30T03:04:18Z,2018-02-15T23:20:47Z,CONTRIBUTOR,"Since the proto is a public-visible and TreePath depending on the proto 

Let's make it public...",2018-02-12T19:07:16Z,15,2,3,5.29697420437337
147,16564,Ndlstm dynamic batch size,"awaiting review,cla: yes,stat:awaiting response","Apparently, lstm2d.separable_lstm doesn't accept dynamic batch sizes (number of images). This pull request addresses issue https://github.com/tensorflow/tensorflow/issues/16510",1,,7,2018-01-30T01:27:17Z,2018-02-12T04:14:02Z,CONTRIBUTOR,"Apparently, lstm2d.separable_lstm doesn't accept dynamic batch sizes (number of images). This pull request addresses issue https://github.com/tensorflow/tensorflow/issues/16510",2018-02-03T08:09:28Z,12,2,2,6.79697420437337
148,16563,Minor refactor to remove redundant test class,cla: yes,"There were two WeightNormLSTMCellTest classes that differed in formatting only.  Removed the (original) one at end of file since based on history, the version above it contained latest formatting updates.",0,,1,2018-01-30T01:10:00Z,2018-02-01T18:20:46Z,CONTRIBUTOR,"There were two WeightNormLSTMCellTest classes that differed in formatting only.  Removed the (original) one at end of file since based on history, the version above it contained latest formatting updates.",2018-02-01T17:53:04Z,1,2,0,2.7969742043733703
149,16560,Add remove_control_dependencies() graph_transform.,"awaiting testing (then merge),cla: yes",Add a graph_transform that can be used to remove control dependencies from the tensorflow graph. This can allow later passes such as strip_unused_nodes to do a better job.  ,1,,4,2018-01-29T23:42:45Z,2018-02-16T20:57:51Z,CONTRIBUTOR,Add a graph_transform that can be used to remove control dependencies from the tensorflow graph. This can allow later passes such as strip_unused_nodes to do a better job.  ,2018-01-30T03:17:32Z,17,2,3,5.294014103795206
150,16557,MKL: Fix for mkl_input conversion for MKL DNN. ,"awaiting testing (then merge),cla: yes",Fix also enables elementwise operations in MKL,0,,2,2018-01-29T23:07:55Z,2018-02-01T18:10:54Z,CONTRIBUTOR,Fix also enables elementwise operations in MKL,2018-01-31T00:30:06Z,2,2,0,3.294014103795206
151,16556,tf.argmax appears to be functioning incorrectly on occasion,"stat:awaiting response,type:bug/performance","**EDIT**

[Link to script and input/label data as pickle files to reproduce the error.](https://drive.google.com/open?id=13Kice6p3IQvRVOKIlW0DSvD9wwJDL-YH)

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
I have written my own code. My code base, data set and batch generating algorithm are quite large, so I am attempting to illustrate this as best as possible. If no mistake of mine can be seen in this post and the examples I have given, then I will provide further code/data. I have asked on StackOverflow, but have got not replies. If  I am missing something simple, then I'm sure it would have been pointed out on StackOverflow by now.

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  
Manjaro 17.1.3 Kernel 4.14

- **TensorFlow installed from (source or binary)**:
python pip

- **TensorFlow version (use command below)**:
tensorflow-gpu 1.5.0

- **Python version**:
3.6.4

- **CUDA/cuDNN version**:
CUDA 9.0
cuDNN 7.0

- **GPU model and memory**: 
Nvidia GeForce GTX 1050 8GB

### Describe the problem
tf.argmax seems to be occasionally producing incorrect results when used on the last axis of a 3-dimensional tensor.

### Source code / logs
To debug this, I have printed out the following operations:

```
print(self.session.run(
    tf.equal(tf.argmax(self.predictions, axis=-1),
             tf.argmax(self.labelsUnrolled, axis=-1)),
    self.batchDict))
print("""")
print(self.session.run(self.predictions, self.batchDict))
print("""")
print(self.session.run(self.labelsUnrolled, self.batchDict))
print(""\n********\n"")
```

Which on two consecutive iterations output the following:

```
[[ True  True  True]
 [ True  True  True]]

[array([[0.06275553, 0.44493628, 0.42474008, 0.06756803],
        [0.06320112, 0.49631155, 0.4021484 , 0.03833894],
        [0.04378054, 0.59403986, 0.3236889 , 0.03849069]], dtype=float32), 
array([[8.1677590e-06, 9.9997127e-01, 2.0200867e-05, 3.6184446e-07],
        [4.3686719e-06, 9.9992716e-01, 6.6905603e-05, 1.5286902e-06],
        [1.3270236e-05, 9.9986196e-01, 1.1622251e-04, 8.5579613e-06]], dtype=float32)]

[array([[0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.]], dtype=float32),
 array([[0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.]], dtype=float32)]

********

[[False False  True]
 [ True  True  True]]

 [array([[0.0466171 , 0.53605616, 0.37778312, 0.03954368],
         [0.05007472, 0.4603508 , 0.44400516, 0.0455693 ],
         [0.06134444, 0.38073638, 0.504286  , 0.05363319]], dtype=float32),
 array([[9.6363285e-05, 9.9861979e-01, 1.2741127e-03, 9.7381862e-06],
         [1.6185455e-05, 9.9977034e-01, 2.0742408e-04, 6.0521238e-06],
         [2.9893983e-05, 9.9954152e-01, 4.2436572e-04, 4.2021661e-06]], dtype=float32)]

[array([[0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.]], dtype=float32), 
 array([[0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.]], dtype=float32)]

********
```

Isn't the very first `True` in the first iteration incorrect? There are many more examples being executed where these are wrong (although it is right the majority of the time). Am I going wrong somewhere with the `tf.argmax` function?

Printing out the same operations, but not inside the session gives the following shapes:

```
Tensor(""Equal_175:0"", shape=(2, ?), dtype=bool)

[<tf.Tensor 'unstack_2:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'unstack_2:1' shape=(?, 4) dtype=float32>]

[<tf.Tensor 'unstack_1:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'unstack_1:1' shape=(?, 4) dtype=float32>]
```

Is it a problem that the number associated with the tensor name ""Equal_XXX:0"" is incrementing each iteration?

I have also tried changing the axis argument in both argmax functions to `axis=2`, giving the ""Equal"" tensor a shape of (2, 3) again, but there are still similar errors.

Here is an example:

```
[[False False  True]
 [ True  True False]]

[array([[0.09075877, 0.41096467, 0.4460272 , 0.05224944],
        [0.04962843, 0.43777955, 0.46654516, 0.04604685],
        [0.07901238, 0.40768984, 0.46641603, 0.04688181]], dtype=float32),
 array([[0.04444276, 0.49195835, 0.42141557, 0.04218334],
        [0.02372498, 0.47147286, 0.4679979 , 0.03680426],
        [0.03707527, 0.435518  , 0.48937747, 0.03802926]], dtype=float32)]

[array([[0., 0., 1., 0.],
        [1., 0., 0., 0.],
        [0., 0., 1., 0.]], dtype=float32),
 array([[0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.]], dtype=float32)]
```

I would expect this to be:

```
[[ True False  True]
 [ True False False]]
```

I thought this may have been a problem with parallel computations on the GPU, but I tried the same execution on just my CPU and got the following, similar miscalculation, too:

```
[[False  True False]
 [False False False]]

[array([[0.07774187, 0.40993363, 0.47022063, 0.04210386],
        [0.04910654, 0.44086066, 0.46013904, 0.04989377],
        [0.06700655, 0.37128285, 0.51324743, 0.04846317]], dtype=float32), 
array([[0.07584244, 0.3863555 , 0.5090046 , 0.02879751],
        [0.06959026, 0.3221606 , 0.5715027 , 0.03674646],
        [0.09042579, 0.32515866, 0.5385905 , 0.04582503]], dtype=float32)]

[array([[0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.]], dtype=float32),
array([[0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.]], dtype=float32)]
```

",0,,6,2018-01-29T22:34:01Z,2018-01-31T02:34:55Z,NONE,"**EDIT**

[Link to script and input/label data as pickle files to reproduce the error.](https://drive.google.com/open?id=13Kice6p3IQvRVOKIlW0DSvD9wwJDL-YH)

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
I have written my own code. My code base, data set and batch generating algorithm are quite large, so I am attempting to illustrate this as best as possible. If no mistake of mine can be seen in this post and the examples I have given, then I will provide further code/data. I have asked on StackOverflow, but have got not replies. If  I am missing something simple, then I'm sure it would have been pointed out on StackOverflow by now.

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  
Manjaro 17.1.3 Kernel 4.14

- **TensorFlow installed from (source or binary)**:
python pip

- **TensorFlow version (use command below)**:
tensorflow-gpu 1.5.0

- **Python version**:
3.6.4

- **CUDA/cuDNN version**:
CUDA 9.0
cuDNN 7.0

- **GPU model and memory**: 
Nvidia GeForce GTX 1050 8GB

### Describe the problem
tf.argmax seems to be occasionally producing incorrect results when used on the last axis of a 3-dimensional tensor.

### Source code / logs
To debug this, I have printed out the following operations:

```
print(self.session.run(
    tf.equal(tf.argmax(self.predictions, axis=-1),
             tf.argmax(self.labelsUnrolled, axis=-1)),
    self.batchDict))
print("""")
print(self.session.run(self.predictions, self.batchDict))
print("""")
print(self.session.run(self.labelsUnrolled, self.batchDict))
print(""\n********\n"")
```

Which on two consecutive iterations output the following:

```
[[ True  True  True]
 [ True  True  True]]

[array([[0.06275553, 0.44493628, 0.42474008, 0.06756803],
        [0.06320112, 0.49631155, 0.4021484 , 0.03833894],
        [0.04378054, 0.59403986, 0.3236889 , 0.03849069]], dtype=float32), 
array([[8.1677590e-06, 9.9997127e-01, 2.0200867e-05, 3.6184446e-07],
        [4.3686719e-06, 9.9992716e-01, 6.6905603e-05, 1.5286902e-06],
        [1.3270236e-05, 9.9986196e-01, 1.1622251e-04, 8.5579613e-06]], dtype=float32)]

[array([[0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.]], dtype=float32),
 array([[0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.]], dtype=float32)]

********

[[False False  True]
 [ True  True  True]]

 [array([[0.0466171 , 0.53605616, 0.37778312, 0.03954368],
         [0.05007472, 0.4603508 , 0.44400516, 0.0455693 ],
         [0.06134444, 0.38073638, 0.504286  , 0.05363319]], dtype=float32),
 array([[9.6363285e-05, 9.9861979e-01, 1.2741127e-03, 9.7381862e-06],
         [1.6185455e-05, 9.9977034e-01, 2.0742408e-04, 6.0521238e-06],
         [2.9893983e-05, 9.9954152e-01, 4.2436572e-04, 4.2021661e-06]], dtype=float32)]

[array([[0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.]], dtype=float32), 
 array([[0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.]], dtype=float32)]

********
```

Isn't the very first `True` in the first iteration incorrect? There are many more examples being executed where these are wrong (although it is right the majority of the time). Am I going wrong somewhere with the `tf.argmax` function?

Printing out the same operations, but not inside the session gives the following shapes:

```
Tensor(""Equal_175:0"", shape=(2, ?), dtype=bool)

[<tf.Tensor 'unstack_2:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'unstack_2:1' shape=(?, 4) dtype=float32>]

[<tf.Tensor 'unstack_1:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'unstack_1:1' shape=(?, 4) dtype=float32>]
```

Is it a problem that the number associated with the tensor name ""Equal_XXX:0"" is incrementing each iteration?

I have also tried changing the axis argument in both argmax functions to `axis=2`, giving the ""Equal"" tensor a shape of (2, 3) again, but there are still similar errors.

Here is an example:

```
[[False False  True]
 [ True  True False]]

[array([[0.09075877, 0.41096467, 0.4460272 , 0.05224944],
        [0.04962843, 0.43777955, 0.46654516, 0.04604685],
        [0.07901238, 0.40768984, 0.46641603, 0.04688181]], dtype=float32),
 array([[0.04444276, 0.49195835, 0.42141557, 0.04218334],
        [0.02372498, 0.47147286, 0.4679979 , 0.03680426],
        [0.03707527, 0.435518  , 0.48937747, 0.03802926]], dtype=float32)]

[array([[0., 0., 1., 0.],
        [1., 0., 0., 0.],
        [0., 0., 1., 0.]], dtype=float32),
 array([[0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.]], dtype=float32)]
```

I would expect this to be:

```
[[ True False  True]
 [ True False False]]
```

I thought this may have been a problem with parallel computations on the GPU, but I tried the same execution on just my CPU and got the following, similar miscalculation, too:

```
[[False  True False]
 [False False False]]

[array([[0.07774187, 0.40993363, 0.47022063, 0.04210386],
        [0.04910654, 0.44086066, 0.46013904, 0.04989377],
        [0.06700655, 0.37128285, 0.51324743, 0.04846317]], dtype=float32), 
array([[0.07584244, 0.3863555 , 0.5090046 , 0.02879751],
        [0.06959026, 0.3221606 , 0.5715027 , 0.03674646],
        [0.09042579, 0.32515866, 0.5385905 , 0.04582503]], dtype=float32)]

[array([[0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.]], dtype=float32),
array([[0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.]], dtype=float32)]
```

",2018-01-30T02:51:17Z,2,1,0,4.294014103795206
152,16552,Sampled softmax loss stops gradients on sampled classes,stat:awaiting tensorflower,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:
Binary
- **TensorFlow version (use command below)**:
1.3
- **Python version**: 
2.7
- **Bazel version (if compiling from source)**:
N/A
- **GCC/Compiler version (if compiling from source)**:
N/A
- **CUDA/cuDNN version**:
CUDA 8 / cuDNN 6
- **GPU model and memory**:
4 x TITAN X (Pascal)

### Describe the problem

The backbone of TensorFlow's sampled loss functions `nce_loss` and `sampled_softmax_loss` is a helper function called `_compute_sampled_logits`https://github.com/tensorflow/tensorflow/blob/8fb12848d3a81a010714a4612ffd735106ea83d8/tensorflow/python/ops/nn_impl.py#L961-L1139.

`_compute_sampled_logits` takes as input:

- weights and biases of the final layer,
- the output labels
- the inputs to the final layer inputs
- the sampled values of the output layer
- a few other things

and returns the logits and labels of only the requested sampled labels.

One of the first ops executed is https://github.com/tensorflow/tensorflow/blob/8fb12848d3a81a010714a4612ffd735106ea83d8/tensorflow/python/ops/nn_impl.py#L1046-L1047 

This line seems like it is stopping gradients flowing back through the sampled values if i'm reading it correctly.

Shouldn't the gradients be stopped from flowing back through the _non-sampled values_ as opposed to the _sampled values_? Why are gradients being stopped at the sampled values? 
",0,,4,2018-01-29T18:24:23Z,2018-02-13T20:40:48Z,NONE,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:
Binary
- **TensorFlow version (use command below)**:
1.3
- **Python version**: 
2.7
- **Bazel version (if compiling from source)**:
N/A
- **GCC/Compiler version (if compiling from source)**:
N/A
- **CUDA/cuDNN version**:
CUDA 8 / cuDNN 6
- **GPU model and memory**:
4 x TITAN X (Pascal)

### Describe the problem

The backbone of TensorFlow's sampled loss functions `nce_loss` and `sampled_softmax_loss` is a helper function called `_compute_sampled_logits`https://github.com/tensorflow/tensorflow/blob/8fb12848d3a81a010714a4612ffd735106ea83d8/tensorflow/python/ops/nn_impl.py#L961-L1139.

`_compute_sampled_logits` takes as input:

- weights and biases of the final layer,
- the output labels
- the inputs to the final layer inputs
- the sampled values of the output layer
- a few other things

and returns the logits and labels of only the requested sampled labels.

One of the first ops executed is https://github.com/tensorflow/tensorflow/blob/8fb12848d3a81a010714a4612ffd735106ea83d8/tensorflow/python/ops/nn_impl.py#L1046-L1047 

This line seems like it is stopping gradients flowing back through the sampled values if i'm reading it correctly.

Shouldn't the gradients be stopped from flowing back through the _non-sampled values_ as opposed to the _sampled values_? Why are gradients being stopped at the sampled values? 
",2018-01-30T02:46:24Z,14,1,2,3.294014103795206
153,16550,Add options to enable new features for cloud-tpu-profiler.,"awaiting review,cla: yes","Add options for the user to manually include dataset ops in trace collection, and to automatically recapture the traces when no trace event is collected.
Also change tf.flags to absl.flags since the former is going to be deprecated. ",0,,1,2018-01-29T17:28:23Z,2018-01-30T06:58:20Z,CONTRIBUTOR,"Add options for the user to manually include dataset ops in trace collection, and to automatically recapture the traces when no trace event is collected.
Also change tf.flags to absl.flags since the former is going to be deprecated. ",2018-01-29T18:07:02Z,1,2,0,2.794014103795206
154,16548,AttributeError: module 'tensorflow.python.layers.layers' has no attribute 'conv_2d',stat:awaiting response,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: see below
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: OSX 10.13.2
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.5.0
- **Python version**: 3.6.3
- **GPU model and memory**: no GPU
- **Exact command to reproduce**:

```python
class my_RNNCell(tf.nn.rnn_cell.RNNCell):
    def __init__(self):
        super(my_RNNCell, self).__init__()
        self._output_size = 2
        self._state_size = 2

    def __call__(self, tensor_in, state):

        output = tf.layers.conv_2d(tensor_in, 1, [1, 10])

        return output, output
    
    @property
    def output_size(self):
        return self._output_size
    @property
    def state_size(self):
        return self._state_size


tf.nn.dynamic_rnn(my_RNNCell(), inputs=tf.placeholder(shape=[None,2,100,3], dtype=tf.float32), initial_state= tf.placeholder(shape=[None,2], dtype=tf.float32))

>>>
AttributeError: module 'tensorflow.python.layers.layers' has no attribute 'conv_2d'
```
",0,,8,2018-01-29T17:01:12Z,2018-01-31T19:21:38Z,NONE,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: see below
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: OSX 10.13.2
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.5.0
- **Python version**: 3.6.3
- **GPU model and memory**: no GPU
- **Exact command to reproduce**:

```python
class my_RNNCell(tf.nn.rnn_cell.RNNCell):
    def __init__(self):
        super(my_RNNCell, self).__init__()
        self._output_size = 2
        self._state_size = 2

    def __call__(self, tensor_in, state):

        output = tf.layers.conv_2d(tensor_in, 1, [1, 10])

        return output, output
    
    @property
    def output_size(self):
        return self._output_size
    @property
    def state_size(self):
        return self._state_size


tf.nn.dynamic_rnn(my_RNNCell(), inputs=tf.placeholder(shape=[None,2,100,3], dtype=tf.float32), initial_state= tf.placeholder(shape=[None,2], dtype=tf.float32))

>>>
AttributeError: module 'tensorflow.python.layers.layers' has no attribute 'conv_2d'
```
",2018-01-30T02:42:37Z,2,1,0,5.294014103795206
155,16544,"deconv_output_length(input_length, filter_size, padding, stride)",,"deconv_output_length(input_length, filter_size, padding, stride) is defined [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/layers/utils.py#L159)

When padding is valid, input_length += max(filter_size - stride, 0), why to use `max` function?
See more details on #2118

For conv2d:
```
output = (input - filter + stride) // stride  # VALID
output = (input + stride - 1) // stride  # SAME
```

For conv2d_transpose:
```
output = input * stride + filter - stride  # VALID
output = input * stride - stride + 1  # SAME 
```

Even when filter_size is less than stride, I think output is also `input * stride + filter - stride` rather `input * stride`, so why to use `max`?

 
",0,,1,2018-01-29T15:12:08Z,2018-01-30T02:37:56Z,NONE,"deconv_output_length(input_length, filter_size, padding, stride) is defined [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/layers/utils.py#L159)

When padding is valid, input_length += max(filter_size - stride, 0), why to use `max` function?
See more details on #2118

For conv2d:
```
output = (input - filter + stride) // stride  # VALID
output = (input + stride - 1) // stride  # SAME
```

For conv2d_transpose:
```
output = input * stride + filter - stride  # VALID
output = input * stride - stride + 1  # SAME 
```

Even when filter_size is less than stride, I think output is also `input * stride + filter - stride` rather `input * stride`, so why to use `max`?

 
",2018-01-30T02:37:56Z,1,1,0,1.794014103795206
156,16539,How to use tensorflow library in c programs??,,"I build shared libraries in tensorflow-serving with bazel.
However, I can not build C programs with the shared libraries.

I build C programs like this:
g++ main.cc -L../../bazel-bin/tensorflow_serving/rnnlm -lrnnlm_client_run -ldata_generator

Details is shown below:

../../bazel-bin/tensorflow_serving/rnnlm/librnnlm_client_run.so: undefined reference to `tensorflow::serving::PredictionService::Stub::Predict(grpc::Cli
    entContext*, tensorflow::serving::PredictRequest const&, tensorflow::serving::PredictResponse*)'
 12 ../../bazel-bin/tensorflow_serving/rnnlm/librnnlm_client_run.so: undefined reference to `google::protobuf::internal::fixed_address_empty_string'
 13 ../../bazel-bin/tensorflow_serving/rnnlm/librnnlm_client_run.so: undefined reference to `tensorflow::Tensor::CheckTypeAndIsAligned(tensorflow::DataType)
     const'
 14 ../../bazel-bin/tensorflow_serving/rnnlm/librnnlm_client_run.so: undefined reference to `tensorflow::serving::PredictRequest::_slow_mutable_model_spec()
    '
 15 ../../bazel-bin/tensorflow_serving/rnnlm/librnnlm_client_run.so: undefined reference to `tensorflow::Tensor::FromProto(tensorflow::TensorProto const&)'
 16 ../../bazel-bin/tensorflow_serving/rnnlm/librnnlm_client_run.so: undefined reference to `tensorflow::serving::PredictResponse::PredictResponse()'
 17 ../../bazel-bin/tensorflow_serving/rnnlm/librnnlm_client_run.so: undefined reference to `tensorflow::TensorShape::CheckDimsEqual(int) const'
 18 ../../bazel-bin/tensorflow_serving/rnnlm/librnnlm_client_run.so: undefined reference to `tensorflow::Tensor::Tensor()'
 19 ../../bazel-bin/tensorflow_serving/rnnlm/librnnlm_client_run.so: undefined reference to `tensorflow::serving::PredictionService::NewStub(std::shared_ptr
    <grpc::ChannelInterface> const&, grpc::StubOptions const&)'
 20 ../../bazel-bin/tensorflow_serving/rnnlm/librnnlm_client_run.so: undefined reference to `tensorflow::TensorShapeProto_Dim::TensorShapeProto_Dim(google::
    protobuf::Arena*)'
 21 ../../bazel-bin/tensorflow_serving/rnnlm/librnnlm_client_run.so: undefined reference to `tensorflow::TensorProto::TensorProto(google::protobuf::Arena*)'
 22 ../../bazel-bin/tensorflow_serving/rnnlm/librnnlm_client_run.so: undefined reference to `google::protobuf::internal::ArenaImpl::AllocateAligned(unsigned
     long)'
 23 ../../bazel-bin/tensorflow_serving/rnnlm/librnnlm_client_run.so: undefined reference to `tensorflow::TensorShapeBase<tensorflow::TensorShape>::dim_size(
    int) const'
 24 ../../bazel-bin/tensorflow_serving/rnnlm/librnnlm_client_run.so: undefined reference to `tensorflow::serving::PredictRequest::~PredictRequest()'
 25 ../../bazel-bin/tensorflow_serving/rnnlm/librnnlm_client_run.so: undefined reference to `google::protobuf::internal::RepeatedPtrFieldBase::Reserve(int)'
 26 ../../bazel-bin/tensorflow_serving/rnnlm/librnnlm_client_run.so: undefined reference to `tensorflow::TensorShapeProto_Dim::TensorShapeProto_Dim()'
 27 ../../bazel-bin/tensorflow_serving/rnnlm/librnnlm_client_run.so: undefined reference to `tensorflow::serving::PredictRequest::PredictRequest()'
 28 ../../bazel-bin/tensorflow_serving/rnnlm/librnnlm_client_run.so: undefined reference to `google::protobuf::Arena::OnArenaAllocation(std::type_info const
    *, unsigned long) const'
 29 ../../bazel-bin/tensorflow_serving/rnnlm/librnnlm_client_run.so: undefined reference to `tensorflow::TensorProto::~TensorProto()'
 30 ../../bazel-bin/tensorflow_serving/rnnlm/librnnlm_client_run.so: undefined reference to `tensorflow::TensorProto::_slow_mutable_tensor_shape()'",0,,2,2018-01-29T12:06:26Z,2018-02-07T00:20:10Z,NONE,"I build shared libraries in tensorflow-serving with bazel.
However, I can not build C programs with the shared libraries.

I build C programs like this:
g++ main.cc -L../../bazel-bin/tensorflow_serving/rnnlm -lrnnlm_client_run -ldata_generator

Details is shown below:

../../bazel-bin/tensorflow_serving/rnnlm/librnnlm_client_run.so: undefined reference to `tensorflow::serving::PredictionService::Stub::Predict(grpc::Cli
    entContext*, tensorflow::serving::PredictRequest const&, tensorflow::serving::PredictResponse*)'
 12 ../../bazel-bin/tensorflow_serving/rnnlm/librnnlm_client_run.so: undefined reference to `google::protobuf::internal::fixed_address_empty_string'
 13 ../../bazel-bin/tensorflow_serving/rnnlm/librnnlm_client_run.so: undefined reference to `tensorflow::Tensor::CheckTypeAndIsAligned(tensorflow::DataType)
     const'
 14 ../../bazel-bin/tensorflow_serving/rnnlm/librnnlm_client_run.so: undefined reference to `tensorflow::serving::PredictRequest::_slow_mutable_model_spec()
    '
 15 ../../bazel-bin/tensorflow_serving/rnnlm/librnnlm_client_run.so: undefined reference to `tensorflow::Tensor::FromProto(tensorflow::TensorProto const&)'
 16 ../../bazel-bin/tensorflow_serving/rnnlm/librnnlm_client_run.so: undefined reference to `tensorflow::serving::PredictResponse::PredictResponse()'
 17 ../../bazel-bin/tensorflow_serving/rnnlm/librnnlm_client_run.so: undefined reference to `tensorflow::TensorShape::CheckDimsEqual(int) const'
 18 ../../bazel-bin/tensorflow_serving/rnnlm/librnnlm_client_run.so: undefined reference to `tensorflow::Tensor::Tensor()'
 19 ../../bazel-bin/tensorflow_serving/rnnlm/librnnlm_client_run.so: undefined reference to `tensorflow::serving::PredictionService::NewStub(std::shared_ptr
    <grpc::ChannelInterface> const&, grpc::StubOptions const&)'
 20 ../../bazel-bin/tensorflow_serving/rnnlm/librnnlm_client_run.so: undefined reference to `tensorflow::TensorShapeProto_Dim::TensorShapeProto_Dim(google::
    protobuf::Arena*)'
 21 ../../bazel-bin/tensorflow_serving/rnnlm/librnnlm_client_run.so: undefined reference to `tensorflow::TensorProto::TensorProto(google::protobuf::Arena*)'
 22 ../../bazel-bin/tensorflow_serving/rnnlm/librnnlm_client_run.so: undefined reference to `google::protobuf::internal::ArenaImpl::AllocateAligned(unsigned
     long)'
 23 ../../bazel-bin/tensorflow_serving/rnnlm/librnnlm_client_run.so: undefined reference to `tensorflow::TensorShapeBase<tensorflow::TensorShape>::dim_size(
    int) const'
 24 ../../bazel-bin/tensorflow_serving/rnnlm/librnnlm_client_run.so: undefined reference to `tensorflow::serving::PredictRequest::~PredictRequest()'
 25 ../../bazel-bin/tensorflow_serving/rnnlm/librnnlm_client_run.so: undefined reference to `google::protobuf::internal::RepeatedPtrFieldBase::Reserve(int)'
 26 ../../bazel-bin/tensorflow_serving/rnnlm/librnnlm_client_run.so: undefined reference to `tensorflow::TensorShapeProto_Dim::TensorShapeProto_Dim()'
 27 ../../bazel-bin/tensorflow_serving/rnnlm/librnnlm_client_run.so: undefined reference to `tensorflow::serving::PredictRequest::PredictRequest()'
 28 ../../bazel-bin/tensorflow_serving/rnnlm/librnnlm_client_run.so: undefined reference to `google::protobuf::Arena::OnArenaAllocation(std::type_info const
    *, unsigned long) const'
 29 ../../bazel-bin/tensorflow_serving/rnnlm/librnnlm_client_run.so: undefined reference to `tensorflow::TensorProto::~TensorProto()'
 30 ../../bazel-bin/tensorflow_serving/rnnlm/librnnlm_client_run.so: undefined reference to `tensorflow::TensorProto::_slow_mutable_tensor_shape()'",2018-01-29T19:23:26Z,8,1,2,2.294014103795206
157,16537,added audio_ops.cc to tf_op_files.txt to fix the Op type not registered DecodeWav error ,cla: yes,- https://github.com/tensorflow/tensorflow/issues/15921,0,,5,2018-01-29T07:38:00Z,2018-02-01T22:31:05Z,CONTRIBUTOR,- https://github.com/tensorflow/tensorflow/issues/15921,2018-01-29T07:39:22Z,2,2,0,4.794014103795206
158,16536,Feature Request: global average pooling layer in tf.layers,type:feature,"Hello,

Can we add an implementation of global_average_pooling to tf.layers or tf.contrib.layers? It can look much like the Keras implementation here: https://www.tensorflow.org/api_docs/python/tf/keras/layers/GlobalMaxPool3D
and essentially just requires calling tf.reduce_mean.

I suspect lots of people have written functions called global_pooling that just call reduce mean, and it would be nice to have a tf.layers function that just does this for consistency/readability.

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: AWS Deep Learning AMI (Conda)
- **TensorFlow version (use command below)**: 1.5
- **Python version**:  3.6
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: K80
- **Exact command to reproduce**: N/A

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",1,,8,2018-01-29T07:36:48Z,2018-02-14T22:58:30Z,NONE,"Hello,

Can we add an implementation of global_average_pooling to tf.layers or tf.contrib.layers? It can look much like the Keras implementation here: https://www.tensorflow.org/api_docs/python/tf/keras/layers/GlobalMaxPool3D
and essentially just requires calling tf.reduce_mean.

I suspect lots of people have written functions called global_pooling that just call reduce mean, and it would be nice to have a tf.layers function that just does this for consistency/readability.

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: AWS Deep Learning AMI (Conda)
- **TensorFlow version (use command below)**: 1.5
- **Python version**:  3.6
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: K80
- **Exact command to reproduce**: N/A

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",2018-01-30T02:28:47Z,15,1,3,6.294014103795206
159,16534,warning with tf.losses.softmax_cross_entropy,stat:contributions welcome,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows and Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.5
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: 9/7
- **GPU model and memory**:
- **Exact command to reproduce**:

### Describe the problem
tf.losses.softmax_cross_entropy calls tf.nn.softmax_cross_entropy_with_logits, in which there is a warning. It's better also provide tf.losses.softmax_cross_entropy_v2 to call tf.nn.softmax_cross_entropy_with_logits_v2.
",0,,5,2018-01-29T06:17:48Z,2018-02-08T00:14:23Z,NONE,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows and Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.5
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: 9/7
- **GPU model and memory**:
- **Exact command to reproduce**:

### Describe the problem
tf.losses.softmax_cross_entropy calls tf.nn.softmax_cross_entropy_with_logits, in which there is a warning. It's better also provide tf.losses.softmax_cross_entropy_v2 to call tf.nn.softmax_cross_entropy_with_logits_v2.
",2018-01-30T02:28:04Z,9,1,2,3.794014103795206
160,16533,Make Lstm1d.ndlstm_base_unrolled use lstm_cell with state_is_tuple = True,"awaiting testing (then merge),cla: yes,stat:awaiting response",This is to address the deprecation warning thrown by using `state_is_tuple` = False.,1,,2,2018-01-29T05:52:17Z,2018-02-12T04:13:53Z,CONTRIBUTOR,This is to address the deprecation warning thrown by using `state_is_tuple` = False.,2018-02-12T04:01:06Z,13,2,2,4.294014103795206
161,16524,MKL: Reverting the switch to max_pool_v2 in python,cla: yes,"A prior commit https://github.com/tensorflow/tensorflow/pull/14983 changed python interface to call max_pool_v2 causing failure in MKL build. Currently MKL doesn't support max_pool_v2. Reverting  the commit  for now, will change it back when MKL implementation is complete.",1,,1,2018-01-28T20:54:41Z,2018-01-29T17:46:06Z,CONTRIBUTOR,"A prior commit https://github.com/tensorflow/tensorflow/pull/14983 changed python interface to call max_pool_v2 causing failure in MKL build. Currently MKL doesn't support max_pool_v2. Reverting  the commit  for now, will change it back when MKL implementation is complete.",2018-01-29T14:44:24Z,1,2,0,3.7912066762199625
162,16517,ou must feed a value for placeholder tensor 'import/Placeholder when i test my frozen model,,"Hello , I trying create mobile app for object recognition for my own created model. I fallow this tutorial https://codelabs.developers.google.com/codelabs/tensorflow-for-poets-2/#2
But when i even get a testing model from step 3 i get error 

```
InvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'import/Placeholder' with dtype float
	 [[Node: import/Placeholder = Placeholder[dtype=DT_FLOAT, shape=<unknown>, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]
```

I'm aware that is wrong node problem, but i trying with other and always i get failure 

My code for model creation 
```

x = tf.placeholder(tf.float32,
                   shape=[None, cons.IMAGE_SIZE, cons.IMAGE_SIZE, 3], name=""x"")
y_ = tf.placeholder(tf.float32, shape=[None, cons.LABELS_NUMB], name=""labels"")

K = 4
L = 8
M = 12
N = 200

x_image = tf.reshape(x, [-1, cons.IMAGE_SIZE, cons.IMAGE_SIZE, 3])
tf.summary.image('input', x_image, 3)
print(""X image "")
print(tf.shape(x_image))

################## first ##############

W_conv1 = weight_variable([5, 5, 3, 32], ""weight1"")
b_conv1 = bias_variable([32], ""bias1"")

h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)
h_pool1 = max_pool_2x2(h_conv1)
print(""W_conv1 "")
print(tf.shape(W_conv1))
print(""b_conv1 "")
print(tf.shape(b_conv1))
print(""h_conv1 "")
print(tf.shape(h_conv1))
print(""h_pool1 "")
print(tf.shape(h_pool1))

################## second ##############

W_conv2 = weight_variable([5, 5, 32, 64], ""weight2"")
b_conv2 = bias_variable([64], ""bias2"")

h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)
tf.summary.histogram(""activations"", h_conv2)


h_pool2 = max_pool_2x2(h_conv2)

print(""W_conv2 "")
print(tf.shape(W_conv2))
print(""b_conv2 "")
print(tf.shape(b_conv2))
print(""h_conv2 "")
print(tf.shape(h_conv2))
print(""h_pool2 "")
print(tf.shape(h_pool2))

################## fully connected 3 ##############

W_fc1 = weight_variable([8 * 8 * 64, 1024], ""Weight3"")
b_fc1 = bias_variable([1024], ""bias3"")

h_pool2_flat = tf.reshape(h_pool2, [-1, 8 * 8 * 64])
h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)

print(""W_fc1 "")
print(tf.shape(W_fc1))
print(""b_fc1 "")
print(tf.shape(b_fc1))
print(""h_pool2_flat "")
print(tf.shape(h_pool2_flat))
print(""h_fc1 "")
print(tf.shape(h_fc1))

################ dropout  4 #################

keep_prob = tf.placeholder(tf.float32)
h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)

################## fully connected 5 ##############

W_fc2 = weight_variable([1024, cons.LABELS_NUMB], ""weight5"")
b_fc2 = bias_variable([cons.LABELS_NUMB], ""bias5"")

Y = tf.matmul(h_fc1_drop, W_fc2) + b_fc2
tf.summary.histogram(""final"", Y)


print(""W_fc2 "")
print(tf.shape(W_fc2))
print(""b_fc2 "")
print(tf.shape(b_fc2))
print(""Y "")
print(tf.shape(Y))

with tf.name_scope(""cross_entropy""):
    cross_entropy = tf.reduce_mean(
        tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=Y))
    tf.summary.scalar(""xent"", cross_entropy)

with tf.name_scope(""train_step""):
    train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)

with tf.name_scope(""Acuracy""):
    correct_prediction = tf.equal(tf.argmax(Y, 1), tf.argmax(y_, 1))
    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
    tf.summary.scalar(""accuracy"", accuracy)

summ = tf.summary.merge_all()
saver = tf.train.Saver()

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())

    writer = tf.summary.FileWriter(""/home/damian/api/mnist_demo/10"")
    writer.add_graph(sess.graph)
    for i in range(1000):
        img, lb = fileCreation.next_batch(100, images32, labels)
        if i % 5 == 0:
            [train_accuracy, s] = sess.run([accuracy, summ], feed_dict={x: img, y_: fileCreation.dense_to_one_hot(lb, cons.LABELS_NUMB), keep_prob: 1.0})
        writer.add_summary(s, i)
        if i % 100 == 0:
            saver.save(sess, '/home/damian/api/checkpoint/my_test_model', global_step=i)
            train_accuracy = accuracy.eval(feed_dict={
                x: img, y_: fileCreation.dense_to_one_hot(lb, cons.LABELS_NUMB), keep_prob: 1.0})
            print('step %d, training accuracy %g' % (i, train_accuracy))
        train_step.run(feed_dict={x: img, y_: fileCreation.dense_to_one_hot(lb, cons.LABELS_NUMB), keep_prob: 0.5})

    print('test accuracy %g' % accuracy.eval(feed_dict={x: test_images32,
                                                        y_: fileCreation.dense_to_one_hot(test_labels,
                                                                                          cons.LABELS_NUMB),
                                                        keep_prob: 0.1}))
```


I'm freeze this model with and i point output_node to 'final'
next i call script from tutorial


```
python -m scripts.label_image \
  --graph=tf_files/frozen_model3.pb  \
  --input_layer=x \
  --output_layer=final \
  --image=tf_files/stop.png  \
  --input_height=32 \
  --input_width=32 \
  --input_mean=16  \
  --input_std=16  
```









",0,,1,2018-01-28T16:15:22Z,2018-01-30T19:10:44Z,NONE,"Hello , I trying create mobile app for object recognition for my own created model. I fallow this tutorial https://codelabs.developers.google.com/codelabs/tensorflow-for-poets-2/#2
But when i even get a testing model from step 3 i get error 

```
InvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'import/Placeholder' with dtype float
	 [[Node: import/Placeholder = Placeholder[dtype=DT_FLOAT, shape=<unknown>, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]
```

I'm aware that is wrong node problem, but i trying with other and always i get failure 

My code for model creation 
```

x = tf.placeholder(tf.float32,
                   shape=[None, cons.IMAGE_SIZE, cons.IMAGE_SIZE, 3], name=""x"")
y_ = tf.placeholder(tf.float32, shape=[None, cons.LABELS_NUMB], name=""labels"")

K = 4
L = 8
M = 12
N = 200

x_image = tf.reshape(x, [-1, cons.IMAGE_SIZE, cons.IMAGE_SIZE, 3])
tf.summary.image('input', x_image, 3)
print(""X image "")
print(tf.shape(x_image))

################## first ##############

W_conv1 = weight_variable([5, 5, 3, 32], ""weight1"")
b_conv1 = bias_variable([32], ""bias1"")

h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)
h_pool1 = max_pool_2x2(h_conv1)
print(""W_conv1 "")
print(tf.shape(W_conv1))
print(""b_conv1 "")
print(tf.shape(b_conv1))
print(""h_conv1 "")
print(tf.shape(h_conv1))
print(""h_pool1 "")
print(tf.shape(h_pool1))

################## second ##############

W_conv2 = weight_variable([5, 5, 32, 64], ""weight2"")
b_conv2 = bias_variable([64], ""bias2"")

h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)
tf.summary.histogram(""activations"", h_conv2)


h_pool2 = max_pool_2x2(h_conv2)

print(""W_conv2 "")
print(tf.shape(W_conv2))
print(""b_conv2 "")
print(tf.shape(b_conv2))
print(""h_conv2 "")
print(tf.shape(h_conv2))
print(""h_pool2 "")
print(tf.shape(h_pool2))

################## fully connected 3 ##############

W_fc1 = weight_variable([8 * 8 * 64, 1024], ""Weight3"")
b_fc1 = bias_variable([1024], ""bias3"")

h_pool2_flat = tf.reshape(h_pool2, [-1, 8 * 8 * 64])
h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)

print(""W_fc1 "")
print(tf.shape(W_fc1))
print(""b_fc1 "")
print(tf.shape(b_fc1))
print(""h_pool2_flat "")
print(tf.shape(h_pool2_flat))
print(""h_fc1 "")
print(tf.shape(h_fc1))

################ dropout  4 #################

keep_prob = tf.placeholder(tf.float32)
h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)

################## fully connected 5 ##############

W_fc2 = weight_variable([1024, cons.LABELS_NUMB], ""weight5"")
b_fc2 = bias_variable([cons.LABELS_NUMB], ""bias5"")

Y = tf.matmul(h_fc1_drop, W_fc2) + b_fc2
tf.summary.histogram(""final"", Y)


print(""W_fc2 "")
print(tf.shape(W_fc2))
print(""b_fc2 "")
print(tf.shape(b_fc2))
print(""Y "")
print(tf.shape(Y))

with tf.name_scope(""cross_entropy""):
    cross_entropy = tf.reduce_mean(
        tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=Y))
    tf.summary.scalar(""xent"", cross_entropy)

with tf.name_scope(""train_step""):
    train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)

with tf.name_scope(""Acuracy""):
    correct_prediction = tf.equal(tf.argmax(Y, 1), tf.argmax(y_, 1))
    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
    tf.summary.scalar(""accuracy"", accuracy)

summ = tf.summary.merge_all()
saver = tf.train.Saver()

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())

    writer = tf.summary.FileWriter(""/home/damian/api/mnist_demo/10"")
    writer.add_graph(sess.graph)
    for i in range(1000):
        img, lb = fileCreation.next_batch(100, images32, labels)
        if i % 5 == 0:
            [train_accuracy, s] = sess.run([accuracy, summ], feed_dict={x: img, y_: fileCreation.dense_to_one_hot(lb, cons.LABELS_NUMB), keep_prob: 1.0})
        writer.add_summary(s, i)
        if i % 100 == 0:
            saver.save(sess, '/home/damian/api/checkpoint/my_test_model', global_step=i)
            train_accuracy = accuracy.eval(feed_dict={
                x: img, y_: fileCreation.dense_to_one_hot(lb, cons.LABELS_NUMB), keep_prob: 1.0})
            print('step %d, training accuracy %g' % (i, train_accuracy))
        train_step.run(feed_dict={x: img, y_: fileCreation.dense_to_one_hot(lb, cons.LABELS_NUMB), keep_prob: 0.5})

    print('test accuracy %g' % accuracy.eval(feed_dict={x: test_images32,
                                                        y_: fileCreation.dense_to_one_hot(test_labels,
                                                                                          cons.LABELS_NUMB),
                                                        keep_prob: 0.1}))
```


I'm freeze this model with and i point output_node to 'final'
next i call script from tutorial


```
python -m scripts.label_image \
  --graph=tf_files/frozen_model3.pb  \
  --input_layer=x \
  --output_layer=final \
  --image=tf_files/stop.png  \
  --input_height=32 \
  --input_width=32 \
  --input_mean=16  \
  --input_std=16  
```









",2018-01-30T19:10:44Z,2,1,0,1.7912066762199625
163,16515,Unable to install/ upgrade tensorflow1.5 on window10,stat:awaiting response,"Exception:
Traceback (most recent call last):
  File ""C:\ProgramData\Anaconda3\lib\site-packages\pip\basecommand.py"", line 215, in main
    status = self.run(options, args)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\pip\commands\install.py"", line 335, in run
    wb.build(autobuilding=True)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\pip\wheel.py"", line 749, in build
    self.requirement_set.prepare_files(self.finder)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\pip\req\req_set.py"", line 380, in prepare_files
    ignore_dependencies=self.ignore_dependencies))
  File ""C:\ProgramData\Anaconda3\lib\site-packages\pip\req\req_set.py"", line 554, in _prepare_file
    require_hashes
  File ""C:\ProgramData\Anaconda3\lib\site-packages\pip\req\req_install.py"", line 278, in populate_link
    self.link = finder.find_requirement(self, upgrade)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\pip\index.py"", line 465, in find_requirement
    all_candidates = self.find_all_candidates(req.name)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\pip\index.py"", line 423, in find_all_candidates
    for page in self._get_pages(url_locations, project_name):
  File ""C:\ProgramData\Anaconda3\lib\site-packages\pip\index.py"", line 568, in _get_pages
    page = self._get_page(location)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\pip\index.py"", line 683, in _get_page
    return HTMLPage.get_page(link, session=self.session)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\pip\index.py"", line 811, in get_page
    inst = cls(resp.content, resp.url, resp.headers)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\pip\index.py"", line 731, in __init__
    namespaceHTMLElements=False,
TypeError: parse() got an unexpected keyword argument 'transport_encoding'",0,,5,2018-01-28T13:38:19Z,2018-02-08T16:54:11Z,NONE,"Exception:
Traceback (most recent call last):
  File ""C:\ProgramData\Anaconda3\lib\site-packages\pip\basecommand.py"", line 215, in main
    status = self.run(options, args)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\pip\commands\install.py"", line 335, in run
    wb.build(autobuilding=True)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\pip\wheel.py"", line 749, in build
    self.requirement_set.prepare_files(self.finder)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\pip\req\req_set.py"", line 380, in prepare_files
    ignore_dependencies=self.ignore_dependencies))
  File ""C:\ProgramData\Anaconda3\lib\site-packages\pip\req\req_set.py"", line 554, in _prepare_file
    require_hashes
  File ""C:\ProgramData\Anaconda3\lib\site-packages\pip\req\req_install.py"", line 278, in populate_link
    self.link = finder.find_requirement(self, upgrade)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\pip\index.py"", line 465, in find_requirement
    all_candidates = self.find_all_candidates(req.name)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\pip\index.py"", line 423, in find_all_candidates
    for page in self._get_pages(url_locations, project_name):
  File ""C:\ProgramData\Anaconda3\lib\site-packages\pip\index.py"", line 568, in _get_pages
    page = self._get_page(location)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\pip\index.py"", line 683, in _get_page
    return HTMLPage.get_page(link, session=self.session)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\pip\index.py"", line 811, in get_page
    inst = cls(resp.content, resp.url, resp.headers)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\pip\index.py"", line 731, in __init__
    namespaceHTMLElements=False,
TypeError: parse() got an unexpected keyword argument 'transport_encoding'",2018-01-28T14:26:30Z,10,1,2,3.7912066762199625
164,16513,TF1.5.0 not working with CUDA 8.0,,"After upgrading to TF 1.5.0, when I import tensorflow, it raises:

```
ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory
```

- System: Ubuntu 14.04.5 LTS (64 bit)
- Python: 2.7.6
- TensorFlow: tensorflow-gpu-1.5.0
- GPU: GeForce GTX TITAN
- CUDA: 8.0",0,,3,2018-01-28T10:24:08Z,2018-01-29T19:34:12Z,NONE,"After upgrading to TF 1.5.0, when I import tensorflow, it raises:

```
ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory
```

- System: Ubuntu 14.04.5 LTS (64 bit)
- Python: 2.7.6
- TensorFlow: tensorflow-gpu-1.5.0
- GPU: GeForce GTX TITAN
- CUDA: 8.0",2018-01-29T19:34:12Z,1,1,0,2.7912066762199625
165,16512,how to install ffmpeg in tensorflow 1.4 binary,,"hi
i want to install ffmpeg in tensorflow 1.4 binary , python 3.5 on ubuntu 16.04 , please help me how do i do ? 
the output type python -c ""from tensorflow.contrib import ffmpeg"" is ok dont have anly error , but i dont know why : 
from tensorflow.contrib import ffmpeg

i get error , 
>>>  from tensorflow.contrib import ffmpeg
  File ""<stdin>"", line 1
    from tensorflow.contrib import ffmpeg
    ^
IndentationError: unexpected indent
",0,,2,2018-01-28T07:56:25Z,2018-01-30T03:34:18Z,NONE,"hi
i want to install ffmpeg in tensorflow 1.4 binary , python 3.5 on ubuntu 16.04 , please help me how do i do ? 
the output type python -c ""from tensorflow.contrib import ffmpeg"" is ok dont have anly error , but i dont know why : 
from tensorflow.contrib import ffmpeg

i get error , 
>>>  from tensorflow.contrib import ffmpeg
  File ""<stdin>"", line 1
    from tensorflow.contrib import ffmpeg
    ^
IndentationError: unexpected indent
",2018-01-28T14:16:58Z,2,1,0,2.2912066762199625
166,16510,Feature Request: Make lstm2d.separable_lstm accept Dynamic Batch Sizes,,"Apparently, lstm2d.separable_lstm doesn't accept dynamic batch sizes (number of images). Whenever I set the shape of a `placeholder` to `(None, height, width, depth)` to be fed into the network , I get this error:

```
Traceback (most recent call last):
  File ""...\tensorflow\contrib\ndlstm\python\lstm2d.py"", line 159, in separable_lstm
    hidden = horizontal_lstm(images, nhidden)
  File ""...\tensorflow\contrib\ndlstm\python\lstm2d.py"", line 82, in horizontal_lstm
    sequence = images_to_sequence(images)
  File ""...\tensorflow\contrib\ndlstm\python\lstm2d.py"", line 47, in images_to_sequence
    [width, num_image_batches * height, depth])
TypeError: unsupported operand type(s) for *: 'NoneType' and 'int'
```

I guess it would be nice to have `separable_lstm` accept dynamic batch sizes so it can be used effectively.",1,,4,2018-01-28T04:46:35Z,2018-02-13T22:57:48Z,CONTRIBUTOR,"Apparently, lstm2d.separable_lstm doesn't accept dynamic batch sizes (number of images). Whenever I set the shape of a `placeholder` to `(None, height, width, depth)` to be fed into the network , I get this error:

```
Traceback (most recent call last):
  File ""...\tensorflow\contrib\ndlstm\python\lstm2d.py"", line 159, in separable_lstm
    hidden = horizontal_lstm(images, nhidden)
  File ""...\tensorflow\contrib\ndlstm\python\lstm2d.py"", line 82, in horizontal_lstm
    sequence = images_to_sequence(images)
  File ""...\tensorflow\contrib\ndlstm\python\lstm2d.py"", line 47, in images_to_sequence
    [width, num_image_batches * height, depth])
TypeError: unsupported operand type(s) for *: 'NoneType' and 'int'
```

I guess it would be nice to have `separable_lstm` accept dynamic batch sizes so it can be used effectively.",2018-01-29T11:06:39Z,15,2,3,5.2912066762199625
167,16507,"ResourceExhaustedError, when running UNET",,"My computer has a gpu GeForce 940MX installed. It has the Memory bandwidth 16.02 GB/s. I'm trying to train LUNA dataset using UNET model using following code.

	from __future__ import print_function

	import numpy as np
	from keras.models import Model
	from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D
	from keras.layers import concatenate
	from keras.optimizers import Adam
	from keras.optimizers import SGD
	from keras.callbacks import ModelCheckpoint, LearningRateScheduler
	from keras import backend as K


	K.set_image_dim_ordering('th')  # Theano dimension ordering in this code

	img_rows = 512
	img_cols = 512

	smooth = 1.


	def dice_coef(y_true, y_pred):
		y_true_f = K.flatten(y_true)
		y_pred_f = K.flatten(y_pred)
		intersection = K.sum(y_true_f * y_pred_f)
		return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)

	def dice_coef_np(y_true,y_pred):
		y_true_f = y_true.flatten()
		y_pred_f = y_pred.flatten()
		intersection = np.sum(y_true_f * y_pred_f)
		return (2. * intersection + smooth) / (np.sum(y_true_f) + np.sum(y_pred_f) + smooth)

	def dice_coef_loss(y_true, y_pred):
		return -dice_coef(y_true, y_pred)


	def get_unet():
		inputs = Input((1,img_rows, img_cols))
		conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)
		conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv1)
		pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)

		conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(pool1)
		conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv2)
		pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)

		conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool2)
		conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv3)
		pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)

		conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(pool3)
		conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv4)
		pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)

		conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(pool4)
		conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(conv5)

		#up6 = merge([UpSampling2D(size=(2, 2))(conv5), conv4], mode='concat', concat_axis=1)
		up6 = concatenate([UpSampling2D(size=(2, 2))(conv5), conv4], axis=1)
		conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(up6)
		conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv6)

		#up7 = merge([UpSampling2D(size=(2, 2))(conv6), conv3], mode='concat', concat_axis=1)
		up7 = concatenate([UpSampling2D(size=(2, 2))(conv6), conv3], axis=1)
		conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(up7)
		conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv7)

		#up8 = merge([UpSampling2D(size=(2, 2))(conv7), conv2], mode='concat', concat_axis=1)
		up8 = concatenate([UpSampling2D(size=(2, 2))(conv7), conv2], axis=1)
		conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(up8)
		conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv8)

		#up9 = merge([UpSampling2D(size=(2, 2))(conv8), conv1], mode='concat', concat_axis=1)
		up9 = concatenate([UpSampling2D(size=(2, 2))(conv8), conv1], axis=1)
		conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(up9)
		conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv9)

		conv10 = Conv2D(1, (1, 1), activation='sigmoid')(conv9)

		model = Model(inputs=inputs, outputs=conv10)

		model.compile(optimizer=Adam(lr=1.0e-5), loss=dice_coef_loss, metrics=[dice_coef])

		return model


	def train_and_predict(use_existing):
		print('-'*30)
		print('Loading and preprocessing train data...')
		print('-'*30)
		imgs_train = np.load(""C:/Users/hirplk/Desktop/unet/Luna2016-Lung-Nodule-Detection-master_new/DATA_PROCESS/scratch/cse/dual/cs5130287/Luna2016/output_final/""+""trainImages.npy"").astype(np.float32)
		imgs_mask_train = np.load(""C:/Users/hirplk/Desktop/unet/Luna2016-Lung-Nodule-Detection-master_new/DATA_PROCESS/scratch/cse/dual/cs5130287/Luna2016/output_final/""+""trainMasks.npy"").astype(np.float32)

		imgs_test = np.load(""C:/Users/hirplk/Desktop/unet/Luna2016-Lung-Nodule-Detection-master_new/DATA_PROCESS/scratch/cse/dual/cs5130287/Luna2016/output_final/""+""testImages.npy"").astype(np.float32)
		imgs_mask_test_true = np.load(""C:/Users/hirplk/Desktop/unet/Luna2016-Lung-Nodule-Detection-master_new/DATA_PROCESS/scratch/cse/dual/cs5130287/Luna2016/output_final/""+""testMasks.npy"").astype(np.float32)
		
		mean = np.mean(imgs_train)  # mean for data centering
		std = np.std(imgs_train)  # std for data normalization

		imgs_train -= mean  # images should already be standardized, but just in case
		imgs_train /= std

		print('-'*30)
		print('Creating and compiling model...')
		print('-'*30)
		model = get_unet()
		# Saving weights to unet.hdf5 at checkpoints
		model_checkpoint = ModelCheckpoint('unet.hdf5', monitor='loss', save_best_only=True)
		#
		# Should we load existing weights? 
		# Set argument for call to train_and_predict to true at end of script
		if use_existing:
			model.load_weights('./unet.hdf5')
			
		# 
		# The final results for this tutorial were produced using a multi-GPU
		# machine using TitanX's.
		# For a home GPU computation benchmark, on my home set up with a GTX970 
		# I was able to run 20 epochs with a training set size of 320 and 
		# batch size of 2 in about an hour. I started getting reseasonable masks 
		# after about 3 hours of training. 
		#
		print('-'*30)
		print('Fitting model...')
		print('-'*30)
		model.fit(imgs_train, imgs_mask_train, batch_size=50, epochs=10, verbose=1, shuffle=True,
				  callbacks=[model_checkpoint])

		# loading best weights from training session
		print('-'*30)
		print('Loading saved weights...')
		print('-'*30)
		model.load_weights('./unet.hdf5')

		print('-'*30)
		print('Predicting masks on test data...')
		print('-'*30)
		num_test = len(imgs_test)
		imgs_mask_test = np.ndarray([num_test,1,512,512],dtype=np.float32)
		for i in range(num_test):
			imgs_mask_test[i] = model.predict([imgs_test[i:i+1]], verbose=0)[0]
		np.save('masksTestPredicted.npy', imgs_mask_test)
		mean = 0.0
		for i in range(num_test):
			mean+=dice_coef_np(imgs_mask_test_true[i,0], imgs_mask_test[i,0])
		mean/=num_test
		print(""Mean Dice Coeff : "",mean)

	if __name__ == '__main__':
		train_and_predict(False)
		
But when running it using GPU I'm getting the following error.

	Warning (from warnings module):
	  File ""C:\Research\Python_installation\lib\site-packages\h5py\__init__.py"", line 36
		from ._conv import register_converters as _register_converters
	FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
	Using TensorFlow backend.
	------------------------------
	Loading and preprocessing train data...
	------------------------------
	------------------------------
	Creating and compiling model...
	------------------------------
	------------------------------
	Fitting model...
	------------------------------
	Epoch 1/10
	Traceback (most recent call last):
	  File ""C:\Research\Python_installation\lib\site-packages\tensorflow\python\client\session.py"", line 1327, in _do_call
		return fn(*args)
	  File ""C:\Research\Python_installation\lib\site-packages\tensorflow\python\client\session.py"", line 1306, in _run_fn
		status, run_metadata)
	  File ""C:\Research\Python_installation\lib\contextlib.py"", line 66, in __exit__
		next(self.gen)
	  File ""C:\Research\Python_installation\lib\site-packages\tensorflow\python\framework\errors_impl.py"", line 466, in raise_exception_on_not_ok_status
		pywrap_tensorflow.TF_GetCode(status))
	tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[50,32,512,512]
		 [[Node: conv2d_1/convolution = Conv2D[T=DT_FLOAT, data_format=""NCHW"", padding=""SAME"", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=""/job:localhost/replica:0/task:0/gpu:0""](_arg_input_1_0_2/_261, conv2d_1/kernel/read)]]
		 [[Node: loss/mul/_273 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_3022_loss/mul"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]

	During handling of the above exception, another exception occurred:

	Traceback (most recent call last):
	  File ""C:\Users\hirplk\Desktop\unet\DSB3Tutorial-master\tutorial_code\LUNA_train_unet.py"", line 150, in <module>
		train_and_predict(False)
	  File ""C:\Users\hirplk\Desktop\unet\DSB3Tutorial-master\tutorial_code\LUNA_train_unet.py"", line 127, in train_and_predict
		callbacks=[model_checkpoint])
	  File ""C:\Research\Python_installation\lib\site-packages\keras\engine\training.py"", line 1657, in fit
		validation_steps=validation_steps)
	  File ""C:\Research\Python_installation\lib\site-packages\keras\engine\training.py"", line 1213, in _fit_loop
		outs = f(ins_batch)
	  File ""C:\Research\Python_installation\lib\site-packages\keras\backend\tensorflow_backend.py"", line 2357, in __call__
		**self.session_kwargs)
	  File ""C:\Research\Python_installation\lib\site-packages\tensorflow\python\client\session.py"", line 895, in run
		run_metadata_ptr)
	  File ""C:\Research\Python_installation\lib\site-packages\tensorflow\python\client\session.py"", line 1124, in _run
		feed_dict_tensor, options, run_metadata)
	  File ""C:\Research\Python_installation\lib\site-packages\tensorflow\python\client\session.py"", line 1321, in _do_run
		options, run_metadata)
	  File ""C:\Research\Python_installation\lib\site-packages\tensorflow\python\client\session.py"", line 1340, in _do_call
		raise type(e)(node_def, op, message)
	tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[50,32,512,512]
		 [[Node: conv2d_1/convolution = Conv2D[T=DT_FLOAT, data_format=""NCHW"", padding=""SAME"", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=""/job:localhost/replica:0/task:0/gpu:0""](_arg_input_1_0_2/_261, conv2d_1/kernel/read)]]
		 [[Node: loss/mul/_273 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_3022_loss/mul"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]

	Caused by op 'conv2d_1/convolution', defined at:
	  File ""<string>"", line 1, in <module>
	  File ""C:\Research\Python_installation\lib\idlelib\run.py"", line 124, in main
		ret = method(*args, **kwargs)
	  File ""C:\Research\Python_installation\lib\idlelib\run.py"", line 351, in runcode
		exec(code, self.locals)
	  File ""C:\Users\hirplk\Desktop\unet\DSB3Tutorial-master\tutorial_code\LUNA_train_unet.py"", line 150, in <module>
		train_and_predict(False)
	  File ""C:\Users\hirplk\Desktop\unet\DSB3Tutorial-master\tutorial_code\LUNA_train_unet.py"", line 106, in train_and_predict
		model = get_unet()
	  File ""C:\Users\hirplk\Desktop\unet\DSB3Tutorial-master\tutorial_code\LUNA_train_unet.py"", line 39, in get_unet
		conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)
	  File ""C:\Research\Python_installation\lib\site-packages\keras\engine\topology.py"", line 603, in __call__
		output = self.call(inputs, **kwargs)
	  File ""C:\Research\Python_installation\lib\site-packages\keras\layers\convolutional.py"", line 164, in call
		dilation_rate=self.dilation_rate)
	  File ""C:\Research\Python_installation\lib\site-packages\keras\backend\tensorflow_backend.py"", line 3195, in conv2d
		data_format=tf_data_format)
	  File ""C:\Research\Python_installation\lib\site-packages\tensorflow\python\ops\nn_ops.py"", line 672, in convolution
		op=op)
	  File ""C:\Research\Python_installation\lib\site-packages\tensorflow\python\ops\nn_ops.py"", line 338, in with_space_to_batch
		return op(input, num_spatial_dims, padding)
	  File ""C:\Research\Python_installation\lib\site-packages\tensorflow\python\ops\nn_ops.py"", line 664, in op
		name=name)
	  File ""C:\Research\Python_installation\lib\site-packages\tensorflow\python\ops\nn_ops.py"", line 131, in _non_atrous_convolution
		name=name)
	  File ""C:\Research\Python_installation\lib\site-packages\tensorflow\python\ops\gen_nn_ops.py"", line 397, in conv2d
		data_format=data_format, name=name)
	  File ""C:\Research\Python_installation\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 767, in apply_op
		op_def=op_def)
	  File ""C:\Research\Python_installation\lib\site-packages\tensorflow\python\framework\ops.py"", line 2630, in create_op
		original_op=self._default_original_op, op_def=op_def)
	  File ""C:\Research\Python_installation\lib\site-packages\tensorflow\python\framework\ops.py"", line 1204, in __init__
		self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

	ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[50,32,512,512]
		 [[Node: conv2d_1/convolution = Conv2D[T=DT_FLOAT, data_format=""NCHW"", padding=""SAME"", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=""/job:localhost/replica:0/task:0/gpu:0""](_arg_input_1_0_2/_261, conv2d_1/kernel/read)]]
		 [[Node: loss/mul/_273 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_3022_loss/mul"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]


Can someone please kindly explain me the reason behind this error, ResourceExhaustedError. Is it because that the memory of GPU is not enough to load the dataset. This worked fine without GPU. But took around 6 hours to finish one epoch",0,,2,2018-01-28T01:43:47Z,2018-01-29T19:47:54Z,NONE,"My computer has a gpu GeForce 940MX installed. It has the Memory bandwidth 16.02 GB/s. I'm trying to train LUNA dataset using UNET model using following code.

	from __future__ import print_function

	import numpy as np
	from keras.models import Model
	from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D
	from keras.layers import concatenate
	from keras.optimizers import Adam
	from keras.optimizers import SGD
	from keras.callbacks import ModelCheckpoint, LearningRateScheduler
	from keras import backend as K


	K.set_image_dim_ordering('th')  # Theano dimension ordering in this code

	img_rows = 512
	img_cols = 512

	smooth = 1.


	def dice_coef(y_true, y_pred):
		y_true_f = K.flatten(y_true)
		y_pred_f = K.flatten(y_pred)
		intersection = K.sum(y_true_f * y_pred_f)
		return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)

	def dice_coef_np(y_true,y_pred):
		y_true_f = y_true.flatten()
		y_pred_f = y_pred.flatten()
		intersection = np.sum(y_true_f * y_pred_f)
		return (2. * intersection + smooth) / (np.sum(y_true_f) + np.sum(y_pred_f) + smooth)

	def dice_coef_loss(y_true, y_pred):
		return -dice_coef(y_true, y_pred)


	def get_unet():
		inputs = Input((1,img_rows, img_cols))
		conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)
		conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv1)
		pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)

		conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(pool1)
		conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv2)
		pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)

		conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool2)
		conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv3)
		pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)

		conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(pool3)
		conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv4)
		pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)

		conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(pool4)
		conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(conv5)

		#up6 = merge([UpSampling2D(size=(2, 2))(conv5), conv4], mode='concat', concat_axis=1)
		up6 = concatenate([UpSampling2D(size=(2, 2))(conv5), conv4], axis=1)
		conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(up6)
		conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv6)

		#up7 = merge([UpSampling2D(size=(2, 2))(conv6), conv3], mode='concat', concat_axis=1)
		up7 = concatenate([UpSampling2D(size=(2, 2))(conv6), conv3], axis=1)
		conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(up7)
		conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv7)

		#up8 = merge([UpSampling2D(size=(2, 2))(conv7), conv2], mode='concat', concat_axis=1)
		up8 = concatenate([UpSampling2D(size=(2, 2))(conv7), conv2], axis=1)
		conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(up8)
		conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv8)

		#up9 = merge([UpSampling2D(size=(2, 2))(conv8), conv1], mode='concat', concat_axis=1)
		up9 = concatenate([UpSampling2D(size=(2, 2))(conv8), conv1], axis=1)
		conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(up9)
		conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv9)

		conv10 = Conv2D(1, (1, 1), activation='sigmoid')(conv9)

		model = Model(inputs=inputs, outputs=conv10)

		model.compile(optimizer=Adam(lr=1.0e-5), loss=dice_coef_loss, metrics=[dice_coef])

		return model


	def train_and_predict(use_existing):
		print('-'*30)
		print('Loading and preprocessing train data...')
		print('-'*30)
		imgs_train = np.load(""C:/Users/hirplk/Desktop/unet/Luna2016-Lung-Nodule-Detection-master_new/DATA_PROCESS/scratch/cse/dual/cs5130287/Luna2016/output_final/""+""trainImages.npy"").astype(np.float32)
		imgs_mask_train = np.load(""C:/Users/hirplk/Desktop/unet/Luna2016-Lung-Nodule-Detection-master_new/DATA_PROCESS/scratch/cse/dual/cs5130287/Luna2016/output_final/""+""trainMasks.npy"").astype(np.float32)

		imgs_test = np.load(""C:/Users/hirplk/Desktop/unet/Luna2016-Lung-Nodule-Detection-master_new/DATA_PROCESS/scratch/cse/dual/cs5130287/Luna2016/output_final/""+""testImages.npy"").astype(np.float32)
		imgs_mask_test_true = np.load(""C:/Users/hirplk/Desktop/unet/Luna2016-Lung-Nodule-Detection-master_new/DATA_PROCESS/scratch/cse/dual/cs5130287/Luna2016/output_final/""+""testMasks.npy"").astype(np.float32)
		
		mean = np.mean(imgs_train)  # mean for data centering
		std = np.std(imgs_train)  # std for data normalization

		imgs_train -= mean  # images should already be standardized, but just in case
		imgs_train /= std

		print('-'*30)
		print('Creating and compiling model...')
		print('-'*30)
		model = get_unet()
		# Saving weights to unet.hdf5 at checkpoints
		model_checkpoint = ModelCheckpoint('unet.hdf5', monitor='loss', save_best_only=True)
		#
		# Should we load existing weights? 
		# Set argument for call to train_and_predict to true at end of script
		if use_existing:
			model.load_weights('./unet.hdf5')
			
		# 
		# The final results for this tutorial were produced using a multi-GPU
		# machine using TitanX's.
		# For a home GPU computation benchmark, on my home set up with a GTX970 
		# I was able to run 20 epochs with a training set size of 320 and 
		# batch size of 2 in about an hour. I started getting reseasonable masks 
		# after about 3 hours of training. 
		#
		print('-'*30)
		print('Fitting model...')
		print('-'*30)
		model.fit(imgs_train, imgs_mask_train, batch_size=50, epochs=10, verbose=1, shuffle=True,
				  callbacks=[model_checkpoint])

		# loading best weights from training session
		print('-'*30)
		print('Loading saved weights...')
		print('-'*30)
		model.load_weights('./unet.hdf5')

		print('-'*30)
		print('Predicting masks on test data...')
		print('-'*30)
		num_test = len(imgs_test)
		imgs_mask_test = np.ndarray([num_test,1,512,512],dtype=np.float32)
		for i in range(num_test):
			imgs_mask_test[i] = model.predict([imgs_test[i:i+1]], verbose=0)[0]
		np.save('masksTestPredicted.npy', imgs_mask_test)
		mean = 0.0
		for i in range(num_test):
			mean+=dice_coef_np(imgs_mask_test_true[i,0], imgs_mask_test[i,0])
		mean/=num_test
		print(""Mean Dice Coeff : "",mean)

	if __name__ == '__main__':
		train_and_predict(False)
		
But when running it using GPU I'm getting the following error.

	Warning (from warnings module):
	  File ""C:\Research\Python_installation\lib\site-packages\h5py\__init__.py"", line 36
		from ._conv import register_converters as _register_converters
	FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
	Using TensorFlow backend.
	------------------------------
	Loading and preprocessing train data...
	------------------------------
	------------------------------
	Creating and compiling model...
	------------------------------
	------------------------------
	Fitting model...
	------------------------------
	Epoch 1/10
	Traceback (most recent call last):
	  File ""C:\Research\Python_installation\lib\site-packages\tensorflow\python\client\session.py"", line 1327, in _do_call
		return fn(*args)
	  File ""C:\Research\Python_installation\lib\site-packages\tensorflow\python\client\session.py"", line 1306, in _run_fn
		status, run_metadata)
	  File ""C:\Research\Python_installation\lib\contextlib.py"", line 66, in __exit__
		next(self.gen)
	  File ""C:\Research\Python_installation\lib\site-packages\tensorflow\python\framework\errors_impl.py"", line 466, in raise_exception_on_not_ok_status
		pywrap_tensorflow.TF_GetCode(status))
	tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[50,32,512,512]
		 [[Node: conv2d_1/convolution = Conv2D[T=DT_FLOAT, data_format=""NCHW"", padding=""SAME"", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=""/job:localhost/replica:0/task:0/gpu:0""](_arg_input_1_0_2/_261, conv2d_1/kernel/read)]]
		 [[Node: loss/mul/_273 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_3022_loss/mul"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]

	During handling of the above exception, another exception occurred:

	Traceback (most recent call last):
	  File ""C:\Users\hirplk\Desktop\unet\DSB3Tutorial-master\tutorial_code\LUNA_train_unet.py"", line 150, in <module>
		train_and_predict(False)
	  File ""C:\Users\hirplk\Desktop\unet\DSB3Tutorial-master\tutorial_code\LUNA_train_unet.py"", line 127, in train_and_predict
		callbacks=[model_checkpoint])
	  File ""C:\Research\Python_installation\lib\site-packages\keras\engine\training.py"", line 1657, in fit
		validation_steps=validation_steps)
	  File ""C:\Research\Python_installation\lib\site-packages\keras\engine\training.py"", line 1213, in _fit_loop
		outs = f(ins_batch)
	  File ""C:\Research\Python_installation\lib\site-packages\keras\backend\tensorflow_backend.py"", line 2357, in __call__
		**self.session_kwargs)
	  File ""C:\Research\Python_installation\lib\site-packages\tensorflow\python\client\session.py"", line 895, in run
		run_metadata_ptr)
	  File ""C:\Research\Python_installation\lib\site-packages\tensorflow\python\client\session.py"", line 1124, in _run
		feed_dict_tensor, options, run_metadata)
	  File ""C:\Research\Python_installation\lib\site-packages\tensorflow\python\client\session.py"", line 1321, in _do_run
		options, run_metadata)
	  File ""C:\Research\Python_installation\lib\site-packages\tensorflow\python\client\session.py"", line 1340, in _do_call
		raise type(e)(node_def, op, message)
	tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[50,32,512,512]
		 [[Node: conv2d_1/convolution = Conv2D[T=DT_FLOAT, data_format=""NCHW"", padding=""SAME"", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=""/job:localhost/replica:0/task:0/gpu:0""](_arg_input_1_0_2/_261, conv2d_1/kernel/read)]]
		 [[Node: loss/mul/_273 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_3022_loss/mul"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]

	Caused by op 'conv2d_1/convolution', defined at:
	  File ""<string>"", line 1, in <module>
	  File ""C:\Research\Python_installation\lib\idlelib\run.py"", line 124, in main
		ret = method(*args, **kwargs)
	  File ""C:\Research\Python_installation\lib\idlelib\run.py"", line 351, in runcode
		exec(code, self.locals)
	  File ""C:\Users\hirplk\Desktop\unet\DSB3Tutorial-master\tutorial_code\LUNA_train_unet.py"", line 150, in <module>
		train_and_predict(False)
	  File ""C:\Users\hirplk\Desktop\unet\DSB3Tutorial-master\tutorial_code\LUNA_train_unet.py"", line 106, in train_and_predict
		model = get_unet()
	  File ""C:\Users\hirplk\Desktop\unet\DSB3Tutorial-master\tutorial_code\LUNA_train_unet.py"", line 39, in get_unet
		conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)
	  File ""C:\Research\Python_installation\lib\site-packages\keras\engine\topology.py"", line 603, in __call__
		output = self.call(inputs, **kwargs)
	  File ""C:\Research\Python_installation\lib\site-packages\keras\layers\convolutional.py"", line 164, in call
		dilation_rate=self.dilation_rate)
	  File ""C:\Research\Python_installation\lib\site-packages\keras\backend\tensorflow_backend.py"", line 3195, in conv2d
		data_format=tf_data_format)
	  File ""C:\Research\Python_installation\lib\site-packages\tensorflow\python\ops\nn_ops.py"", line 672, in convolution
		op=op)
	  File ""C:\Research\Python_installation\lib\site-packages\tensorflow\python\ops\nn_ops.py"", line 338, in with_space_to_batch
		return op(input, num_spatial_dims, padding)
	  File ""C:\Research\Python_installation\lib\site-packages\tensorflow\python\ops\nn_ops.py"", line 664, in op
		name=name)
	  File ""C:\Research\Python_installation\lib\site-packages\tensorflow\python\ops\nn_ops.py"", line 131, in _non_atrous_convolution
		name=name)
	  File ""C:\Research\Python_installation\lib\site-packages\tensorflow\python\ops\gen_nn_ops.py"", line 397, in conv2d
		data_format=data_format, name=name)
	  File ""C:\Research\Python_installation\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 767, in apply_op
		op_def=op_def)
	  File ""C:\Research\Python_installation\lib\site-packages\tensorflow\python\framework\ops.py"", line 2630, in create_op
		original_op=self._default_original_op, op_def=op_def)
	  File ""C:\Research\Python_installation\lib\site-packages\tensorflow\python\framework\ops.py"", line 1204, in __init__
		self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

	ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[50,32,512,512]
		 [[Node: conv2d_1/convolution = Conv2D[T=DT_FLOAT, data_format=""NCHW"", padding=""SAME"", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=""/job:localhost/replica:0/task:0/gpu:0""](_arg_input_1_0_2/_261, conv2d_1/kernel/read)]]
		 [[Node: loss/mul/_273 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_3022_loss/mul"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]


Can someone please kindly explain me the reason behind this error, ResourceExhaustedError. Is it because that the memory of GPU is not enough to load the dataset. This worked fine without GPU. But took around 6 hours to finish one epoch",2018-01-29T19:47:54Z,1,1,0,2.2912066762199625
168,16506,Feature request: Have Estimator display Loss and Metrics for Every Epoch and not Every Step,,"Most of the papers Ive read measure the time it takes to train a model with every epoch and not every step. If it isnt possible to display the loss only for every epoch, I think it would be nice to print when an epoch has passed.",0,,1,2018-01-28T00:03:46Z,2018-01-30T03:31:05Z,CONTRIBUTOR,"Most of the papers Ive read measure the time it takes to train a model with every epoch and not every step. If it isnt possible to display the loss only for every epoch, I think it would be nice to print when an epoch has passed.",2018-01-30T03:31:05Z,2,2,0,2.7912066762199625
169,16500,contrib/learn: Typo in variable name x_exrta --> x_extra,"awaiting testing (then merge),cla: yes","flake8 testing of https://github.com/tensorflow/tensorflow

$ __flake8 . --count --select=E901,E999,F821,F822,F823 --show-source --statistics__
```
./tensorflow/contrib/learn/python/learn/datasets/synthetic.py:156:32: F821 undefined name 'x_extra'
    spir_x = np.append(spir_x, x_extra)
                               ^
```",1,,4,2018-01-27T17:58:50Z,2018-01-29T21:50:40Z,CONTRIBUTOR,"flake8 testing of https://github.com/tensorflow/tensorflow

$ __flake8 . --count --select=E901,E999,F821,F822,F823 --show-source --statistics__
```
./tensorflow/contrib/learn/python/learn/datasets/synthetic.py:156:32: F821 undefined name 'x_extra'
    spir_x = np.append(spir_x, x_extra)
                               ^
```",2018-01-27T23:04:23Z,2,2,0,5.288539008177793
170,16498,Bounding box do not remove,,"Hi there,
After I detected my tv, it showed up but when i move it to other place, it do not remove the bounding box even though i put my camera on the table. Is this a bug?

![2018-01-28-00-34-50](https://user-images.githubusercontent.com/32919949/35474032-9eb4827e-03c3-11e8-8768-c4c81f9ad391.png)
",1,,7,2018-01-27T16:39:37Z,2018-02-15T23:20:35Z,NONE,"Hi there,
After I detected my tv, it showed up but when i move it to other place, it do not remove the bounding box even though i put my camera on the table. Is this a bug?

![2018-01-28-00-34-50](https://user-images.githubusercontent.com/32919949/35474032-9eb4827e-03c3-11e8-8768-c4c81f9ad391.png)
",2018-01-30T03:24:36Z,18,1,3,5.788539008177793
171,16496,"Feature Suggestion: ""Float-bit-strings""",type:feature,"### System information (Not really relevant ...)
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.4
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: NA
- **GCC/Compiler version (if compiling from source)**: NA
- **CUDA/cuDNN version**: 8 (?)
- **GPU model and memory**: GTX 1070
- **Exact command to reproduce**: NA

### Summary

This proposes the use of what I call ""float-bit-strings"" or ""float-bits"" instead of one-hot-encoded arrays so as to greatly reduce the memory and computational usage e.g. in language models.

I don't think this preliminary discussion belongs on StackOverflow so I hope it is OK to post it here. It is a new feature that could be added to TensorFlow. There's quite likely somebody on the TensorFlow dev-team or in the community who has already thought of this. But I have searched the internet and cannot find any mentioning of a similar idea.


### Background

I have started looking at language-models using e.g. LSTM and encoder-decoder architectures. There are some aspects that seem to be incredibly wasteful and limiting. Let me briefly describe this and please forgive me if I am ignorant, I have only spent a week or two on studying LSTM and language models so far :-)

For example in Machine Translation we typically have the text-data for the source- and target-languages as lists of integer-tokens, where each integer maps to a word in the vocabulary. There may be e.g. 100k different words so these integer-tokens can take on values between zero and 100k. This data cannot be input directly to a Neural Network so we use an embedding layer to convert these integers to n-dimensional vectors with values between zero and one, according to a mapping-function that may either be loaded from disk or trained along with the rest of the Neural Network; if I understand correctly.

For the decoder in a language model, we have a similar problem where we must somehow convert integer-tokens to data that the neural network can work on. A typical way of doing this seems to be a one-hot encoding; if I understand correctly. (This could also be done for the encoder-part, but it doesn't seem to be necessary).

I can't figure out what the max-size of one-hot encodings are in TensorFlow and whether it can even handle 100k one-hot encoded tensors. But it is obviously an extremely wasteful data-mapping. For example, for a vocab of 100k words we only need 17-bits (log2(100k)) to represent each integer-token - but for a one-hot encoding using 32-bit floats we need 32 x 100k bits!

I can't figure out what people normally do, but it seems like the common practice is to limit the vocab to a smaller number of words, e.g. 1k or 10k. It appears that Google Translate runs on multiple GPU's and maybe that's why they can handle extremely large vocabs with one-hot encoded tensors?


### Float-bit-strings

I thought it might be possible to use a bit-string-like representation inside a TensorFlow model. I have searched the internet and cannot find anyone who has proposed a similar idea.

The idea is to convert each integer-token to what I call a ""float-bit-string"" or ""float-bits"". For example, the number 123 has the bit-string 01111011. We can then make a corresponding tensor with floats [0., 1., 1., 1., 1., 0., 1., 1.] and input this to the TensorFlow model.

In a language model we would then have to input and output these ""float-bits"" instead of one-hot encoded arrays. This would dramatically reduce the memory and computational requirements of the models.


### Test

I have hacked together a little test using numpy and Keras / TensorFlow. The idea is to see if we can learn to map integers x with values between 0 and 10k to y = 123 * x using these ""float-bit"" encodings. And it works as you can see by running the code further below! That is perhaps not a surprise as neural networks are general function approximators, but it's not always that they work according to theory :-)

However, the network cannot learn the arithmetic mapping of e.g. y = 123 * x when x and y are ""float-bits"". This means it cannot generalize to data it hasn't seen during training in the arithmetic manner we might expect. But I don't think that is necessary for use in e.g. language models where we merely want to be able to map some tensor from e.g. an LSTM to an integer-token from the vocabulary.


### Loss Functions

I have tested this with both MSE and binary cross-entropy in Keras, which unfortunately isn't documented so I'm not completely sure what it does. But in both cases it works and the model trains to get the bit-wise mapping correct.

There might be cases where you are more concerned about the MSE between the actual integer-values instead of their ""float-bit-string"" representations, in which case we would need a TensorFlow method to convert ""float-bits"" to integers and then take the MSE of the resulting integer and the true integer from the data-set. This is not relevant for language models, because the proximity of integer-keys do not correspond to words that are necessarily similar in meaning. But it could be useful in other applications.


### TensorFlow Implementation

In order to make this work in TensorFlow it seems that we just need a couple of TensorFlow-methods for converting between integers and ""float-bit-strings"". I have hacked this together using numpy but I'm sure somebody on the dev-team can make a super-fast native TensorFlow implementation. Then we just need a wrapper in Keras and that might be enough to do e.g. language models with gigantic vocabs.


### Test-Code

    import numpy as np
    from tensorflow.python.keras.models import Sequential
    from tensorflow.python.keras.layers import InputLayer
    from tensorflow.python.keras.layers import Dense
    from tensorflow.python.keras.optimizers import RMSprop
    
    
    # Number of bits to use in our ""float-bit-strings"".
    num_bits = 32
    
    def int_to_floatbits(value):
        """"""
        Convert a single integer value to an array of 0.0 and 1.0 floats
        corresponding to the bit-string.
    
        Example: value==123 gives [0.  ... 0.  1.  1.  1.  1.  0.  1.  1.]
        """"""
    
        # Convert the integer value to a bit-string.
        # NOTE: This has been fixed to 32-bit length.
        bitstr = ""{0:032b}"".format(value)
    
        # Convert the bit-string to an array of equivalent float-values.
        floatbits = np.array([1.0 if bit == '1' else 0.0 for bit in bitstr])
    
        return floatbits
    
    
    def floatbits_to_strbits(floatbits):
        """"""
        Convert an array of floats to a bit-string.
        A float value greater than 0.5 results in 1.0
        and a float value less or equal to 0.5 results in 0.0
    
        Example: [0.1, 0.49, 0.51, 0.9, 1.1, -2.3] gives ""001110""
        """"""
    
        # Convert the float-array to a list of bit-characters '0' or '1'.
        charbits = ['1' if floatbit > 0.5 else '0' for floatbit in floatbits]
    
        # Convert the bit-characters to a string.
        strbits = """".join(charbits)
    
        return strbits
    
    def floatbits_to_int(floatbits):
        """"""
        Convert a float-array to an integer, assuming each element
        of the float-array corresponds to a bit.
        
        Example: [0.1, 0.49, 0.51, 0.9, 1.1, -2.3] corresponds to
        the bit-string ""001110"" which is the integer 14.
        """"""
    
        # Convert the float-array to a bit-string.
        strbits = floatbits_to_strbits(floatbits=floatbits)
    
        # Convert the bit-string to an integer value.
        value = int(strbits, base=2)
    
        return value
    
    
    # Various tests of the above functions.
    if True:
        foo = int_to_floatbits(123)
        print(foo)
        print(floatbits_to_strbits(foo))
        print(floatbits_to_int(foo))
    
        bar = [0.3,  0.9,  0.8,  0.51,  0.501,  0.4999,  0.999,  1.1]
        print(floatbits_to_strbits(bar))
        print(floatbits_to_int(bar))
    
        baz = [0.1, 0.49, 0.51, 0.9, 1.1, -2.3]
        print(floatbits_to_strbits(baz))
        print(floatbits_to_int(baz))
    
    # quit()
    
    # We will now train a TensorFlow / Keras model
    # that maps integers between 0 and 10000 to
    # the same numbers multiplied by 123.
    # If we were to use one-hot encoding then we would
    # need 10000 inputs to the Neural Network and
    # 1230000 outputs if using the full output range.
    # Using ""bit-strings"" encoded as floats, we only need
    # 14 bits for the input and 21 bits for the output.
    # We round it up to 32-bits.
    
    # The dataset as integers,
    # we want the Neural Network to map from x to y.
    x_int = np.arange(10000, dtype=int)
    y_int = 123 * x_int
    
    # Convert the dataset to ""float-bit-strings"" (aka. float-bits).
    x = np.array(list(map(int_to_floatbits, x_int)))
    y_true = np.array(list(map(int_to_floatbits, y_int)))
    
    # Check the mapping is correct. E.g. if the number of required bits
    # exceeds num_bits then these may not create numpy matrices correctly.
    if False:
        print(x.shape)
        print(y_true.shape)
        print(x[0:10])
        print(y_true[0:10])
    
    # Start construction of the Keras Sequential model.
    model = Sequential()
    
    # Add an input layer to the model.
    model.add(InputLayer(input_shape=(num_bits,)))
    
    # Fully-connected / dense layers with ReLU-activation.
    model.add(Dense(512, activation='relu'))
    model.add(Dense(512, activation='relu'))
    
    # Last fully-connected / dense layer with sigmoid-activation
    # so the output is between 0.0 and 1.0
    model.add(Dense(num_bits, activation='sigmoid'))
    
    optimizer = RMSprop(lr=1e-3)
    
    if True:
        # Loss is MSE.
        model.compile(optimizer=optimizer,
                      loss='mean_squared_error')
    else:
        # Loss is Binary Crossentropy, but also report MSE.
        model.compile(optimizer=optimizer,
                      loss='binary_crossentropy',
                      metrics=['mse'])
    
    epochs = 50
    
    if True:
        # Fit the model using the entire data-set.
        model.fit(x, y_true, epochs=epochs)
    else:
        # Fit the model using the data-set split into training and validation.
        # You will see that the validation-error is high so the model
        # has not learned the arithmetic function of the data-set.
        model.fit(x, y_true, epochs=epochs, validation_split=0.2)
    
    # Use the model to predict the output for a part of the data-set.
    y_pred = model.predict(x[0:10])
    
    # The true output for this part of the data-set.
    y_true_subset = y_true[0:10]
    
    # Map the ""float-bit-strings"" to integers.
    y_pred_int = list(map(floatbits_to_int, y_pred))
    y_true_int = list(map(floatbits_to_int, y_true_subset))
    
    # Print the predicted and true integers.
    print(*zip(y_pred_int, y_true_int))
    
    # Round the float-bit-strings to 2 decimals for pretty printing.
    def rounded(numbers):
        return np.array([[""{:.2f}"".format(x) for x in row] for row in numbers])
    y_pred_rounded = rounded(y_pred)
    y_true_rounded = rounded(y_true_subset)
    
    # Print the predicted and true float-bit-strings.
    # (I know it is bad to reuse the same variable-names here ...)
    for y_pred_int, y_true_int, y_pred_rounded, y_true_rounded \
        in zip(y_pred_int, y_true_int, y_pred_rounded, y_true_rounded):
    
        print(y_true_int, ""\t"", y_true_rounded)
        print(y_pred_int, ""\t"", y_pred_rounded)
        print()


### Output

True integer and true ""float-bit-string"" (note that the numbers are all exactly 0.00 or 1.00):

	738 	 ['0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '1.00' '0.00' '1.00' '1.00' '1.00' '0.00' '0.00' '0.00' '1.00' '0.00']

Predicted integer and predicted ""float-bit-string"" (note that the numbers a **not** all exactly 0.00 or 1.00):

	738 	 ['0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.03' '0.00' '0.00' '0.00' '0.01' '0.00' '0.00' '0.00' '0.00' '0.99' '0.00' '1.00' '1.00' '1.00' '0.00' '0.00' '0.00' '1.00' '0.00']
",0,,3,2018-01-27T15:23:29Z,2018-01-30T03:22:21Z,CONTRIBUTOR,"### System information (Not really relevant ...)
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.4
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: NA
- **GCC/Compiler version (if compiling from source)**: NA
- **CUDA/cuDNN version**: 8 (?)
- **GPU model and memory**: GTX 1070
- **Exact command to reproduce**: NA

### Summary

This proposes the use of what I call ""float-bit-strings"" or ""float-bits"" instead of one-hot-encoded arrays so as to greatly reduce the memory and computational usage e.g. in language models.

I don't think this preliminary discussion belongs on StackOverflow so I hope it is OK to post it here. It is a new feature that could be added to TensorFlow. There's quite likely somebody on the TensorFlow dev-team or in the community who has already thought of this. But I have searched the internet and cannot find any mentioning of a similar idea.


### Background

I have started looking at language-models using e.g. LSTM and encoder-decoder architectures. There are some aspects that seem to be incredibly wasteful and limiting. Let me briefly describe this and please forgive me if I am ignorant, I have only spent a week or two on studying LSTM and language models so far :-)

For example in Machine Translation we typically have the text-data for the source- and target-languages as lists of integer-tokens, where each integer maps to a word in the vocabulary. There may be e.g. 100k different words so these integer-tokens can take on values between zero and 100k. This data cannot be input directly to a Neural Network so we use an embedding layer to convert these integers to n-dimensional vectors with values between zero and one, according to a mapping-function that may either be loaded from disk or trained along with the rest of the Neural Network; if I understand correctly.

For the decoder in a language model, we have a similar problem where we must somehow convert integer-tokens to data that the neural network can work on. A typical way of doing this seems to be a one-hot encoding; if I understand correctly. (This could also be done for the encoder-part, but it doesn't seem to be necessary).

I can't figure out what the max-size of one-hot encodings are in TensorFlow and whether it can even handle 100k one-hot encoded tensors. But it is obviously an extremely wasteful data-mapping. For example, for a vocab of 100k words we only need 17-bits (log2(100k)) to represent each integer-token - but for a one-hot encoding using 32-bit floats we need 32 x 100k bits!

I can't figure out what people normally do, but it seems like the common practice is to limit the vocab to a smaller number of words, e.g. 1k or 10k. It appears that Google Translate runs on multiple GPU's and maybe that's why they can handle extremely large vocabs with one-hot encoded tensors?


### Float-bit-strings

I thought it might be possible to use a bit-string-like representation inside a TensorFlow model. I have searched the internet and cannot find anyone who has proposed a similar idea.

The idea is to convert each integer-token to what I call a ""float-bit-string"" or ""float-bits"". For example, the number 123 has the bit-string 01111011. We can then make a corresponding tensor with floats [0., 1., 1., 1., 1., 0., 1., 1.] and input this to the TensorFlow model.

In a language model we would then have to input and output these ""float-bits"" instead of one-hot encoded arrays. This would dramatically reduce the memory and computational requirements of the models.


### Test

I have hacked together a little test using numpy and Keras / TensorFlow. The idea is to see if we can learn to map integers x with values between 0 and 10k to y = 123 * x using these ""float-bit"" encodings. And it works as you can see by running the code further below! That is perhaps not a surprise as neural networks are general function approximators, but it's not always that they work according to theory :-)

However, the network cannot learn the arithmetic mapping of e.g. y = 123 * x when x and y are ""float-bits"". This means it cannot generalize to data it hasn't seen during training in the arithmetic manner we might expect. But I don't think that is necessary for use in e.g. language models where we merely want to be able to map some tensor from e.g. an LSTM to an integer-token from the vocabulary.


### Loss Functions

I have tested this with both MSE and binary cross-entropy in Keras, which unfortunately isn't documented so I'm not completely sure what it does. But in both cases it works and the model trains to get the bit-wise mapping correct.

There might be cases where you are more concerned about the MSE between the actual integer-values instead of their ""float-bit-string"" representations, in which case we would need a TensorFlow method to convert ""float-bits"" to integers and then take the MSE of the resulting integer and the true integer from the data-set. This is not relevant for language models, because the proximity of integer-keys do not correspond to words that are necessarily similar in meaning. But it could be useful in other applications.


### TensorFlow Implementation

In order to make this work in TensorFlow it seems that we just need a couple of TensorFlow-methods for converting between integers and ""float-bit-strings"". I have hacked this together using numpy but I'm sure somebody on the dev-team can make a super-fast native TensorFlow implementation. Then we just need a wrapper in Keras and that might be enough to do e.g. language models with gigantic vocabs.


### Test-Code

    import numpy as np
    from tensorflow.python.keras.models import Sequential
    from tensorflow.python.keras.layers import InputLayer
    from tensorflow.python.keras.layers import Dense
    from tensorflow.python.keras.optimizers import RMSprop
    
    
    # Number of bits to use in our ""float-bit-strings"".
    num_bits = 32
    
    def int_to_floatbits(value):
        """"""
        Convert a single integer value to an array of 0.0 and 1.0 floats
        corresponding to the bit-string.
    
        Example: value==123 gives [0.  ... 0.  1.  1.  1.  1.  0.  1.  1.]
        """"""
    
        # Convert the integer value to a bit-string.
        # NOTE: This has been fixed to 32-bit length.
        bitstr = ""{0:032b}"".format(value)
    
        # Convert the bit-string to an array of equivalent float-values.
        floatbits = np.array([1.0 if bit == '1' else 0.0 for bit in bitstr])
    
        return floatbits
    
    
    def floatbits_to_strbits(floatbits):
        """"""
        Convert an array of floats to a bit-string.
        A float value greater than 0.5 results in 1.0
        and a float value less or equal to 0.5 results in 0.0
    
        Example: [0.1, 0.49, 0.51, 0.9, 1.1, -2.3] gives ""001110""
        """"""
    
        # Convert the float-array to a list of bit-characters '0' or '1'.
        charbits = ['1' if floatbit > 0.5 else '0' for floatbit in floatbits]
    
        # Convert the bit-characters to a string.
        strbits = """".join(charbits)
    
        return strbits
    
    def floatbits_to_int(floatbits):
        """"""
        Convert a float-array to an integer, assuming each element
        of the float-array corresponds to a bit.
        
        Example: [0.1, 0.49, 0.51, 0.9, 1.1, -2.3] corresponds to
        the bit-string ""001110"" which is the integer 14.
        """"""
    
        # Convert the float-array to a bit-string.
        strbits = floatbits_to_strbits(floatbits=floatbits)
    
        # Convert the bit-string to an integer value.
        value = int(strbits, base=2)
    
        return value
    
    
    # Various tests of the above functions.
    if True:
        foo = int_to_floatbits(123)
        print(foo)
        print(floatbits_to_strbits(foo))
        print(floatbits_to_int(foo))
    
        bar = [0.3,  0.9,  0.8,  0.51,  0.501,  0.4999,  0.999,  1.1]
        print(floatbits_to_strbits(bar))
        print(floatbits_to_int(bar))
    
        baz = [0.1, 0.49, 0.51, 0.9, 1.1, -2.3]
        print(floatbits_to_strbits(baz))
        print(floatbits_to_int(baz))
    
    # quit()
    
    # We will now train a TensorFlow / Keras model
    # that maps integers between 0 and 10000 to
    # the same numbers multiplied by 123.
    # If we were to use one-hot encoding then we would
    # need 10000 inputs to the Neural Network and
    # 1230000 outputs if using the full output range.
    # Using ""bit-strings"" encoded as floats, we only need
    # 14 bits for the input and 21 bits for the output.
    # We round it up to 32-bits.
    
    # The dataset as integers,
    # we want the Neural Network to map from x to y.
    x_int = np.arange(10000, dtype=int)
    y_int = 123 * x_int
    
    # Convert the dataset to ""float-bit-strings"" (aka. float-bits).
    x = np.array(list(map(int_to_floatbits, x_int)))
    y_true = np.array(list(map(int_to_floatbits, y_int)))
    
    # Check the mapping is correct. E.g. if the number of required bits
    # exceeds num_bits then these may not create numpy matrices correctly.
    if False:
        print(x.shape)
        print(y_true.shape)
        print(x[0:10])
        print(y_true[0:10])
    
    # Start construction of the Keras Sequential model.
    model = Sequential()
    
    # Add an input layer to the model.
    model.add(InputLayer(input_shape=(num_bits,)))
    
    # Fully-connected / dense layers with ReLU-activation.
    model.add(Dense(512, activation='relu'))
    model.add(Dense(512, activation='relu'))
    
    # Last fully-connected / dense layer with sigmoid-activation
    # so the output is between 0.0 and 1.0
    model.add(Dense(num_bits, activation='sigmoid'))
    
    optimizer = RMSprop(lr=1e-3)
    
    if True:
        # Loss is MSE.
        model.compile(optimizer=optimizer,
                      loss='mean_squared_error')
    else:
        # Loss is Binary Crossentropy, but also report MSE.
        model.compile(optimizer=optimizer,
                      loss='binary_crossentropy',
                      metrics=['mse'])
    
    epochs = 50
    
    if True:
        # Fit the model using the entire data-set.
        model.fit(x, y_true, epochs=epochs)
    else:
        # Fit the model using the data-set split into training and validation.
        # You will see that the validation-error is high so the model
        # has not learned the arithmetic function of the data-set.
        model.fit(x, y_true, epochs=epochs, validation_split=0.2)
    
    # Use the model to predict the output for a part of the data-set.
    y_pred = model.predict(x[0:10])
    
    # The true output for this part of the data-set.
    y_true_subset = y_true[0:10]
    
    # Map the ""float-bit-strings"" to integers.
    y_pred_int = list(map(floatbits_to_int, y_pred))
    y_true_int = list(map(floatbits_to_int, y_true_subset))
    
    # Print the predicted and true integers.
    print(*zip(y_pred_int, y_true_int))
    
    # Round the float-bit-strings to 2 decimals for pretty printing.
    def rounded(numbers):
        return np.array([[""{:.2f}"".format(x) for x in row] for row in numbers])
    y_pred_rounded = rounded(y_pred)
    y_true_rounded = rounded(y_true_subset)
    
    # Print the predicted and true float-bit-strings.
    # (I know it is bad to reuse the same variable-names here ...)
    for y_pred_int, y_true_int, y_pred_rounded, y_true_rounded \
        in zip(y_pred_int, y_true_int, y_pred_rounded, y_true_rounded):
    
        print(y_true_int, ""\t"", y_true_rounded)
        print(y_pred_int, ""\t"", y_pred_rounded)
        print()


### Output

True integer and true ""float-bit-string"" (note that the numbers are all exactly 0.00 or 1.00):

	738 	 ['0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '1.00' '0.00' '1.00' '1.00' '1.00' '0.00' '0.00' '0.00' '1.00' '0.00']

Predicted integer and predicted ""float-bit-string"" (note that the numbers a **not** all exactly 0.00 or 1.00):

	738 	 ['0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.03' '0.00' '0.00' '0.00' '0.01' '0.00' '0.00' '0.00' '0.00' '0.99' '0.00' '1.00' '1.00' '1.00' '0.00' '0.00' '0.00' '1.00' '0.00']
",2018-01-30T03:22:21Z,3,2,1,3.7885390081777928
172,16486,Change RELEASE.md to specify CUDA 9.0,cla: yes,PR for https://github.com/tensorflow/tensorflow/issues/16348 (tinyest PR ever?),0,,3,2018-01-27T09:17:33Z,2018-02-01T03:22:55Z,CONTRIBUTOR,PR for https://github.com/tensorflow/tensorflow/issues/16348 (tinyest PR ever?),2018-01-27T09:19:21Z,4,2,1,3.7885390081777928
173,16485,resolve undefined name array_ops,cla: yes,"flake8 testing of https://github.com/tensorflow/tensorflow on Python 2.7.14

$ __flake8 . --count --select=E901,E999,F821,F822,F823 --show-source --statistics__
```
./tensorflow/contrib/framework/python/ops/accumulate_n_v2.py:94:12: F821 undefined name 'array_ops'
    return array_ops.identity(inputs[0], name=name)
           ^
```",1,,1,2018-01-27T09:07:33Z,2018-02-08T00:35:30Z,CONTRIBUTOR,"flake8 testing of https://github.com/tensorflow/tensorflow on Python 2.7.14

$ __flake8 . --count --select=E901,E999,F821,F822,F823 --show-source --statistics__
```
./tensorflow/contrib/framework/python/ops/accumulate_n_v2.py:94:12: F821 undefined name 'array_ops'
    return array_ops.identity(inputs[0], name=name)
           ^
```",2018-02-02T21:25:32Z,11,2,2,3.7885390081777928
174,16484,use gather_nd to _gather_states in LSTMBlockWapper,"awaiting testing (then merge),cla: yes",no need for calculate mod_indices and reshape data .,1,,1,2018-01-27T08:52:23Z,2018-02-15T21:55:48Z,CONTRIBUTOR,no need for calculate mod_indices and reshape data .,2018-02-15T06:01:47Z,18,2,3,3.7885390081777928
175,16481,Container localhost does not exist.,"stat:awaiting tensorflower,type:bug/performance","Hi,

I upgraded from 1.5.0-rc1 to the current master branch and I started receiving the following error:

```
2018-01-27 02:48:38.928667: W tensorflow/core/framework/op_kernel.cc:1201] OP_REQUIRES failed at lookup_table_op.cc:656 : Not found: Container localhost does not exist. (Could not find resource: localhost/hash_table_/Users/anthony/Development/GitHub/symphony-mt/temp/data/iwslt-15/vocab.vi_WHOLE_LINE_LINE_NUMBER)
2018-01-27 02:48:38.928786: W tensorflow/core/framework/op_kernel.cc:1201] OP_REQUIRES failed at iterator_ops.cc:855 : Not found: Container localhost does not exist. (Could not find resource: localhost/hash_table_/Users/anthony/Development/GitHub/symphony-mt/temp/data/iwslt-15/vocab.vi_WHOLE_LINE_LINE_NUMBER)
	 [[Node: Lookup_1/LookupTableFind = LookupTableFindV2[Tin=DT_STRING, Tout=DT_INT64](lookup_1_placeholder, input_1, lookup_1_placeholder_1)]]
Exception in thread ""main"" org.platanios.tensorflow.jni.NotFoundException: Container localhost does not exist. (Could not find resource: localhost/hash_table_/Users/anthony/Development/GitHub/symphony-mt/temp/data/iwslt-15/vocab.vi_WHOLE_LINE_LINE_NUMBER)
	 [[Node: Lookup_1/LookupTableFind = LookupTableFindV2[Tin=DT_STRING, Tout=DT_INT64](lookup_1_placeholder, input_1, lookup_1_placeholder_1)]]
	 [[Node: Model/Model/Iterator/Next = IteratorGetNext[output_shapes=[[?,?], [?], [?,?], [?,?], [?]], output_types=[DT_INT32, DT_INT32, DT_INT32, DT_INT32, DT_INT32], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](Model/Model/Iterator)]]
```

It's hard to reproduce this error but a summary of the context is that I have a lookup table op inside a dataset map operator and I get this error when I try to execute the corresponding iterator ""GetNext"" op. I'm looking for information in how to parse and debug this error. I never explicitly set any containers for my variables or lookup tables (i.e., leave them to the default value; an empty string). Were there any changes introduced recently that could result in this error? Note that this happens with my Scala API but not with the Python API and so it may be that I haven't updated something in my code. I just don't really know where to look for this.

Thanks!",1,,12,2018-01-27T07:58:18Z,2018-02-02T02:58:59Z,CONTRIBUTOR,"Hi,

I upgraded from 1.5.0-rc1 to the current master branch and I started receiving the following error:

```
2018-01-27 02:48:38.928667: W tensorflow/core/framework/op_kernel.cc:1201] OP_REQUIRES failed at lookup_table_op.cc:656 : Not found: Container localhost does not exist. (Could not find resource: localhost/hash_table_/Users/anthony/Development/GitHub/symphony-mt/temp/data/iwslt-15/vocab.vi_WHOLE_LINE_LINE_NUMBER)
2018-01-27 02:48:38.928786: W tensorflow/core/framework/op_kernel.cc:1201] OP_REQUIRES failed at iterator_ops.cc:855 : Not found: Container localhost does not exist. (Could not find resource: localhost/hash_table_/Users/anthony/Development/GitHub/symphony-mt/temp/data/iwslt-15/vocab.vi_WHOLE_LINE_LINE_NUMBER)
	 [[Node: Lookup_1/LookupTableFind = LookupTableFindV2[Tin=DT_STRING, Tout=DT_INT64](lookup_1_placeholder, input_1, lookup_1_placeholder_1)]]
Exception in thread ""main"" org.platanios.tensorflow.jni.NotFoundException: Container localhost does not exist. (Could not find resource: localhost/hash_table_/Users/anthony/Development/GitHub/symphony-mt/temp/data/iwslt-15/vocab.vi_WHOLE_LINE_LINE_NUMBER)
	 [[Node: Lookup_1/LookupTableFind = LookupTableFindV2[Tin=DT_STRING, Tout=DT_INT64](lookup_1_placeholder, input_1, lookup_1_placeholder_1)]]
	 [[Node: Model/Model/Iterator/Next = IteratorGetNext[output_shapes=[[?,?], [?], [?,?], [?,?], [?]], output_types=[DT_INT32, DT_INT32, DT_INT32, DT_INT32, DT_INT32], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](Model/Model/Iterator)]]
```

It's hard to reproduce this error but a summary of the context is that I have a lookup table op inside a dataset map operator and I get this error when I try to execute the corresponding iterator ""GetNext"" op. I'm looking for information in how to parse and debug this error. I never explicitly set any containers for my variables or lookup tables (i.e., leave them to the default value; an empty string). Were there any changes introduced recently that could result in this error? Note that this happens with my Scala API but not with the Python API and so it may be that I haven't updated something in my code. I just don't really know where to look for this.

Thanks!",2018-01-27T15:54:41Z,5,2,1,9.288539008177793
176,16479,Does 1.5.0 not suppurt CUDA 9.1? It worked with CUDA 9.0 but not 9.1,,"I installed 1.5.0, and tying to import tensorflow, but it said that 'cannot find cudart64_90.dll'. 
Then I installed the CUDA 9.0, and then everything works fine. 
So I want to make sure than does 1.5.0 not support CUDA 9.1 or I have something installed wrong? 
",0,,6,2018-01-27T02:22:43Z,2018-01-27T23:01:32Z,NONE,"I installed 1.5.0, and tying to import tensorflow, but it said that 'cannot find cudart64_90.dll'. 
Then I installed the CUDA 9.0, and then everything works fine. 
So I want to make sure than does 1.5.0 not support CUDA 9.1 or I have something installed wrong? 
",2018-01-27T03:21:39Z,0,1,0,4.288539008177793
177,16478,Failed install on Windows,type:bug/performance,"Python 3.6.4

There is a strange error when I install tensorflow 1.5.

```
Collecting tensorflow
  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/34/96/11f048eca7b4d6da3084ca49c636b9e720e9dd1483c0c4e9ba3cf5037564/tensorflow-1.5.0-cp36-cp36m-win_amd64.whl
Requirement already up-to-date: wheel>=0.26 in d:\python\python36\lib\site-packages (from tensorflow)
Requirement already up-to-date: numpy>=1.12.1 in d:\python\python36\lib\site-packages (from tensorflow)
Collecting absl-py>=0.1.6 (from tensorflow)
  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/42/3c/1985d86a44bfe44fd060c02807336f840a509bfaa2d340860fba7d22da39/absl-py-0.1.9.tar.gz
Requirement already up-to-date: protobuf>=3.4.0 in d:\python\python36\lib\site-packages (from tensorflow)
Requirement already up-to-date: six>=1.10.0 in d:\python\python36\lib\site-packages (from tensorflow)
Collecting tensorflow-tensorboard<1.6.0,>=1.5.0 (from tensorflow)
  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/43/69/82e2a368076c94edbba3cd15804103bf1f31486d69e11551b71fa1d1f384/tensorflow_tensorboard-1.5.0-py3-none-any.whl
Requirement already up-to-date: setuptools in d:\python\python36\lib\site-packages (from protobuf>=3.4.0->tensorflow)
Requirement already up-to-date: bleach==1.5.0 in d:\python\python36\lib\site-packages (from tensorflow-tensorboard<1.6.0,>=1.5.0->tensorflow)
Requirement already up-to-date: markdown>=2.6.8 in d:\python\python36\lib\site-packages (from tensorflow-tensorboard<1.6.0,>=1.5.0->tensorflow)
Requirement already up-to-date: werkzeug>=0.11.10 in d:\python\python36\lib\site-packages (from tensorflow-tensorboard<1.6.0,>=1.5.0->tensorflow)
Requirement already up-to-date: html5lib==0.9999999 in d:\python\python36\lib\site-packages (from tensorflow-tensorboard<1.6.0,>=1.5.0->tensorflow)
Collecting futures>=3.1.1 (from tensorflow-tensorboard<1.6.0,>=1.5.0->tensorflow)
  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/1f/9e/7b2ff7e965fc654592269f2906ade1c7d705f1bf25b7d469fa153f7d19eb/futures-3.2.0.tar.gz
Unknown requires Python '>=2.6, <3' but the running Python is 3.6.4
```

Why the dependency is *futures*? It doesn't have a verion of Python 3.6.4.",0,,15,2018-01-27T02:08:44Z,2018-01-27T10:16:51Z,NONE,"Python 3.6.4

There is a strange error when I install tensorflow 1.5.

```
Collecting tensorflow
  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/34/96/11f048eca7b4d6da3084ca49c636b9e720e9dd1483c0c4e9ba3cf5037564/tensorflow-1.5.0-cp36-cp36m-win_amd64.whl
Requirement already up-to-date: wheel>=0.26 in d:\python\python36\lib\site-packages (from tensorflow)
Requirement already up-to-date: numpy>=1.12.1 in d:\python\python36\lib\site-packages (from tensorflow)
Collecting absl-py>=0.1.6 (from tensorflow)
  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/42/3c/1985d86a44bfe44fd060c02807336f840a509bfaa2d340860fba7d22da39/absl-py-0.1.9.tar.gz
Requirement already up-to-date: protobuf>=3.4.0 in d:\python\python36\lib\site-packages (from tensorflow)
Requirement already up-to-date: six>=1.10.0 in d:\python\python36\lib\site-packages (from tensorflow)
Collecting tensorflow-tensorboard<1.6.0,>=1.5.0 (from tensorflow)
  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/43/69/82e2a368076c94edbba3cd15804103bf1f31486d69e11551b71fa1d1f384/tensorflow_tensorboard-1.5.0-py3-none-any.whl
Requirement already up-to-date: setuptools in d:\python\python36\lib\site-packages (from protobuf>=3.4.0->tensorflow)
Requirement already up-to-date: bleach==1.5.0 in d:\python\python36\lib\site-packages (from tensorflow-tensorboard<1.6.0,>=1.5.0->tensorflow)
Requirement already up-to-date: markdown>=2.6.8 in d:\python\python36\lib\site-packages (from tensorflow-tensorboard<1.6.0,>=1.5.0->tensorflow)
Requirement already up-to-date: werkzeug>=0.11.10 in d:\python\python36\lib\site-packages (from tensorflow-tensorboard<1.6.0,>=1.5.0->tensorflow)
Requirement already up-to-date: html5lib==0.9999999 in d:\python\python36\lib\site-packages (from tensorflow-tensorboard<1.6.0,>=1.5.0->tensorflow)
Collecting futures>=3.1.1 (from tensorflow-tensorboard<1.6.0,>=1.5.0->tensorflow)
  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/1f/9e/7b2ff7e965fc654592269f2906ade1c7d705f1bf25b7d469fa153f7d19eb/futures-3.2.0.tar.gz
Unknown requires Python '>=2.6, <3' but the running Python is 3.6.4
```

Why the dependency is *futures*? It doesn't have a verion of Python 3.6.4.",2018-01-27T03:28:37Z,0,1,0,8.788539008177793
178,16477,"Windows Installation tutorial has wrong cuda version requirement, 9.0 required for latest version.",,"I just ran the installation validation and it's telling me I need 9.0, the tutorial says we must use 8.0. I don't have cheap access to Internet, now I have to find 1GB+ plus of data without paying $15 to use my phone's data. Please update the page to recommend 9.0.

Thank you.",1,,14,2018-01-27T01:54:37Z,2018-02-12T07:11:15Z,NONE,"I just ran the installation validation and it's telling me I need 9.0, the tutorial says we must use 8.0. I don't have cheap access to Internet, now I have to find 1GB+ plus of data without paying $15 to use my phone's data. Please update the page to recommend 9.0.

Thank you.",2018-01-28T14:18:17Z,15,1,3,9.288539008177793
179,16474,MKL: Making MKL-DNN default,"awaiting testing (then merge),cla: yes",Make Tensroflow use MKL DNN by default if --config=mkl is used when building,1,,10,2018-01-26T22:08:58Z,2018-01-26T23:43:30Z,CONTRIBUTOR,Make Tensroflow use MKL DNN by default if --config=mkl is used when building,2018-01-26T22:14:09Z,0,2,0,8.285999667502674
180,16473,Fixing hard_sigmoid's documentation to match impl,cla: no,,1,,3,2018-01-26T21:25:42Z,2018-01-30T07:15:07Z,NONE,,2018-01-26T21:34:42Z,4,1,1,3.7859996675026744
181,16464,AssignAddVariableOp has no output,"stat:awaiting tensorflower,type:docs","
------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Arch Linux
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.5.0-rc1
- **Python version**: NA (Using Go bindings)
- **Bazel version (if compiling from source)**: 0.9.0
- **GCC/Compiler version (if compiling from source)**: 7.2.1
- **CUDA/cuDNN version**: 9.1 / 7.0
- **GPU model and memory**: GTX 1060 6GB
- **Exact command to reproduce**: See below


### Describe the problem
According to the docs, AssignAddVariableOp ""Outputs the incremented value, which can be used to totally order the increments to this variable."". Without this feature, I get non deterministic behavior when reading the value of the variable at the same time as I update it. However, at least in the Go bindings, it returns an operation which has no outputs. I can work around this problem by using two calls to `sess.Run()`, but this is inelegant.

### Source code / logs
```
package main

import (
	""fmt""

	tf ""github.com/tensorflow/tensorflow/tensorflow/go""
	""github.com/tensorflow/tensorflow/tensorflow/go/op""
)

func main() {
	s := op.NewScope()
	value1 := op.Const(s.SubScope(""zero""), float32(0))
	value2 := op.Const(s, float32(3.1415))
	handle := op.VarHandleOp(s, tf.Float, tf.ScalarShape())
	init := op.AssignVariableOp(s, handle, value1)
	update := op.AssignAddVariableOp(s, handle, value2)
	fmt.Println(""NumOutputs:"", update.NumOutputs())
	graph, err := s.Finalize()
	if err != nil {
		panic(err)
	}
	sess, err := tf.NewSession(graph, nil)
	if err != nil {
		panic(err)
	}
	_, err = sess.Run(nil, nil, []*tf.Operation{init})
	if err != nil {
		panic(err)
	}
	_, err = sess.Run(nil, []tf.Output{update.Output(0)}, nil)
	if err != nil {
		panic(err)
	}
}
```
```
$ go run assign_demo.go 
NumOutputs: 0
panic: Tried to fetch data for 'AssignAddVariableOp:0', which produces no output.  To run to a node but not fetch any data, pass 'AssignAddVariableOp:0' as an argument to the 'target_node_names' argument of the Session::Run API.

goroutine 1 [running]:
main.main()
	/home/isaac/go/src/github.com/is8ac/gotf/assign_demo.go:32 +0x448
exit status 2
```",1,,4,2018-01-26T17:33:42Z,2018-02-01T06:41:53Z,NONE,"
------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Arch Linux
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.5.0-rc1
- **Python version**: NA (Using Go bindings)
- **Bazel version (if compiling from source)**: 0.9.0
- **GCC/Compiler version (if compiling from source)**: 7.2.1
- **CUDA/cuDNN version**: 9.1 / 7.0
- **GPU model and memory**: GTX 1060 6GB
- **Exact command to reproduce**: See below


### Describe the problem
According to the docs, AssignAddVariableOp ""Outputs the incremented value, which can be used to totally order the increments to this variable."". Without this feature, I get non deterministic behavior when reading the value of the variable at the same time as I update it. However, at least in the Go bindings, it returns an operation which has no outputs. I can work around this problem by using two calls to `sess.Run()`, but this is inelegant.

### Source code / logs
```
package main

import (
	""fmt""

	tf ""github.com/tensorflow/tensorflow/tensorflow/go""
	""github.com/tensorflow/tensorflow/tensorflow/go/op""
)

func main() {
	s := op.NewScope()
	value1 := op.Const(s.SubScope(""zero""), float32(0))
	value2 := op.Const(s, float32(3.1415))
	handle := op.VarHandleOp(s, tf.Float, tf.ScalarShape())
	init := op.AssignVariableOp(s, handle, value1)
	update := op.AssignAddVariableOp(s, handle, value2)
	fmt.Println(""NumOutputs:"", update.NumOutputs())
	graph, err := s.Finalize()
	if err != nil {
		panic(err)
	}
	sess, err := tf.NewSession(graph, nil)
	if err != nil {
		panic(err)
	}
	_, err = sess.Run(nil, nil, []*tf.Operation{init})
	if err != nil {
		panic(err)
	}
	_, err = sess.Run(nil, []tf.Output{update.Output(0)}, nil)
	if err != nil {
		panic(err)
	}
}
```
```
$ go run assign_demo.go 
NumOutputs: 0
panic: Tried to fetch data for 'AssignAddVariableOp:0', which produces no output.  To run to a node but not fetch any data, pass 'AssignAddVariableOp:0' as an argument to the 'target_node_names' argument of the Session::Run API.

goroutine 1 [running]:
main.main()
	/home/isaac/go/src/github.com/is8ac/gotf/assign_demo.go:32 +0x448
exit status 2
```",2018-01-27T05:35:56Z,5,1,1,4.285999667502674
182,16463,Improve profiler error message when graph_path is not available.,"awaiting testing (then merge),cla: yes,kokoro:run","This fix tries to address the issue raised in #16451 to provide a better error message when graph_path is not available for profiler.

Previously if graph_path is not available, the process will crash
with not very imformative message and a core dump:
```
2018-01-26 01:43:29.458032: F tensorflow/core/profiler/profiler.cc:206] Non-OK-status: ReadProtoFile(Env::Default(), FLAGS_graph_path, graph.get(), false) status: Not found: ; No such file or directory
Aborted (core dumped)
```

With this fix, the error message is improved to:
```
Failed to read graph_path: Invalid argument: Cannot parse proto file.
```
and the process exit with 1.

This fix fixes #16451.

Signed-off-by: Yong Tang <yong.tang.github@outlook.com>",1,,1,2018-01-26T16:55:04Z,2018-01-26T18:11:51Z,MEMBER,"This fix tries to address the issue raised in #16451 to provide a better error message when graph_path is not available for profiler.

Previously if graph_path is not available, the process will crash
with not very imformative message and a core dump:
```
2018-01-26 01:43:29.458032: F tensorflow/core/profiler/profiler.cc:206] Non-OK-status: ReadProtoFile(Env::Default(), FLAGS_graph_path, graph.get(), false) status: Not found: ; No such file or directory
Aborted (core dumped)
```

With this fix, the error message is improved to:
```
Failed to read graph_path: Invalid argument: Cannot parse proto file.
```
and the process exit with 1.

This fix fixes #16451.

Signed-off-by: Yong Tang <yong.tang.github@outlook.com>",2018-01-26T17:13:08Z,0,3,0,4.785999667502674
183,16462,How to create a model checkpoint based on each step rather than time interval.? using TensorFlow-Slim api.,"stat:awaiting tensorflower,type:support","slim.learning.train(
    train_op,
    logdir,
    number_of_steps=1000,
    **save_summaries_secs=300**,
    **save_interval_secs=600**):

The above api only supports to capture model checpoints periodically, but I need to checkpoint based on each step. How do achieve this using TesorFlow-Slim API.?

I am looking for parameters like this:

save_summaries_steps = 10,
save_interval_steps=10
    where the value 10 is the number of steps and that should be configurable.",0,,3,2018-01-26T16:48:53Z,2018-01-27T01:03:49Z,NONE,"slim.learning.train(
    train_op,
    logdir,
    number_of_steps=1000,
    **save_summaries_secs=300**,
    **save_interval_secs=600**):

The above api only supports to capture model checpoints periodically, but I need to checkpoint based on each step. How do achieve this using TesorFlow-Slim API.?

I am looking for parameters like this:

save_summaries_steps = 10,
save_interval_steps=10
    where the value 10 is the number of steps and that should be configurable.",2018-01-27T01:03:49Z,1,1,0,2.7859996675026744
184,16460,Fix missing .,"awaiting testing (then merge),cla: yes",,1,,8,2018-01-26T16:19:31Z,2018-02-08T20:17:37Z,CONTRIBUTOR,,2018-01-26T16:27:11Z,12,2,2,7.285999667502674
185,16458,How to parse multivalve feature using tf.feature_column and tf.data API ??,,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",0,,2,2018-01-26T15:58:52Z,2018-01-27T01:02:00Z,NONE,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",2018-01-27T01:02:00Z,1,1,0,2.2859996675026744
186,16456,'InputFnOps' object has no attribute 'receiver_tensors',stat:awaiting response,"
### System information
==TensorFlow installed from (source or binary)==
Source
== Python version ==
Python 2.7.13
== cat /etc/issue ==
Linux orion 4.9.0-3-amd64 #1 SMP Debian 4.9.30-2+deb9u5 (2017-09-19) x86_64 GNU/Linux
VERSION_ID=""9""
VERSION=""9 (stretch)""
== are we in docker ==
No
== compiler ==
c++ (Debian 6.3.0-18) 6.3.0 20170516
Copyright (C) 2016 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
== uname -a ==
Linux orion 4.9.0-3-amd64 #1 SMP Debian 4.9.30-2+deb9u5 (2017-09-19) x86_64 GNU/Linux
== check pips ==
numpy (1.12.1)
protobuf (3.5.1)
tensorflow (1.3.0)
tensorflow-tensorboard (0.1.8)
tensorflow-transform (0.3.1)
== check for virtualenv ==
True
== tensorflow import ==
tf.VERSION = 1.3.0
tf.GIT_VERSION = v1.3.0-rc2-20-g0787eee
tf.COMPILER_VERSION = v1.3.0-rc2-20-g0787eee
Sanity check: array([1], dtype=int32)
== env ==
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

### Describe the problem
I ran into some incompatibility issues running tf.estimator.DNNClassifier with tf.contrib.learn.InputFnOps and tf.contrib.learn.Experiment.  
There has been a separate [solved issue](https://github.com/tensorflow/transform/issues/36): 
I have tried the bundle version, but it doesn't work for me:
tensorflow==1.3
tensorflow_transform==0.3.1
six==1.10.0
However, if I switch to tf.contrib.learn.DNNClassifier, the issue go away.
It is also suggested to use tf.estimator.DNNClassifier rather than tf.contrib.learn.DNNClassifier.  I would like to get tf.estimator.DNNClassifier work.

### Source code / logs
```
def build_estimator(config):
    m = tf.estimator.DNNClassifier(
   # m = tf.contrib.learn.DNNClassifier(
    					config=config,
    					feature_columns=deep_columns,
                        hidden_units=[100, 100, 100],
                        n_classes=2,
                        optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.01)
                        )
    return m
```
```
def json_serving_input_fn():
	""""""Build the serving inputs.""""""
	inputs = {}
	for feat in INPUT_COLUMNS:
		inputs[feat.name] = tf.placeholder(shape=[None], dtype=feat.dtype)

	features = {
	  key: tf.expand_dims(tensor, -1)
	  for key, tensor in inputs.iteritems()
	}
	return tf.contrib.learn.InputFnOps(features, None, inputs)
```
```
def _experiment_fn(run_config, hparams):
    # num_epochs can control duration if train_steps isn't
    # passed to Experiment
    train_input = lambda: model.generate_input_fn(
        hparams.train_files,
        num_epochs=hparams.num_epochs,
        batch_size=hparams.train_batch_size,
    )
    # Don't shuffle evaluation data
    eval_input = lambda: model.generate_input_fn(
        hparams.eval_files,
        batch_size=hparams.eval_batch_size,
        shuffle=False
    )
    return tf.contrib.learn.Experiment(
        model.build_estimator(
            config=run_config
        )
```


Error message:
local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 440, in export_savedmodel
    serving_input_receiver.receiver_tensors,
AttributeError: 'InputFnOps' object has no attribute 'receiver_tensors'
",0,,3,2018-01-26T15:07:52Z,2018-02-12T22:11:17Z,NONE,"
### System information
==TensorFlow installed from (source or binary)==
Source
== Python version ==
Python 2.7.13
== cat /etc/issue ==
Linux orion 4.9.0-3-amd64 #1 SMP Debian 4.9.30-2+deb9u5 (2017-09-19) x86_64 GNU/Linux
VERSION_ID=""9""
VERSION=""9 (stretch)""
== are we in docker ==
No
== compiler ==
c++ (Debian 6.3.0-18) 6.3.0 20170516
Copyright (C) 2016 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
== uname -a ==
Linux orion 4.9.0-3-amd64 #1 SMP Debian 4.9.30-2+deb9u5 (2017-09-19) x86_64 GNU/Linux
== check pips ==
numpy (1.12.1)
protobuf (3.5.1)
tensorflow (1.3.0)
tensorflow-tensorboard (0.1.8)
tensorflow-transform (0.3.1)
== check for virtualenv ==
True
== tensorflow import ==
tf.VERSION = 1.3.0
tf.GIT_VERSION = v1.3.0-rc2-20-g0787eee
tf.COMPILER_VERSION = v1.3.0-rc2-20-g0787eee
Sanity check: array([1], dtype=int32)
== env ==
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

### Describe the problem
I ran into some incompatibility issues running tf.estimator.DNNClassifier with tf.contrib.learn.InputFnOps and tf.contrib.learn.Experiment.  
There has been a separate [solved issue](https://github.com/tensorflow/transform/issues/36): 
I have tried the bundle version, but it doesn't work for me:
tensorflow==1.3
tensorflow_transform==0.3.1
six==1.10.0
However, if I switch to tf.contrib.learn.DNNClassifier, the issue go away.
It is also suggested to use tf.estimator.DNNClassifier rather than tf.contrib.learn.DNNClassifier.  I would like to get tf.estimator.DNNClassifier work.

### Source code / logs
```
def build_estimator(config):
    m = tf.estimator.DNNClassifier(
   # m = tf.contrib.learn.DNNClassifier(
    					config=config,
    					feature_columns=deep_columns,
                        hidden_units=[100, 100, 100],
                        n_classes=2,
                        optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.01)
                        )
    return m
```
```
def json_serving_input_fn():
	""""""Build the serving inputs.""""""
	inputs = {}
	for feat in INPUT_COLUMNS:
		inputs[feat.name] = tf.placeholder(shape=[None], dtype=feat.dtype)

	features = {
	  key: tf.expand_dims(tensor, -1)
	  for key, tensor in inputs.iteritems()
	}
	return tf.contrib.learn.InputFnOps(features, None, inputs)
```
```
def _experiment_fn(run_config, hparams):
    # num_epochs can control duration if train_steps isn't
    # passed to Experiment
    train_input = lambda: model.generate_input_fn(
        hparams.train_files,
        num_epochs=hparams.num_epochs,
        batch_size=hparams.train_batch_size,
    )
    # Don't shuffle evaluation data
    eval_input = lambda: model.generate_input_fn(
        hparams.eval_files,
        batch_size=hparams.eval_batch_size,
        shuffle=False
    )
    return tf.contrib.learn.Experiment(
        model.build_estimator(
            config=run_config
        )
```


Error message:
local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 440, in export_savedmodel
    serving_input_receiver.receiver_tensors,
AttributeError: 'InputFnOps' object has no attribute 'receiver_tensors'
",2018-01-27T05:37:44Z,16,1,3,2.7859996675026744
187,16454,ValueError: Dimensions 1069539296 and 13528529576648672 are not compatible,stat:awaiting response,"Traceback (most recent call last):
  File ""/home/lihua/.local/lib/python3.5/site-packages/tensorflow/python/framework/tensor_shape.py"", line 558, in merge_with
    new_dims.append(dim.merge_with(other[i]))
  File ""/home/lihua/.local/lib/python3.5/site-packages/tensorflow/python/framework/tensor_shape.py"", line 133, in merge_with
    self.assert_is_compatible_with(other)
  File ""/home/lihua/.local/lib/python3.5/site-packages/tensorflow/python/framework/tensor_shape.py"", line 106, in assert_is_compatible_with
    other))
ValueError: Dimensions 1069539296 and 13528529576648672 are not compatible

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""Train.py"", line 117, in <module>
    train()
  File ""Train.py"", line 54, in train
    train_op=Evaluation.trainning(loss=loss1, learning_rate=0.0001)
  File ""/home/lihua/Documents/Projects/Project2018/trafficSignClassification/Evaluation.py"", line 36, in trainning
    train_op = optimizer.minimize(loss, global_step= global_step)
  File ""/home/lihua/.local/lib/python3.5/site-packages/tensorflow/python/training/optimizer.py"", line 315, in minimize
    grad_loss=grad_loss)
  File ""/home/lihua/.local/lib/python3.5/site-packages/tensorflow/python/training/optimizer.py"", line 386, in compute_gradients
    colocate_gradients_with_ops=colocate_gradients_with_ops)
  File ""/home/lihua/.local/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py"", line 560, in gradients
    in_grad.set_shape(t_in.get_shape())
  File ""/home/lihua/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 443, in set_shape
    self._shape = self._shape.merge_with(shape)
  File ""/home/lihua/.local/lib/python3.5/site-packages/tensorflow/python/framework/tensor_shape.py"", line 561, in merge_with
    raise ValueError(""Shapes %s and %s are not compatible"" % (self, other))
ValueError: Shapes (128, 4, 4, 1069539296) and (128, 4, 4, 13528529576648672) are not compatible


ubuntu16.04
tensorflow 1.4


",0,,1,2018-01-26T13:37:31Z,2018-01-27T02:52:08Z,NONE,"Traceback (most recent call last):
  File ""/home/lihua/.local/lib/python3.5/site-packages/tensorflow/python/framework/tensor_shape.py"", line 558, in merge_with
    new_dims.append(dim.merge_with(other[i]))
  File ""/home/lihua/.local/lib/python3.5/site-packages/tensorflow/python/framework/tensor_shape.py"", line 133, in merge_with
    self.assert_is_compatible_with(other)
  File ""/home/lihua/.local/lib/python3.5/site-packages/tensorflow/python/framework/tensor_shape.py"", line 106, in assert_is_compatible_with
    other))
ValueError: Dimensions 1069539296 and 13528529576648672 are not compatible

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""Train.py"", line 117, in <module>
    train()
  File ""Train.py"", line 54, in train
    train_op=Evaluation.trainning(loss=loss1, learning_rate=0.0001)
  File ""/home/lihua/Documents/Projects/Project2018/trafficSignClassification/Evaluation.py"", line 36, in trainning
    train_op = optimizer.minimize(loss, global_step= global_step)
  File ""/home/lihua/.local/lib/python3.5/site-packages/tensorflow/python/training/optimizer.py"", line 315, in minimize
    grad_loss=grad_loss)
  File ""/home/lihua/.local/lib/python3.5/site-packages/tensorflow/python/training/optimizer.py"", line 386, in compute_gradients
    colocate_gradients_with_ops=colocate_gradients_with_ops)
  File ""/home/lihua/.local/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py"", line 560, in gradients
    in_grad.set_shape(t_in.get_shape())
  File ""/home/lihua/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 443, in set_shape
    self._shape = self._shape.merge_with(shape)
  File ""/home/lihua/.local/lib/python3.5/site-packages/tensorflow/python/framework/tensor_shape.py"", line 561, in merge_with
    raise ValueError(""Shapes %s and %s are not compatible"" % (self, other))
ValueError: Shapes (128, 4, 4, 1069539296) and (128, 4, 4, 13528529576648672) are not compatible


ubuntu16.04
tensorflow 1.4


",2018-01-27T02:52:08Z,1,1,0,1.7859996675026744
188,16451,Parameter parsing error messages,,"Parameter parsing error messages probably can be improved, e.g. 

`bazel-bin/tensorflow/core/profiler/profiler --profile_path=/tmp/for_tfprof/profile_20`

runs ok, but if cd to bazel-bin/tensorflow/core/profiler/ and

`./profiler --profile_path /tmp/for_tfprof/profile_20`

results in 

> ./profiler
> --profile_path
> /tmp/for_tfprof/profile_20
> Reading Files...
> Try to use a single --profile_path instead of graph_path,op_log_path,run_meta_path
> 2018-01-26 01:43:29.458032: F tensorflow/core/profiler/profiler.cc:206] Non-OK-status: ReadProtoFile(Env::Default(), FLAGS_graph_path, graph.get(), false) status: Not found: ; No such file or directory
> Aborted (core dumped)

",0,,1,2018-01-26T10:01:37Z,2018-01-26T18:11:51Z,CONTRIBUTOR,"Parameter parsing error messages probably can be improved, e.g. 

`bazel-bin/tensorflow/core/profiler/profiler --profile_path=/tmp/for_tfprof/profile_20`

runs ok, but if cd to bazel-bin/tensorflow/core/profiler/ and

`./profiler --profile_path /tmp/for_tfprof/profile_20`

results in 

> ./profiler
> --profile_path
> /tmp/for_tfprof/profile_20
> Reading Files...
> Try to use a single --profile_path instead of graph_path,op_log_path,run_meta_path
> 2018-01-26 01:43:29.458032: F tensorflow/core/profiler/profiler.cc:206] Non-OK-status: ReadProtoFile(Env::Default(), FLAGS_graph_path, graph.get(), false) status: Not found: ; No such file or directory
> Aborted (core dumped)

",2018-01-26T16:55:34Z,0,2,0,2.7859996675026744
189,16450,InvalidArgumentError (see above for traceback): sequence_length(0) <= 80 thrown by ctc_loss,stat:awaiting response,"I am encountering this error thrown by `ctc_loss` and I have no idea what it means nor how to resolve it.

```
InvalidArgumentError (see above for traceback): sequence_length(0) <= 80
	 [[Node: CTCLoss = CTCLoss[ctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=true, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](transpose_1/_455, Where/_445, GatherNd, reshape_1/_457)]]
	 [[Node: CTCLoss/_459 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device_incarnation=1, tensor_name=""edge_7988_CTCLoss"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]

```",0,,1,2018-01-26T09:49:04Z,2018-01-27T03:31:47Z,CONTRIBUTOR,"I am encountering this error thrown by `ctc_loss` and I have no idea what it means nor how to resolve it.

```
InvalidArgumentError (see above for traceback): sequence_length(0) <= 80
	 [[Node: CTCLoss = CTCLoss[ctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=true, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](transpose_1/_455, Where/_445, GatherNd, reshape_1/_457)]]
	 [[Node: CTCLoss/_459 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device_incarnation=1, tensor_name=""edge_7988_CTCLoss"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]

```",2018-01-27T03:31:47Z,1,2,0,2.7859996675026744
190,16448,embedding lookup table in tensorflow serving,,"Hi, I am trying to serve a NLP model in tensorflow serving. I am wondering how embedding matrix is being stored in tensorflow serving. If I deploy model to two servers, will the embedding matrix be a distributed table with sharding for looking up?",0,,2,2018-01-26T08:36:01Z,2018-01-27T03:32:39Z,NONE,"Hi, I am trying to serve a NLP model in tensorflow serving. I am wondering how embedding matrix is being stored in tensorflow serving. If I deploy model to two servers, will the embedding matrix be a distributed table with sharding for looking up?",2018-01-27T03:32:39Z,1,1,0,2.2859996675026744
191,16446,use tflite bilinear op to resize input of label_image,"awaiting testing (then merge),cla: yes,comp:lite",replace previous naive `downsize()` function with a `resize()` using TF Lite RESIZE_BILINEAR operator,1,,1,2018-01-26T07:44:36Z,2018-01-31T19:22:18Z,CONTRIBUTOR,replace previous naive `downsize()` function with a `resize()` using TF Lite RESIZE_BILINEAR operator,2018-01-30T18:02:50Z,5,2,1,3.7859996675026744
192,16444,Documentation update,,"The mean squared error is described as following

mean_squared_error(
    labels,
    predictions,
    weights=1.0,
    scope=None,
    loss_collection=tf.GraphKeys.LOSSES,
    reduction=Reduction.SUM_BY_NONZERO_WEIGHTS
)

The default reduction method is MEAN in v1.4",0,,6,2018-01-26T06:42:29Z,2018-01-26T12:08:15Z,NONE,"The mean squared error is described as following

mean_squared_error(
    labels,
    predictions,
    weights=1.0,
    scope=None,
    loss_collection=tf.GraphKeys.LOSSES,
    reduction=Reduction.SUM_BY_NONZERO_WEIGHTS
)

The default reduction method is MEAN in v1.4",2018-01-26T07:05:20Z,0,1,0,4.285999667502674
193,16443,Disable AWS S3 virtual addressing,"awaiting testing (then merge),cla: yes","The fix disables the virtual addressing of AWS S3, as was suggested in the comment https://github.com/tensorflow/tensorflow/issues/16397#issuecomment-360654674

Signed-off-by: Yong Tang <yong.tang.github@outlook.com>
",1,,4,2018-01-26T05:58:11Z,2018-01-29T16:24:14Z,MEMBER,"The fix disables the virtual addressing of AWS S3, as was suggested in the comment https://github.com/tensorflow/tensorflow/issues/16397#issuecomment-360654674

Signed-off-by: Yong Tang <yong.tang.github@outlook.com>
",2018-01-26T18:52:02Z,3,3,1,6.285999667502674
194,16442,"why save model and deploy in android device, the outputs are not the same as in ubuntu?",,"OS Platform and Distribution :Ubuntu 14.04.5 LTS   && Android 8.0
TensorFlow installed from source
TensorFlow version :1..4.0
Python version : 2.7.6
Bazel version :0.4.5
GCC/Compiler version:4.8.4
CUDA/cuDNN version:8.0
GPU model and memory: GTX1080, 8G
Exact command to reproduce:

step 1. clone code from https://github.com/davidsandberg/facenet
step 2. in the file src/compare.py, add the follow code after line 90, after align.detect_face.create_mtcnn 
            output_node_names=['pnet/prob1','pnet/conv4-2/BiasAdd','pnet/conv1/BiasAdd','rnet/prob1','rnet/conv5-2/conv5-2','onet/prob1','onet/conv6-2/conv6-2','onet/conv6-3/conv6-3']
            output_graph_def = tf.graph_util.convert_variables_to_constants(sess, sess.graph.as_graph_def(), output_node_names) #sess.graph_def,
            with tf.gfile.FastGFile(""mtcnn.pb"", mode = 'wb') as f:
                f.write(output_graph_def.SerializeToString())

step 3. run the command:  python src/compare.py ./data/20170512-110547 ./data/images/Anthony_Hopkins_0001.jpg ./data/images/Anthony_Hopkins_0002.jpg
will create the model file mtcnn.pb

step 4 deploy the file mtcnn.pb to android app, validate with the file Anthony_Hopkins_0001.jpg, indeed it can fetch the results for the outputs such as 'pnet/prob1','pnet/conv4-2/BiasAdd',  but the values are difference with the results from the facenet project run on ubuntu,

the query is what is the cause to the difference? the  way to create the mtcnn.pb is wrong? still need to optimize it to adapt android device? or there is something wrong with Tensorflow for mobile device?

the follow is the log show the difference:
with the same input:00.28515625,-0.24609375,-0.59765625
but the output is difference

Android output
 	Line 5555: 01-26 11:34:20.793 I/lxr     (22967): img00.28515625,-0.24609375,-0.59765625
	Line 5795: 01-26 11:34:21.326 I/lxr     (22967): mapWidth 70 mapHeight 70
	Line 5796: 01-26 11:34:21.327 I/lxr     (22967): outValue:0.9998832,1.16751995E-4
	Line 5797: 01-26 11:34:21.327 I/lxr     (22967): outReg:-0.068167016,-0.2052449,0.06884944,0.1512082

Ubuntu output
img_y (1, 150, 150, 3)
img_y0 [ 0.28515625 -0.24609375 -0.59765625]
out0 shape (1, 70, 70, 4)
out1 shape (1, 70, 70, 2)
out0 [-0.07926445 -0.20101449  0.06468102  0.16017048]
out1 [  9.99792397e-01   2.07666759e-04]
",1,,5,2018-01-26T05:50:11Z,2018-02-15T23:21:27Z,NONE,"OS Platform and Distribution :Ubuntu 14.04.5 LTS   && Android 8.0
TensorFlow installed from source
TensorFlow version :1..4.0
Python version : 2.7.6
Bazel version :0.4.5
GCC/Compiler version:4.8.4
CUDA/cuDNN version:8.0
GPU model and memory: GTX1080, 8G
Exact command to reproduce:

step 1. clone code from https://github.com/davidsandberg/facenet
step 2. in the file src/compare.py, add the follow code after line 90, after align.detect_face.create_mtcnn 
            output_node_names=['pnet/prob1','pnet/conv4-2/BiasAdd','pnet/conv1/BiasAdd','rnet/prob1','rnet/conv5-2/conv5-2','onet/prob1','onet/conv6-2/conv6-2','onet/conv6-3/conv6-3']
            output_graph_def = tf.graph_util.convert_variables_to_constants(sess, sess.graph.as_graph_def(), output_node_names) #sess.graph_def,
            with tf.gfile.FastGFile(""mtcnn.pb"", mode = 'wb') as f:
                f.write(output_graph_def.SerializeToString())

step 3. run the command:  python src/compare.py ./data/20170512-110547 ./data/images/Anthony_Hopkins_0001.jpg ./data/images/Anthony_Hopkins_0002.jpg
will create the model file mtcnn.pb

step 4 deploy the file mtcnn.pb to android app, validate with the file Anthony_Hopkins_0001.jpg, indeed it can fetch the results for the outputs such as 'pnet/prob1','pnet/conv4-2/BiasAdd',  but the values are difference with the results from the facenet project run on ubuntu,

the query is what is the cause to the difference? the  way to create the mtcnn.pb is wrong? still need to optimize it to adapt android device? or there is something wrong with Tensorflow for mobile device?

the follow is the log show the difference:
with the same input:00.28515625,-0.24609375,-0.59765625
but the output is difference

Android output
 	Line 5555: 01-26 11:34:20.793 I/lxr     (22967): img00.28515625,-0.24609375,-0.59765625
	Line 5795: 01-26 11:34:21.326 I/lxr     (22967): mapWidth 70 mapHeight 70
	Line 5796: 01-26 11:34:21.327 I/lxr     (22967): outValue:0.9998832,1.16751995E-4
	Line 5797: 01-26 11:34:21.327 I/lxr     (22967): outReg:-0.068167016,-0.2052449,0.06884944,0.1512082

Ubuntu output
img_y (1, 150, 150, 3)
img_y0 [ 0.28515625 -0.24609375 -0.59765625]
out0 shape (1, 70, 70, 4)
out1 shape (1, 70, 70, 2)
out0 [-0.07926445 -0.20101449  0.06468102  0.16017048]
out1 [  9.99792397e-01   2.07666759e-04]
",2018-01-30T03:49:38Z,19,1,3,4.785999667502674
195,16438,xrange() was removed in Python 3,"awaiting testing (then merge),cla: yes",Each of these files contains at least one call to the Python 2-only builtin function __xrange()__ which was removed in Python 3 in favor of __range()__.  To each of these files we add the line [__from six.moves import xrange__](https://pythonhosted.org/six/#module-six.moves) for compatibility with both Python 2 and Python 3.,1,,1,2018-01-26T03:43:34Z,2018-01-26T18:10:56Z,CONTRIBUTOR,Each of these files contains at least one call to the Python 2-only builtin function __xrange()__ which was removed in Python 3 in favor of __range()__.  To each of these files we add the line [__from six.moves import xrange__](https://pythonhosted.org/six/#module-six.moves) for compatibility with both Python 2 and Python 3.,2018-01-26T06:41:59Z,0,2,0,3.7859996675026744
196,16436,replace deprecated keep_dims with keepdims in keras.backend,"awaiting review,cla: yes",This may not be important but it sometimes triggers warnings with conv nets built with keras.,1,,1,2018-01-26T03:37:11Z,2018-01-26T20:22:54Z,CONTRIBUTOR,This may not be important but it sometimes triggers warnings with conv nets built with keras.,2018-01-26T20:22:54Z,0,2,0,3.7859996675026744
197,16434, Imported lstm1d and lstm2d in ndlstm __init__.py.,"awaiting testing (then merge),cla: yes",Makes importing ndlstm modules easier.,1,,1,2018-01-26T02:58:34Z,2018-01-26T18:11:11Z,CONTRIBUTOR,Makes importing ndlstm modules easier.,2018-01-26T05:38:52Z,0,2,0,3.7859996675026744
198,16433,Fix an imperfect implementation of tf.losses.mean_pairwise_squared_error,"awaiting testing (then merge),cla: yes","Here is a fix for the issue [Imperfect implementation of tf.losses.mean_pairwise_squared_error (#15968)](https://github.com/tensorflow/tensorflow/issues/15968)

RELNOTES: Fixed wrong normalization in tf.losses.mean_pairwise_squared_error to conform to the math and documentation. Numerical results will be different.",1,,5,2018-01-26T02:42:02Z,2018-02-01T18:32:37Z,CONTRIBUTOR,"Here is a fix for the issue [Imperfect implementation of tf.losses.mean_pairwise_squared_error (#15968)](https://github.com/tensorflow/tensorflow/issues/15968)

RELNOTES: Fixed wrong normalization in tf.losses.mean_pairwise_squared_error to conform to the math and documentation. Numerical results will be different.",2018-02-01T01:27:39Z,5,2,1,5.785999667502674
199,16427,Fix build errors in contrib/mpi introduced by commit 6042b5d267f,"awaiting testing (then merge),cla: yes","The commit https://github.com/tensorflow/tensorflow/commit/6042b5d267f42d004087b44c29525951700579f9#diff-7c00d4a3caee74eedf5bb638bce23e5a 
* Introduced code to `tensorflow/contrib/mpi/mpi_rendezvous_mgr.h` to use the type `RecentRequestIds` without including the header `tensorflow/core/distributed_runtime/recent_request_ids.h`.
```
ERROR: /opt/tensorflow/tensorflow/contrib/mpi/BUILD:60:1: C++ compilation of rule '//tensorflow/contrib/mpi:mpi_rendezvous_mgr' failed (Exit 1)
In file included from tensorflow/contrib/mpi/mpi_rendezvous_mgr.cc:18:0:
./tensorflow/contrib/mpi/mpi_rendezvous_mgr.h:182:3: error: 'RecentRequestIds' does not name a type
   RecentRequestIds recv_tensor_recent_request_ids_;
   ^
```
* Probably a typo in `tensorflow/contrib/mpi/mpi_rendezvous_mgr.cc : MPIRendezvousMgr::AddRequest()`. The variable `req` was probably meant to be `request` as per the commit message.
```
ERROR: /opt/tensorflow/tensorflow/contrib/mpi/BUILD:60:1: C++ compilation of rule '//tensorflow/contrib/mpi:mpi_rendezvous_mgr' failed (Exit 1)
In file included from ./tensorflow/core/framework/variant.h:29:0,
                 from ./tensorflow/core/framework/allocator.h:26,
                 from ./tensorflow/core/framework/tensor.h:20,
                 from ./tensorflow/core/framework/device_base.h:23,
                 from ./tensorflow/core/framework/rendezvous.h:22,
                 from ./tensorflow/core/distributed_runtime/rendezvous_mgr_interface.h:22,
                 from ./tensorflow/core/distributed_runtime/base_rendezvous_mgr.h:22,
                 from ./tensorflow/contrib/mpi/mpi_rendezvous_mgr.h:35,
                 from tensorflow/contrib/mpi/mpi_rendezvous_mgr.cc:18:
tensorflow/contrib/mpi/mpi_rendezvous_mgr.cc: In member function 'void tensorflow::MPIRendezvousMgr::AddRequest(tensorflow::RecvTensorRequest, int)':
tensorflow/contrib/mpi/mpi_rendezvous_mgr.cc:155:7: error: 'req' was not declared in this scope
       req.request_id(), ""RecvTensor (MPIRendezvousMgr)"", req));
       ^
```

I compiled with following commands:
```
echo ""deb [arch=amd64] http://storage.googleapis.com/bazel-apt stable jdk1.8"" > /etc/apt/sources.list.d/bazel.list
curl https://bazel.build/bazel-release.pub.gpg | apt-key add -
git clone https://github.com/tensorflow/tensorflow .
export PYTHON_BIN_PATH=/path/to/python ## python 2.7.14
export USE_DEFAULT_PYTHON_LIB_PATH=1
export TF_NEED_JEMALLOC=1
export TF_NEED_GCP=0
export TF_NEED_HDFS=1
export TF_ENABLE_XLA=1
export TF_NEED_OPENCL=0
export TF_NEED_S3=0
export TF_NEED_GDR=0
export TF_NEED_VERBS=0
export TF_NEED_OPENCL_SYCL=0
export TF_NEED_CUDA=1
export TF_CUDA_VERSION=8.0
export CUDA_TOOLKIT_PATH=/path/to/cuda
export TF_CUDNN_VERSION=7
export CUDNN_INSTALL_PATH=/path/to/cudnn
export TF_CUDA_COMPUTE_CAPABILITIES=""3.5,5.2,6.0,6.1""
export TF_CUDA_CLANG=0
export GCC_HOST_COMPILER_PATH=/path/to/gcc
export TF_NEED_MPI=1
export MPI_HOME=/path/to/openmpi
export CC_OPT_FLAGS=""-march=native""
export TF_SET_ANDROID_WORKSPACE=0
./configure
bazel build --config=mkl --config=opt --config=cuda \
          //tensorflow/tools/pip_package:build_pip_package && \
bazel-bin/tensorflow/tools/pip_package/build_pip_package ./tensorflow_pkg
pip install -v ./tensorflow_pkg/tensorflow-*.whl
```",1,,6,2018-01-26T01:43:01Z,2018-01-26T19:28:34Z,CONTRIBUTOR,"The commit https://github.com/tensorflow/tensorflow/commit/6042b5d267f42d004087b44c29525951700579f9#diff-7c00d4a3caee74eedf5bb638bce23e5a 
* Introduced code to `tensorflow/contrib/mpi/mpi_rendezvous_mgr.h` to use the type `RecentRequestIds` without including the header `tensorflow/core/distributed_runtime/recent_request_ids.h`.
```
ERROR: /opt/tensorflow/tensorflow/contrib/mpi/BUILD:60:1: C++ compilation of rule '//tensorflow/contrib/mpi:mpi_rendezvous_mgr' failed (Exit 1)
In file included from tensorflow/contrib/mpi/mpi_rendezvous_mgr.cc:18:0:
./tensorflow/contrib/mpi/mpi_rendezvous_mgr.h:182:3: error: 'RecentRequestIds' does not name a type
   RecentRequestIds recv_tensor_recent_request_ids_;
   ^
```
* Probably a typo in `tensorflow/contrib/mpi/mpi_rendezvous_mgr.cc : MPIRendezvousMgr::AddRequest()`. The variable `req` was probably meant to be `request` as per the commit message.
```
ERROR: /opt/tensorflow/tensorflow/contrib/mpi/BUILD:60:1: C++ compilation of rule '//tensorflow/contrib/mpi:mpi_rendezvous_mgr' failed (Exit 1)
In file included from ./tensorflow/core/framework/variant.h:29:0,
                 from ./tensorflow/core/framework/allocator.h:26,
                 from ./tensorflow/core/framework/tensor.h:20,
                 from ./tensorflow/core/framework/device_base.h:23,
                 from ./tensorflow/core/framework/rendezvous.h:22,
                 from ./tensorflow/core/distributed_runtime/rendezvous_mgr_interface.h:22,
                 from ./tensorflow/core/distributed_runtime/base_rendezvous_mgr.h:22,
                 from ./tensorflow/contrib/mpi/mpi_rendezvous_mgr.h:35,
                 from tensorflow/contrib/mpi/mpi_rendezvous_mgr.cc:18:
tensorflow/contrib/mpi/mpi_rendezvous_mgr.cc: In member function 'void tensorflow::MPIRendezvousMgr::AddRequest(tensorflow::RecvTensorRequest, int)':
tensorflow/contrib/mpi/mpi_rendezvous_mgr.cc:155:7: error: 'req' was not declared in this scope
       req.request_id(), ""RecvTensor (MPIRendezvousMgr)"", req));
       ^
```

I compiled with following commands:
```
echo ""deb [arch=amd64] http://storage.googleapis.com/bazel-apt stable jdk1.8"" > /etc/apt/sources.list.d/bazel.list
curl https://bazel.build/bazel-release.pub.gpg | apt-key add -
git clone https://github.com/tensorflow/tensorflow .
export PYTHON_BIN_PATH=/path/to/python ## python 2.7.14
export USE_DEFAULT_PYTHON_LIB_PATH=1
export TF_NEED_JEMALLOC=1
export TF_NEED_GCP=0
export TF_NEED_HDFS=1
export TF_ENABLE_XLA=1
export TF_NEED_OPENCL=0
export TF_NEED_S3=0
export TF_NEED_GDR=0
export TF_NEED_VERBS=0
export TF_NEED_OPENCL_SYCL=0
export TF_NEED_CUDA=1
export TF_CUDA_VERSION=8.0
export CUDA_TOOLKIT_PATH=/path/to/cuda
export TF_CUDNN_VERSION=7
export CUDNN_INSTALL_PATH=/path/to/cudnn
export TF_CUDA_COMPUTE_CAPABILITIES=""3.5,5.2,6.0,6.1""
export TF_CUDA_CLANG=0
export GCC_HOST_COMPILER_PATH=/path/to/gcc
export TF_NEED_MPI=1
export MPI_HOME=/path/to/openmpi
export CC_OPT_FLAGS=""-march=native""
export TF_SET_ANDROID_WORKSPACE=0
./configure
bazel build --config=mkl --config=opt --config=cuda \
          //tensorflow/tools/pip_package:build_pip_package && \
bazel-bin/tensorflow/tools/pip_package/build_pip_package ./tensorflow_pkg
pip install -v ./tensorflow_pkg/tensorflow-*.whl
```",2018-01-26T02:07:18Z,0,2,0,6.285999667502674
200,16424,Dependency on old version of bleach (1.5),type:build/install,"Bleach 1.5 came out Nov 4th 2016 and this is old enough to cause dependency issues for projects that stayed up to date with Bleach.

In particular, this causes issues for Jupyter users.",1,,11,2018-01-25T23:20:59Z,2018-02-02T22:16:28Z,NONE,"Bleach 1.5 came out Nov 4th 2016 and this is old enough to cause dependency issues for projects that stayed up to date with Bleach.

In particular, this causes issues for Jupyter users.",2018-01-27T06:01:01Z,7,1,1,7.783578492051333
201,16423,Windows 10 Cmake GPU nvcc.exe error,type:build/install,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Windows 10
- **TensorFlow installed from (source or binary)**:
source
- **TensorFlow version (use command below)**:
r1.5
- **Python version**: 
3.6
- **GCC/Compiler version (if compiling from source)**:
Visual Studio 2017
- **CUDA/cuDNN version**:
9.1
- **GPU model and memory**:
1080Ti
- **Exact command to reproduce**:

**Cmake Command:**
```
cmake -G ""Visual Studio 15 2017 Win64"" -T host=x64 -DCMAKE_BUILD_TYPE=""
Release"" -DSWIG_EXECUTABLE='C:\ProgramData\Chocolatey\bin\swig.exe' -Dtensorflow_ENABLE_GPU=ON -Dtensorflow_CUDA_VERSION
=9.1 -Dtensorflow_CUDNN_VERSION=7 -Dtensorflow_WIN_CPU_SIMD_OPTIONS=""/arch:AVX2"" -DCUDA_CUDART_LIBRARY=D:\NVIDIA\CUDA\v9
.1 -DCUDNN_HOME='D:\NVIDIA\CUDA\v9.1' ..
```

**Build command**
```
""C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\MSBuild\15.0\Bin\amd64\MSBuild.exe"" /m:4 /p:Configuration=Release .\tf_core_gpu_kernels.vcxproj
```
### Describe the problem
Cmake creates a bad command to send to nvcc.exe

In tf_core_gpu_kernels_generated_adjust_contrast_op_gpu.cu.cc.obj.Release.cmake:202L
there is an error with the resulting command.

There is issues with spacing, "";"" in between arguments and others.

**Command Ran**
`C:/NVIDIA/CUDA/v9.1/bin/nvcc.exe -M -D__CUDACC__ D:/tensorflow/tensorflow/core/kernels/adjust_contrast_op_gpu.cu.cc -o D:/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/tf_core_gpu_kernels.dir/__/__/core/kernels/tf_core_gpu_kernels_generated_adjust_contrast_op_gpu.cu.cc.obj.NVCC-depend -ccbin;C:/Program Files (x86)/Microsoft Visual Studio/2017/Community/VC/bin -m64;-DSQLITE_OMIT_LOAD_EXTENSION;-DEIGEN_AVOID_STL_ARRAY;-DNOMINMAX;-D_WIN32_WINNT=0x0A00;-DLANG_CXX11;-DCOMPILER_MSVC;-DWIN32;-DOS_WIN;-D_MBCS;-DWIN64;-DWIN32_LEAN_AND_MEAN;-DNOGDI;-DPLATFORM_WINDOWS;-DTENSORFLOW_USE_EIGEN_THREADPOOL;-DEIGEN_HAS_C99_MATH;-DTF_COMPILE_LIBRARY;-DGRPC_ARES=0;-DTF_USE_SNAPPY;-DGOOGLE_CUDA=1;-DTF_EXTRA_CUDA_CAPABILITIES=6.1 -Xcompiler;,""/DWIN32"",""/D_WINDOWS"",""/W3"",""/GR"",""/EHsc"",""/MP"",""/arch:AVX2"",""/MD"",""/O2"",""/Ob2"",""/DNDEBUG"",""/D_ITERATOR_DEBUG_LEVEL=0""  -gencode;arch=compute_61,code=""sm_61,compute_61"";--include-path;D:/tensorflow/tensorflow/contrib/cmake/build/Release;--expt-relaxed-constexpr;-ftz=true;  -DNVCC -IC:/NVIDIA/CUDA/v9.1/include ;-ID:/tensorflow ;-ID:/tensorflow/tensorflow/contrib/cmake/build ;-ID:/tensorflow/tensorflow/contrib/cmake/build/external/zlib_archive ;-ID:/tensorflow/tensorflow/contrib/cmake/build/external/gif_archive/giflib-5.1.4 ;-ID:/tensorflow/tensorflow/contrib/cmake/build/external/png_archive ;-ID:/tensorflow/tensorflow/contrib/cmake/build/external/jpeg_archive ;-ID:/tensorflow/tensorflow/contrib/cmake/build/external/lmdb ;-ID:/tensorflow/tensorflow/contrib/cmake/build/external/eigen_archive ;-ID:/tensorflow/third_party/eigen3 ;-ID:/tensorflow/tensorflow/contrib/cmake/build/gemmlowp/src/gemmlowp ;-ID:/tensorflow/tensorflow/contrib/cmake/build/jsoncpp/src/jsoncpp ;-ID:/tensorflow/tensorflow/contrib/cmake/build/external/farmhash_archive ;-ID:/tensorflow/tensorflow/contrib/cmake/build/external/farmhash_archive/util ;-ID:/tensorflow/tensorflow/contrib/cmake/build/external/highwayhash ;-ID:/tensorflow/tensorflow/contrib/cmake/build/cub/src/cub ;-ID:/tensorflow/tensorflow/contrib/cmake/build/external/nsync/public ;-ID:/tensorflow/tensorflow/contrib/cmake/build/protobuf/src/protobuf/src ;-ID:/tensorflow/tensorflow/contrib/cmake/build/re2/install/include ;-ID:/tensorflow/tensorflow/contrib/cmake/build/external/sqlite ;-ID:/tensorflow/tensorflow/contrib/cmake/build/grpc/src/grpc/include ;-ID:/tensorflow/tensorflow/contrib/cmake/build/snappy/src/snappy ;-IC:/NVIDIA/CUDA/v9.1 ;-IC:/NVIDIA/CUDA/v9.1/extras/CUPTI/include ;-ID:/tensorflow/third_party/gpus`

Multable invalid cmake varables

```
${CCBIN} = -ccbin;C:/Program Files (x86)/Microsoft Visual Studio/2017/Community/VC/bin 
${nvcc_flags} = -m64;-DSQLITE_OMIT_LOAD_EXTENSION;-DEIGEN_AVOID_STL_ARRAY;-DNOMINMAX;-D_WIN32_WINNT=0x0A00;-DLANG_CXX11;-DCOMPILER_MSVC;-DWIN32;-DOS_WIN;-D_MBCS;-DWIN64;-DWIN32_LEAN_AND_MEAN;-DNOGDI;-DPLATFORM_WINDOWS;-DTENSORFLOW_USE_EIGEN_THREADPOOL;-DEIGEN_HAS_C99_MATH;-DTF_COMPILE_LIBRARY;-DGRPC_ARES=0;-DTF_USE_SNAPPY;-DGOOGLE_CUDA=1;-DTF_EXTRA_CUDA_CAPABILITIES=6.1
${nvcc_host_compiler_flags} = -Xcompiler;,""/DWIN32"",""/D_WINDOWS"",""/W3"",""/GR"",""/EHsc"",""/MP"",""/arch:AVX2"",""/MD"",""/O2"",""/Ob2"",""/DNDEBUG"",""/D_ITERATOR_DEBUG_LEVEL=0""  
${depends_CUDA_NVCC_FLAGS} = -gencode;arch=compute_61,code=""sm_61,compute_61"";--include-path;D:/tensorflow/tensorflow/contrib/cmake/build/Release;--expt-relaxed-constexpr;-ftz=true
${CUDA_NVCC_INCLUDE_ARGS} = -IC:/NVIDIA/CUDA/v9.1/include ;-ID:/tensorflow ;-ID:/tensorflow/tensorflow/contrib/cmake/build ;-ID:/tensorflow/tensorflow/contrib/cmake/build/external/zlib_archive ;-ID:/tensorflow/tensorflow/contrib/cmake/build/external/gif_archive/giflib-5.1.4 ;-ID:/tensorflow/tensorflow/contrib/cmake/build/external/png_archive ;-ID:/tensorflow/tensorflow/contrib/cmake/build/external/jpeg_archive ;-ID:/tensorflow/tensorflow/contrib/cmake/build/external/lmdb ;-ID:/tensorflow/tensorflow/contrib/cmake/build/external/eigen_archive ;-ID:/tensorflow/third_party/eigen3 ;-ID:/tensorflow/tensorflow/contrib/cmake/build/gemmlowp/src/gemmlowp ;-ID:/tensorflow/tensorflow/contrib/cmake/build/jsoncpp/src/jsoncpp ;-ID:/tensorflow/tensorflow/contrib/cmake/build/external/farmhash_archive ;-ID:/tensorflow/tensorflow/contrib/cmake/build/external/farmhash_archive/util ;-ID:/tensorflow/tensorflow/contrib/cmake/build/external/highwayhash ;-ID:/tensorflow/tensorflow/contrib/cmake/build/cub/src/cub ;-ID:/tensorflow/tensorflow/contrib/cmake/build/external/nsync/public ;-ID:/tensorflow/tensorflow/contrib/cmake/build/protobuf/src/protobuf/src ;-ID:/tensorflow/tensorflow/contrib/cmake/build/re2/install/include ;-ID:/tensorflow/tensorflow/contrib/cmake/build/external/sqlite ;-ID:/tensorflow/tensorflow/contrib/cmake/build/grpc/src/grpc/include ;-ID:/tensorflow/tensorflow/contrib/cmake/build/snappy/src/snappy ;-IC:/NVIDIA/CUDA/v9.1 ;-IC:/NVIDIA/CUDA/v9.1/extras/CUPTI/include ;-ID:/tensorflow/third_party/gpus
```


Should be
```
${CCBIN} = -ccbin ""C:/Program Files (x86)/Microsoft Visual Studio/2017/Community/VC/bin""
${nvcc_flags} = -m64 -DSQLITE_OMIT_LOAD_EXTENSION -DEIGEN_AVOID_STL_ARRAY -DNOMINMAX -D_WIN32_WINNT=0x0A00 -DLANG_CXX11 -DCOMPILER_MSVC -DWIN32 -DOS_WIN -D_MBCS -DWIN64 -DWIN32_LEAN_AND_MEAN -DNOGDI -DPLATFORM_WINDOWS -DTENSORFLOW_USE_EIGEN_THREADPOOL -DEIGEN_HAS_C99_MATH -DTF_COMPILE_LIBRARY -DGRPC_ARES=0 -DTF_USE_SNAPPY -DGOOGLE_CUDA=1 -DTF_EXTRA_CUDA_CAPABILITIES=6.1 
${nvcc_host_compiler_flags} = -Xcompiler ""/DWIN32,/D_WINDOWS,/W3,/GR,/EHsc,/MP,/arch:AVX2,/MD,/O2,/Ob2,/DNDEBUG,/D_ITERATOR_DEBUG_LEVEL=0"" 
${depends_CUDA_NVCC_FLAGS} = -gencode arch=compute_61,code=\""sm_61,compute_61\"" --include-path D:/tensorflow/tensorflow/contrib/cmake/build/Release --expt-relaxed-constexpr -ftz=true
${CUDA_NVCC_INCLUDE_ARGS} = -IC:/NVIDIA/CUDA/v9.1/include -ID:/tensorflow -ID:/tensorflow/tensorflow/contrib/cmake/build -ID:/tensorflow/tensorflow/contrib/cmake/build/external/zlib_archive -ID:/tensorflow/tensorflow/contrib/cmake/build/external/gif_archive/giflib-5.1.4 -ID:/tensorflow/tensorflow/contrib/cmake/build/external/png_archive -ID:/tensorflow/tensorflow/contrib/cmake/build/external/jpeg_archive -ID:/tensorflow/tensorflow/contrib/cmake/build/external/lmdb -ID:/tensorflow/tensorflow/contrib/cmake/build/external/eigen_archive -ID:/tensorflow/third_party/eigen3 -ID:/tensorflow/tensorflow/contrib/cmake/build/gemmlowp/src/gemmlowp -ID:/tensorflow/tensorflow/contrib/cmake/build/jsoncpp/src/jsoncpp -ID:/tensorflow/tensorflow/contrib/cmake/build/external/farmhash_archive -ID:/tensorflow/tensorflow/contrib/cmake/build/external/farmhash_archive/util -ID:/tensorflow/tensorflow/contrib/cmake/build/external/highwayhash -ID:/tensorflow/tensorflow/contrib/cmake/build/cub/src/cub -ID:/tensorflow/tensorflow/contrib/cmake/build/external/nsync/public -ID:/tensorflow/tensorflow/contrib/cmake/build/protobuf/src/protobuf/src -ID:/tensorflow/tensorflow/contrib/cmake/build/re2/install/include -ID:/tensorflow/tensorflow/contrib/cmake/build/external/sqlite -ID:/tensorflow/tensorflow/contrib/cmake/build/grpc/src/grpc/include -ID:/tensorflow/tensorflow/contrib/cmake/build/snappy/src/snappy -IC:/NVIDIA/CUDA/v9.1 -IC:/NVIDIA/CUDA/v9.1/extras/CUPTI/include -ID:/tensorflow/third_party/gpus

```",0,,1,2018-01-25T21:55:09Z,2018-01-26T01:19:15Z,NONE,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Windows 10
- **TensorFlow installed from (source or binary)**:
source
- **TensorFlow version (use command below)**:
r1.5
- **Python version**: 
3.6
- **GCC/Compiler version (if compiling from source)**:
Visual Studio 2017
- **CUDA/cuDNN version**:
9.1
- **GPU model and memory**:
1080Ti
- **Exact command to reproduce**:

**Cmake Command:**
```
cmake -G ""Visual Studio 15 2017 Win64"" -T host=x64 -DCMAKE_BUILD_TYPE=""
Release"" -DSWIG_EXECUTABLE='C:\ProgramData\Chocolatey\bin\swig.exe' -Dtensorflow_ENABLE_GPU=ON -Dtensorflow_CUDA_VERSION
=9.1 -Dtensorflow_CUDNN_VERSION=7 -Dtensorflow_WIN_CPU_SIMD_OPTIONS=""/arch:AVX2"" -DCUDA_CUDART_LIBRARY=D:\NVIDIA\CUDA\v9
.1 -DCUDNN_HOME='D:\NVIDIA\CUDA\v9.1' ..
```

**Build command**
```
""C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\MSBuild\15.0\Bin\amd64\MSBuild.exe"" /m:4 /p:Configuration=Release .\tf_core_gpu_kernels.vcxproj
```
### Describe the problem
Cmake creates a bad command to send to nvcc.exe

In tf_core_gpu_kernels_generated_adjust_contrast_op_gpu.cu.cc.obj.Release.cmake:202L
there is an error with the resulting command.

There is issues with spacing, "";"" in between arguments and others.

**Command Ran**
`C:/NVIDIA/CUDA/v9.1/bin/nvcc.exe -M -D__CUDACC__ D:/tensorflow/tensorflow/core/kernels/adjust_contrast_op_gpu.cu.cc -o D:/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/tf_core_gpu_kernels.dir/__/__/core/kernels/tf_core_gpu_kernels_generated_adjust_contrast_op_gpu.cu.cc.obj.NVCC-depend -ccbin;C:/Program Files (x86)/Microsoft Visual Studio/2017/Community/VC/bin -m64;-DSQLITE_OMIT_LOAD_EXTENSION;-DEIGEN_AVOID_STL_ARRAY;-DNOMINMAX;-D_WIN32_WINNT=0x0A00;-DLANG_CXX11;-DCOMPILER_MSVC;-DWIN32;-DOS_WIN;-D_MBCS;-DWIN64;-DWIN32_LEAN_AND_MEAN;-DNOGDI;-DPLATFORM_WINDOWS;-DTENSORFLOW_USE_EIGEN_THREADPOOL;-DEIGEN_HAS_C99_MATH;-DTF_COMPILE_LIBRARY;-DGRPC_ARES=0;-DTF_USE_SNAPPY;-DGOOGLE_CUDA=1;-DTF_EXTRA_CUDA_CAPABILITIES=6.1 -Xcompiler;,""/DWIN32"",""/D_WINDOWS"",""/W3"",""/GR"",""/EHsc"",""/MP"",""/arch:AVX2"",""/MD"",""/O2"",""/Ob2"",""/DNDEBUG"",""/D_ITERATOR_DEBUG_LEVEL=0""  -gencode;arch=compute_61,code=""sm_61,compute_61"";--include-path;D:/tensorflow/tensorflow/contrib/cmake/build/Release;--expt-relaxed-constexpr;-ftz=true;  -DNVCC -IC:/NVIDIA/CUDA/v9.1/include ;-ID:/tensorflow ;-ID:/tensorflow/tensorflow/contrib/cmake/build ;-ID:/tensorflow/tensorflow/contrib/cmake/build/external/zlib_archive ;-ID:/tensorflow/tensorflow/contrib/cmake/build/external/gif_archive/giflib-5.1.4 ;-ID:/tensorflow/tensorflow/contrib/cmake/build/external/png_archive ;-ID:/tensorflow/tensorflow/contrib/cmake/build/external/jpeg_archive ;-ID:/tensorflow/tensorflow/contrib/cmake/build/external/lmdb ;-ID:/tensorflow/tensorflow/contrib/cmake/build/external/eigen_archive ;-ID:/tensorflow/third_party/eigen3 ;-ID:/tensorflow/tensorflow/contrib/cmake/build/gemmlowp/src/gemmlowp ;-ID:/tensorflow/tensorflow/contrib/cmake/build/jsoncpp/src/jsoncpp ;-ID:/tensorflow/tensorflow/contrib/cmake/build/external/farmhash_archive ;-ID:/tensorflow/tensorflow/contrib/cmake/build/external/farmhash_archive/util ;-ID:/tensorflow/tensorflow/contrib/cmake/build/external/highwayhash ;-ID:/tensorflow/tensorflow/contrib/cmake/build/cub/src/cub ;-ID:/tensorflow/tensorflow/contrib/cmake/build/external/nsync/public ;-ID:/tensorflow/tensorflow/contrib/cmake/build/protobuf/src/protobuf/src ;-ID:/tensorflow/tensorflow/contrib/cmake/build/re2/install/include ;-ID:/tensorflow/tensorflow/contrib/cmake/build/external/sqlite ;-ID:/tensorflow/tensorflow/contrib/cmake/build/grpc/src/grpc/include ;-ID:/tensorflow/tensorflow/contrib/cmake/build/snappy/src/snappy ;-IC:/NVIDIA/CUDA/v9.1 ;-IC:/NVIDIA/CUDA/v9.1/extras/CUPTI/include ;-ID:/tensorflow/third_party/gpus`

Multable invalid cmake varables

```
${CCBIN} = -ccbin;C:/Program Files (x86)/Microsoft Visual Studio/2017/Community/VC/bin 
${nvcc_flags} = -m64;-DSQLITE_OMIT_LOAD_EXTENSION;-DEIGEN_AVOID_STL_ARRAY;-DNOMINMAX;-D_WIN32_WINNT=0x0A00;-DLANG_CXX11;-DCOMPILER_MSVC;-DWIN32;-DOS_WIN;-D_MBCS;-DWIN64;-DWIN32_LEAN_AND_MEAN;-DNOGDI;-DPLATFORM_WINDOWS;-DTENSORFLOW_USE_EIGEN_THREADPOOL;-DEIGEN_HAS_C99_MATH;-DTF_COMPILE_LIBRARY;-DGRPC_ARES=0;-DTF_USE_SNAPPY;-DGOOGLE_CUDA=1;-DTF_EXTRA_CUDA_CAPABILITIES=6.1
${nvcc_host_compiler_flags} = -Xcompiler;,""/DWIN32"",""/D_WINDOWS"",""/W3"",""/GR"",""/EHsc"",""/MP"",""/arch:AVX2"",""/MD"",""/O2"",""/Ob2"",""/DNDEBUG"",""/D_ITERATOR_DEBUG_LEVEL=0""  
${depends_CUDA_NVCC_FLAGS} = -gencode;arch=compute_61,code=""sm_61,compute_61"";--include-path;D:/tensorflow/tensorflow/contrib/cmake/build/Release;--expt-relaxed-constexpr;-ftz=true
${CUDA_NVCC_INCLUDE_ARGS} = -IC:/NVIDIA/CUDA/v9.1/include ;-ID:/tensorflow ;-ID:/tensorflow/tensorflow/contrib/cmake/build ;-ID:/tensorflow/tensorflow/contrib/cmake/build/external/zlib_archive ;-ID:/tensorflow/tensorflow/contrib/cmake/build/external/gif_archive/giflib-5.1.4 ;-ID:/tensorflow/tensorflow/contrib/cmake/build/external/png_archive ;-ID:/tensorflow/tensorflow/contrib/cmake/build/external/jpeg_archive ;-ID:/tensorflow/tensorflow/contrib/cmake/build/external/lmdb ;-ID:/tensorflow/tensorflow/contrib/cmake/build/external/eigen_archive ;-ID:/tensorflow/third_party/eigen3 ;-ID:/tensorflow/tensorflow/contrib/cmake/build/gemmlowp/src/gemmlowp ;-ID:/tensorflow/tensorflow/contrib/cmake/build/jsoncpp/src/jsoncpp ;-ID:/tensorflow/tensorflow/contrib/cmake/build/external/farmhash_archive ;-ID:/tensorflow/tensorflow/contrib/cmake/build/external/farmhash_archive/util ;-ID:/tensorflow/tensorflow/contrib/cmake/build/external/highwayhash ;-ID:/tensorflow/tensorflow/contrib/cmake/build/cub/src/cub ;-ID:/tensorflow/tensorflow/contrib/cmake/build/external/nsync/public ;-ID:/tensorflow/tensorflow/contrib/cmake/build/protobuf/src/protobuf/src ;-ID:/tensorflow/tensorflow/contrib/cmake/build/re2/install/include ;-ID:/tensorflow/tensorflow/contrib/cmake/build/external/sqlite ;-ID:/tensorflow/tensorflow/contrib/cmake/build/grpc/src/grpc/include ;-ID:/tensorflow/tensorflow/contrib/cmake/build/snappy/src/snappy ;-IC:/NVIDIA/CUDA/v9.1 ;-IC:/NVIDIA/CUDA/v9.1/extras/CUPTI/include ;-ID:/tensorflow/third_party/gpus
```


Should be
```
${CCBIN} = -ccbin ""C:/Program Files (x86)/Microsoft Visual Studio/2017/Community/VC/bin""
${nvcc_flags} = -m64 -DSQLITE_OMIT_LOAD_EXTENSION -DEIGEN_AVOID_STL_ARRAY -DNOMINMAX -D_WIN32_WINNT=0x0A00 -DLANG_CXX11 -DCOMPILER_MSVC -DWIN32 -DOS_WIN -D_MBCS -DWIN64 -DWIN32_LEAN_AND_MEAN -DNOGDI -DPLATFORM_WINDOWS -DTENSORFLOW_USE_EIGEN_THREADPOOL -DEIGEN_HAS_C99_MATH -DTF_COMPILE_LIBRARY -DGRPC_ARES=0 -DTF_USE_SNAPPY -DGOOGLE_CUDA=1 -DTF_EXTRA_CUDA_CAPABILITIES=6.1 
${nvcc_host_compiler_flags} = -Xcompiler ""/DWIN32,/D_WINDOWS,/W3,/GR,/EHsc,/MP,/arch:AVX2,/MD,/O2,/Ob2,/DNDEBUG,/D_ITERATOR_DEBUG_LEVEL=0"" 
${depends_CUDA_NVCC_FLAGS} = -gencode arch=compute_61,code=\""sm_61,compute_61\"" --include-path D:/tensorflow/tensorflow/contrib/cmake/build/Release --expt-relaxed-constexpr -ftz=true
${CUDA_NVCC_INCLUDE_ARGS} = -IC:/NVIDIA/CUDA/v9.1/include -ID:/tensorflow -ID:/tensorflow/tensorflow/contrib/cmake/build -ID:/tensorflow/tensorflow/contrib/cmake/build/external/zlib_archive -ID:/tensorflow/tensorflow/contrib/cmake/build/external/gif_archive/giflib-5.1.4 -ID:/tensorflow/tensorflow/contrib/cmake/build/external/png_archive -ID:/tensorflow/tensorflow/contrib/cmake/build/external/jpeg_archive -ID:/tensorflow/tensorflow/contrib/cmake/build/external/lmdb -ID:/tensorflow/tensorflow/contrib/cmake/build/external/eigen_archive -ID:/tensorflow/third_party/eigen3 -ID:/tensorflow/tensorflow/contrib/cmake/build/gemmlowp/src/gemmlowp -ID:/tensorflow/tensorflow/contrib/cmake/build/jsoncpp/src/jsoncpp -ID:/tensorflow/tensorflow/contrib/cmake/build/external/farmhash_archive -ID:/tensorflow/tensorflow/contrib/cmake/build/external/farmhash_archive/util -ID:/tensorflow/tensorflow/contrib/cmake/build/external/highwayhash -ID:/tensorflow/tensorflow/contrib/cmake/build/cub/src/cub -ID:/tensorflow/tensorflow/contrib/cmake/build/external/nsync/public -ID:/tensorflow/tensorflow/contrib/cmake/build/protobuf/src/protobuf/src -ID:/tensorflow/tensorflow/contrib/cmake/build/re2/install/include -ID:/tensorflow/tensorflow/contrib/cmake/build/external/sqlite -ID:/tensorflow/tensorflow/contrib/cmake/build/grpc/src/grpc/include -ID:/tensorflow/tensorflow/contrib/cmake/build/snappy/src/snappy -IC:/NVIDIA/CUDA/v9.1 -IC:/NVIDIA/CUDA/v9.1/extras/CUPTI/include -ID:/tensorflow/third_party/gpus

```",2018-01-26T01:19:15Z,1,1,0,1.7835784920513333
202,16419,R1.4,cla: no,want to test ,0,,2,2018-01-25T20:09:27Z,2018-01-25T21:33:32Z,NONE,want to test ,2018-01-25T21:33:32Z,0,1,0,2.2835784920513333
203,16418,common global variable with constant.py,cla: no,Made a common `constant.py` for global commonly used variables.,1,,7,2018-01-25T19:56:38Z,2018-02-08T00:37:02Z,CONTRIBUTOR,Made a common `constant.py` for global commonly used variables.,2018-01-25T19:58:30Z,13,2,2,6.783578492051333
204,16417,Add missing library in Dockerfile,cla: yes,The local Dockerfile does not have all the dependencies for running the exercise notebooks in udacity assignments.,1,,5,2018-01-25T19:42:22Z,2018-01-25T23:31:26Z,CONTRIBUTOR,The local Dockerfile does not have all the dependencies for running the exercise notebooks in udacity assignments.,2018-01-25T19:44:00Z,0,2,0,5.783578492051333
205,16416,Making global constant.py file for ops,cla: no,Created a separate common `constants.py` which can be used globally.,0,,2,2018-01-25T19:39:27Z,2018-01-25T19:39:59Z,CONTRIBUTOR,Created a separate common `constants.py` which can be used globally.,2018-01-25T19:39:59Z,0,2,0,3.2835784920513333
206,16413,Separate constant file for global variables,cla: no,"Created a separate common `constants.py` which can be used globally under `ops.`

**P.S:** I created the same pull request [#16401](https://github.com/tensorflow/tensorflow/pull/16401) as after pushing my latest changes I was facing CLA issues.",0,,4,2018-01-25T18:55:02Z,2018-01-25T19:29:35Z,CONTRIBUTOR,"Created a separate common `constants.py` which can be used globally under `ops.`

**P.S:** I created the same pull request [#16401](https://github.com/tensorflow/tensorflow/pull/16401) as after pushing my latest changes I was facing CLA issues.",2018-01-25T19:04:36Z,0,2,0,4.283578492051333
207,16412,Documentation on build from source is unclear,type:support,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh


python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
('v1.4.1-7-gaa03bfc', '1.4.1')
built and installed from source with
git checkout r1.4
bazel build -c opt --copt=-march=""haswell"" --config=cuda --verbose_failures --incompatible_load_argument_is_label=false //tensorflow/tools/pip_package:build_pip_package >pip_package_build2.log 2>&1
note: incompatible path flag is required with R1.4 at this time per https://github.com/tensorflow/tensorflow/issues/15492
ubuntu 16.04
Cuda 9.1, cudnn 7.0.4
gcc --version
gcc (Ubuntu 5.4.0-6ubuntu1~16.04.5) 5.4.0 20160609
uname -r
4.4.0-104-generic
Bazel 0.9

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
It is unclear how to build and install the entire package purely from source
I will attempt to log what I have done so far
clone and build TF R1.4 for cuda
install wheel into local directory  (sudo pip install /tmp/tensorflow-pkg/tensorflow*.whl -t ~/mytf_r1.4_c9.1
export PYTHONPATH=~/mytf_r1.4_c9.1
move tensorflow directory

install common_voice files to ~/Common_voice

per native client build from source instructions: https://github.com/mozilla/DeepSpeech/blob/master/native_client/README.md
git clone tensorflow
cd tensorflow
git checkout r1.4
ln -s ../DeepSpeech/native_client ./
./configure
edit native_client/BUILD
comment out the following:
#    tfcompile_flags = select({
#        ""//tensorflow:rpi3"": str('--target_triple=""armv6-linux-gnueabihf"" --target_cpu=""cortex-a53"" --target_features=""+neon-fp-armv8""'),
#        ""//conditions:default"": str('')
#    }),
bazel build -c opt --copt=-O3 --incompatible_load_argument_is_label=false //tensorflow:libtensorflow_cc.so //tensorflow:libtensorflow_framework.so //native_client:deepspeech //native_client:deepspeech_utils //native_client:libctc_decoder_with_kenlm.so //native_client:generate_trie
at this point all the native client binaries are in
~/tensorflow/bazel-bin/native_client
levinth@zt-gpu-lin:~/DeepSpeech/native_client$ ls ~/tensorflow/bazel-bin/native_client/
generate_trie
generate_trie-2.params
generate_trie.runfiles
generate_trie.runfiles_manifest
libctc_decoder_with_kenlm.so
libctc_decoder_with_kenlm.so-2.params
libctc_decoder_with_kenlm.so.runfiles
libctc_decoder_with_kenlm.so.runfiles_manifest
libdeepspeech.a
libdeepspeech.a-2.params
libdeepspeech.pic.a
libdeepspeech.pic.a-2.params
libdeepspeech.so
libdeepspeech.so-2.params
libdeepspeech_utils.a
libdeepspeech_utils.a-2.params
libdeepspeech_utils.pic.a
libdeepspeech_utils.pic.a-2.params
libdeepspeech_utils.so
libdeepspeech_utils.so-2.params
_objs

cd ../Deepspeech/native_client
export TFDIR ~/tensorflow
make deepspeech


at this point however the native client shared objects are still in bazel-bin/native client and have not been installed. the invocation of Deepspeech.py fails as it cannot find the shared objects
python DeepSpeech.py --train_files ../Common_voice/cv-valid-train.csv,../Common_voice/cv-other-train.csv --dev_files ../Common_voice/cv-valid-dev.csv --test_files ../Common_voice/cv-valid-test.csv >deepspeech_1.log 2>&1
tensorflow.python.framework.errors_impl.NotFoundError: native_client/libctc_decoder_with_kenlm.so: cannot open shared object file: No such file or directory

the native_client/Makefile has sections for bindings and install..so try
sudo make install
and this still generates the error as install does not put
~/tensorflow/bazel-bin/native_client/libctc_decoder_with_kenlm.so
into /usr/local/lib 
though deepspeech.so and deepspeech_utils.so are installed there.

manually copy /tensorflow/bazel-bin/native_client/libctc_decoder_with_kenlm.so to ~/DeepSpeech/native_client and set permissions
at this point the invocation now starts running but complains about
------------------------------------------------------------------------
WARNING: libdeepspeech failed to load, resorting to deprecated code
         Refer to README.md for instructions on installing libdeepspeech
------------------------------------------------------------------------
even though /usr/local/lib is in the $LD_LIBRARY_PATH

invoking
python DeepSpeech.py --train_files ../Common_voice/cv-valid-train.csv,../Common_voice/cv-other-train.csv --dev_files ../Common_voice/cv-valid-dev.csv --test_files ../Common_voice/cv-valid-test.csv --display_step 1 --validation_step 10

------------------------------------------------------------------------
WARNING: libdeepspeech failed to load, resorting to deprecated code
         Refer to README.md for instructions on installing libdeepspeech
------------------------------------------------------------------------
I STARTING Optimization
Loading the LM will be faster if you build a binary file.
Reading data/lm/lm.binary
----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100
terminate called after throwing an instance of 'lm::FormatLoadException'
  what():  native_client/kenlm/lm/read_arpa.cc:65 in void lm::ReadARPACounts(util::FilePiece&, std::vector<long unsigned int>&) threw FormatLoadException.
first non-empty line was ""version https://git-lfs.github.com/spec/v1"" not \data\. Byte: 43

I clearly have not figured this out
:-)
 
",0,,3,2018-01-25T17:58:48Z,2018-01-25T19:45:03Z,NONE,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh


python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
('v1.4.1-7-gaa03bfc', '1.4.1')
built and installed from source with
git checkout r1.4
bazel build -c opt --copt=-march=""haswell"" --config=cuda --verbose_failures --incompatible_load_argument_is_label=false //tensorflow/tools/pip_package:build_pip_package >pip_package_build2.log 2>&1
note: incompatible path flag is required with R1.4 at this time per https://github.com/tensorflow/tensorflow/issues/15492
ubuntu 16.04
Cuda 9.1, cudnn 7.0.4
gcc --version
gcc (Ubuntu 5.4.0-6ubuntu1~16.04.5) 5.4.0 20160609
uname -r
4.4.0-104-generic
Bazel 0.9

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
It is unclear how to build and install the entire package purely from source
I will attempt to log what I have done so far
clone and build TF R1.4 for cuda
install wheel into local directory  (sudo pip install /tmp/tensorflow-pkg/tensorflow*.whl -t ~/mytf_r1.4_c9.1
export PYTHONPATH=~/mytf_r1.4_c9.1
move tensorflow directory

install common_voice files to ~/Common_voice

per native client build from source instructions: https://github.com/mozilla/DeepSpeech/blob/master/native_client/README.md
git clone tensorflow
cd tensorflow
git checkout r1.4
ln -s ../DeepSpeech/native_client ./
./configure
edit native_client/BUILD
comment out the following:
#    tfcompile_flags = select({
#        ""//tensorflow:rpi3"": str('--target_triple=""armv6-linux-gnueabihf"" --target_cpu=""cortex-a53"" --target_features=""+neon-fp-armv8""'),
#        ""//conditions:default"": str('')
#    }),
bazel build -c opt --copt=-O3 --incompatible_load_argument_is_label=false //tensorflow:libtensorflow_cc.so //tensorflow:libtensorflow_framework.so //native_client:deepspeech //native_client:deepspeech_utils //native_client:libctc_decoder_with_kenlm.so //native_client:generate_trie
at this point all the native client binaries are in
~/tensorflow/bazel-bin/native_client
levinth@zt-gpu-lin:~/DeepSpeech/native_client$ ls ~/tensorflow/bazel-bin/native_client/
generate_trie
generate_trie-2.params
generate_trie.runfiles
generate_trie.runfiles_manifest
libctc_decoder_with_kenlm.so
libctc_decoder_with_kenlm.so-2.params
libctc_decoder_with_kenlm.so.runfiles
libctc_decoder_with_kenlm.so.runfiles_manifest
libdeepspeech.a
libdeepspeech.a-2.params
libdeepspeech.pic.a
libdeepspeech.pic.a-2.params
libdeepspeech.so
libdeepspeech.so-2.params
libdeepspeech_utils.a
libdeepspeech_utils.a-2.params
libdeepspeech_utils.pic.a
libdeepspeech_utils.pic.a-2.params
libdeepspeech_utils.so
libdeepspeech_utils.so-2.params
_objs

cd ../Deepspeech/native_client
export TFDIR ~/tensorflow
make deepspeech


at this point however the native client shared objects are still in bazel-bin/native client and have not been installed. the invocation of Deepspeech.py fails as it cannot find the shared objects
python DeepSpeech.py --train_files ../Common_voice/cv-valid-train.csv,../Common_voice/cv-other-train.csv --dev_files ../Common_voice/cv-valid-dev.csv --test_files ../Common_voice/cv-valid-test.csv >deepspeech_1.log 2>&1
tensorflow.python.framework.errors_impl.NotFoundError: native_client/libctc_decoder_with_kenlm.so: cannot open shared object file: No such file or directory

the native_client/Makefile has sections for bindings and install..so try
sudo make install
and this still generates the error as install does not put
~/tensorflow/bazel-bin/native_client/libctc_decoder_with_kenlm.so
into /usr/local/lib 
though deepspeech.so and deepspeech_utils.so are installed there.

manually copy /tensorflow/bazel-bin/native_client/libctc_decoder_with_kenlm.so to ~/DeepSpeech/native_client and set permissions
at this point the invocation now starts running but complains about
------------------------------------------------------------------------
WARNING: libdeepspeech failed to load, resorting to deprecated code
         Refer to README.md for instructions on installing libdeepspeech
------------------------------------------------------------------------
even though /usr/local/lib is in the $LD_LIBRARY_PATH

invoking
python DeepSpeech.py --train_files ../Common_voice/cv-valid-train.csv,../Common_voice/cv-other-train.csv --dev_files ../Common_voice/cv-valid-dev.csv --test_files ../Common_voice/cv-valid-test.csv --display_step 1 --validation_step 10

------------------------------------------------------------------------
WARNING: libdeepspeech failed to load, resorting to deprecated code
         Refer to README.md for instructions on installing libdeepspeech
------------------------------------------------------------------------
I STARTING Optimization
Loading the LM will be faster if you build a binary file.
Reading data/lm/lm.binary
----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100
terminate called after throwing an instance of 'lm::FormatLoadException'
  what():  native_client/kenlm/lm/read_arpa.cc:65 in void lm::ReadARPACounts(util::FilePiece&, std::vector<long unsigned int>&) threw FormatLoadException.
first non-empty line was ""version https://git-lfs.github.com/spec/v1"" not \data\. Byte: 43

I clearly have not figured this out
:-)
 
",2018-01-25T19:45:03Z,0,1,0,2.7835784920513333
208,16409,Build libjpeg-turbo ALTIVEC SIMD,"awaiting review,cla: yes","The libjpeg-turbo package has ALTIVEC SIMD and this updates the
third_party build to build the ALTIVEC SIMD on the appropriate
platform.",1,,1,2018-01-25T16:12:29Z,2018-01-25T18:39:32Z,CONTRIBUTOR,"The libjpeg-turbo package has ALTIVEC SIMD and this updates the
third_party build to build the ALTIVEC SIMD on the appropriate
platform.",2018-01-25T16:59:47Z,0,2,0,3.7835784920513333
209,16407,Creating placeholder with `np.uint32` dtype fails,,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
yes custom snippet below
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Windows 7
- **TensorFlow installed from (source or binary)**:
binary
- **TensorFlow version (use command below)**:
1.4.0
- **Python version**: 
3.6
- **Bazel version (if compiling from source)**:
not applicable
- **GCC/Compiler version (if compiling from source)**:
not applicable
- **CUDA/cuDNN version**:
CUDA 8.5
- **GPU model and memory**:
Titan X
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
np.uint32 dtype is not supported while creating placeholder

### Source code / logs
```python
import tensorflow as tf
import numpy as np
print(tf.placeholder(np.int32, [None], 'ph1'))
print(tf.placeholder(np.uint32, [None], 'ph2'))
```
Line 3 works, line 4 fails.

Console Output:
```txt
Tensor(""ph1:0"", shape=(?,), dtype=int32)
Traceback (most recent call last):
  File ""...\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\eager\execute.py"", line 126, in make_type
    v = dtypes.as_dtype(v).base_dtype
  File ""...\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\framework\dtypes.py"", line 595, in as_dtype
    ""Cannot convert value %r to a TensorFlow DType."" % type_value)
TypeError: Cannot convert value <class 'numpy.uint32'> to a TensorFlow DType.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""E:/scaffold-ext/scaffold_ext/analysis/ann/bug.py"", line 4, in <module>
    print(tf.placeholder(np.uint32, [None], 'ph2'))
  File ""...\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\ops\array_ops.py"", line 1599, in placeholder
    return gen_array_ops._placeholder(dtype=dtype, shape=shape, name=name)
  File ""...\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\ops\gen_array_ops.py"", line 3083, in _placeholder
    dtype = _execute.make_type(dtype, ""dtype"")
  File ""...\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\eager\execute.py"", line 129, in make_type
    (arg_name, repr(v)))
TypeError: Expected DataType for argument 'dtype' not <class 'numpy.uint32'>.
```",0,,2,2018-01-25T14:50:18Z,2018-01-25T22:20:04Z,NONE,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
yes custom snippet below
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Windows 7
- **TensorFlow installed from (source or binary)**:
binary
- **TensorFlow version (use command below)**:
1.4.0
- **Python version**: 
3.6
- **Bazel version (if compiling from source)**:
not applicable
- **GCC/Compiler version (if compiling from source)**:
not applicable
- **CUDA/cuDNN version**:
CUDA 8.5
- **GPU model and memory**:
Titan X
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
np.uint32 dtype is not supported while creating placeholder

### Source code / logs
```python
import tensorflow as tf
import numpy as np
print(tf.placeholder(np.int32, [None], 'ph1'))
print(tf.placeholder(np.uint32, [None], 'ph2'))
```
Line 3 works, line 4 fails.

Console Output:
```txt
Tensor(""ph1:0"", shape=(?,), dtype=int32)
Traceback (most recent call last):
  File ""...\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\eager\execute.py"", line 126, in make_type
    v = dtypes.as_dtype(v).base_dtype
  File ""...\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\framework\dtypes.py"", line 595, in as_dtype
    ""Cannot convert value %r to a TensorFlow DType."" % type_value)
TypeError: Cannot convert value <class 'numpy.uint32'> to a TensorFlow DType.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""E:/scaffold-ext/scaffold_ext/analysis/ann/bug.py"", line 4, in <module>
    print(tf.placeholder(np.uint32, [None], 'ph2'))
  File ""...\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\ops\array_ops.py"", line 1599, in placeholder
    return gen_array_ops._placeholder(dtype=dtype, shape=shape, name=name)
  File ""...\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\ops\gen_array_ops.py"", line 3083, in _placeholder
    dtype = _execute.make_type(dtype, ""dtype"")
  File ""...\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\eager\execute.py"", line 129, in make_type
    (arg_name, repr(v)))
TypeError: Expected DataType for argument 'dtype' not <class 'numpy.uint32'>.
```",2018-01-25T22:14:40Z,0,1,0,2.2835784920513333
210,16405,tf.contrib.framework.sort failing with <<...has no attribute 'sort'>> in TF windows ,,"### [Problem] : Can't use tf.contrib.framework.sort in my tensorflow code as it's failing to find the 'sort' attribute

Source code:
```
import tensorflow as tf

x = tf.placeholder(tf.float32, shape=(10, 10))
y = tf.contrib.framework.sort(x)

with tf.Session() as sess:
    rand_array = np.random.rand(10, 10)
    print(sess.run(y, feed_dict={x: rand_array})) 
```

Error log:
```
 <module>
    y = tf.contrib.framework.sort(x)
AttributeError: module 'tensorflow.contrib.framework' has no attribute 'sort'
```

### System information
- I've been trying to use the tf.contrib.framework.sort  within a custom loss function but issue being reproduced with a simple call to the tf.contrib.framework.sort
- Windows 10 64 bit
- TF installed with native pip3
- TF version: 1.4.0
- Python version: 3.6.2 
- TF with CPU support only
",0,,1,2018-01-25T14:17:01Z,2018-01-25T23:57:30Z,NONE,"### [Problem] : Can't use tf.contrib.framework.sort in my tensorflow code as it's failing to find the 'sort' attribute

Source code:
```
import tensorflow as tf

x = tf.placeholder(tf.float32, shape=(10, 10))
y = tf.contrib.framework.sort(x)

with tf.Session() as sess:
    rand_array = np.random.rand(10, 10)
    print(sess.run(y, feed_dict={x: rand_array})) 
```

Error log:
```
 <module>
    y = tf.contrib.framework.sort(x)
AttributeError: module 'tensorflow.contrib.framework' has no attribute 'sort'
```

### System information
- I've been trying to use the tf.contrib.framework.sort  within a custom loss function but issue being reproduced with a simple call to the tf.contrib.framework.sort
- Windows 10 64 bit
- TF installed with native pip3
- TF version: 1.4.0
- Python version: 3.6.2 
- TF with CPU support only
",2018-01-25T23:57:30Z,0,1,0,1.7835784920513333
211,16404,remove SRU num_units == x.shape[-1] restriction,"awaiting testing (then merge),cla: yes","Based on the [author's response](https://github.com/taolei87/sru/issues/12), the restriction is unnecessary. Simply add a linear transform to the input will solve the issue

#13094 ",1,,7,2018-01-25T13:06:58Z,2018-01-29T21:33:49Z,CONTRIBUTOR,"Based on the [author's response](https://github.com/taolei87/sru/issues/12), the restriction is unnecessary. Simply add a linear transform to the input will solve the issue

#13094 ",2018-01-25T16:55:25Z,4,2,1,6.783578492051333
212,16403,1D Convolution in Tensorflow Serving,stat:awaiting response,"### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Arch Linux
- **TensorFlow installed from (source or binary)**: tensorflow binary
- **TensorFlow version (use command below)**: 1.4.0
- **Python version**: 3.6
- **CUDA/cuDNN version**: 9.0, 7.0
- **GPU model and memory**: GTX 1050

### Describe the problem
The Problem is a little bit hard to reproduce, I guess because so many steps are involved.
So, the basic scenario is, that I am using keras to train a model in python. Here is the model I am using:

`
           input = Input(shape=(200, 8))
            x = Conv1D(filters=128, kernel_size=7, activation=""relu"", padding=""same"")(input)
            x = Conv1D(filters=128, kernel_size=7, activation=""relu"", padding=""same"")(x)
            x = Conv1D(filters=128, kernel_size=3, activation=""relu"", padding=""same"")(x)
            x = Conv1D(filters=128, kernel_size=3, activation=""relu"", padding=""same"")(x)
            x = Conv1D(filters=128, kernel_size=3, activation=""relu"", padding=""same"")(x)
            x = Conv1D(filters=2, kernel_size=1, activation=""softmax"")(x)

`

Now, I extract the graph and I am saving graph and weights with the ModelBundleBuilder:

`
session = K.get_session()

        signature = tf.saved_model.signature_def_utils.build_signature_def(
            inputs={'input': tf.saved_model.utils.build_tensor_info(self._get_model().inputs[0])},
            outputs={'output': tf.saved_model.utils.build_tensor_info(self._get_model().outputs[0])},
            method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME
        )

        b = tf.saved_model.builder.SavedModelBuilder(filename)
        legacy_init_op = tf.group(tf.tables_initializer(), name='legacy_init_op')
        b.add_meta_graph_and_variables(session,
                                       [tf.saved_model.tag_constants.SERVING],
                                       signature_def_map={
                                           tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY: signature},
                                       legacy_init_op=legacy_init_op)
        b.save()
`

If I am loading the model via python, everything works as expected.

Now I am deploying the model into TF serving and using protobuf / gRPC to make the prediction via Java. I am converting a 3D float array to a TensorProto like this:

`
TensorShapeProto.Dim dim1 = TensorShapeProto.Dim.newBuilder()
                .setSize(data.length).build();

        TensorShapeProto.Dim dim2 = TensorShapeProto.Dim.newBuilder()
                .setSize(data[0].length).build();

        TensorShapeProto.Dim dim3 = TensorShapeProto.Dim.newBuilder()
                .setSize(data[0][0].length).build();

        TensorShapeProto shape = TensorShapeProto.newBuilder()
                .addDim(dim1).addDim(dim2).addDim(dim3).build();

        TensorProto.Builder builder = TensorProto.newBuilder()
                .setDtype(DataType.DT_FLOAT)
                .setTensorShape(shape);

        for(int i = 0; i < data.length; i++) {
            for(int j = 0; j < data[0].length; j++) {
                for(int k = 0; k < data[0][0].length; k++) {
                    builder.addFloatVal(data[k][j][i]);
                }
            }
        }

        return builder.build();
`

And do the predicition like this:

`
public class ModelClientImpl implements ModelClient {

    private String host;
    private Integer port;
    private ManagedChannel channel;
    private PredictionServiceGrpc.PredictionServiceBlockingStub stub;

    public void init() {
        channel = ManagedChannelBuilder
                .forAddress(getHost(), getPort())
                .usePlaintext(true)
                .build();

        stub = PredictionServiceGrpc.newBlockingStub(channel);
    }

    @Override
    public Map<String, TensorProto> predict(final String signatureName, Map<String, TensorProto> inputs) {
        final Predict.PredictResponse response = stub.predict(createRequest(signatureName, inputs));

        return response.getOutputsMap();
    }

    protected Predict.PredictRequest createRequest(final String signatureName, final Map<String, TensorProto> inputs) {
        final Model.ModelSpec modelSpec = Model.ModelSpec.newBuilder()
                .setName(signatureName)
                .setSignatureName(""serving_default"").build();

        final Predict.PredictRequest.Builder builder = Predict.PredictRequest.newBuilder()
                .setModelSpec(modelSpec)
                .putAllInputs(inputs);

        return builder.build();
    }

    public String getHost() {
        return host;
    }

    public void setHost(String host) {
        this.host = host;
    }

    public Integer getPort() {
        return port;
    }

    public void setPort(Integer port) {
        this.port = port;
    }

    @Override
    public void close() throws Exception {
        channel.shutdown().awaitTermination(5, TimeUnit.DAYS);
    }
}

`

But the prediction is totally different from python. Does anybody know if this is a bug or is something wromg with 1dconv?
",0,,2,2018-01-25T12:50:42Z,2018-01-26T01:32:27Z,NONE,"### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Arch Linux
- **TensorFlow installed from (source or binary)**: tensorflow binary
- **TensorFlow version (use command below)**: 1.4.0
- **Python version**: 3.6
- **CUDA/cuDNN version**: 9.0, 7.0
- **GPU model and memory**: GTX 1050

### Describe the problem
The Problem is a little bit hard to reproduce, I guess because so many steps are involved.
So, the basic scenario is, that I am using keras to train a model in python. Here is the model I am using:

`
           input = Input(shape=(200, 8))
            x = Conv1D(filters=128, kernel_size=7, activation=""relu"", padding=""same"")(input)
            x = Conv1D(filters=128, kernel_size=7, activation=""relu"", padding=""same"")(x)
            x = Conv1D(filters=128, kernel_size=3, activation=""relu"", padding=""same"")(x)
            x = Conv1D(filters=128, kernel_size=3, activation=""relu"", padding=""same"")(x)
            x = Conv1D(filters=128, kernel_size=3, activation=""relu"", padding=""same"")(x)
            x = Conv1D(filters=2, kernel_size=1, activation=""softmax"")(x)

`

Now, I extract the graph and I am saving graph and weights with the ModelBundleBuilder:

`
session = K.get_session()

        signature = tf.saved_model.signature_def_utils.build_signature_def(
            inputs={'input': tf.saved_model.utils.build_tensor_info(self._get_model().inputs[0])},
            outputs={'output': tf.saved_model.utils.build_tensor_info(self._get_model().outputs[0])},
            method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME
        )

        b = tf.saved_model.builder.SavedModelBuilder(filename)
        legacy_init_op = tf.group(tf.tables_initializer(), name='legacy_init_op')
        b.add_meta_graph_and_variables(session,
                                       [tf.saved_model.tag_constants.SERVING],
                                       signature_def_map={
                                           tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY: signature},
                                       legacy_init_op=legacy_init_op)
        b.save()
`

If I am loading the model via python, everything works as expected.

Now I am deploying the model into TF serving and using protobuf / gRPC to make the prediction via Java. I am converting a 3D float array to a TensorProto like this:

`
TensorShapeProto.Dim dim1 = TensorShapeProto.Dim.newBuilder()
                .setSize(data.length).build();

        TensorShapeProto.Dim dim2 = TensorShapeProto.Dim.newBuilder()
                .setSize(data[0].length).build();

        TensorShapeProto.Dim dim3 = TensorShapeProto.Dim.newBuilder()
                .setSize(data[0][0].length).build();

        TensorShapeProto shape = TensorShapeProto.newBuilder()
                .addDim(dim1).addDim(dim2).addDim(dim3).build();

        TensorProto.Builder builder = TensorProto.newBuilder()
                .setDtype(DataType.DT_FLOAT)
                .setTensorShape(shape);

        for(int i = 0; i < data.length; i++) {
            for(int j = 0; j < data[0].length; j++) {
                for(int k = 0; k < data[0][0].length; k++) {
                    builder.addFloatVal(data[k][j][i]);
                }
            }
        }

        return builder.build();
`

And do the predicition like this:

`
public class ModelClientImpl implements ModelClient {

    private String host;
    private Integer port;
    private ManagedChannel channel;
    private PredictionServiceGrpc.PredictionServiceBlockingStub stub;

    public void init() {
        channel = ManagedChannelBuilder
                .forAddress(getHost(), getPort())
                .usePlaintext(true)
                .build();

        stub = PredictionServiceGrpc.newBlockingStub(channel);
    }

    @Override
    public Map<String, TensorProto> predict(final String signatureName, Map<String, TensorProto> inputs) {
        final Predict.PredictResponse response = stub.predict(createRequest(signatureName, inputs));

        return response.getOutputsMap();
    }

    protected Predict.PredictRequest createRequest(final String signatureName, final Map<String, TensorProto> inputs) {
        final Model.ModelSpec modelSpec = Model.ModelSpec.newBuilder()
                .setName(signatureName)
                .setSignatureName(""serving_default"").build();

        final Predict.PredictRequest.Builder builder = Predict.PredictRequest.newBuilder()
                .setModelSpec(modelSpec)
                .putAllInputs(inputs);

        return builder.build();
    }

    public String getHost() {
        return host;
    }

    public void setHost(String host) {
        this.host = host;
    }

    public Integer getPort() {
        return port;
    }

    public void setPort(Integer port) {
        this.port = port;
    }

    @Override
    public void close() throws Exception {
        channel.shutdown().awaitTermination(5, TimeUnit.DAYS);
    }
}

`

But the prediction is totally different from python. Does anybody know if this is a bug or is something wromg with 1dconv?
",2018-01-26T01:32:27Z,1,1,0,2.2835784920513333
213,16402,Modified Implementation of ndlstm_base_dynamic.,"awaiting review,cla: yes",It now uses a `BasicLSTMCell` that has `state_is_tuple=True` to address the deprecation thrown by having `state_is_tuple=False`.,1,,1,2018-01-25T12:13:47Z,2018-01-25T18:41:31Z,CONTRIBUTOR,It now uses a `BasicLSTMCell` that has `state_is_tuple=True` to address the deprecation thrown by having `state_is_tuple=False`.,2018-01-25T16:40:02Z,0,2,0,3.7835784920513333
214,16401,Separate constant file for global variables,cla: no,Created a separate common `constants.py` which can be used globally under `ops`.,1,,3,2018-01-25T11:36:25Z,2018-01-25T18:48:26Z,CONTRIBUTOR,Created a separate common `constants.py` which can be used globally under `ops`.,2018-01-25T16:52:38Z,0,2,0,4.783578492051333
215,16400,"[doc] link to ""How to Use t-SNE Effectively"" from embeddings is broken",,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)  NO (since Web page problem)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04) Windows 7**:
- **TensorFlow installed from (source or binary) binary**:
- **TensorFlow version (use command below) 1.5.0rc0 **:
- **Python version  3.5.1**: 
- **Bazel version (if compiling from source) NOT USED**:
- **GCC/Compiler version (if compiling from source) NOT USED**:
- **CUDA/cuDNN version NOT USED**:
- **GPU model and memory NOT USED **:
- **Exact command to reproduce DOC Problem. Just look https://www.tensorflow.org/programmers_guide/distill.pub/2016/misread-tsne/**:

### Describe the problem
- Link to to ""How to Use t-SNE Effectively"" is broken.
- The page link is follows (before junmping)
  - https://www.tensorflow.org/programmers_guide/embedding
  - 404 page is following URL 
     - https://www.tensorflow.org/programmers_guide/distill.pub/2016/misread-tsne/
- Original page should be follows. (the URL in embedding.md should rewrite to follows)
  -   https://distill.pub/2016/misread-tsne/

### Source code / logs
- The problem code is follows.
  - https://github.com/tensorflow/tensorflow/blame/v1.5.0-rc1/tensorflow/docs_src/programmers_guide/embedding.md#L123

",0,,1,2018-01-25T10:17:07Z,2018-01-25T16:06:01Z,NONE,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)  NO (since Web page problem)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04) Windows 7**:
- **TensorFlow installed from (source or binary) binary**:
- **TensorFlow version (use command below) 1.5.0rc0 **:
- **Python version  3.5.1**: 
- **Bazel version (if compiling from source) NOT USED**:
- **GCC/Compiler version (if compiling from source) NOT USED**:
- **CUDA/cuDNN version NOT USED**:
- **GPU model and memory NOT USED **:
- **Exact command to reproduce DOC Problem. Just look https://www.tensorflow.org/programmers_guide/distill.pub/2016/misread-tsne/**:

### Describe the problem
- Link to to ""How to Use t-SNE Effectively"" is broken.
- The page link is follows (before junmping)
  - https://www.tensorflow.org/programmers_guide/embedding
  - 404 page is following URL 
     - https://www.tensorflow.org/programmers_guide/distill.pub/2016/misread-tsne/
- Original page should be follows. (the URL in embedding.md should rewrite to follows)
  -   https://distill.pub/2016/misread-tsne/

### Source code / logs
- The problem code is follows.
  - https://github.com/tensorflow/tensorflow/blame/v1.5.0-rc1/tensorflow/docs_src/programmers_guide/embedding.md#L123

",2018-01-25T16:06:00Z,0,1,0,1.7835784920513333
216,16399,Does TensorFlow 1.5 support CUDA 9.1?,,"My notebook has an MX150 display adapter, someone said that it's available with CUDA 9.1",0,,3,2018-01-25T08:28:58Z,2018-01-26T01:42:20Z,NONE,"My notebook has an MX150 display adapter, someone said that it's available with CUDA 9.1",2018-01-25T10:52:35Z,1,1,0,2.7835784920513333
217,16398,Compare_and_bitpack function for bool for big endian,"cla: yes,stat:awaiting response","Added condition for endianness check and related conversion for Big Endian.
Removed the note from file: 
`// NOTE(ebrevdo): This assumes memory is little-endian.`
Please let me know your feedback.",1,,1,2018-01-25T08:22:19Z,2018-01-26T16:48:27Z,CONTRIBUTOR,"Added condition for endianness check and related conversion for Big Endian.
Removed the note from file: 
`// NOTE(ebrevdo): This assumes memory is little-endian.`
Please let me know your feedback.",2018-01-25T16:51:11Z,1,2,0,3.7835784920513333
218,16397,"S3 accessing reports ""Curl returned error code 6"" after AWS SDK upgrading to 1.3.15",stat:contributions welcome,"### System information

- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: CentOS 7.2
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.5.0rc1 (tag) and master (2e5ff39e)
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: 0.9.0
- **GCC/Compiler version (if compiling from source)**: 4.8
- **CUDA/cuDNN version**: 8
- **GPU model and memory**: /
- **Exact command to reproduce**: 

### Describe the problem

TensorFlow 1.4.X was working well with S3 in my environment. After upgrading to 1.5.0rc1, I found that S3 could not be accessed. ""Curl returned error code 6"" is reported.

I noticed that AWS SDK had been upgraded from 1.0.90 to 1.3.15 in r1.5 and master branches. Thus, I pulled the master (2e5ff39e) and tried to change AWS SDK 1.3.15 into 1.0.90 in `tensorflow/workspace.bzl`. After this modification, it works well!

I tried with both AWS S3 (with http proxy) and Minio (localhost), and the results are the same. (AWS SDK 1.0.90 is Ok, but 1.3.15 reports error)

I guess there might be some incompatible changes after AWS SDK 1.3.15. Could you please take a look? Thanks! @yongtang

I noticed that AWS SDK required gcc 4.9, but I was using 4.8. Thus, this issue might be related to the old versions of gcc and glib on my server. I will try with some new systems as well.

### Source code / logs

Logs when using TensorFlow master (2e5ff39e):

```
>>> import tensorflow as tf
>>> tf.gfile.Exists('s3://xxxxxxxxx/f.txt')
2018-01-25 15:34:45.145317: I tensorflow/core/platform/s3/aws_logging.cc:54] Initializing config loader against fileName /home/xxxxxxxx//.aws/config and using profilePrefix = 1
2018-01-25 15:34:45.145354: I tensorflow/core/platform/s3/aws_logging.cc:54] Initializing config loader against fileName /home/xxxxxxxx//.aws/credentials and using profilePrefix = 0
2018-01-25 15:34:45.145367: I tensorflow/core/platform/s3/aws_logging.cc:54] Setting provider to read credentials from /home/xxxxxxxx//.aws/credentials for credentials file and /home/xxxxxxxx//.aws/config for the config file , for use with profile default
2018-01-25 15:34:45.145383: I tensorflow/core/platform/s3/aws_logging.cc:54] Creating HttpClient with max connections2 and scheme http
2018-01-25 15:34:45.145401: I tensorflow/core/platform/s3/aws_logging.cc:54] Initializing CurlHandleContainer with size 2
2018-01-25 15:34:45.145414: I tensorflow/core/platform/s3/aws_logging.cc:54] Creating Instance with default EC2MetadataClient and refresh rate 900000
2018-01-25 15:34:45.145456: I tensorflow/core/platform/s3/aws_logging.cc:54] Unable to open config file /home/xxxxxxxx//.aws/credentials for reading.
2018-01-25 15:34:45.145468: I tensorflow/core/platform/s3/aws_logging.cc:54] Failed to reload configuration.
2018-01-25 15:34:45.145479: I tensorflow/core/platform/s3/aws_logging.cc:54] Unable to open config file /home/xxxxxxxx//.aws/config for reading.
2018-01-25 15:34:45.145487: I tensorflow/core/platform/s3/aws_logging.cc:54] Failed to reload configuration.
2018-01-25 15:34:45.145495: I tensorflow/core/platform/s3/aws_logging.cc:54] Credentials have expired attempting to repull from EC2 Metadata Service.
2018-01-25 15:34:45.145614: I tensorflow/core/platform/s3/aws_logging.cc:54] Pool grown by 2
2018-01-25 15:34:45.145628: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing.
2018-01-25 15:34:46.146625: E tensorflow/core/platform/s3/aws_logging.cc:60] Curl returned error code 28
2018-01-25 15:34:46.146655: E tensorflow/core/platform/s3/aws_logging.cc:60] Http request to Ec2MetadataService failed.
2018-01-25 15:34:46.146666: I tensorflow/core/platform/s3/aws_logging.cc:54] Failed to reload configuration.
2018-01-25 15:34:46.146715: I tensorflow/core/platform/s3/aws_logging.cc:54] Initializing CurlHandleContainer with size 25
2018-01-25 15:34:46.146849: I tensorflow/core/platform/s3/aws_logging.cc:54] Pool grown by 2
2018-01-25 15:34:46.146863: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing.
2018-01-25 15:34:46.147677: E tensorflow/core/platform/s3/aws_logging.cc:60] Curl returned error code 6
2018-01-25 15:34:46.147704: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer.
2018-01-25 15:34:46.147716: W tensorflow/core/platform/s3/aws_logging.cc:57] Request failed, now waiting 0 ms before attempting again.
2018-01-25 15:34:46.147802: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing.
2018-01-25 15:34:46.148107: E tensorflow/core/platform/s3/aws_logging.cc:60] Curl returned error code 6
...
False
```

Logs after replacing AWS SDK 1.3.15 with 1.0.90:

```
>>> import tensorflow as tf
>>> tf.gfile.Exists('s3://xxxxxxxxx/f.txt')
2018-01-25 15:44:29.134077: I tensorflow/core/platform/s3/aws_logging.cc:54] Initializing config loader against fileName /home/xxxxxxxx//.aws/config and using profilePrefix = 1
2018-01-25 15:44:29.134108: I tensorflow/core/platform/s3/aws_logging.cc:54] Initializing config loader against fileName /home/xxxxxxxx//.aws/credentials and using profilePrefix = 0
2018-01-25 15:44:29.134121: I tensorflow/core/platform/s3/aws_logging.cc:54] Setting provider to read credentials from /home/xxxxxxxx//.aws/credentials for credentials file and /home/xxxxxxxx//.aws/config for the config file , for use with profile default
2018-01-25 15:44:29.134133: I tensorflow/core/platform/s3/aws_logging.cc:54] Creating HttpClient with max connections -864887560 and scheme Creating HttpClient with max connections %d and scheme %s
2018-01-25 15:44:29.134147: I tensorflow/core/platform/s3/aws_logging.cc:54] Initializing CurlHandleContainer with size 2
2018-01-25 15:44:29.134157: I tensorflow/core/platform/s3/aws_logging.cc:54] Creating Instance with default EC2MetadataClient and refresh rate 900000
2018-01-25 15:44:29.134176: I tensorflow/core/platform/s3/aws_logging.cc:54] Found credential in environment with access key id ********************
2018-01-25 15:44:29.134184: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key
2018-01-25 15:44:29.134223: I tensorflow/core/platform/s3/aws_logging.cc:54] Initializing CurlHandleContainer with size 25
2018-01-25 15:44:29.134264: I tensorflow/core/platform/s3/aws_logging.cc:54] Found credential in environment with access key id ********************
2018-01-25 15:44:29.134273: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key
2018-01-25 15:44:29.134429: I tensorflow/core/platform/s3/aws_logging.cc:54] Pool successfully grown by 2
2018-01-25 15:44:29.134442: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing.
2018-01-25 15:44:30.048051: I tensorflow/core/platform/s3/aws_logging.cc:54] Found credential in environment with access key id ********************
2018-01-25 15:44:30.048084: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key
2018-01-25 15:44:30.048159: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing.
True
```",0,,10,2018-01-25T08:00:10Z,2018-02-06T11:05:58Z,CONTRIBUTOR,"### System information

- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: CentOS 7.2
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.5.0rc1 (tag) and master (2e5ff39e)
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: 0.9.0
- **GCC/Compiler version (if compiling from source)**: 4.8
- **CUDA/cuDNN version**: 8
- **GPU model and memory**: /
- **Exact command to reproduce**: 

### Describe the problem

TensorFlow 1.4.X was working well with S3 in my environment. After upgrading to 1.5.0rc1, I found that S3 could not be accessed. ""Curl returned error code 6"" is reported.

I noticed that AWS SDK had been upgraded from 1.0.90 to 1.3.15 in r1.5 and master branches. Thus, I pulled the master (2e5ff39e) and tried to change AWS SDK 1.3.15 into 1.0.90 in `tensorflow/workspace.bzl`. After this modification, it works well!

I tried with both AWS S3 (with http proxy) and Minio (localhost), and the results are the same. (AWS SDK 1.0.90 is Ok, but 1.3.15 reports error)

I guess there might be some incompatible changes after AWS SDK 1.3.15. Could you please take a look? Thanks! @yongtang

I noticed that AWS SDK required gcc 4.9, but I was using 4.8. Thus, this issue might be related to the old versions of gcc and glib on my server. I will try with some new systems as well.

### Source code / logs

Logs when using TensorFlow master (2e5ff39e):

```
>>> import tensorflow as tf
>>> tf.gfile.Exists('s3://xxxxxxxxx/f.txt')
2018-01-25 15:34:45.145317: I tensorflow/core/platform/s3/aws_logging.cc:54] Initializing config loader against fileName /home/xxxxxxxx//.aws/config and using profilePrefix = 1
2018-01-25 15:34:45.145354: I tensorflow/core/platform/s3/aws_logging.cc:54] Initializing config loader against fileName /home/xxxxxxxx//.aws/credentials and using profilePrefix = 0
2018-01-25 15:34:45.145367: I tensorflow/core/platform/s3/aws_logging.cc:54] Setting provider to read credentials from /home/xxxxxxxx//.aws/credentials for credentials file and /home/xxxxxxxx//.aws/config for the config file , for use with profile default
2018-01-25 15:34:45.145383: I tensorflow/core/platform/s3/aws_logging.cc:54] Creating HttpClient with max connections2 and scheme http
2018-01-25 15:34:45.145401: I tensorflow/core/platform/s3/aws_logging.cc:54] Initializing CurlHandleContainer with size 2
2018-01-25 15:34:45.145414: I tensorflow/core/platform/s3/aws_logging.cc:54] Creating Instance with default EC2MetadataClient and refresh rate 900000
2018-01-25 15:34:45.145456: I tensorflow/core/platform/s3/aws_logging.cc:54] Unable to open config file /home/xxxxxxxx//.aws/credentials for reading.
2018-01-25 15:34:45.145468: I tensorflow/core/platform/s3/aws_logging.cc:54] Failed to reload configuration.
2018-01-25 15:34:45.145479: I tensorflow/core/platform/s3/aws_logging.cc:54] Unable to open config file /home/xxxxxxxx//.aws/config for reading.
2018-01-25 15:34:45.145487: I tensorflow/core/platform/s3/aws_logging.cc:54] Failed to reload configuration.
2018-01-25 15:34:45.145495: I tensorflow/core/platform/s3/aws_logging.cc:54] Credentials have expired attempting to repull from EC2 Metadata Service.
2018-01-25 15:34:45.145614: I tensorflow/core/platform/s3/aws_logging.cc:54] Pool grown by 2
2018-01-25 15:34:45.145628: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing.
2018-01-25 15:34:46.146625: E tensorflow/core/platform/s3/aws_logging.cc:60] Curl returned error code 28
2018-01-25 15:34:46.146655: E tensorflow/core/platform/s3/aws_logging.cc:60] Http request to Ec2MetadataService failed.
2018-01-25 15:34:46.146666: I tensorflow/core/platform/s3/aws_logging.cc:54] Failed to reload configuration.
2018-01-25 15:34:46.146715: I tensorflow/core/platform/s3/aws_logging.cc:54] Initializing CurlHandleContainer with size 25
2018-01-25 15:34:46.146849: I tensorflow/core/platform/s3/aws_logging.cc:54] Pool grown by 2
2018-01-25 15:34:46.146863: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing.
2018-01-25 15:34:46.147677: E tensorflow/core/platform/s3/aws_logging.cc:60] Curl returned error code 6
2018-01-25 15:34:46.147704: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer.
2018-01-25 15:34:46.147716: W tensorflow/core/platform/s3/aws_logging.cc:57] Request failed, now waiting 0 ms before attempting again.
2018-01-25 15:34:46.147802: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing.
2018-01-25 15:34:46.148107: E tensorflow/core/platform/s3/aws_logging.cc:60] Curl returned error code 6
...
False
```

Logs after replacing AWS SDK 1.3.15 with 1.0.90:

```
>>> import tensorflow as tf
>>> tf.gfile.Exists('s3://xxxxxxxxx/f.txt')
2018-01-25 15:44:29.134077: I tensorflow/core/platform/s3/aws_logging.cc:54] Initializing config loader against fileName /home/xxxxxxxx//.aws/config and using profilePrefix = 1
2018-01-25 15:44:29.134108: I tensorflow/core/platform/s3/aws_logging.cc:54] Initializing config loader against fileName /home/xxxxxxxx//.aws/credentials and using profilePrefix = 0
2018-01-25 15:44:29.134121: I tensorflow/core/platform/s3/aws_logging.cc:54] Setting provider to read credentials from /home/xxxxxxxx//.aws/credentials for credentials file and /home/xxxxxxxx//.aws/config for the config file , for use with profile default
2018-01-25 15:44:29.134133: I tensorflow/core/platform/s3/aws_logging.cc:54] Creating HttpClient with max connections -864887560 and scheme Creating HttpClient with max connections %d and scheme %s
2018-01-25 15:44:29.134147: I tensorflow/core/platform/s3/aws_logging.cc:54] Initializing CurlHandleContainer with size 2
2018-01-25 15:44:29.134157: I tensorflow/core/platform/s3/aws_logging.cc:54] Creating Instance with default EC2MetadataClient and refresh rate 900000
2018-01-25 15:44:29.134176: I tensorflow/core/platform/s3/aws_logging.cc:54] Found credential in environment with access key id ********************
2018-01-25 15:44:29.134184: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key
2018-01-25 15:44:29.134223: I tensorflow/core/platform/s3/aws_logging.cc:54] Initializing CurlHandleContainer with size 25
2018-01-25 15:44:29.134264: I tensorflow/core/platform/s3/aws_logging.cc:54] Found credential in environment with access key id ********************
2018-01-25 15:44:29.134273: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key
2018-01-25 15:44:29.134429: I tensorflow/core/platform/s3/aws_logging.cc:54] Pool successfully grown by 2
2018-01-25 15:44:29.134442: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing.
2018-01-25 15:44:30.048051: I tensorflow/core/platform/s3/aws_logging.cc:54] Found credential in environment with access key id ********************
2018-01-25 15:44:30.048084: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key
2018-01-25 15:44:30.048159: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing.
True
```",2018-01-25T16:43:23Z,11,2,2,7.283578492051333
219,16395,change from deprecated version to a new version,"awaiting testing (then merge),cla: yes",,1,,6,2018-01-25T07:32:11Z,2018-02-15T17:37:07Z,CONTRIBUTOR,,2018-02-09T19:13:17Z,20,2,3,6.283578492051333
220,16394,cmake gpu build improvement,"awaiting review,cla: no","cmake build pass with gpu enabled

python binding option can change to off now",1,,4,2018-01-25T06:23:27Z,2018-01-27T03:56:45Z,CONTRIBUTOR,"cmake build pass with gpu enabled

python binding option can change to off now",2018-01-26T16:49:11Z,2,2,0,5.283578492051333
221,16391,Added early stopping and CheckpointSaverListeners to train and evaluate,"awaiting review,cla: yes,stat:awaiting tensorflower",Addresses the issue at https://github.com/tensorflow/tensorflow/issues/16203,1,,8,2018-01-25T06:00:13Z,2018-02-11T23:44:37Z,NONE,Addresses the issue at https://github.com/tensorflow/tensorflow/issues/16203,2018-01-29T19:32:49Z,16,1,3,6.283578492051333
222,16389,cherrypick bfloat16 changes,cla: yes,,1,,2,2018-01-25T04:12:26Z,2018-01-25T18:20:31Z,NONE,,2018-01-25T18:13:12Z,0,1,0,3.2835784920513333
223,16387,Tensorflow not using GPU,,"I use windows in my laptop. Initially tensorflow worked well with GPU. I don't know why suddenly it's not detecting the GPU. I am using tensorflow-gpu 1.4 version (installed with pip install tensorflow-gpu) with CUDA 8.0 and cudnn 6.0. I also tried with other versions but the problem persists. Attached is the error message shown. I appreciate any help :)

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: just called a session
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows
- **TensorFlow installed from (source or binary)**: pip install tensorflow-gpu
- **TensorFlow version (use command below)**: 1.4
- **Python version**: 3.5.2
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: 8.0/6.0
- **GPU model and memory**: NVIDIA GeForce 1050 2GB
- **Exact command to reproduce**: sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))

(https://user-images.githubusercont
![tferror](https://user-images.githubusercontent.com/19821962/35368823-48f11022-0153-11e8-87c3-41e41118a3fa.png)
ent.com/19821962/35368747-e2d4ba8c-0152-11e8-871a-a6a5aedf6663.png)
",0,,4,2018-01-25T03:09:07Z,2018-02-07T00:11:42Z,NONE,"I use windows in my laptop. Initially tensorflow worked well with GPU. I don't know why suddenly it's not detecting the GPU. I am using tensorflow-gpu 1.4 version (installed with pip install tensorflow-gpu) with CUDA 8.0 and cudnn 6.0. I also tried with other versions but the problem persists. Attached is the error message shown. I appreciate any help :)

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: just called a session
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows
- **TensorFlow installed from (source or binary)**: pip install tensorflow-gpu
- **TensorFlow version (use command below)**: 1.4
- **Python version**: 3.5.2
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: 8.0/6.0
- **GPU model and memory**: NVIDIA GeForce 1050 2GB
- **Exact command to reproduce**: sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))

(https://user-images.githubusercont
![tferror](https://user-images.githubusercontent.com/19821962/35368823-48f11022-0153-11e8-87c3-41e41118a3fa.png)
ent.com/19821962/35368747-e2d4ba8c-0152-11e8-871a-a6a5aedf6663.png)
",2018-01-25T09:14:58Z,12,1,2,3.2835784920513333
224,16386,Using keras layers within an Estimator either causes training where it shouldn't or corrupts weights,,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Debian 3.16.36
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: ('v1.4.0-19-ga52c8d9', '1.4.1')
- **Python version**: 2.7.9
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: See gist

### Describe the problem

Hey there,

So I have a `tf.keras` model that for use on Amazon SageMaker I'm trying to convert into an Estimator. I know there's `tf.keras.estimator.model_to_estimator` but, I'm having separate [issues with that](https://github.com/tensorflow/tensorflow/issues/16385).

In the [easily run reproduction here](https://gist.github.com/zmjjmz/667d0d7e6b6c97c49b3aaf4d67b03d2c) I have (as a demonstration) a `tf.keras.layers.Embedding` which is initialized with all zeros and has `trainable=False`. Followed by that is a `Dense` layer with `use_bias=False` because I couldn't figure out how to get predictions out of an Estimator without training it first (and I can't train nothing apparently). Since all of the embeddings are zero however and can't be trained, the `Dense` layer should always produce a zero, even after training it. Instead, it produces garbage!

In fact, I've taken a few steps to ensure that no training takes place, although ideally I'd be able to just run the estimator without training:
1) I've set the loss to be 0 initially (l2_norm of what should start out as 0)
2) Optimize with SGD using a learning rate of 0
3) One training example that should have zero loss...

The output I actually get is very much non-zero. If I inspect the `embed/embeddings:0` tensor in `tfbdg`, I see this:
```
array([[ 0.14387012,  0.83495581,  0.44025695,  0.25154734,  0.7214781 ,  0.40229702,  0.82108581,  0.12210274,  0.43861651,  0.39615464],                
       [ 0.81636655,  0.48157215,  0.48987687,  0.48775947,  0.62187696,  0.25421095,  0.64555049,  0.97305572,  0.53352964,  0.34286666],                
       [ 0.82881641,  0.80365777,  0.4596678 ,  0.21614265,  0.22256434,  0.07986271,  0.92880177,  0.64946997,  0.89239001,  0.13793337],                
       [ 0.98491704,  0.15281868,  0.77106941,  0.30048406,  0.86042607,  0.88010466,  0.64362776,  0.70185173,  0.49912012,  0.61521161]], dtype=float32)
```

Even though it shouldn't have budged from all zeroes! 

So, something about how I'm doing this is fundamentally broken. I suspect that the issue is in line `61` where I start a new Session -- however this appears to be necessary, since I need to ensure that the `keras` backend is using the same `Graph` as `tensorflow.get_default_graph()` due to the peculiarities of how the Estimator calls the `model_fn`.

Notably those values hold between:
1) Runs of the estimator
2) Successive runs with the same `tf.set_random_seed` value

The latter makes me think that somehow the `Embedding` layer is receiving *a* gradient despite my best efforts, although it's hard to test this versus some sort of memory corruption. 

If you need me to provide any more information let me know -- I'm sure I've left something out.

",0,,5,2018-01-25T03:06:57Z,2018-01-25T17:23:21Z,NONE,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Debian 3.16.36
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: ('v1.4.0-19-ga52c8d9', '1.4.1')
- **Python version**: 2.7.9
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: See gist

### Describe the problem

Hey there,

So I have a `tf.keras` model that for use on Amazon SageMaker I'm trying to convert into an Estimator. I know there's `tf.keras.estimator.model_to_estimator` but, I'm having separate [issues with that](https://github.com/tensorflow/tensorflow/issues/16385).

In the [easily run reproduction here](https://gist.github.com/zmjjmz/667d0d7e6b6c97c49b3aaf4d67b03d2c) I have (as a demonstration) a `tf.keras.layers.Embedding` which is initialized with all zeros and has `trainable=False`. Followed by that is a `Dense` layer with `use_bias=False` because I couldn't figure out how to get predictions out of an Estimator without training it first (and I can't train nothing apparently). Since all of the embeddings are zero however and can't be trained, the `Dense` layer should always produce a zero, even after training it. Instead, it produces garbage!

In fact, I've taken a few steps to ensure that no training takes place, although ideally I'd be able to just run the estimator without training:
1) I've set the loss to be 0 initially (l2_norm of what should start out as 0)
2) Optimize with SGD using a learning rate of 0
3) One training example that should have zero loss...

The output I actually get is very much non-zero. If I inspect the `embed/embeddings:0` tensor in `tfbdg`, I see this:
```
array([[ 0.14387012,  0.83495581,  0.44025695,  0.25154734,  0.7214781 ,  0.40229702,  0.82108581,  0.12210274,  0.43861651,  0.39615464],                
       [ 0.81636655,  0.48157215,  0.48987687,  0.48775947,  0.62187696,  0.25421095,  0.64555049,  0.97305572,  0.53352964,  0.34286666],                
       [ 0.82881641,  0.80365777,  0.4596678 ,  0.21614265,  0.22256434,  0.07986271,  0.92880177,  0.64946997,  0.89239001,  0.13793337],                
       [ 0.98491704,  0.15281868,  0.77106941,  0.30048406,  0.86042607,  0.88010466,  0.64362776,  0.70185173,  0.49912012,  0.61521161]], dtype=float32)
```

Even though it shouldn't have budged from all zeroes! 

So, something about how I'm doing this is fundamentally broken. I suspect that the issue is in line `61` where I start a new Session -- however this appears to be necessary, since I need to ensure that the `keras` backend is using the same `Graph` as `tensorflow.get_default_graph()` due to the peculiarities of how the Estimator calls the `model_fn`.

Notably those values hold between:
1) Runs of the estimator
2) Successive runs with the same `tf.set_random_seed` value

The latter makes me think that somehow the `Embedding` layer is receiving *a* gradient despite my best efforts, although it's hard to test this versus some sort of memory corruption. 

If you need me to provide any more information let me know -- I'm sure I've left something out.

",2018-01-25T09:48:41Z,0,1,0,3.7835784920513333
225,16385,Estimator built with keras.estimator.model_to_estimator fails on Estimator.export_savedmodel,stat:awaiting response,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Debian 3.16.36
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: ('v1.4.0-19-ga52c8d9', '1.4.1')
- **Python version**: 2.7.9
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: See gist

### Describe the problem

If I create a model with `tf.keras`, compile it and then turn it into an estimator by simply passing it thru to `tf.keras.estimator.model_to_estimator`, I am able to train and evaluate the model just fine -- however when I got to export it with `Estimator.export_savedmodel`, I get the following error:

```
Traceback (most recent call last):
  File ""endtoend_noweights_trainer_keras.py"", line 302, in <module>
    get_serving_input_fn(hyperparameters),
  File ""/home/u1/zach/proj/dataplayground2/local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 517, in export_savedmodel
    serving_input_receiver.receiver_tensors_alternatives)
  File ""/home/u1/zach/proj/dataplayground2/local/lib/python2.7/site-packages/tensorflow/python/estimator/export/export.py"", line 193, in build_all_signature_defs
    raise ValueError('export_outputs must be a dict.')
ValueError: export_outputs must be a dict.
```

I'm not sure what `export_outputs` is, but if I had to guess it should be a mapping of output names to output tensors from the `keras` model.

Here's the (very unclean) [code](https://gist.github.com/zmjjmz/8e3a7e5430f2e700a9e89bf2b4f6259b) I'm using to get to this, although there's a lot of dependencies that won't work for y'all. If you need a repro I can take the time to put it together, just let me know. Notably the error occurs on line `300`.",1,,12,2018-01-25T02:20:52Z,2018-02-14T00:37:53Z,NONE,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Debian 3.16.36
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: ('v1.4.0-19-ga52c8d9', '1.4.1')
- **Python version**: 2.7.9
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: See gist

### Describe the problem

If I create a model with `tf.keras`, compile it and then turn it into an estimator by simply passing it thru to `tf.keras.estimator.model_to_estimator`, I am able to train and evaluate the model just fine -- however when I got to export it with `Estimator.export_savedmodel`, I get the following error:

```
Traceback (most recent call last):
  File ""endtoend_noweights_trainer_keras.py"", line 302, in <module>
    get_serving_input_fn(hyperparameters),
  File ""/home/u1/zach/proj/dataplayground2/local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 517, in export_savedmodel
    serving_input_receiver.receiver_tensors_alternatives)
  File ""/home/u1/zach/proj/dataplayground2/local/lib/python2.7/site-packages/tensorflow/python/estimator/export/export.py"", line 193, in build_all_signature_defs
    raise ValueError('export_outputs must be a dict.')
ValueError: export_outputs must be a dict.
```

I'm not sure what `export_outputs` is, but if I had to guess it should be a mapping of output names to output tensors from the `keras` model.

Here's the (very unclean) [code](https://gist.github.com/zmjjmz/8e3a7e5430f2e700a9e89bf2b4f6259b) I'm using to get to this, although there's a lot of dependencies that won't work for y'all. If you need a repro I can take the time to put it together, just let me know. Notably the error occurs on line `300`.",2018-01-26T01:53:29Z,19,1,3,8.283578492051333
226,16384,fix typos,cla: yes,fix typos,1,,1,2018-01-25T01:28:59Z,2018-01-25T18:52:07Z,CONTRIBUTOR,fix typos,2018-01-25T18:52:17Z,0,2,0,3.7835784920513333
227,16382,Disable bfloat16 for sparse_matmul for 1.5.0,cla: yes,"I'm using this PR to test out simple workarounds for the sparse_matmul problem.
It doesn't need a reviewer yet.

Note: it looks like we're going to try to release 1.5.0 with the fix anyway. I will keep this PR available until that's finished.",1,,3,2018-01-25T00:00:18Z,2018-01-25T19:17:06Z,MEMBER,"I'm using this PR to test out simple workarounds for the sparse_matmul problem.
It doesn't need a reviewer yet.

Note: it looks like we're going to try to release 1.5.0 with the fix anyway. I will keep this PR available until that's finished.",2018-01-25T01:29:34Z,0,3,0,5.783578492051333
228,16379,No module named 'tensorflow'. Anaconda+windows10+tensorflow-gpu+cuda8+cudnn6,type:build/install,"I had anaconda on my windows 10.
I installed CUDA 8.0 with cuDNN 6 and then followed [http://blog.nitishmutha.com/tensorflow/2017/01/22/TensorFlow-with-gpu-for-windows.html](url) to activate tensorflow-gpu environment. Now when I import tensorflow in the console, it works but with jupyter notebook opened right in this environment it throws the error. I even upgraded setuptools as mentioned in a previous issue.
![capture5](https://user-images.githubusercontent.com/10391022/35360420-a5cd8f98-0183-11e8-919a-53da56df5ee8.JPG)
![capture6](https://user-images.githubusercontent.com/10391022/35360450-c27e44e8-0183-11e8-9c0a-5aaaa05000c4.JPG)
![capture7](https://user-images.githubusercontent.com/10391022/35360455-c7e24a42-0183-11e8-964b-7186fb233078.JPG)
![capture8](https://user-images.githubusercontent.com/10391022/35360549-174292c2-0184-11e8-9d6a-93ba6139a275.JPG)


",0,,3,2018-01-24T22:27:17Z,2018-01-25T12:25:09Z,NONE,"I had anaconda on my windows 10.
I installed CUDA 8.0 with cuDNN 6 and then followed [http://blog.nitishmutha.com/tensorflow/2017/01/22/TensorFlow-with-gpu-for-windows.html](url) to activate tensorflow-gpu environment. Now when I import tensorflow in the console, it works but with jupyter notebook opened right in this environment it throws the error. I even upgraded setuptools as mentioned in a previous issue.
![capture5](https://user-images.githubusercontent.com/10391022/35360420-a5cd8f98-0183-11e8-919a-53da56df5ee8.JPG)
![capture6](https://user-images.githubusercontent.com/10391022/35360450-c27e44e8-0183-11e8-9c0a-5aaaa05000c4.JPG)
![capture7](https://user-images.githubusercontent.com/10391022/35360455-c7e24a42-0183-11e8-964b-7186fb233078.JPG)
![capture8](https://user-images.githubusercontent.com/10391022/35360549-174292c2-0184-11e8-9d6a-93ba6139a275.JPG)


",2018-01-24T23:43:04Z,1,1,0,2.7812664140627286
229,16377,R1.4,cla: no,,0,,2,2018-01-24T21:03:18Z,2018-01-24T22:17:26Z,NONE,,2018-01-24T22:17:26Z,0,1,0,2.2812664140627286
230,16375,Branch 183115307,cla: yes,,0,,4,2018-01-24T20:39:34Z,2018-01-24T22:47:19Z,CONTRIBUTOR,,2018-01-24T21:21:17Z,0,2,0,4.281266414062729
231,16371,Tegra Nvidia Jetson TX2 build python 2.7 new CUDA and CUDNN,"stat:awaiting response,type:build/install","Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux4Tegra 28.2
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.5-rc1
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: 0.9
- **GCC/Compiler version (if compiling from source)**: 5.4
- **CUDA/cuDNN version**: 9.0/7.0
- **GPU model and memory**: Denver2 8GB
- **Exact command to reproduce**: import tensorflow

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
Clean installation with the new CUDA 9 and cudnn 7 from nvidia
### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

```
Collecting system information...
Traceback (most recent call last):
  File ""/tmp/check_tf.py"", line 1, in <module>
    import tensorflow as tf;
  File ""/home/ubuntu/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""/home/ubuntu/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/home/ubuntu/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/home/ubuntu/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/home/ubuntu/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/home/ubuntu/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
ImportError: /home/ubuntu/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _ZN3Aws4Time9LocalTimeEP2tml


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
Wrote environment to tf_env.txt. You can review the contents of that file.
and use it to populate the fields in the github issue template.

cat tf_env.txt

```
",0,,6,2018-01-24T17:58:38Z,2018-01-26T15:45:08Z,NONE,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux4Tegra 28.2
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.5-rc1
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: 0.9
- **GCC/Compiler version (if compiling from source)**: 5.4
- **CUDA/cuDNN version**: 9.0/7.0
- **GPU model and memory**: Denver2 8GB
- **Exact command to reproduce**: import tensorflow

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
Clean installation with the new CUDA 9 and cudnn 7 from nvidia
### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

```
Collecting system information...
Traceback (most recent call last):
  File ""/tmp/check_tf.py"", line 1, in <module>
    import tensorflow as tf;
  File ""/home/ubuntu/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""/home/ubuntu/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/home/ubuntu/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/home/ubuntu/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/home/ubuntu/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/home/ubuntu/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
ImportError: /home/ubuntu/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _ZN3Aws4Time9LocalTimeEP2tml


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
Wrote environment to tf_env.txt. You can review the contents of that file.
and use it to populate the fields in the github issue template.

cat tf_env.txt

```
",2018-01-26T02:09:31Z,2,1,0,4.281266414062729
232,16369,Unittesting Models with Tensorflow - How to clear the existing graph ?,stat:awaiting response,"Hello dear tensorflowers,

I have already asked the question of [StackOverflow](https://stackoverflow.com/questions/48421308/tensorflow-and-unittests-layer-already-defined), however it seams like nobody can answer my question.

So I hope you will forgive me about reposting it here:

I am developing unittests for a product I implemented with TF.

Each part of the model is tested separately then all together in different conditions.

Let's take the example of a simple GAN, I have the following tests:

 - **GeneratorTest** Class: With all tests concerning G inside
 - **DiscriminatorTest** Class: With all tests concerning D inside
 - **GAN_Train_Test** Class: G and D connected all together: 1 training step is tested.
 - **GAN_Inference_Test** Class: G and D connecteed all together: 1 inference run is tested.

------------

When the files are executed independently, everything is working nicely and fine. Tests are all fine.

Problems start occuring when I try to create one file to launch them all from one master file.

**master_test_launcher.py:**

```python
import unittest
import time
    
import tensorflow as tf
    
from tests.test_generator import GeneratorTest
from tests.test_discriminator import DiscriminatorTest
from tests.test_anovae_model import GAN_Train_Test
from tests.test_inference import GAN_Inference_Test
    
runner = unittest.TextTestRunner(verbosity=2)
    
if __name__ == '__main__':
   tf.logging.set_verbosity(tf.logging.DEBUG)
    
    for test in [GeneratorTest, DiscriminatorTest, GAN_Train_Test, GAN_Inference_Test]:
        tf.logging.debug(""Running tests for: %s ..."" % test.__str__())
    
        tf.reset_default_graph()
    
        time.sleep(2)
    
        test_suite = unittest.TestSuite()
        test_suite.addTest(unittest.makeSuite(test))
    
        runner.run(test_suite)
```

I repeatedly obtain the same error when I run the tests related to G and D connected together: 

```python
Exception: Layer 'encoder/input' already exists, please choice other 'name' or reuse this layer
Hint : Use different name for different 'Layer' (The name is used to control parameter sharing)
```

The error is quite simple to understand, each test file is independant and thus create its own session and graph. While testing only G or D, there is no problem because they have different name_scope/variable_scope. However, when testing the whole model *Layers* already have been defined by previous tests and thus leading to issue.

I would like to find a way to completely drop the graph and reset the whole TF state as **brand new and clean**. However, everything I try seem to  fail.

I would like to avoid creating a new graph for each test, leaving the old one in memory (could lead to very high amount of memory waste after a few tests).


So my question is easy: ** How can I reset the whole TF state and internal vars as ""clean"" as if you relaunch a new python shell ? By some black-magic I can't find any way doing it (after looking for it for hours).

For information here are the things I tried and which failed:
- tf.reset_default_graph()
- cleaning everything in graph collections
- creating a new graph + new session before executing each Test File: A graph is still built somewhere containing my Layers and I can't manage to find it.
- reading the TF code and trying to find any __exit__ or close function which I didn't find

Thanks a lot,

Jonathan D.",0,,1,2018-01-24T15:28:36Z,2018-01-25T07:03:15Z,CONTRIBUTOR,"Hello dear tensorflowers,

I have already asked the question of [StackOverflow](https://stackoverflow.com/questions/48421308/tensorflow-and-unittests-layer-already-defined), however it seams like nobody can answer my question.

So I hope you will forgive me about reposting it here:

I am developing unittests for a product I implemented with TF.

Each part of the model is tested separately then all together in different conditions.

Let's take the example of a simple GAN, I have the following tests:

 - **GeneratorTest** Class: With all tests concerning G inside
 - **DiscriminatorTest** Class: With all tests concerning D inside
 - **GAN_Train_Test** Class: G and D connected all together: 1 training step is tested.
 - **GAN_Inference_Test** Class: G and D connecteed all together: 1 inference run is tested.

------------

When the files are executed independently, everything is working nicely and fine. Tests are all fine.

Problems start occuring when I try to create one file to launch them all from one master file.

**master_test_launcher.py:**

```python
import unittest
import time
    
import tensorflow as tf
    
from tests.test_generator import GeneratorTest
from tests.test_discriminator import DiscriminatorTest
from tests.test_anovae_model import GAN_Train_Test
from tests.test_inference import GAN_Inference_Test
    
runner = unittest.TextTestRunner(verbosity=2)
    
if __name__ == '__main__':
   tf.logging.set_verbosity(tf.logging.DEBUG)
    
    for test in [GeneratorTest, DiscriminatorTest, GAN_Train_Test, GAN_Inference_Test]:
        tf.logging.debug(""Running tests for: %s ..."" % test.__str__())
    
        tf.reset_default_graph()
    
        time.sleep(2)
    
        test_suite = unittest.TestSuite()
        test_suite.addTest(unittest.makeSuite(test))
    
        runner.run(test_suite)
```

I repeatedly obtain the same error when I run the tests related to G and D connected together: 

```python
Exception: Layer 'encoder/input' already exists, please choice other 'name' or reuse this layer
Hint : Use different name for different 'Layer' (The name is used to control parameter sharing)
```

The error is quite simple to understand, each test file is independant and thus create its own session and graph. While testing only G or D, there is no problem because they have different name_scope/variable_scope. However, when testing the whole model *Layers* already have been defined by previous tests and thus leading to issue.

I would like to find a way to completely drop the graph and reset the whole TF state as **brand new and clean**. However, everything I try seem to  fail.

I would like to avoid creating a new graph for each test, leaving the old one in memory (could lead to very high amount of memory waste after a few tests).


So my question is easy: ** How can I reset the whole TF state and internal vars as ""clean"" as if you relaunch a new python shell ? By some black-magic I can't find any way doing it (after looking for it for hours).

For information here are the things I tried and which failed:
- tf.reset_default_graph()
- cleaning everything in graph collections
- creating a new graph + new session before executing each Test File: A graph is still built somewhere containing my Layers and I can't manage to find it.
- reading the TF code and trying to find any __exit__ or close function which I didn't find

Thanks a lot,

Jonathan D.",2018-01-25T07:03:13Z,1,2,0,2.7812664140627286
233,16366,I have an issue with http://projector.tensorflow.org/ always getting stuck,,"Can anyone help me with this issue? I start up the _**Visualizing High-Dimensional Space**_ webstie and it loads until it says something about metadata and never does anything from there. Plese, help. I'm so confused?",0,,1,2018-01-24T14:35:43Z,2018-01-24T17:41:16Z,NONE,"Can anyone help me with this issue? I start up the _**Visualizing High-Dimensional Space**_ webstie and it loads until it says something about metadata and never does anything from there. Plese, help. I'm so confused?",2018-01-24T17:41:16Z,0,1,0,1.7812664140627283
234,16362,`tf.foldl` should have more robust input handling (like `tf.scan`),"stat:contributions welcome,type:feature","### System information
- Windows 10 x64
- Installed from binary
- TensorFlow 1.4.0 (Cpu version)
- Python 3.6.1

### Bug Description
`tf.foldl` (and `tf.foldr`) are conceptually very very similar to `tf.scan`. Therefore the implementations are also very similar. However, `tf.scan` accepts initializer lists or tuples with varying type arguments, while `tf.foldl` does not. I think this is a simple oversight, and it seems that cutting and pasting some code from `tf.scan` to `tf.foldl` fixes this problem. Specifically. the master `tf.foldl `code is (after removing the docstring):

```
def foldl(fn, elems, initializer=None, parallel_iterations=10, back_prop=True,
          swap_memory=False, name=None):
  if not callable(fn):
    raise TypeError(""fn must be callable."")

  with ops.name_scope(name, ""foldl"", [elems]):
    # Any get_variable calls in fn will cache the first call locally
    # and not issue repeated network I/O requests for each iteration.
    varscope = vs.get_variable_scope()
    varscope_caching_device_was_none = False
    if varscope.caching_device is None:
      # TODO(ebrevdo): Change to using colocate_with here and in other methods.
      varscope.set_caching_device(lambda op: op.device)
      varscope_caching_device_was_none = True

    # Convert elems to tensor array.
    elems = ops.convert_to_tensor(elems, name=""elems"")
    n = array_ops.shape(elems)[0]
    elems_ta = tensor_array_ops.TensorArray(dtype=elems.dtype, size=n,
                                            dynamic_size=False,
                                            infer_shape=True)
    elems_ta = elems_ta.unstack(elems)

    if initializer is None:
      a = elems_ta.read(0)
      i = constant_op.constant(1)
    else:
      a = ops.convert_to_tensor(initializer)
      i = constant_op.constant(0)

    def compute(i, a):
      a = fn(a, elems_ta.read(i))
      return [i + 1, a]
    _, r_a = control_flow_ops.while_loop(
        lambda i, a: i < n, compute, [i, a],
        parallel_iterations=parallel_iterations,
        back_prop=back_prop,
        swap_memory=swap_memory)

    if varscope_caching_device_was_none:
      varscope.set_caching_device(None)
    return r_a

```

Modifying the code in the following manner seems to allow non-homgoenous initializer lists (tuples do not work for some reason). Note that you can toggle the mofidication with the ""useModifications"" flag:

```
def foldl(fn, elems, initializer=None, parallel_iterations=10, back_prop=True,
          swap_memory=False, name=None):
    if not callable(fn):
        raise TypeError(""fn must be callable."")

    with ops.name_scope(name, ""foldl"", [elems]):
        # Any get_variable calls in fn will cache the first call locally
        # and not issue repeated network I/O requests for each iteration.
        varscope = vs.get_variable_scope()
        varscope_caching_device_was_none = False
        if varscope.caching_device is None:
            # TODO(ebrevdo): Change to using colocate_with here and in other methods.
            varscope.set_caching_device(lambda op: op.device)
            varscope_caching_device_was_none = True

        # Convert elems to tensor array.
        elems = ops.convert_to_tensor(elems, name=""elems"")
        n = array_ops.shape(elems)[0]
        elems_ta = tensor_array_ops.TensorArray(dtype=elems.dtype, size=n,
                                                dynamic_size=False,
                                                infer_shape=True)
        elems_ta = elems_ta.unstack(elems)

        if initializer is None:
            a = elems_ta.read(0)
            i = constant_op.constant(1)
        else:
            useModifications = True
            if useModifications:
                output_is_sequence = nest.is_sequence(initializer)
                output_flatten = lambda x: nest.flatten(x) if output_is_sequence else [x]
                initializer_flat = output_flatten(initializer)
                a = [ops.convert_to_tensor(init) for init in initializer_flat]
            else:
                a = ops.convert_to_tensor(initializer)

            i = constant_op.constant(0)

        def compute(i, a):
            a = fn(a, elems_ta.read(i))
            return [i + 1, a]

        _, r_a = control_flow_ops.while_loop(
            lambda i, a: i < n, compute, (i, a),
            parallel_iterations=parallel_iterations,
            back_prop=back_prop,
            swap_memory=swap_memory)

        if varscope_caching_device_was_none:
            varscope.set_caching_device(None)
        return r_a
```

Here is a MWE:

```
import tensorflow as tf

a = tf.constant( 1, dtype = tf.float32 )
b = tf.constant( 2, dtype = tf.int64   )

useTuple = False

def body( ab, i ):
    a = ab[0]
    b = ab[1]
    if useTuple:
        return (a,b)
    else:
        return [a,b]

N = 3
with tf.Session() as sess:
    if useTuple:
        ab = (a,b)
    else:
        ab = [a,b]
    print( ""new foldl :"", sess.run(   foldl(  body, tf.range(N), ab ) ) )  
    print( ""tf.scan   :"", sess.run( tf.scan(  body, tf.range(N), ab ) ) )
    print( ""tf.foldl  :"", sess.run( tf.foldl( body, tf.range(N), ab ) ) )
```

with useTuple = False, this returns 

```
new foldl : [1.0, 2]
tf.scan   : [array([1., 1., 1.], dtype=float32), array([2, 2, 2], dtype=int64)]
# Crash for tf.foldl with error: 
TypeError: Cannot convert a list containing a tensor of dtype <dtype: 'int64'> to <dtype: 'float32'> (Tensor is: <tf.Tensor 'Const_5:0' shape=() dtype=int64>)

```",0,,1,2018-01-24T12:58:43Z,2018-01-25T00:37:32Z,CONTRIBUTOR,"### System information
- Windows 10 x64
- Installed from binary
- TensorFlow 1.4.0 (Cpu version)
- Python 3.6.1

### Bug Description
`tf.foldl` (and `tf.foldr`) are conceptually very very similar to `tf.scan`. Therefore the implementations are also very similar. However, `tf.scan` accepts initializer lists or tuples with varying type arguments, while `tf.foldl` does not. I think this is a simple oversight, and it seems that cutting and pasting some code from `tf.scan` to `tf.foldl` fixes this problem. Specifically. the master `tf.foldl `code is (after removing the docstring):

```
def foldl(fn, elems, initializer=None, parallel_iterations=10, back_prop=True,
          swap_memory=False, name=None):
  if not callable(fn):
    raise TypeError(""fn must be callable."")

  with ops.name_scope(name, ""foldl"", [elems]):
    # Any get_variable calls in fn will cache the first call locally
    # and not issue repeated network I/O requests for each iteration.
    varscope = vs.get_variable_scope()
    varscope_caching_device_was_none = False
    if varscope.caching_device is None:
      # TODO(ebrevdo): Change to using colocate_with here and in other methods.
      varscope.set_caching_device(lambda op: op.device)
      varscope_caching_device_was_none = True

    # Convert elems to tensor array.
    elems = ops.convert_to_tensor(elems, name=""elems"")
    n = array_ops.shape(elems)[0]
    elems_ta = tensor_array_ops.TensorArray(dtype=elems.dtype, size=n,
                                            dynamic_size=False,
                                            infer_shape=True)
    elems_ta = elems_ta.unstack(elems)

    if initializer is None:
      a = elems_ta.read(0)
      i = constant_op.constant(1)
    else:
      a = ops.convert_to_tensor(initializer)
      i = constant_op.constant(0)

    def compute(i, a):
      a = fn(a, elems_ta.read(i))
      return [i + 1, a]
    _, r_a = control_flow_ops.while_loop(
        lambda i, a: i < n, compute, [i, a],
        parallel_iterations=parallel_iterations,
        back_prop=back_prop,
        swap_memory=swap_memory)

    if varscope_caching_device_was_none:
      varscope.set_caching_device(None)
    return r_a

```

Modifying the code in the following manner seems to allow non-homgoenous initializer lists (tuples do not work for some reason). Note that you can toggle the mofidication with the ""useModifications"" flag:

```
def foldl(fn, elems, initializer=None, parallel_iterations=10, back_prop=True,
          swap_memory=False, name=None):
    if not callable(fn):
        raise TypeError(""fn must be callable."")

    with ops.name_scope(name, ""foldl"", [elems]):
        # Any get_variable calls in fn will cache the first call locally
        # and not issue repeated network I/O requests for each iteration.
        varscope = vs.get_variable_scope()
        varscope_caching_device_was_none = False
        if varscope.caching_device is None:
            # TODO(ebrevdo): Change to using colocate_with here and in other methods.
            varscope.set_caching_device(lambda op: op.device)
            varscope_caching_device_was_none = True

        # Convert elems to tensor array.
        elems = ops.convert_to_tensor(elems, name=""elems"")
        n = array_ops.shape(elems)[0]
        elems_ta = tensor_array_ops.TensorArray(dtype=elems.dtype, size=n,
                                                dynamic_size=False,
                                                infer_shape=True)
        elems_ta = elems_ta.unstack(elems)

        if initializer is None:
            a = elems_ta.read(0)
            i = constant_op.constant(1)
        else:
            useModifications = True
            if useModifications:
                output_is_sequence = nest.is_sequence(initializer)
                output_flatten = lambda x: nest.flatten(x) if output_is_sequence else [x]
                initializer_flat = output_flatten(initializer)
                a = [ops.convert_to_tensor(init) for init in initializer_flat]
            else:
                a = ops.convert_to_tensor(initializer)

            i = constant_op.constant(0)

        def compute(i, a):
            a = fn(a, elems_ta.read(i))
            return [i + 1, a]

        _, r_a = control_flow_ops.while_loop(
            lambda i, a: i < n, compute, (i, a),
            parallel_iterations=parallel_iterations,
            back_prop=back_prop,
            swap_memory=swap_memory)

        if varscope_caching_device_was_none:
            varscope.set_caching_device(None)
        return r_a
```

Here is a MWE:

```
import tensorflow as tf

a = tf.constant( 1, dtype = tf.float32 )
b = tf.constant( 2, dtype = tf.int64   )

useTuple = False

def body( ab, i ):
    a = ab[0]
    b = ab[1]
    if useTuple:
        return (a,b)
    else:
        return [a,b]

N = 3
with tf.Session() as sess:
    if useTuple:
        ab = (a,b)
    else:
        ab = [a,b]
    print( ""new foldl :"", sess.run(   foldl(  body, tf.range(N), ab ) ) )  
    print( ""tf.scan   :"", sess.run( tf.scan(  body, tf.range(N), ab ) ) )
    print( ""tf.foldl  :"", sess.run( tf.foldl( body, tf.range(N), ab ) ) )
```

with useTuple = False, this returns 

```
new foldl : [1.0, 2]
tf.scan   : [array([1., 1., 1.], dtype=float32), array([2, 2, 2], dtype=int64)]
# Crash for tf.foldl with error: 
TypeError: Cannot convert a list containing a tensor of dtype <dtype: 'int64'> to <dtype: 'float32'> (Tensor is: <tf.Tensor 'Const_5:0' shape=() dtype=int64>)

```",2018-01-25T00:37:32Z,1,2,0,2.7812664140627286
235,16361,Fix typo,"awaiting testing (then merge),cla: yes",fix typo,1,,1,2018-01-24T12:25:33Z,2018-01-24T18:29:36Z,CONTRIBUTOR,fix typo,2018-01-24T17:26:08Z,0,2,0,3.7812664140627286
236,16360,"Python: Make an alias for ""tf.variable"" (with a lower ""v"") so the naming of it is consistent with ""tf.placeholder""/""tf.constant""",type:support,"
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N/A
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: N/A
- **TensorFlow installed from (source or binary)**: N/A
- **TensorFlow version (use command below)**: N/A
- **Python version**:  N/A
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A

### Describe the problem
Many developers learn the naming standards of the software so they can write code faster. It does not make any sense to have to things ""tf.placeholder"" and ""tf.Variable"" named using different schema. Constant, Placeholder and Variable are similar entities and can be used interchangeably. They should be named in same style even if tf.Variable is a class. 

https://www.tensorflow.org/api_docs/python/tf/placeholder
https://www.tensorflow.org/api_docs/python/tf/Variable
https://www.tensorflow.org/api_docs/python/tf/constant

### Source code / logs
N/A
",0,,1,2018-01-24T12:20:59Z,2018-01-25T01:01:53Z,NONE,"
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N/A
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: N/A
- **TensorFlow installed from (source or binary)**: N/A
- **TensorFlow version (use command below)**: N/A
- **Python version**:  N/A
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A

### Describe the problem
Many developers learn the naming standards of the software so they can write code faster. It does not make any sense to have to things ""tf.placeholder"" and ""tf.Variable"" named using different schema. Constant, Placeholder and Variable are similar entities and can be used interchangeably. They should be named in same style even if tf.Variable is a class. 

https://www.tensorflow.org/api_docs/python/tf/placeholder
https://www.tensorflow.org/api_docs/python/tf/Variable
https://www.tensorflow.org/api_docs/python/tf/constant

### Source code / logs
N/A
",2018-01-25T01:01:53Z,1,1,0,1.7812664140627283
237,16359,Return type annotation,"awaiting testing (then merge),cla: yes","Added type annotations in the docstring to the return types of dataset functions.
Presented like this, they can automatically be read by tools (I have tested that this works in PyCharm) to improve auto-completion when coding.

This is really useful in case of datasets, because they often result in long chained calls (something like `Dataset.generate...(...).map(...).repeat(...).batch(...)`). With this patch, code completion works after every `.` (again, I have only tested PyCharm).",1,,1,2018-01-24T11:42:09Z,2018-01-24T19:58:28Z,CONTRIBUTOR,"Added type annotations in the docstring to the return types of dataset functions.
Presented like this, they can automatically be read by tools (I have tested that this works in PyCharm) to improve auto-completion when coding.

This is really useful in case of datasets, because they often result in long chained calls (something like `Dataset.generate...(...).map(...).repeat(...).batch(...)`). With this patch, code completion works after every `.` (again, I have only tested PyCharm).",2018-01-24T17:25:35Z,0,2,0,3.7812664140627286
238,16358,Request for updating keras/datasets files to r1.5,type:bug/performance,"### System information
- **executes Keras sample code imdb_fasttext.py https://github.com/keras-team/keras/blob/master/examples/imdb_fasttext.py**:
- **Windows 7**:
- **TensorFlow installed from binary**:
- **TensorFlow version 1.5.0rc0**:
- **Python version 3.5.1**: 

### Describe the problem
Keras sample program does not work.
 There is a bug for numpy arange method wrong usage.
   (Need to fix from arrange to arange) 
This issue is already solved on master branch. (not in 1.5.0rc1)
Would you update these source codes?

### Source code / logs
Error messages are follows
===
C:\Users\sakaia\work\tensorflow\keras>python imdb_fasttext.py
Loading data...
Traceback (most recent call last):
  File ""imdb_fasttext.py"", line 75, in <module>
    (x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features
)
  File ""C:\Program Files\Python35\lib\site-packages\tensorflow\python\keras\_imp
l\keras\datasets\imdb.py"", line 77, in load_data
    indices = np.arrange(len(x_train))
AttributeError: module 'numpy' has no attribute 'arrange'
===

Following are just checking np.arrange (not np.arange)
>git branch r1.5
>grep -rn np.arrange *
tensorflow/python/keras/_impl/keras/datasets/boston_housing.py:51:  indices = np.arrange(len(x))
tensorflow/python/keras/_impl/keras/datasets/reuters.py:76:  indices = np.arrange(len(xs))
tensorflow/python/keras/_impl/keras/datasets/imdb.py:77:  indices = np.arrange(len(x_train))
tensorflow/python/keras/_impl/keras/datasets/imdb.py:82:  indices = np.arrange(len(x_test))
>git branch -
>grep -rn np.arrange *
(This line is intentionally blank)",0,,5,2018-01-24T10:39:52Z,2018-01-27T06:18:20Z,NONE,"### System information
- **executes Keras sample code imdb_fasttext.py https://github.com/keras-team/keras/blob/master/examples/imdb_fasttext.py**:
- **Windows 7**:
- **TensorFlow installed from binary**:
- **TensorFlow version 1.5.0rc0**:
- **Python version 3.5.1**: 

### Describe the problem
Keras sample program does not work.
 There is a bug for numpy arange method wrong usage.
   (Need to fix from arrange to arange) 
This issue is already solved on master branch. (not in 1.5.0rc1)
Would you update these source codes?

### Source code / logs
Error messages are follows
===
C:\Users\sakaia\work\tensorflow\keras>python imdb_fasttext.py
Loading data...
Traceback (most recent call last):
  File ""imdb_fasttext.py"", line 75, in <module>
    (x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features
)
  File ""C:\Program Files\Python35\lib\site-packages\tensorflow\python\keras\_imp
l\keras\datasets\imdb.py"", line 77, in load_data
    indices = np.arrange(len(x_train))
AttributeError: module 'numpy' has no attribute 'arrange'
===

Following are just checking np.arrange (not np.arange)
>git branch r1.5
>grep -rn np.arrange *
tensorflow/python/keras/_impl/keras/datasets/boston_housing.py:51:  indices = np.arrange(len(x))
tensorflow/python/keras/_impl/keras/datasets/reuters.py:76:  indices = np.arrange(len(xs))
tensorflow/python/keras/_impl/keras/datasets/imdb.py:77:  indices = np.arrange(len(x_train))
tensorflow/python/keras/_impl/keras/datasets/imdb.py:82:  indices = np.arrange(len(x_test))
>git branch -
>grep -rn np.arrange *
(This line is intentionally blank)",2018-01-24T19:56:43Z,3,1,1,3.7812664140627286
239,16357,Increase tolerance in `losses_impl_test.py`. fixes #16238,cla: no,,1,,2,2018-01-24T07:53:22Z,2018-01-31T15:24:35Z,CONTRIBUTOR,,2018-01-24T17:26:57Z,7,2,1,4.281266414062729
240,16355,minor spelling tweaks for eager execution docs,"awaiting testing (then merge),cla: yes",,1,,1,2018-01-24T06:35:52Z,2018-01-24T18:45:52Z,CONTRIBUTOR,,2018-01-24T17:24:04Z,0,2,0,3.7812664140627286
241,16351,TfLiteCameraDemo failed to work with NNAPI after commit e6ff665dbe4888aa5fdff8f34c44405acca2ddd1,comp:lite,"I am testing NNAPI by forcing TfLiteCameraDemo to invoking libneuralnetworks.so. It worked correctly though slower. But since commit e6ff665dbe4888aa5fdff8f34c44405acca2ddd1, TfLiteCameraDemo crashes with error message like,

	01-24 03:39:36.393 19136 19153 E AndroidRuntime: FATAL EXCEPTION: CameraBackground
	01-24 03:39:36.393 19136 19153 E AndroidRuntime: Process: com.example.android.tflitecamerademo, PID: 19136
	01-24 03:39:36.393 19136 19153 E AndroidRuntime: java.lang.IllegalArgumentException: Failed to run on the given Interpreter: NNAPI was requested, but dependent sized tensors being used.
	01-24 03:39:36.393 19136 19153 E AndroidRuntime: 
	01-24 03:39:36.393 19136 19153 E AndroidRuntime: 	at org.tensorflow.lite.NativeInterpreterWrapper.run(Native Method)
	01-24 03:39:36.393 19136 19153 E AndroidRuntime: 	at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:95)
	01-24 03:39:36.393 19136 19153 E AndroidRuntime: 	at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:123)
	01-24 03:39:36.393 19136 19153 E AndroidRuntime: 	at org.tensorflow.lite.Interpreter.run(Interpreter.java:104)
	01-24 03:39:36.393 19136 19153 E AndroidRuntime: 	at com.example.android.tflitecamerademo.ImageClassifier.classifyFrame(ImageClassifier.java:114)
	01-24 03:39:36.393 19136 19153 E AndroidRuntime: 	at com.example.android.tflitecamerademo.Camera2BasicFragment.classifyFrame(Camera2BasicFragment.java:663)
	01-24 03:39:36.393 19136 19153 E AndroidRuntime: 	at com.example.android.tflitecamerademo.Camera2BasicFragment.access$900(Camera2BasicFragment.java:69)
	01-24 03:39:36.393 19136 19153 E AndroidRuntime: 	at com.example.android.tflitecamerademo.Camera2BasicFragment$5.run(Camera2BasicFragment.java:558)
	01-24 03:39:36.393 19136 19153 E AndroidRuntime: 	at android.os.Handler.handleCallback(Handler.java:790)
	01-24 03:39:36.393 19136 19153 E AndroidRuntime: 	at android.os.Handler.dispatchMessage(Handler.java:99)
	01-24 03:39:36.393 19136 19153 E AndroidRuntime: 	at android.os.Looper.loop(Looper.java:164)
	01-24 03:39:36.393 19136 19153 E AndroidRuntime: 	at android.os.HandlerThread.run(HandlerThread.java:65)
	01-24 03:39:36.396   626   871 W ActivityManager:   Force finishing activity com.example.android.tflitecamerademo/.CameraActivity

Here is my patch

	 index e44c5ae..1ed88eb 100644
	 ---a/tensorflow/contrib/lite/java/demo/app/src/main/java/com/example/android/tflitecamerademo/ImageClassifier.java
	 +++b/tensorflow/contrib/lite/java/demo/app/src/main/java/com/example/android/tflitecamerademo/ImageClassifier.java
	 @@ -91,7 +91,7 @@ public class ImageClassifier {
		
	   /** Initializes an {@code ImageClassifier}. */
	   ImageClassifier(Activity activity) throws IOException {
	-    tflite = new Interpreter(loadModelFile(activity));
	+    tflite = new Interpreter(loadModelFile(activity), true);
	     labelList = loadLabelList(activity);
	     imgData =
	         ByteBuffer.allocateDirect(
	diff --git a/tensorflow/contrib/lite/java/demo/build.gradle b/tensorflow/contrib/lite/java/demo/build.gradle
	index dd883d6..9361c71 100644
	--- a/tensorflow/contrib/lite/java/src/main/java/org/tensorflow/lite/Interpreter.java
	+++ b/tensorflow/contrib/lite/java/src/main/java/org/tensorflow/lite/Interpreter.java
	@@ -66,6 +66,13 @@ public final class Interpreter implements AutoCloseable {
	     }
	     wrapper = new NativeInterpreterWrapper(modelFile.getAbsolutePath());
	   }
	+  public Interpreter(@NotNull File modelFile, boolean nn) {
	+    if (modelFile == null) {
	+      return;
	+    }
	+    wrapper = new NativeInterpreterWrapper(modelFile.getAbsolutePath());
	+    wrapper.setUseNNAPI(nn);
	+  }
	
	   /**
	    * Initializes a {@code Interpreter} with a {@code MappedByteBuffer} to the model file.
	@@ -76,6 +83,10 @@ public final class Interpreter implements AutoCloseable {
	   public Interpreter(@NotNull MappedByteBuffer mappedByteBuffer) {
	     wrapper = new NativeInterpreterWrapper(mappedByteBuffer);
	   }
	+  public Interpreter(@NotNull MappedByteBuffer mappedByteBuffer,  boolean nn) {
	+    wrapper = new NativeInterpreterWrapper(mappedByteBuffer);
	+    wrapper.setUseNNAPI(nn);
	+  }
	 
	   /**
	    * Runs model inference if the model takes only one input, and provides only one output.
	   /**
	    * Runs model inference if the model takes only one input, and provides only one output.
",0,,9,2018-01-24T03:55:51Z,2018-01-26T02:12:56Z,NONE,"I am testing NNAPI by forcing TfLiteCameraDemo to invoking libneuralnetworks.so. It worked correctly though slower. But since commit e6ff665dbe4888aa5fdff8f34c44405acca2ddd1, TfLiteCameraDemo crashes with error message like,

	01-24 03:39:36.393 19136 19153 E AndroidRuntime: FATAL EXCEPTION: CameraBackground
	01-24 03:39:36.393 19136 19153 E AndroidRuntime: Process: com.example.android.tflitecamerademo, PID: 19136
	01-24 03:39:36.393 19136 19153 E AndroidRuntime: java.lang.IllegalArgumentException: Failed to run on the given Interpreter: NNAPI was requested, but dependent sized tensors being used.
	01-24 03:39:36.393 19136 19153 E AndroidRuntime: 
	01-24 03:39:36.393 19136 19153 E AndroidRuntime: 	at org.tensorflow.lite.NativeInterpreterWrapper.run(Native Method)
	01-24 03:39:36.393 19136 19153 E AndroidRuntime: 	at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:95)
	01-24 03:39:36.393 19136 19153 E AndroidRuntime: 	at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:123)
	01-24 03:39:36.393 19136 19153 E AndroidRuntime: 	at org.tensorflow.lite.Interpreter.run(Interpreter.java:104)
	01-24 03:39:36.393 19136 19153 E AndroidRuntime: 	at com.example.android.tflitecamerademo.ImageClassifier.classifyFrame(ImageClassifier.java:114)
	01-24 03:39:36.393 19136 19153 E AndroidRuntime: 	at com.example.android.tflitecamerademo.Camera2BasicFragment.classifyFrame(Camera2BasicFragment.java:663)
	01-24 03:39:36.393 19136 19153 E AndroidRuntime: 	at com.example.android.tflitecamerademo.Camera2BasicFragment.access$900(Camera2BasicFragment.java:69)
	01-24 03:39:36.393 19136 19153 E AndroidRuntime: 	at com.example.android.tflitecamerademo.Camera2BasicFragment$5.run(Camera2BasicFragment.java:558)
	01-24 03:39:36.393 19136 19153 E AndroidRuntime: 	at android.os.Handler.handleCallback(Handler.java:790)
	01-24 03:39:36.393 19136 19153 E AndroidRuntime: 	at android.os.Handler.dispatchMessage(Handler.java:99)
	01-24 03:39:36.393 19136 19153 E AndroidRuntime: 	at android.os.Looper.loop(Looper.java:164)
	01-24 03:39:36.393 19136 19153 E AndroidRuntime: 	at android.os.HandlerThread.run(HandlerThread.java:65)
	01-24 03:39:36.396   626   871 W ActivityManager:   Force finishing activity com.example.android.tflitecamerademo/.CameraActivity

Here is my patch

	 index e44c5ae..1ed88eb 100644
	 ---a/tensorflow/contrib/lite/java/demo/app/src/main/java/com/example/android/tflitecamerademo/ImageClassifier.java
	 +++b/tensorflow/contrib/lite/java/demo/app/src/main/java/com/example/android/tflitecamerademo/ImageClassifier.java
	 @@ -91,7 +91,7 @@ public class ImageClassifier {
		
	   /** Initializes an {@code ImageClassifier}. */
	   ImageClassifier(Activity activity) throws IOException {
	-    tflite = new Interpreter(loadModelFile(activity));
	+    tflite = new Interpreter(loadModelFile(activity), true);
	     labelList = loadLabelList(activity);
	     imgData =
	         ByteBuffer.allocateDirect(
	diff --git a/tensorflow/contrib/lite/java/demo/build.gradle b/tensorflow/contrib/lite/java/demo/build.gradle
	index dd883d6..9361c71 100644
	--- a/tensorflow/contrib/lite/java/src/main/java/org/tensorflow/lite/Interpreter.java
	+++ b/tensorflow/contrib/lite/java/src/main/java/org/tensorflow/lite/Interpreter.java
	@@ -66,6 +66,13 @@ public final class Interpreter implements AutoCloseable {
	     }
	     wrapper = new NativeInterpreterWrapper(modelFile.getAbsolutePath());
	   }
	+  public Interpreter(@NotNull File modelFile, boolean nn) {
	+    if (modelFile == null) {
	+      return;
	+    }
	+    wrapper = new NativeInterpreterWrapper(modelFile.getAbsolutePath());
	+    wrapper.setUseNNAPI(nn);
	+  }
	
	   /**
	    * Initializes a {@code Interpreter} with a {@code MappedByteBuffer} to the model file.
	@@ -76,6 +83,10 @@ public final class Interpreter implements AutoCloseable {
	   public Interpreter(@NotNull MappedByteBuffer mappedByteBuffer) {
	     wrapper = new NativeInterpreterWrapper(mappedByteBuffer);
	   }
	+  public Interpreter(@NotNull MappedByteBuffer mappedByteBuffer,  boolean nn) {
	+    wrapper = new NativeInterpreterWrapper(mappedByteBuffer);
	+    wrapper.setUseNNAPI(nn);
	+  }
	 
	   /**
	    * Runs model inference if the model takes only one input, and provides only one output.
	   /**
	    * Runs model inference if the model takes only one input, and provides only one output.
",2018-01-24T12:14:24Z,2,1,0,5.781266414062729
242,16347,Golang API to serialize data into tf.Example protos(tfrecords),stat:awaiting response,"The Golang api [WriteContentsTo](https://godoc.org/github.com/tensorflow/tensorflow/tensorflow/go#Tensor.WriteContentsTo) can be used to writes the serialized contents of a tensor to io.Writer, where the tensor is built from golang scalars, slices, and arrays. 

Yet there's not a Golang API to serialize data into tf.Example protos(tfrecords).

For example, when i want serialize a libsvm into tf.Example protos, i can do this by:
```
def libsvm2tfrecords(data_source, target_dir, delimiter='\t'):
    """"""
    a single file should not contain lines more than 1000,000, or we should use libsvm2proto_par
    :param data_source: libsvm file path
    :param target_dir: dir to storage the serialize proto file
    :param delimiter: delimiter for csv reader
    :return:
    """"""
    if not os.path.isfile(data_source):
        raise ValueError('data file passed do not exist or not a file')

    file_name = os.path.join(target_dir, os.path.splitext(
        os.path.basename(data_source))[0] + '.tfrecords')
    writer = tf.python_io.TFRecordWriter(file_name)
    start = datetime.now()
    line_c = 0
    with open(data_source, 'rb') as rf:
        f_reader = csv.reader(rf, delimiter=delimiter, quotechar='|')
        for row in f_reader:
            line_c += 1
            feature = dict()
            indexes = []
            values = []
            feature.update({'label': _float_feature([float(row[0])])})
            for e in row[1:]:
                index, value = e.split(':')
                indexes.append(int(index))
                values.append(float(value))
                feature.update({'index': _int64_feature(indexes)})
                feature.update({'value': _float_feature(values)})

            example = tf.train.Example(features=tf.train.Features(feature=feature))
            writer.write(example.SerializeToString())

        writer.close()
        end = datetime.now()

        print(""- consumed time: %ds for %s"" % ((end-start).seconds, data_source))
```
BUT Golang api does not seem to be able to achieve this. ",0,,2,2018-01-24T02:53:13Z,2018-01-25T06:51:38Z,NONE,"The Golang api [WriteContentsTo](https://godoc.org/github.com/tensorflow/tensorflow/tensorflow/go#Tensor.WriteContentsTo) can be used to writes the serialized contents of a tensor to io.Writer, where the tensor is built from golang scalars, slices, and arrays. 

Yet there's not a Golang API to serialize data into tf.Example protos(tfrecords).

For example, when i want serialize a libsvm into tf.Example protos, i can do this by:
```
def libsvm2tfrecords(data_source, target_dir, delimiter='\t'):
    """"""
    a single file should not contain lines more than 1000,000, or we should use libsvm2proto_par
    :param data_source: libsvm file path
    :param target_dir: dir to storage the serialize proto file
    :param delimiter: delimiter for csv reader
    :return:
    """"""
    if not os.path.isfile(data_source):
        raise ValueError('data file passed do not exist or not a file')

    file_name = os.path.join(target_dir, os.path.splitext(
        os.path.basename(data_source))[0] + '.tfrecords')
    writer = tf.python_io.TFRecordWriter(file_name)
    start = datetime.now()
    line_c = 0
    with open(data_source, 'rb') as rf:
        f_reader = csv.reader(rf, delimiter=delimiter, quotechar='|')
        for row in f_reader:
            line_c += 1
            feature = dict()
            indexes = []
            values = []
            feature.update({'label': _float_feature([float(row[0])])})
            for e in row[1:]:
                index, value = e.split(':')
                indexes.append(int(index))
                values.append(float(value))
                feature.update({'index': _int64_feature(indexes)})
                feature.update({'value': _float_feature(values)})

            example = tf.train.Example(features=tf.train.Features(feature=feature))
            writer.write(example.SerializeToString())

        writer.close()
        end = datetime.now()

        print(""- consumed time: %ds for %s"" % ((end-start).seconds, data_source))
```
BUT Golang api does not seem to be able to achieve this. ",2018-01-24T19:57:06Z,1,1,0,2.2812664140627286
243,16342,Fixed documentation formatting,cla: no,,0,,3,2018-01-24T01:03:41Z,2018-02-13T00:22:53Z,NONE,,2018-01-24T17:23:16Z,19,1,3,2.7812664140627286
244,16341,"[Bazel/Windows] Don't use -Wl, -lpthread and -lm on Windows","awaiting testing (then merge),cla: yes",,1,,1,2018-01-24T00:48:57Z,2018-01-24T18:37:25Z,CONTRIBUTOR,,2018-01-24T17:29:07Z,0,2,0,3.7812664140627286
245,16339,Remove path_to_str from the public API,"awaiting testing (then merge),cla: yes",@martinwicke fyi,0,,3,2018-01-23T23:48:50Z,2018-01-24T17:20:46Z,OWNER,@martinwicke fyi,2018-01-24T17:20:42Z,1,4,0,5.779055313275624
246,16337,Fix a bug that capture_tpu_profile only takes absolute logdir path.,"awaiting testing (then merge),cla: yes",Also removed package dependancy on tensorflow for better compatibility.,1,,1,2018-01-23T21:23:02Z,2018-01-23T22:42:42Z,CONTRIBUTOR,Also removed package dependancy on tensorflow for better compatibility.,2018-01-23T22:32:09Z,0,2,0,3.7790553132756237
247,16336,Apply final cherry-picks for 1.5.0 release.,"awaiting review,cla: yes",,1,,5,2018-01-23T21:10:12Z,2018-01-24T18:47:16Z,MEMBER,,2018-01-23T22:59:59Z,1,3,0,6.779055313275624
248,16332,Fixes #16314,"awaiting testing (then merge),cla: yes",Fixes #16314.,1,,3,2018-01-23T18:47:02Z,2018-01-23T19:50:27Z,CONTRIBUTOR,Fixes #16314.,2018-01-23T19:26:44Z,0,2,0,4.779055313275624
249,16331,Compilation failure with gcc-6.4 (gcc-7.2 and clang-4) in ubuntu 17.10,stat:awaiting response,"When compiling master in ubuntu 17.10, compilation fails due to 'too perfect forwarding' in variant_op_registry
`
std::unordered_map<std::tuple<VariantXOp, StringPiece, StringPiece>, 
                     VariantXOpFn, TupleHash>
`
A workaround, by replacing tuple with a struct, provided in PR #16309 for your review.

Thanks,
Sami
",0,,2,2018-01-23T17:17:44Z,2018-01-24T16:40:37Z,CONTRIBUTOR,"When compiling master in ubuntu 17.10, compilation fails due to 'too perfect forwarding' in variant_op_registry
`
std::unordered_map<std::tuple<VariantXOp, StringPiece, StringPiece>, 
                     VariantXOpFn, TupleHash>
`
A workaround, by replacing tuple with a struct, provided in PR #16309 for your review.

Thanks,
Sami
",2018-01-24T13:08:45Z,1,2,0,3.2790553132756237
250,16328,Gradient computation across multi-GPU,,"
------------------------

### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.4
- **Python version**: 2.7.6
- **CUDA/cuDNN version**: 8.0/6.0

I am trying to compute global mean and global variance for batch normalization layer across GPUs, both forward and backward should be considered. With `\sigma^2 = mean(x^2) - mean(x)^2`, the gradient w.r.t. each `x` can be computed independently in the GPU that `x` is attached to. 

However, when computing the gradients, I met a problem: without specifying GPU device, `tf.gradient` will use the `\gpu:0`. I cannot specify each operation of gradient computation because the gradients are computed automatically by the `optimizer` and only gradients of parameters are computed.

My question is that if a node is explicitly attached to a GPU device, why the gradient can not be attached to the same GPU device?

I tried this code and get two timeline files [timelines.zip](https://github.com/tensorflow/tensorflow/files/1649923/timelines.zip) and two snapshots bellow.

    import tensorflow as tf
    import numpy as np
    from tensorflow.python.client import timeline
    
    N_SAMPLES = 100000000
    
    
    def all_reduce(gpu_num):
        means = []
        x2s = []
        axs = []
        for i in range(gpu_num):
            with tf.device('/cpu:0'):
                x = tf.placeholder(dtype=tf.float32, shape=[N_SAMPLES], name='local_input_%d' % i)
            with tf.device('/gpu:%d'%i):
                ax = tf.multiply(10.0, x, name='local_multiply_%d'%i)
                mean = tf.reduce_mean(ax, name='local_mean_%d'%i)
                x2 = tf.square(ax, name='local_square_%d'%i)
                axs.append(ax)
                means.append(mean)
                x2s.append(x2)
    
        with tf.device('/gpu:0'):
            global_mean = tf.reduce_mean(means, name='global_mean')
            global_var = tf.subtract(tf.reduce_mean(x2s, name='global_x2'),
                                     tf.square(global_mean, name='global_mean_square'),
                                     name='global_sub')
            print global_var.get_shape()
    
        gs = []
        # manually
        # for i in range(gpu_num):
        #     with tf.device('/gpu:%d'%i):
        #         gradient_wrt_mean = tf.gradients(global_mean, axs[i])
        #         gradient_wrt_var = tf.gradients(global_var, axs[i])
        #         gs.append(gradient_wrt_mean)
        #         gs.append(gradient_wrt_var)
    
        # auto by tf
        gradient_wrt_mean = tf.gradients(global_mean, axs)
        gradient_wrt_var = tf.gradients(global_var, axs)
        gs.append(gradient_wrt_var)
        gs.append(gradient_wrt_mean)
    
        for n in tf.get_default_graph().as_graph_def().node:
            print [n.name, n.device]
    
        return global_mean, global_var, axs, gs
    
    
    def main(_):
        gpu_num = 2
        mean_op, var_op, xs, gs = all_reduce(gpu_num)
        x = np.random.randn(N_SAMPLES*gpu_num)
        print np.mean(x), np.var(x)
        feed_dict = dict()
        for i in range(gpu_num):
            feed_dict[xs[i]] = x[i*N_SAMPLES:(i+1)*N_SAMPLES]
    
        run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)
        run_metadata = tf.RunMetadata()
        gpu_options = tf.GPUOptions(allow_growth=False)
        config = tf.ConfigProto(log_device_placement=False, gpu_options=gpu_options)
        sess = tf.Session(config=config)
    
        # mean, var, g = sess.run([
        #     mean_op, var_op, gs
        # ], feed_dict=feed_dict, options=run_options, run_metadata=run_metadata)
        # print mean, var
    
        g = sess.run([
            gs
        ], feed_dict=feed_dict, options=run_options, run_metadata=run_metadata)
    
        # Create the Timeline object, and write it to a json
        tl = timeline.Timeline(run_metadata.step_stats)
        ctf = tl.generate_chrome_trace_format()
        with open('timeline.json', 'w') as f:
            f.write(ctf)
    
    
    if __name__ == '__main__':
        tf.app.run()

Two figures:
auto, without specifying GPU device.
![image](https://user-images.githubusercontent.com/13829174/35196526-76c1fa20-fed3-11e7-995f-6c63813acc83.png)

manually specifying GPU device.
![image](https://user-images.githubusercontent.com/13829174/35196537-93757eee-fed3-11e7-8df5-986a90a8c0f8.png)

If using `tf.gradient` without specifying GPU devices, only a `tf.reduce_mean` operation is done in `/gpu:1`. So is there some easy way that the operations of gradient computation can be assigned automatically to the corresponded GPU device?


",0,,2,2018-01-23T15:30:55Z,2018-01-24T14:15:40Z,NONE,"
------------------------

### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.4
- **Python version**: 2.7.6
- **CUDA/cuDNN version**: 8.0/6.0

I am trying to compute global mean and global variance for batch normalization layer across GPUs, both forward and backward should be considered. With `\sigma^2 = mean(x^2) - mean(x)^2`, the gradient w.r.t. each `x` can be computed independently in the GPU that `x` is attached to. 

However, when computing the gradients, I met a problem: without specifying GPU device, `tf.gradient` will use the `\gpu:0`. I cannot specify each operation of gradient computation because the gradients are computed automatically by the `optimizer` and only gradients of parameters are computed.

My question is that if a node is explicitly attached to a GPU device, why the gradient can not be attached to the same GPU device?

I tried this code and get two timeline files [timelines.zip](https://github.com/tensorflow/tensorflow/files/1649923/timelines.zip) and two snapshots bellow.

    import tensorflow as tf
    import numpy as np
    from tensorflow.python.client import timeline
    
    N_SAMPLES = 100000000
    
    
    def all_reduce(gpu_num):
        means = []
        x2s = []
        axs = []
        for i in range(gpu_num):
            with tf.device('/cpu:0'):
                x = tf.placeholder(dtype=tf.float32, shape=[N_SAMPLES], name='local_input_%d' % i)
            with tf.device('/gpu:%d'%i):
                ax = tf.multiply(10.0, x, name='local_multiply_%d'%i)
                mean = tf.reduce_mean(ax, name='local_mean_%d'%i)
                x2 = tf.square(ax, name='local_square_%d'%i)
                axs.append(ax)
                means.append(mean)
                x2s.append(x2)
    
        with tf.device('/gpu:0'):
            global_mean = tf.reduce_mean(means, name='global_mean')
            global_var = tf.subtract(tf.reduce_mean(x2s, name='global_x2'),
                                     tf.square(global_mean, name='global_mean_square'),
                                     name='global_sub')
            print global_var.get_shape()
    
        gs = []
        # manually
        # for i in range(gpu_num):
        #     with tf.device('/gpu:%d'%i):
        #         gradient_wrt_mean = tf.gradients(global_mean, axs[i])
        #         gradient_wrt_var = tf.gradients(global_var, axs[i])
        #         gs.append(gradient_wrt_mean)
        #         gs.append(gradient_wrt_var)
    
        # auto by tf
        gradient_wrt_mean = tf.gradients(global_mean, axs)
        gradient_wrt_var = tf.gradients(global_var, axs)
        gs.append(gradient_wrt_var)
        gs.append(gradient_wrt_mean)
    
        for n in tf.get_default_graph().as_graph_def().node:
            print [n.name, n.device]
    
        return global_mean, global_var, axs, gs
    
    
    def main(_):
        gpu_num = 2
        mean_op, var_op, xs, gs = all_reduce(gpu_num)
        x = np.random.randn(N_SAMPLES*gpu_num)
        print np.mean(x), np.var(x)
        feed_dict = dict()
        for i in range(gpu_num):
            feed_dict[xs[i]] = x[i*N_SAMPLES:(i+1)*N_SAMPLES]
    
        run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)
        run_metadata = tf.RunMetadata()
        gpu_options = tf.GPUOptions(allow_growth=False)
        config = tf.ConfigProto(log_device_placement=False, gpu_options=gpu_options)
        sess = tf.Session(config=config)
    
        # mean, var, g = sess.run([
        #     mean_op, var_op, gs
        # ], feed_dict=feed_dict, options=run_options, run_metadata=run_metadata)
        # print mean, var
    
        g = sess.run([
            gs
        ], feed_dict=feed_dict, options=run_options, run_metadata=run_metadata)
    
        # Create the Timeline object, and write it to a json
        tl = timeline.Timeline(run_metadata.step_stats)
        ctf = tl.generate_chrome_trace_format()
        with open('timeline.json', 'w') as f:
            f.write(ctf)
    
    
    if __name__ == '__main__':
        tf.app.run()

Two figures:
auto, without specifying GPU device.
![image](https://user-images.githubusercontent.com/13829174/35196526-76c1fa20-fed3-11e7-995f-6c63813acc83.png)

manually specifying GPU device.
![image](https://user-images.githubusercontent.com/13829174/35196537-93757eee-fed3-11e7-8df5-986a90a8c0f8.png)

If using `tf.gradient` without specifying GPU devices, only a `tf.reduce_mean` operation is done in `/gpu:1`. So is there some easy way that the operations of gradient computation can be assigned automatically to the corresponded GPU device?


",2018-01-23T19:17:33Z,1,1,0,2.2790553132756237
251,16326,Making string values in constant,"awaiting testing (then merge),cla: yes",We already have a `constant.py` in session_bundle since adding these string values as a constant.,1,,3,2018-01-23T15:10:05Z,2018-01-24T19:24:31Z,CONTRIBUTOR,We already have a `constant.py` in session_bundle since adding these string values as a constant.,2018-01-24T17:45:49Z,1,2,0,4.779055313275624
252,16324,Using math_ops instead of defining separate mulop function,"awaiting review,cla: yes",,1,,1,2018-01-23T14:31:41Z,2018-01-23T18:10:16Z,CONTRIBUTOR,,2018-01-23T18:10:31Z,0,2,0,3.7790553132756237
253,16323,Does Broadcast in TF copy first or just do ops along the axis,,"For example, we have
tensor a with shape (100, 100, 5) and tensor b with shape (1, 1, 5)
when running
c = tf.multiply(a, b)

Is b first copied 100 * 100 times for the **big** dot multiply with a (GPU memory consuming),
or the dot multiply is done with the original b along axis 0 and 1?

The tf.multiply page refers to **numpy multiply** that says it won't copy, just loop.
https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html

I guess it's copied first that I run into GPU memory problem by adjusting a bit the b.
How's it implemented in TF? Couldn't find the source gen_math_ops

Issue template update:
Have I written custom code No
OS Platform and Distribution: Windows 10 x64 Home version
TensorFlow installed from pip (anaconda with python 3.6.3)
TensorFlow version: 1.4.1
Bazel version: N/A
CUDA/cuDNN version: CUDA 8.0, cuDNN 6
GPU model and memory: GTX 1050Ti, 4 GB memory (3.3 GB available)
Exact command to reproduce N/A (not relevant to the question)",0,,4,2018-01-23T11:52:14Z,2018-02-09T17:40:02Z,NONE,"For example, we have
tensor a with shape (100, 100, 5) and tensor b with shape (1, 1, 5)
when running
c = tf.multiply(a, b)

Is b first copied 100 * 100 times for the **big** dot multiply with a (GPU memory consuming),
or the dot multiply is done with the original b along axis 0 and 1?

The tf.multiply page refers to **numpy multiply** that says it won't copy, just loop.
https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html

I guess it's copied first that I run into GPU memory problem by adjusting a bit the b.
How's it implemented in TF? Couldn't find the source gen_math_ops

Issue template update:
Have I written custom code No
OS Platform and Distribution: Windows 10 x64 Home version
TensorFlow installed from pip (anaconda with python 3.6.3)
TensorFlow version: 1.4.1
Bazel version: N/A
CUDA/cuDNN version: CUDA 8.0, cuDNN 6
GPU model and memory: GTX 1050Ti, 4 GB memory (3.3 GB available)
Exact command to reproduce N/A (not relevant to the question)",2018-01-24T13:08:36Z,16,1,3,3.2790553132756237
254,16322,py_func convert unicode string results to bytes for python2,"awaiting testing (then merge),cla: yes",Fix #16320,1,,1,2018-01-23T10:34:23Z,2018-02-08T16:54:08Z,CONTRIBUTOR,Fix #16320,2018-02-08T16:53:59Z,15,2,3,3.7790553132756237
255,16321,tpu contrib fix,"awaiting testing (then merge),cla: yes",fix the issue in https://github.com/tensorflow/tensorflow/issues/16262,1,,3,2018-01-23T10:33:16Z,2018-01-23T18:51:28Z,CONTRIBUTOR,fix the issue in https://github.com/tensorflow/tensorflow/issues/16262,2018-01-23T10:37:03Z,0,2,0,4.779055313275624
256,16318,import tensorflow as tf,cla: yes,These five files do not explicitly `import tensorflow as tf` yet they use they use __tf.__ methods or functions which drives linters like pylint and flake8 crazy unless special directives are put in place.,1,,4,2018-01-23T09:51:42Z,2018-01-26T00:31:42Z,CONTRIBUTOR,These five files do not explicitly `import tensorflow as tf` yet they use they use __tf.__ methods or functions which drives linters like pylint and flake8 crazy unless special directives are put in place.,2018-01-25T17:11:48Z,3,2,1,5.279055313275624
257,16316,Lack of clarity in tf.while_loop documentation,type:docs,"I believe that the documentation for tf.while_loop is lacking usage clarity, and actually provides contradictory statements. 

Specifically, it seems that many people are using the tf.while_loop as a ""for loop"" ([see stackoverflow](https://stackoverflow.com/questions/35330117/how-can-i-run-a-loop-with-a-tensor-as-its-range-in-tensorflow)). However, the [tf.while_loop](https://www.tensorflow.org/versions/r0.12/api_docs/python/control_flow_ops/control_flow_operations#while_loop) docs state:

> For correct programs, while_loop should return the same result for any parallel_iterations > 0.

A loop counter inside of the ""while loop"" body, seems to violate this constraint despite the fact that this is given as an example usage in the docs:

> python i = tf.constant(0) c = lambda i: tf.less(i, 10) b = lambda i: tf.add(i, 1) r = tf.while_loop(c, b, [i])

So it seems that there are two bad outcomes here:

1. If this is indeed the canonical way of creating a ""for loop"", then the example explicitly creates a dependency between iterations, meaning that the ""while loop"" iterations cannot be run in parallel. 

1. The example is incorrect? 

It seems like the while_loop docs should have an example which better illustrates how to use it as a ""for loop"", if such usage is indeed intended, or a warning on the implications of the provided example.  
",0,,3,2018-01-23T08:07:11Z,2018-01-27T00:00:02Z,CONTRIBUTOR,"I believe that the documentation for tf.while_loop is lacking usage clarity, and actually provides contradictory statements. 

Specifically, it seems that many people are using the tf.while_loop as a ""for loop"" ([see stackoverflow](https://stackoverflow.com/questions/35330117/how-can-i-run-a-loop-with-a-tensor-as-its-range-in-tensorflow)). However, the [tf.while_loop](https://www.tensorflow.org/versions/r0.12/api_docs/python/control_flow_ops/control_flow_operations#while_loop) docs state:

> For correct programs, while_loop should return the same result for any parallel_iterations > 0.

A loop counter inside of the ""while loop"" body, seems to violate this constraint despite the fact that this is given as an example usage in the docs:

> python i = tf.constant(0) c = lambda i: tf.less(i, 10) b = lambda i: tf.add(i, 1) r = tf.while_loop(c, b, [i])

So it seems that there are two bad outcomes here:

1. If this is indeed the canonical way of creating a ""for loop"", then the example explicitly creates a dependency between iterations, meaning that the ""while loop"" iterations cannot be run in parallel. 

1. The example is incorrect? 

It seems like the while_loop docs should have an example which better illustrates how to use it as a ""for loop"", if such usage is indeed intended, or a warning on the implications of the provided example.  
",2018-01-24T01:26:10Z,4,2,1,3.7790553132756237
258,16315,Remove Variables from a TF Server (e.g.),,"I have a cluster of long-lived TensorFlow servers  (//tensorflow/core/distributed_runtime/rpc/grpc_tensorflow_server).
My problem is how to reset variables on these server. 

There is a behavior in distributed TensorFlow in which a variable defined on a worker (e.g. PS) outlives the session which defines it. I understand this behavior is intentional to support between graph model-replica.

However, In my use case this behavior causes unexpected problem. I have not found a mechanism to override this. It there is, I believe it is helpful to better reflect it in the documentation, if there is not, I hope I can make a case to motivate its existence.

In my use case different training jobs are ran _sequentially_ (i.e. one training job at a time) on this cluster, each using one client (which connects to only one master). 

The problem I have is if a variable is defined in two training job with a same name but different shape sizes, the latter client gets the following error on ""Session"" creation:
```
InvalidArgumentError (see above for traceback): Assign requires shapes of both tensors to match. lhs shape= [100] rhs shape= [200]%0A%09 [[Node: a/Assign = Assign[T=DT_FLOAT, _class=[""loc:@a""], use_locking=true, validate_shape=true, _device=""/job:worker/replica:0/task:0/device:GPU:0""](a, a/Initializer/random_uniform)]]
```

`tf.reset_default_graph` does not help. The solution to this problem could be a mechanism similar `tf.reset_default_graph` that resets variables in all the workers.

To replicate this problem let say we have two workers: (one PS, and on Worker)

Worker 1:
```bash
#worker 1
./bazel-bin/tensorflow/core/distributed_runtime/rpc/grpc_tensorflow_server --cluster_spec=""ps|localhost:2222,worker|localhost:2223"" --job_name=worker --task_id=0 &
#worker 2
./bazel-bin/tensorflow/core/distributed_runtime/rpc/grpc_tensorflow_server --cluster_spec=""ps|localhost:2222,worker|localhost:2223"" --job_name=worker --task_id=0
```
(Same result with `tf.train.Server` workers.)

Then run the simple code:
```python
import tensorflow as tf
var = tf.get_variable(""A"", shape=(100,))
with tf.train.MonitoredTrainingSession(master=""localhost:2223"") as sess:
   pass
```
It should work just fine.
Then when this code (which is identical except the variable shape) is ran:
```python
import tensorflow as tf
var = tf.get_variable(""A"", shape=(5000,))
with tf.train.MonitoredTrainingSession(master=""localhost:2223"") as sess:
   pass
```
This example fails.
",0,,2,2018-01-23T07:33:05Z,2018-01-23T21:29:30Z,CONTRIBUTOR,"I have a cluster of long-lived TensorFlow servers  (//tensorflow/core/distributed_runtime/rpc/grpc_tensorflow_server).
My problem is how to reset variables on these server. 

There is a behavior in distributed TensorFlow in which a variable defined on a worker (e.g. PS) outlives the session which defines it. I understand this behavior is intentional to support between graph model-replica.

However, In my use case this behavior causes unexpected problem. I have not found a mechanism to override this. It there is, I believe it is helpful to better reflect it in the documentation, if there is not, I hope I can make a case to motivate its existence.

In my use case different training jobs are ran _sequentially_ (i.e. one training job at a time) on this cluster, each using one client (which connects to only one master). 

The problem I have is if a variable is defined in two training job with a same name but different shape sizes, the latter client gets the following error on ""Session"" creation:
```
InvalidArgumentError (see above for traceback): Assign requires shapes of both tensors to match. lhs shape= [100] rhs shape= [200]%0A%09 [[Node: a/Assign = Assign[T=DT_FLOAT, _class=[""loc:@a""], use_locking=true, validate_shape=true, _device=""/job:worker/replica:0/task:0/device:GPU:0""](a, a/Initializer/random_uniform)]]
```

`tf.reset_default_graph` does not help. The solution to this problem could be a mechanism similar `tf.reset_default_graph` that resets variables in all the workers.

To replicate this problem let say we have two workers: (one PS, and on Worker)

Worker 1:
```bash
#worker 1
./bazel-bin/tensorflow/core/distributed_runtime/rpc/grpc_tensorflow_server --cluster_spec=""ps|localhost:2222,worker|localhost:2223"" --job_name=worker --task_id=0 &
#worker 2
./bazel-bin/tensorflow/core/distributed_runtime/rpc/grpc_tensorflow_server --cluster_spec=""ps|localhost:2222,worker|localhost:2223"" --job_name=worker --task_id=0
```
(Same result with `tf.train.Server` workers.)

Then run the simple code:
```python
import tensorflow as tf
var = tf.get_variable(""A"", shape=(100,))
with tf.train.MonitoredTrainingSession(master=""localhost:2223"") as sess:
   pass
```
It should work just fine.
Then when this code (which is identical except the variable shape) is ran:
```python
import tensorflow as tf
var = tf.get_variable(""A"", shape=(5000,))
with tf.train.MonitoredTrainingSession(master=""localhost:2223"") as sess:
   pass
```
This example fails.
",2018-01-23T07:33:40Z,0,2,0,3.2790553132756237
259,16314,Published libtensorflow_framework.so binaries ABI Problem,,"The distributed `libtensorflow_framework.so` included in the JAR files published to Maven are built using the C++ 11 ABI, in contrast to the main TF build. I think that affects all continuous integration builds of the shared objects. I believe the `-D_GLIBCXX_USE_CXX11_ABI=0` compiler flag should be used for the CI builds as is done for the main build. An example of its use is shown in commit 550df413158b32645ca5df4dcaabc67f1a48964d. This causes some trouble when using these shared objects and developing custom ops, as those are required to be built using that compiler flag. It would be great if the use of the flag was consistent and all binaries were built using the same ABI.

Thanks! ",0,,1,2018-01-23T06:57:28Z,2018-01-23T19:50:27Z,CONTRIBUTOR,"The distributed `libtensorflow_framework.so` included in the JAR files published to Maven are built using the C++ 11 ABI, in contrast to the main TF build. I think that affects all continuous integration builds of the shared objects. I believe the `-D_GLIBCXX_USE_CXX11_ABI=0` compiler flag should be used for the CI builds as is done for the main build. An example of its use is shown in commit 550df413158b32645ca5df4dcaabc67f1a48964d. This causes some trouble when using these shared objects and developing custom ops, as those are required to be built using that compiler flag. It would be great if the use of the flag was consistent and all binaries were built using the same ABI.

Thanks! ",2018-01-23T18:47:30Z,0,2,0,2.7790553132756237
260,16313,Bug of tf.data.TFRecordDataset? Couldn't use tf.reshape after the operations of tf.data.TFRecordDataset,,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.4.1
- **Python version**: 2.7.12
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: 8.0/6.0
- **GPU model and memory**: Nvidia GeForce GTX TITAN X 12GB
- **Exact command to reproduce**:


### Describe the problem
I want to use  function **tf.profiler.ProfileOptionBuilder.float_operation** to show the flops of the model. But it need a certain input shape while the the output shape of **tf.data.TFrecordDataset** is like (?, 32,32,3). When I want to use tf.reshape to reshape the output of **tf.data.TFrecordDataset**, it generates an error ""Input to reshape is a tensor with 64512 values, but the requested shape has 98304"".  

### Source code 

  def dataset_input(self, dataset_type):
    with tf.variable_scope(""batch_"" + dataset_type):
        def parser(record):
            features = tf.parse_single_example(
                record,
                features={
                    'image': tf.FixedLenFeature([], tf.string),
                    'label': tf.FixedLenFeature([], tf.int64)
                })
            image, label = features['image'], features['label']
            height, width, channels = self.input_size, self.input_size, self.input_dim
            image = tf.decode_raw(image, tf.uint8)
            image = tf.reshape(image, [height, width, channels])
            return image, label
        dataset = tf.data.TFRecordDataset([self.dataset_dir[dataset_type]])
        dataset = dataset.map(parser)
        dataset = dataset.shuffle(buffer_size=50000)
        dataset = dataset.batch(self.batch_size)
        dataset = dataset.repeat()
        iterator = dataset.make_one_shot_iterator()
        features, labels = iterator.get_next()
        features = tf.reshape(features, [self.batch_size, self.input_size, self.input_size, self.input_dim])
        return features, labels
  ",0,,4,2018-01-23T06:26:49Z,2018-01-23T22:45:02Z,NONE,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.4.1
- **Python version**: 2.7.12
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: 8.0/6.0
- **GPU model and memory**: Nvidia GeForce GTX TITAN X 12GB
- **Exact command to reproduce**:


### Describe the problem
I want to use  function **tf.profiler.ProfileOptionBuilder.float_operation** to show the flops of the model. But it need a certain input shape while the the output shape of **tf.data.TFrecordDataset** is like (?, 32,32,3). When I want to use tf.reshape to reshape the output of **tf.data.TFrecordDataset**, it generates an error ""Input to reshape is a tensor with 64512 values, but the requested shape has 98304"".  

### Source code 

  def dataset_input(self, dataset_type):
    with tf.variable_scope(""batch_"" + dataset_type):
        def parser(record):
            features = tf.parse_single_example(
                record,
                features={
                    'image': tf.FixedLenFeature([], tf.string),
                    'label': tf.FixedLenFeature([], tf.int64)
                })
            image, label = features['image'], features['label']
            height, width, channels = self.input_size, self.input_size, self.input_dim
            image = tf.decode_raw(image, tf.uint8)
            image = tf.reshape(image, [height, width, channels])
            return image, label
        dataset = tf.data.TFRecordDataset([self.dataset_dir[dataset_type]])
        dataset = dataset.map(parser)
        dataset = dataset.shuffle(buffer_size=50000)
        dataset = dataset.batch(self.batch_size)
        dataset = dataset.repeat()
        iterator = dataset.make_one_shot_iterator()
        features, labels = iterator.get_next()
        features = tf.reshape(features, [self.batch_size, self.input_size, self.input_size, self.input_dim])
        return features, labels
  ",2018-01-23T22:45:02Z,0,1,0,3.2790553132756237
261,16312,Allow step callback for scipy SLSQP,"awaiting testing (then merge),cla: yes,stat:awaiting response",This simple fix allows `SLSQP` method of scipy optimizer to use step callback as reported in issue [#16294](https://github.com/tensorflow/tensorflow/issues/16294). ,1,,2,2018-01-23T03:56:13Z,2018-01-29T21:36:18Z,CONTRIBUTOR,This simple fix allows `SLSQP` method of scipy optimizer to use step callback as reported in issue [#16294](https://github.com/tensorflow/tensorflow/issues/16294). ,2018-01-28T20:23:04Z,6,2,1,4.279055313275624
262,16309,Workaround 'too perfect forwarding' issue in variant_op_registry,"awaiting testing (then merge),cla: yes","In variant_op_registry.h 
```
std::unordered_map<std::tuple<VariantXOp, StringPiece, StringPiece>, 
                     VariantXOpFn, TupleHash>
```
seems to be falling victim to 'too perfect forwarding' issue ( [SO link](https://stackoverflow.com/questions/44475317/variadic-template-issue), [Andrzej's blog](https://akrzemi1.wordpress.com/2013/10/10/too-perfect-forwarding/)) with gcc-6.4, gcc-7.2 and clang-4 in ubuntu-17.10 (and possibly others). This PR works around the issue by replacing std::tuple with a simple struct.",1,,3,2018-01-23T03:08:35Z,2018-01-23T23:13:15Z,CONTRIBUTOR,"In variant_op_registry.h 
```
std::unordered_map<std::tuple<VariantXOp, StringPiece, StringPiece>, 
                     VariantXOpFn, TupleHash>
```
seems to be falling victim to 'too perfect forwarding' issue ( [SO link](https://stackoverflow.com/questions/44475317/variadic-template-issue), [Andrzej's blog](https://akrzemi1.wordpress.com/2013/10/10/too-perfect-forwarding/)) with gcc-6.4, gcc-7.2 and clang-4 in ubuntu-17.10 (and possibly others). This PR works around the issue by replacing std::tuple with a simple struct.",2018-01-23T17:46:51Z,0,2,0,4.779055313275624
263,16307,Fix Conv3DTranspose in tf.keras,"awaiting testing (then merge),cla: yes",,1,,6,2018-01-23T02:53:57Z,2018-01-25T17:01:38Z,CONTRIBUTOR,,2018-01-23T02:59:05Z,2,2,0,6.279055313275624
264,16303,Don't load libcupti.so from regular path on Android,"awaiting testing (then merge),cla: yes,kokoro:run","Open to alternatives, but as other methods in this file work similarly this doesn't seem too bad.",1,,11,2018-01-23T00:52:20Z,2018-01-25T19:56:23Z,MEMBER,"Open to alternatives, but as other methods in this file work similarly this doesn't seem too bad.",2018-01-23T21:44:33Z,2,3,0,9.779055313275624
265,16298,Bug of tf.data.TFRecordDataset? or my codes wrong?,,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Windows 10
- **TensorFlow installed from (source or binary)**:  binary
- **TensorFlow version (use command below)**:  1.4
- **Python version**: 3.6
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: 8.0/6.0
- **GPU model and memory**: Nvidia Quadro K4000
- **Exact command to reproduce**: 


I tested to write dynamic numbers of variables into tfrecord. But when I use the tf.data.TFRecordDataset to read VarLenFeature, the program crashes. However, if I do not use dataset, but just tf.python_io.tf_record_iterator. The program works without problem. I wonder whether this is a bug of tf.data.TFRecordDataset, or there is something wrong in my codes?

My writing codes are 

    def test_write():
      writer = tf.python_io.TFRecordWriter('test.tfrecord')

      for i in range(3):
        val_list = []
        for j in range(i+1):
          val_list.append(i+j)
        feature_dict = {
          'val': tf.train.Feature(int64_list=tf.train.Int64List(value=val_list)),
        }
    
        example = tf.train.Example(features=tf.train.Features(feature=feature_dict))
        writer.write(example.SerializeToString())

      writer.close()

The reading codes using tf.data.TFRecordDataset and causing error are

	def parse_test(example):
	  features = {
		'val': tf.VarLenFeature(dtype=tf.int64)
	  }
	  parsed_features = tf.parse_single_example(example, features)

	  return parsed_features

	def test_read():
	  dataset = tf.data.TFRecordDataset(['test.tfrecord'])
	  dataset = dataset.map(parse_test)
	  dataset = dataset.batch(1)

	  iterator = dataset.make_one_shot_iterator()
	  feature_dict =  iterator.get_next()

	  with tf.Session() as sess:
		for _ in range(3):
		  curr_dict = sess.run(feature_dict)
		  print([curr_dict['val']])

The error message is:

	TypeError: Failed to convert object of type <class 'tensorflow.python.framework.sparse_tensor.SparseTensor'> to Tensor. Contents: SparseTensor(indices=Tensor(""ParseSingleExample/Slice_Indices_val:0"", shape=(?, 1), dtype=int64), values=Tensor(""ParseSingleExample/ParseExample/ParseExample:1"", shape=(?,), dtype=int64), dense_shape=Tensor(""ParseSingleExample/Squeeze_Shape_val:0"", shape=(1,), dtype=int64)). Consider casting elements to a supported type.


 The successful reading codes without using tf.data.TFRecordDataset are as below

	def test_read2():
	  with tf.Session() as sess:
		for serialized_example in tf.python_io.tf_record_iterator('test.tfrecord'):
		  features = tf.parse_single_example(serialized_example,
			features={
			  'val': tf.VarLenFeature(dtype=tf.int64),
			}
		  )

		  temp = features['val']

		  values = sess.run(temp)
		  print(values)

This code successfully print out

	SparseTensorValue(indices=array([[0]], dtype=int64), values=array([0], dtype=int64), dense_shape=array([1], dtype=int64))
	SparseTensorValue(indices=array([[0],
		   [1]], dtype=int64), values=array([1, 2], dtype=int64), dense_shape=array([2], dtype=int64))
	SparseTensorValue(indices=array([[0],
		   [1],
		   [2]], dtype=int64), values=array([2, 3, 4], dtype=int64), dense_shape=array([3], dtype=int64))

However, I am still hoping to use the dataset structure to deal with the VarLenFeature. Is there anything wrong with my reading codes or there is a bug in tf.data.TFRecordDataset? Thank you.

",0,,1,2018-01-22T19:48:46Z,2018-01-23T22:48:02Z,NONE,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Windows 10
- **TensorFlow installed from (source or binary)**:  binary
- **TensorFlow version (use command below)**:  1.4
- **Python version**: 3.6
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: 8.0/6.0
- **GPU model and memory**: Nvidia Quadro K4000
- **Exact command to reproduce**: 


I tested to write dynamic numbers of variables into tfrecord. But when I use the tf.data.TFRecordDataset to read VarLenFeature, the program crashes. However, if I do not use dataset, but just tf.python_io.tf_record_iterator. The program works without problem. I wonder whether this is a bug of tf.data.TFRecordDataset, or there is something wrong in my codes?

My writing codes are 

    def test_write():
      writer = tf.python_io.TFRecordWriter('test.tfrecord')

      for i in range(3):
        val_list = []
        for j in range(i+1):
          val_list.append(i+j)
        feature_dict = {
          'val': tf.train.Feature(int64_list=tf.train.Int64List(value=val_list)),
        }
    
        example = tf.train.Example(features=tf.train.Features(feature=feature_dict))
        writer.write(example.SerializeToString())

      writer.close()

The reading codes using tf.data.TFRecordDataset and causing error are

	def parse_test(example):
	  features = {
		'val': tf.VarLenFeature(dtype=tf.int64)
	  }
	  parsed_features = tf.parse_single_example(example, features)

	  return parsed_features

	def test_read():
	  dataset = tf.data.TFRecordDataset(['test.tfrecord'])
	  dataset = dataset.map(parse_test)
	  dataset = dataset.batch(1)

	  iterator = dataset.make_one_shot_iterator()
	  feature_dict =  iterator.get_next()

	  with tf.Session() as sess:
		for _ in range(3):
		  curr_dict = sess.run(feature_dict)
		  print([curr_dict['val']])

The error message is:

	TypeError: Failed to convert object of type <class 'tensorflow.python.framework.sparse_tensor.SparseTensor'> to Tensor. Contents: SparseTensor(indices=Tensor(""ParseSingleExample/Slice_Indices_val:0"", shape=(?, 1), dtype=int64), values=Tensor(""ParseSingleExample/ParseExample/ParseExample:1"", shape=(?,), dtype=int64), dense_shape=Tensor(""ParseSingleExample/Squeeze_Shape_val:0"", shape=(1,), dtype=int64)). Consider casting elements to a supported type.


 The successful reading codes without using tf.data.TFRecordDataset are as below

	def test_read2():
	  with tf.Session() as sess:
		for serialized_example in tf.python_io.tf_record_iterator('test.tfrecord'):
		  features = tf.parse_single_example(serialized_example,
			features={
			  'val': tf.VarLenFeature(dtype=tf.int64),
			}
		  )

		  temp = features['val']

		  values = sess.run(temp)
		  print(values)

This code successfully print out

	SparseTensorValue(indices=array([[0]], dtype=int64), values=array([0], dtype=int64), dense_shape=array([1], dtype=int64))
	SparseTensorValue(indices=array([[0],
		   [1]], dtype=int64), values=array([1, 2], dtype=int64), dense_shape=array([2], dtype=int64))
	SparseTensorValue(indices=array([[0],
		   [1],
		   [2]], dtype=int64), values=array([2, 3, 4], dtype=int64), dense_shape=array([3], dtype=int64))

However, I am still hoping to use the dataset structure to deal with the VarLenFeature. Is there anything wrong with my reading codes or there is a bug in tf.data.TFRecordDataset? Thank you.

",2018-01-23T22:48:02Z,1,1,0,1.7769378934088573
266,16294,ScipyOptimizer SLSQP supporting callback,"stat:contributions welcome,type:bug/performance","The callback is deprecated when `SLSQP` method in scipy optimizer is selected (see [here](https://github.com/tensorflow/tensorflow/blob/04b5c75aae4bdbdac7c713714a369f9b360daf70/tensorflow/contrib/opt/python/training/external_optimizer.py#L400)). Actually, `SLSQP` does support callback, so 
```python 
if method == 'SLSQP':
  # SLSQP doesn't support step callbacks. Obviate associated warning
  # message.
  del minimize_kwargs['callback']
```
in the above linked file could be removed. 

The following example shows that `SLSQP` do support callback. 

```python 
from scipy.optimize import minimize, rosen, rosen_der

def callback(xk, step=[0]):
  print step[0], xk[0]
  step[0] += 1
  
x0 = [1.3, 0.7, 0.8, 1.9, 1.2]
res = minimize(rosen, x0, callback=callback, method='SLSQP',
    options={'ftol': 1e-6, 'disp': True})
 
print res.x[0]
```






",0,,3,2018-01-22T18:49:22Z,2018-01-23T05:37:22Z,CONTRIBUTOR,"The callback is deprecated when `SLSQP` method in scipy optimizer is selected (see [here](https://github.com/tensorflow/tensorflow/blob/04b5c75aae4bdbdac7c713714a369f9b360daf70/tensorflow/contrib/opt/python/training/external_optimizer.py#L400)). Actually, `SLSQP` does support callback, so 
```python 
if method == 'SLSQP':
  # SLSQP doesn't support step callbacks. Obviate associated warning
  # message.
  del minimize_kwargs['callback']
```
in the above linked file could be removed. 

The following example shows that `SLSQP` do support callback. 

```python 
from scipy.optimize import minimize, rosen, rosen_der

def callback(xk, step=[0]):
  print step[0], xk[0]
  step[0] += 1
  
x0 = [1.3, 0.7, 0.8, 1.9, 1.2]
res = minimize(rosen, x0, callback=callback, method='SLSQP',
    options={'ftol': 1e-6, 'disp': True})
 
print res.x[0]
```






",2018-01-23T02:15:44Z,1,2,0,3.7769378934088573
267,16288,Tensorflow works in command prompt but not in Spyder,type:build/install,"Hello.
I'm new to Python so maybe I've missed something but anyway, here is my problem.
I've installed tensorflow in Anaconda prompt by using 
```
C:\WINDOWS\system32> conda create -n tensorflow python=3.6
C:\WINDOWS\system32> activate tensorflow
(tensorflow)C:\WINDOWS\system32> pip install --ignore-installed --upgrade tensorflow
```
Instalation was succesful, then I opened python and tried to import tensorflow to verify installation
```
(base) C:\WINDOWS\system32>python
Python 3.6.3 |Anaconda custom (64-bit)| (default, Oct 15 2017, 03:27:45) [MSC v.1900 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
ModuleNotFoundError: No module named 'tensorflow'
```

After activating tensorflow, it works.
```
(tensorflow) C:\WINDOWS\system32>python
Python 3.6.4 |Anaconda, Inc.| (default, Jan 16 2018, 10:22:32) [MSC v.1900 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow
>>>
```
But I cannot find a way to make it work in Spyder IDE. I always get error:
```ModuleNotFoundError: No module named 'tensorflow'```",0,,5,2018-01-22T14:47:27Z,2018-01-25T19:40:16Z,NONE,"Hello.
I'm new to Python so maybe I've missed something but anyway, here is my problem.
I've installed tensorflow in Anaconda prompt by using 
```
C:\WINDOWS\system32> conda create -n tensorflow python=3.6
C:\WINDOWS\system32> activate tensorflow
(tensorflow)C:\WINDOWS\system32> pip install --ignore-installed --upgrade tensorflow
```
Instalation was succesful, then I opened python and tried to import tensorflow to verify installation
```
(base) C:\WINDOWS\system32>python
Python 3.6.3 |Anaconda custom (64-bit)| (default, Oct 15 2017, 03:27:45) [MSC v.1900 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
ModuleNotFoundError: No module named 'tensorflow'
```

After activating tensorflow, it works.
```
(tensorflow) C:\WINDOWS\system32>python
Python 3.6.4 |Anaconda, Inc.| (default, Jan 16 2018, 10:22:32) [MSC v.1900 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow
>>>
```
But I cannot find a way to make it work in Spyder IDE. I always get error:
```ModuleNotFoundError: No module named 'tensorflow'```",2018-01-22T17:36:48Z,3,1,1,3.7769378934088573
268,16287,[Bug] LuongMonotonicAttention in contrib/seq2seq/python/ops/attention_wrapper.py,stat:awaiting tensorflower,"`LuongMonotonicAttention.__init__(...)` calls its parent `_BaseAttentionMechanism` with `query_layer` as follows:
```
        query_layer=layers_core.Dense(
            num_units, name=""query_layer"", use_bias=False),
```
But, it doesn't apply it on query in `LuongMonotonicAttention.__call__(...)`.
```
  def __call__(self, query, previous_alignments):
    """"""...
    """"""
    with variable_scope.variable_scope(None, ""luong_monotonic_attention"",
                                       [query]):
      score = _luong_score(query, self._keys, self._scale)
      score_bias = variable_scope.get_variable(
          ""attention_score_bias"", dtype=query.dtype,
          initializer=self._score_bias_init)
      score += score_bias
    alignments = self._probability_fn(score, previous_alignments)
    return alignments
```
Guessing from the way `LuongAttention` works, there should be `query_layer=None` in `LuongMonotonicAttention.__init__(...)`.",0,,3,2018-01-22T13:50:55Z,2018-02-01T03:00:34Z,NONE,"`LuongMonotonicAttention.__init__(...)` calls its parent `_BaseAttentionMechanism` with `query_layer` as follows:
```
        query_layer=layers_core.Dense(
            num_units, name=""query_layer"", use_bias=False),
```
But, it doesn't apply it on query in `LuongMonotonicAttention.__call__(...)`.
```
  def __call__(self, query, previous_alignments):
    """"""...
    """"""
    with variable_scope.variable_scope(None, ""luong_monotonic_attention"",
                                       [query]):
      score = _luong_score(query, self._keys, self._scale)
      score_bias = variable_scope.get_variable(
          ""attention_score_bias"", dtype=query.dtype,
          initializer=self._score_bias_init)
      score += score_bias
    alignments = self._probability_fn(score, previous_alignments)
    return alignments
```
Guessing from the way `LuongAttention` works, there should be `query_layer=None` in `LuongMonotonicAttention.__init__(...)`.",2018-01-26T22:10:26Z,9,1,2,2.7769378934088573
269,16286,Removes redundant variable assignment,"awaiting testing (then merge),cla: yes","Addresses alert raised by lgtm.com:
https://lgtm.com/projects/g/tensorflow/tensorflow/snapshot/e6183fbeecf069148371be83988e8e5db2b14185/files/tensorflow/python/framework/constant_op.py#xb77a2f6647d782be:1

It doesn't seem like assigning `attr_tshape = attr_tshape` does anything, so there's no need to keep it in.",1,,5,2018-01-22T13:26:57Z,2018-01-22T20:55:02Z,CONTRIBUTOR,"Addresses alert raised by lgtm.com:
https://lgtm.com/projects/g/tensorflow/tensorflow/snapshot/e6183fbeecf069148371be83988e8e5db2b14185/files/tensorflow/python/framework/constant_op.py#xb77a2f6647d782be:1

It doesn't seem like assigning `attr_tshape = attr_tshape` does anything, so there's no need to keep it in.",2018-01-22T13:28:34Z,0,2,0,5.776937893408857
270,16281,"After Working with Tensorflow cpu version for 2 days, it gave me an error on installation today","stat:awaiting response,type:build/install","I had installed and used Tensorflow successfully but today when I opened my computer it gave me this error message:
Traceback (most recent call last):
  File ""C:\Users\asus\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 18, in swig_import_helper
    fp, pathname, description = imp.find_module('_pywrap_tensorflow', [dirname(__file__)])
  File ""C:\Users\asus\AppData\Local\Programs\Python\Python36-32\lib\imp.py"", line 297, in find_module
    raise ImportError(_ERR_MSG.format(name), name=name)
ImportError: No module named '_pywrap_tensorflow'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\asus\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\python\__init__.py"", line 54, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\asus\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 28, in <module>
    _pywrap_tensorflow = swig_import_helper()
  File ""C:\Users\asus\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 20, in swig_import_helper
    import _pywrap_tensorflow
ModuleNotFoundError: No module named '_pywrap_tensorflow'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:/Users/asus/PycharmProjects/untitled3/CNNCIFARTFNEW.py"", line 2, in <module>
    import tensorflow as tf
  File ""C:\Users\asus\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""C:\Users\asus\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\python\__init__.py"", line 60, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\asus\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 18, in swig_import_helper
    fp, pathname, description = imp.find_module('_pywrap_tensorflow', [dirname(__file__)])
  File ""C:\Users\asus\AppData\Local\Programs\Python\Python36-32\lib\imp.py"", line 297, in find_module
    raise ImportError(_ERR_MSG.format(name), name=name)
ImportError: No module named '_pywrap_tensorflow'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\asus\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\python\__init__.py"", line 54, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\asus\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 28, in <module>
    _pywrap_tensorflow = swig_import_helper()
  File ""C:\Users\asus\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 20, in swig_import_helper
    import _pywrap_tensorflow
ModuleNotFoundError: No module named '_pywrap_tensorflow'


Error importing tensorflow.  Unless you are using bazel,
you should not try to import tensorflow from its source directory;
please exit the tensorflow source tree, and relaunch your python interpreter
from there.


Please help. This is urgent.",0,,2,2018-01-22T07:38:03Z,2018-01-23T04:58:10Z,NONE,"I had installed and used Tensorflow successfully but today when I opened my computer it gave me this error message:
Traceback (most recent call last):
  File ""C:\Users\asus\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 18, in swig_import_helper
    fp, pathname, description = imp.find_module('_pywrap_tensorflow', [dirname(__file__)])
  File ""C:\Users\asus\AppData\Local\Programs\Python\Python36-32\lib\imp.py"", line 297, in find_module
    raise ImportError(_ERR_MSG.format(name), name=name)
ImportError: No module named '_pywrap_tensorflow'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\asus\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\python\__init__.py"", line 54, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\asus\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 28, in <module>
    _pywrap_tensorflow = swig_import_helper()
  File ""C:\Users\asus\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 20, in swig_import_helper
    import _pywrap_tensorflow
ModuleNotFoundError: No module named '_pywrap_tensorflow'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:/Users/asus/PycharmProjects/untitled3/CNNCIFARTFNEW.py"", line 2, in <module>
    import tensorflow as tf
  File ""C:\Users\asus\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""C:\Users\asus\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\python\__init__.py"", line 60, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\asus\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 18, in swig_import_helper
    fp, pathname, description = imp.find_module('_pywrap_tensorflow', [dirname(__file__)])
  File ""C:\Users\asus\AppData\Local\Programs\Python\Python36-32\lib\imp.py"", line 297, in find_module
    raise ImportError(_ERR_MSG.format(name), name=name)
ImportError: No module named '_pywrap_tensorflow'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\asus\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\python\__init__.py"", line 54, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\asus\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 28, in <module>
    _pywrap_tensorflow = swig_import_helper()
  File ""C:\Users\asus\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 20, in swig_import_helper
    import _pywrap_tensorflow
ModuleNotFoundError: No module named '_pywrap_tensorflow'


Error importing tensorflow.  Unless you are using bazel,
you should not try to import tensorflow from its source directory;
please exit the tensorflow source tree, and relaunch your python interpreter
from there.


Please help. This is urgent.",2018-01-22T19:38:09Z,1,1,0,2.2769378934088573
271,16280,Repair compilation error of tensorflow built with MKL-DNN,"cla: yes,stat:awaiting response","When we compile tensorflow with Intel **MKL-DNN**, it will meet a failure:

`bazel build --copt -O3 --copt=-DINTEL_MKL_DNN --config=mkl -c opt //tensorflow/tools/pip_package:build_pip_package`

**error: 'mkldnn::algorithm' is not a namespace
  using mkldnn::algorithm::lrn_across_channels;**

 Removing the 'algorithm' field in _tensorflow/core/kernels/mkl_lrn_op.cc_ can solve this problem and lead to successful compilation.",1,,7,2018-01-22T07:27:10Z,2018-01-23T07:02:18Z,CONTRIBUTOR,"When we compile tensorflow with Intel **MKL-DNN**, it will meet a failure:

`bazel build --copt -O3 --copt=-DINTEL_MKL_DNN --config=mkl -c opt //tensorflow/tools/pip_package:build_pip_package`

**error: 'mkldnn::algorithm' is not a namespace
  using mkldnn::algorithm::lrn_across_channels;**

 Removing the 'algorithm' field in _tensorflow/core/kernels/mkl_lrn_op.cc_ can solve this problem and lead to successful compilation.",2018-01-22T07:56:56Z,1,2,0,6.776937893408857
272,16278,__init__() got multiple values for argument 'strides',type:support,"    model = Sequential()
    model.add(ZeroPadding2D((1, 1), input_shape=(img_width, img_height, 3)))
    print(model.output_shape)
    model.add(Convolution2D(64, 3, 3, strides=(
        1, 1), activation='relu', name='conv1_1'))
above is my code, I got error:
__init__() got multiple values for argument 'strides'
If i don't use 'strides', it's fine. but the stride is 3. How should I set strides?",0,,2,2018-01-22T06:57:46Z,2018-01-25T01:13:49Z,NONE,"    model = Sequential()
    model.add(ZeroPadding2D((1, 1), input_shape=(img_width, img_height, 3)))
    print(model.output_shape)
    model.add(Convolution2D(64, 3, 3, strides=(
        1, 1), activation='relu', name='conv1_1'))
above is my code, I got error:
__init__() got multiple values for argument 'strides'
If i don't use 'strides', it's fine. but the stride is 3. How should I set strides?",2018-01-25T01:13:49Z,3,1,1,2.2769378934088573
273,16277,Control dependency does not ensure write observed by read,,"TF version 1.3.0

```python
def sleep(t):
    '''TF sleep'''
    import time
    def f(t):
        time.sleep(t)
        return np.array([], dtype=np.float32)
    return tf.py_func(f, [t], [tf.float32])[0]

with tf.device('gpu'):
    x = tf.Variable(0.)
with tf.control_dependencies([tf.identity(sleep(0.1))]):
    with tf.device('gpu'):
        mod = tf.assign(x, 100.)
with tf.device('cpu'):
    a = x+1.
    with tf.control_dependencies([tf.identity(mod)]):
        b = x+2.
        with tf.device('gpu'):
            c = x+3.

x.initializer.run()
sess.run([a, b, c])
# [1.0, 2.0, 103.0]
```

When a variable is read on another device, TF seems to copy once regardless of dependencies. I understand this is how TF works, but I think it would be nice to have dependencies ensure memory access order.",0,,2,2018-01-22T06:22:40Z,2018-01-26T21:40:10Z,CONTRIBUTOR,"TF version 1.3.0

```python
def sleep(t):
    '''TF sleep'''
    import time
    def f(t):
        time.sleep(t)
        return np.array([], dtype=np.float32)
    return tf.py_func(f, [t], [tf.float32])[0]

with tf.device('gpu'):
    x = tf.Variable(0.)
with tf.control_dependencies([tf.identity(sleep(0.1))]):
    with tf.device('gpu'):
        mod = tf.assign(x, 100.)
with tf.device('cpu'):
    a = x+1.
    with tf.control_dependencies([tf.identity(mod)]):
        b = x+2.
        with tf.device('gpu'):
            c = x+3.

x.initializer.run()
sess.run([a, b, c])
# [1.0, 2.0, 103.0]
```

When a variable is read on another device, TF seems to copy once regardless of dependencies. I understand this is how TF works, but I think it would be nice to have dependencies ensure memory access order.",2018-01-23T19:21:05Z,4,2,1,3.2769378934088573
274,16274,R1.4,cla: no,,0,,2,2018-01-22T01:06:46Z,2018-01-23T01:02:40Z,NONE,,2018-01-23T01:03:04Z,1,1,0,2.2769378934088573
275,16272,Add a rnn example on mnist dataset using tf library,"awaiting review,cla: yes,stat:awaiting tensorflower","A Recurrent Neural Network (LSTM) implementation example using TensorFlow library.
This example is using the MNIST database of handwritten digits (http://yann.lecun.com/exdb/mnist/)
Links:
    [Long Short Term Memory](http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf)
    [MNIST Dataset](http://yann.lecun.com/exdb/mnist/).

Training and evaluation log:
Extracting /tmp/mnist_data/train-images-idx3-ubyte.gz
Extracting /tmp/mnist_data/train-labels-idx1-ubyte.gz
Extracting /tmp/mnist_data/t10k-images-idx3-ubyte.gz
Extracting /tmp/mnist_data/t10k-labels-idx1-ubyte.gz
Step 1, Minibatch Loss= 2.5586, Training Accuracy= 0.258
Step 200, Minibatch Loss= 0.2419, Training Accuracy= 0.930
Step 400, Minibatch Loss= 0.1863, Training Accuracy= 0.938
Step 600, Minibatch Loss= 0.1000, Training Accuracy= 0.969
Step 800, Minibatch Loss= 0.0935, Training Accuracy= 0.977
Step 1000, Minibatch Loss= 0.0773, Training Accuracy= 0.969
Step 1200, Minibatch Loss= 0.0500, Training Accuracy= 0.984
Step 1400, Minibatch Loss= 0.0550, Training Accuracy= 0.977
Step 1600, Minibatch Loss= 0.0615, Training Accuracy= 0.984
Step 1800, Minibatch Loss= 0.0635, Training Accuracy= 0.977
Step 2000, Minibatch Loss= 0.0550, Training Accuracy= 0.992
Training Finished!
Testing Accuracy: 0.992188",1,,8,2018-01-21T18:23:28Z,2018-01-28T01:41:04Z,CONTRIBUTOR,"A Recurrent Neural Network (LSTM) implementation example using TensorFlow library.
This example is using the MNIST database of handwritten digits (http://yann.lecun.com/exdb/mnist/)
Links:
    [Long Short Term Memory](http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf)
    [MNIST Dataset](http://yann.lecun.com/exdb/mnist/).

Training and evaluation log:
Extracting /tmp/mnist_data/train-images-idx3-ubyte.gz
Extracting /tmp/mnist_data/train-labels-idx1-ubyte.gz
Extracting /tmp/mnist_data/t10k-images-idx3-ubyte.gz
Extracting /tmp/mnist_data/t10k-labels-idx1-ubyte.gz
Step 1, Minibatch Loss= 2.5586, Training Accuracy= 0.258
Step 200, Minibatch Loss= 0.2419, Training Accuracy= 0.930
Step 400, Minibatch Loss= 0.1863, Training Accuracy= 0.938
Step 600, Minibatch Loss= 0.1000, Training Accuracy= 0.969
Step 800, Minibatch Loss= 0.0935, Training Accuracy= 0.977
Step 1000, Minibatch Loss= 0.0773, Training Accuracy= 0.969
Step 1200, Minibatch Loss= 0.0500, Training Accuracy= 0.984
Step 1400, Minibatch Loss= 0.0550, Training Accuracy= 0.977
Step 1600, Minibatch Loss= 0.0615, Training Accuracy= 0.984
Step 1800, Minibatch Loss= 0.0635, Training Accuracy= 0.977
Step 2000, Minibatch Loss= 0.0550, Training Accuracy= 0.992
Training Finished!
Testing Accuracy: 0.992188",2018-01-22T18:23:04Z,7,2,1,7.274907577742494
276,16271,"tf.pow(x, y) edge case with negative x (Bug)",type:support,"I am using tf.pow for my project, but my losses are 'nan', so I setup the test cases as shown below.
I found that whenever x is negative, tf.pow seems to output nan instead of the correct answer.

>>> r = tf.pow(0.4,0.4)
>>> r2 = tf.pow(-0.4,-0.4)
>>> r3 = tf.pow(0.4,-0.4)
>>> r4 = tf.pow(-0.4,0.4)
>>> sess.run(r)
0.69314486
>>> sess.run(r2)
nan
>>> sess.run(r3)
1.4426999
>>> sess.run(r4)
nan

I appreciate for anyone of the community who can address this issue.

Respectfully,",0,,3,2018-01-21T18:03:16Z,2018-01-22T03:45:00Z,NONE,"I am using tf.pow for my project, but my losses are 'nan', so I setup the test cases as shown below.
I found that whenever x is negative, tf.pow seems to output nan instead of the correct answer.

>>> r = tf.pow(0.4,0.4)
>>> r2 = tf.pow(-0.4,-0.4)
>>> r3 = tf.pow(0.4,-0.4)
>>> r4 = tf.pow(-0.4,0.4)
>>> sess.run(r)
0.69314486
>>> sess.run(r2)
nan
>>> sess.run(r3)
1.4426999
>>> sess.run(r4)
nan

I appreciate for anyone of the community who can address this issue.

Respectfully,",2018-01-22T03:44:57Z,1,1,0,2.7749075777424936
277,16269,ContentTooShortError: <urlopen error retrieval incomplete: got only 246506328 out of 247336696 bytes>,,"I have started UDACITY deep learning course.
I was copying the assignment 1 codes then I got the this error during downloading the notMNIST_large.tar.gz
Here are some screenshots of the code.
I am doing this on jupyter python3 notebook in ubuntu.

![1](https://user-images.githubusercontent.com/25321783/35194207-ca56cdc6-fed5-11e7-9fe7-3232c3741689.png)
![2](https://user-images.githubusercontent.com/25321783/35194208-cacbbc6c-fed5-11e7-9ff0-600527e4990a.png)
![error](https://user-images.githubusercontent.com/25321783/35194209-cb3fbd6a-fed5-11e7-8162-9b999b6240bb.png)
",0,,1,2018-01-21T12:36:47Z,2018-01-22T04:12:24Z,NONE,"I have started UDACITY deep learning course.
I was copying the assignment 1 codes then I got the this error during downloading the notMNIST_large.tar.gz
Here are some screenshots of the code.
I am doing this on jupyter python3 notebook in ubuntu.

![1](https://user-images.githubusercontent.com/25321783/35194207-ca56cdc6-fed5-11e7-9fe7-3232c3741689.png)
![2](https://user-images.githubusercontent.com/25321783/35194208-cacbbc6c-fed5-11e7-9ff0-600527e4990a.png)
![error](https://user-images.githubusercontent.com/25321783/35194209-cb3fbd6a-fed5-11e7-8162-9b999b6240bb.png)
",2018-01-22T04:12:24Z,1,1,0,1.7749075777424936
278,16265,ImportError: cannot import name tf,"stat:awaiting response,type:support","### System information
- **Os version**: Linux Ubuntu 14.04
- **TensorFlow installed from**: binary (sudo pip  install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.4.1-cp27-none-linux_x86_64.whl
)
- **TensorFlow version** :1.4.1
- **Python version**: 2.7

### Describe the problem
After Install when i run the following command it throws error;
`from tensorflow import tf`

It throws
`Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
ImportError: cannot import name tf
`
",0,,2,2018-01-21T06:37:52Z,2018-01-22T03:52:51Z,NONE,"### System information
- **Os version**: Linux Ubuntu 14.04
- **TensorFlow installed from**: binary (sudo pip  install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.4.1-cp27-none-linux_x86_64.whl
)
- **TensorFlow version** :1.4.1
- **Python version**: 2.7

### Describe the problem
After Install when i run the following command it throws error;
`from tensorflow import tf`

It throws
`Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
ImportError: cannot import name tf
`
",2018-01-22T03:46:25Z,1,1,0,2.2749075777424936
279,16262,"Cannot opened include file ""tensorflow/contrib/tpu/proto/tpu_embedding_config.pb.h"": no such file or directory",,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **TensorFlow installed from (source or binary)**: source 
- **TensorFlow version (use command below)**: current git master branch, should be v1.4.1 or v1.5.0rc1?
- **Python version**: N/A
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: MSVC2015
- **CUDA/cuDNN version**: CPU build only, gpu function is off
- **GPU model and memory**: N/A
- **Exact command to reproduce**: Trying a minimal build with cmake, with only snappy support and optimize for native arch turned on

### Describe the problem
Build failing due to missing header files ""tensorflow/contrib/tpu/proto/tpu_embedding_config.pb.h"".

Everything build succesful except for tpu project. I don't really know how to generate the pb.h file from protoc manually. I trying to fix the problem by chaning .cmake files, but not sure which one is for tpu.

### Source code / logs
133>D:\MSVC-source\tensorflow\tensorflow\contrib\tpu\ops\tpu_embedding_ops.cc(16): fatal error C1083: Cannot open include file: 'tensorflow/contrib/tpu/proto/tpu_embedding_config.pb.h': No such file or directory

",0,,6,2018-01-21T02:28:38Z,2018-01-23T21:28:01Z,CONTRIBUTOR,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **TensorFlow installed from (source or binary)**: source 
- **TensorFlow version (use command below)**: current git master branch, should be v1.4.1 or v1.5.0rc1?
- **Python version**: N/A
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: MSVC2015
- **CUDA/cuDNN version**: CPU build only, gpu function is off
- **GPU model and memory**: N/A
- **Exact command to reproduce**: Trying a minimal build with cmake, with only snappy support and optimize for native arch turned on

### Describe the problem
Build failing due to missing header files ""tensorflow/contrib/tpu/proto/tpu_embedding_config.pb.h"".

Everything build succesful except for tpu project. I don't really know how to generate the pb.h file from protoc manually. I trying to fix the problem by chaning .cmake files, but not sure which one is for tpu.

### Source code / logs
133>D:\MSVC-source\tensorflow\tensorflow\contrib\tpu\ops\tpu_embedding_ops.cc(16): fatal error C1083: Cannot open include file: 'tensorflow/contrib/tpu/proto/tpu_embedding_config.pb.h': No such file or directory

",2018-01-22T00:27:42Z,2,2,0,5.274907577742494
280,16255,tf.scatter_update Error ,,"Hi, 

I use tf.scatter_update to update non-trainable variables AS and AO in a code. As I found, when one uses scatter_update, the gradient misses, so that there is no gradient. 
Because of that I set both AL and AO as non-trainable variables (actually they are non-trainable), called the optimizer: tf.train.AdamOptimizer(config.actor_lr0,0.9,0.999,1e-8).minimize(actor_loss), and I thought everything should be fine. 
However, I am getting error:
LookupError: No gradient defined for operation 'actor/encoder/beer_game_flow_8/next_scat_j_2' (op type: ScatterUpdate). 
Here are the lines of the code that I update AO and AS that gives the error:

self.players[k-1].AS = tf.scatter_update(self.players[k-1].AS, 
                    self.curTime + leadTimeIn, 
                    tf.add(self.players[k-1].AS[self.curTime + leadTimeIn], possible_shipment), name='next_scat_j' )                   

self.players[k+1].AO = tf.scatter_update(self.players[k+1].AO, 
                    self.curTime + leadTime, tf.add(self.players[k+1].AO[self.curTime + leadTime], 
                    self.players[k].actionValue(self.curTime, self.playType))
                    , name='handle_scat_j')

Since both AS and AO are non-trainable, I do not need their gradient, and AS and AO are the only variable in this op. So, I was wondering why TensorFlow want to obtain the gradient, since there is no trainable variable here? 
Is it something that you can fix it, or is there any reason behind this behavior? 

BTW, I use python 2.7 with tf 1.4.0 on Debian 8.7 with a K80 with 12GB of memory. 

Thanks, 
Afshin
",0,,3,2018-01-19T23:39:34Z,2018-01-26T18:31:42Z,NONE,"Hi, 

I use tf.scatter_update to update non-trainable variables AS and AO in a code. As I found, when one uses scatter_update, the gradient misses, so that there is no gradient. 
Because of that I set both AL and AO as non-trainable variables (actually they are non-trainable), called the optimizer: tf.train.AdamOptimizer(config.actor_lr0,0.9,0.999,1e-8).minimize(actor_loss), and I thought everything should be fine. 
However, I am getting error:
LookupError: No gradient defined for operation 'actor/encoder/beer_game_flow_8/next_scat_j_2' (op type: ScatterUpdate). 
Here are the lines of the code that I update AO and AS that gives the error:

self.players[k-1].AS = tf.scatter_update(self.players[k-1].AS, 
                    self.curTime + leadTimeIn, 
                    tf.add(self.players[k-1].AS[self.curTime + leadTimeIn], possible_shipment), name='next_scat_j' )                   

self.players[k+1].AO = tf.scatter_update(self.players[k+1].AO, 
                    self.curTime + leadTime, tf.add(self.players[k+1].AO[self.curTime + leadTime], 
                    self.players[k].actionValue(self.curTime, self.playType))
                    , name='handle_scat_j')

Since both AS and AO are non-trainable, I do not need their gradient, and AS and AO are the only variable in this op. So, I was wondering why TensorFlow want to obtain the gradient, since there is no trainable variable here? 
Is it something that you can fix it, or is there any reason behind this behavior? 

BTW, I use python 2.7 with tf 1.4.0 on Debian 8.7 with a K80 with 12GB of memory. 

Thanks, 
Afshin
",2018-01-24T13:08:14Z,7,1,1,2.771085030681817
281,16254,adding placeholder_with_default in order to feed both via dataset and placeholders produces error on GPU,,"
Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**:1.4.0
- **Python version**: 3.6.2
- **Bazel version (if compiling from source)**:0.9.0
- **GCC/Compiler version (if compiling from source)**:5.4.0
- **CUDA/cuDNN version**:9.1.85
- **GPU model and memory**:Titan V, 12G
- **Exact command to reproduce**: See below

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

The issue is when you want to train a classifier that takes both placeholder and dataset via queue to feed input. The reason one may want to do that is to run inference via placeholders. 

I added the following line under define the model section of train_image_classifier.py of slim library

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.


_images = tf.placeholder_with_default(image, shape=[...], name='input)
 I get an error of the following kind when running on GPU:
Cannot assign a device for operation 'input': Could not satisfy explicit device specification '/device:GPU:0' 


",1,,5,2018-01-19T23:37:05Z,2018-01-22T00:02:36Z,NONE,"
Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**:1.4.0
- **Python version**: 3.6.2
- **Bazel version (if compiling from source)**:0.9.0
- **GCC/Compiler version (if compiling from source)**:5.4.0
- **CUDA/cuDNN version**:9.1.85
- **GPU model and memory**:Titan V, 12G
- **Exact command to reproduce**: See below

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

The issue is when you want to train a classifier that takes both placeholder and dataset via queue to feed input. The reason one may want to do that is to run inference via placeholders. 

I added the following line under define the model section of train_image_classifier.py of slim library

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.


_images = tf.placeholder_with_default(image, shape=[...], name='input)
 I get an error of the following kind when running on GPU:
Cannot assign a device for operation 'input': Could not satisfy explicit device specification '/device:GPU:0' 


",2018-01-22T00:02:36Z,3,1,1,4.771085030681817
282,16253,Introducing TensorRT operator,"awaiting testing (then merge),cla: yes","This PR introduces a new op that wraps around an highly optimized TensorRT engine and provides a seamless integration between TensorRT and TensorFlow.
- Add a TRTEngineOp that encapsulates a TensorRT executable.
- Add CreateInferenceGraph to contract a TensorRT-compilable subgraph to a TRTEngineOp.
- Update BUILD files to include new contrib package
- Add tensorflow.contrib.tensorrt python package to expose API to python
",2,,73,2018-01-19T23:09:29Z,2018-02-13T00:46:31Z,CONTRIBUTOR,"This PR introduces a new op that wraps around an highly optimized TensorRT engine and provides a seamless integration between TensorRT and TensorFlow.
- Add a TRTEngineOp that encapsulates a TensorRT executable.
- Add CreateInferenceGraph to contract a TensorRT-compilable subgraph to a TRTEngineOp.
- Update BUILD files to include new contrib package
- Add tensorflow.contrib.tensorrt python package to expose API to python
",2018-01-23T17:31:54Z,24,2,3,40.77108503068182
283,16249,Branch 182554969,cla: yes,,0,,1,2018-01-19T19:00:38Z,2018-01-19T19:27:04Z,MEMBER,,2018-01-19T19:01:11Z,0,3,0,3.771085030681817
284,16248,[bug?] Error in `python': malloc(): memory corruption,"stat:awaiting response,type:bug/performance","Error in `python': malloc(): memory corruption: 0x00000000723f9040

Strangely encountered this error when the training was going, it happened after a certain number of iterations (~7000 iterations with image batch size of 1 using coco dataset).  I have also attached the memory map that showed up after the backtrace.

```
*** Error in `python': malloc(): memory corruption: 0x00000000723f9040 ***
======= Backtrace: =========
/lib/x86_64-linux-gnu/libc.so.6(+0x777e5)[0x7f9d5715a7e5]
/lib/x86_64-linux-gnu/libc.so.6(+0x8213e)[0x7f9d5716513e]
/lib/x86_64-linux-gnu/libc.so.6(__libc_malloc+0x54)[0x7f9d57167184]
/usr/local/lib/python2.7/dist-packages/numpy/core/multiarray.so(+0x1ec51)[0x7f9d560f0c51]
/usr/local/lib/python2.7/dist-packages/numpy/core/multiarray.so(+0x842c8)[0x7f9d561562c8]
/usr/local/lib/python2.7/dist-packages/numpy/core/multiarray.so(+0x844a4)[0x7f9d561564a4]
/usr/local/lib/python2.7/dist-packages/numpy/core/multiarray.so(+0x84920)[0x7f9d56156920]
/usr/local/lib/python2.7/dist-packages/numpy/core/multiarray.so(+0x85105)[0x7f9d56157105]
/usr/local/lib/python2.7/dist-packages/numpy/core/multiarray.so(+0x11b16d)[0x7f9d561ed16d]
/home/user/tools/../data/coco/PythonAPI/pycocotools/_mask.so(+0x9406)[0x7f9d1f276406]
/home/user/tools/../data/coco/PythonAPI/pycocotools/_mask.so(+0x1c05f)[0x7f9d1f28905f]
python(PyEval_EvalFrameEx+0x615e)[0x4ca15e]
python(PyEval_EvalFrameEx+0x5d8f)[0x4c9d8f]
python(PyEval_EvalCodeEx+0x255)[0x4c2765]
python(PyEval_EvalFrameEx+0x68d1)[0x4ca8d1]
python(PyEval_EvalCodeEx+0x255)[0x4c2765]
python(PyEval_EvalFrameEx+0x68d1)[0x4ca8d1]
python(PyEval_EvalCodeEx+0x255)[0x4c2765]
python(PyEval_EvalFrameEx+0x68d1)[0x4ca8d1]
python(PyEval_EvalCodeEx+0x255)[0x4c2765]
python(PyEval_EvalFrameEx+0x68d1)[0x4ca8d1]
python(PyEval_EvalCodeEx+0x255)[0x4c2765]
python(PyEval_EvalFrameEx+0x6099)[0x4ca099]
python(PyEval_EvalCodeEx+0x255)[0x4c2765]
python(PyEval_EvalCode+0x19)[0x4c2509]
python[0x4f1def]
python(PyRun_FileExFlags+0x82)[0x4ec652]
python(PyRun_SimpleFileExFlags+0x191)[0x4eae31]
python(Py_Main+0x68a)[0x49e14a]
/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0xf0)[0x7f9d57103830]
python(_start+0x29)[0x49d9d9]
```


```
System Information
VERSION=""16.04.3 LTS (Xenial Xerus)""
VERSION_ID=""16.04""
VERSION_CODENAME=xenial

Compiler:
c++ (Ubuntu 5.4.0-6ubuntu1~16.04.4) 5.4.0 20160609

PIPs:
msgpack-numpy (0.4.2)
protobuf (3.5.0.post1)
tensorflow-gpu (1.4.1)
tensorflow-tensorboard (0.4.0rc3)

TensorFlow:
tf.VERSION = 1.4.0
tf.GIT_VERSION = v1.4.0-rc1-11-g130a514
tf.COMPILER_VERSION = v1.4.0-rc1-11-g130a514
Sanity check: array([1], dtype=int32)

Env:
LD_LIBRARY_PATH /usr/lib/x86_64-linux-gnu:/usr/local/lib:/usr/local/cuda/lib64:
DYLD_LIBRARY_PATH is unset

GPU:
TITAN Xp

CUDA Lib:
/usr/local/cuda-8.0/lib64/libcudart.so.8.0.61
/usr/local/cuda-8.0/lib64/libcudart_static.a
/usr/local/cuda-8.0/doc/man/man7/libcudart.7
/usr/local/cuda-8.0/doc/man/man7/libcudart.so.7
```
[memory map.txt](https://github.com/tensorflow/tensorflow/files/1647515/memory.map.txt)

",0,,3,2018-01-19T18:21:55Z,2018-02-07T19:57:04Z,NONE,"Error in `python': malloc(): memory corruption: 0x00000000723f9040

Strangely encountered this error when the training was going, it happened after a certain number of iterations (~7000 iterations with image batch size of 1 using coco dataset).  I have also attached the memory map that showed up after the backtrace.

```
*** Error in `python': malloc(): memory corruption: 0x00000000723f9040 ***
======= Backtrace: =========
/lib/x86_64-linux-gnu/libc.so.6(+0x777e5)[0x7f9d5715a7e5]
/lib/x86_64-linux-gnu/libc.so.6(+0x8213e)[0x7f9d5716513e]
/lib/x86_64-linux-gnu/libc.so.6(__libc_malloc+0x54)[0x7f9d57167184]
/usr/local/lib/python2.7/dist-packages/numpy/core/multiarray.so(+0x1ec51)[0x7f9d560f0c51]
/usr/local/lib/python2.7/dist-packages/numpy/core/multiarray.so(+0x842c8)[0x7f9d561562c8]
/usr/local/lib/python2.7/dist-packages/numpy/core/multiarray.so(+0x844a4)[0x7f9d561564a4]
/usr/local/lib/python2.7/dist-packages/numpy/core/multiarray.so(+0x84920)[0x7f9d56156920]
/usr/local/lib/python2.7/dist-packages/numpy/core/multiarray.so(+0x85105)[0x7f9d56157105]
/usr/local/lib/python2.7/dist-packages/numpy/core/multiarray.so(+0x11b16d)[0x7f9d561ed16d]
/home/user/tools/../data/coco/PythonAPI/pycocotools/_mask.so(+0x9406)[0x7f9d1f276406]
/home/user/tools/../data/coco/PythonAPI/pycocotools/_mask.so(+0x1c05f)[0x7f9d1f28905f]
python(PyEval_EvalFrameEx+0x615e)[0x4ca15e]
python(PyEval_EvalFrameEx+0x5d8f)[0x4c9d8f]
python(PyEval_EvalCodeEx+0x255)[0x4c2765]
python(PyEval_EvalFrameEx+0x68d1)[0x4ca8d1]
python(PyEval_EvalCodeEx+0x255)[0x4c2765]
python(PyEval_EvalFrameEx+0x68d1)[0x4ca8d1]
python(PyEval_EvalCodeEx+0x255)[0x4c2765]
python(PyEval_EvalFrameEx+0x68d1)[0x4ca8d1]
python(PyEval_EvalCodeEx+0x255)[0x4c2765]
python(PyEval_EvalFrameEx+0x68d1)[0x4ca8d1]
python(PyEval_EvalCodeEx+0x255)[0x4c2765]
python(PyEval_EvalFrameEx+0x6099)[0x4ca099]
python(PyEval_EvalCodeEx+0x255)[0x4c2765]
python(PyEval_EvalCode+0x19)[0x4c2509]
python[0x4f1def]
python(PyRun_FileExFlags+0x82)[0x4ec652]
python(PyRun_SimpleFileExFlags+0x191)[0x4eae31]
python(Py_Main+0x68a)[0x49e14a]
/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0xf0)[0x7f9d57103830]
python(_start+0x29)[0x49d9d9]
```


```
System Information
VERSION=""16.04.3 LTS (Xenial Xerus)""
VERSION_ID=""16.04""
VERSION_CODENAME=xenial

Compiler:
c++ (Ubuntu 5.4.0-6ubuntu1~16.04.4) 5.4.0 20160609

PIPs:
msgpack-numpy (0.4.2)
protobuf (3.5.0.post1)
tensorflow-gpu (1.4.1)
tensorflow-tensorboard (0.4.0rc3)

TensorFlow:
tf.VERSION = 1.4.0
tf.GIT_VERSION = v1.4.0-rc1-11-g130a514
tf.COMPILER_VERSION = v1.4.0-rc1-11-g130a514
Sanity check: array([1], dtype=int32)

Env:
LD_LIBRARY_PATH /usr/lib/x86_64-linux-gnu:/usr/local/lib:/usr/local/cuda/lib64:
DYLD_LIBRARY_PATH is unset

GPU:
TITAN Xp

CUDA Lib:
/usr/local/cuda-8.0/lib64/libcudart.so.8.0.61
/usr/local/cuda-8.0/lib64/libcudart_static.a
/usr/local/cuda-8.0/doc/man/man7/libcudart.7
/usr/local/cuda-8.0/doc/man/man7/libcudart.so.7
```
[memory map.txt](https://github.com/tensorflow/tensorflow/files/1647515/memory.map.txt)

",2018-01-19T20:42:31Z,18,1,3,2.771085030681817
285,16246,Failed to build error: mismatched argument pack lenghts...,,"### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 4.14.13-1-ARCH
- **TensorFlow installed from (source or binary)**: git
- **Python version**: 3.6.4
- **Bazel version (if compiling from source)**: 0.9.0-1
- **GCC/Compiler version (if compiling from source)**: 6.4.1
- **CUDA/cuDNN version**:  9.1.85-1 / 7.0.5-2
- **Exact command to reproduce**:
./configure
bazel build --config=opt --config=cuda --jobs 12 //tensorflow/tools/pip_package:build_pip_package


### Describe the problem
failed to build

### Source code / logs

> /usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/tuple:489:65: error: mismatched argument pack lengths while expanding 'std::is_convertible<_UElements&&, _Elements>'
>        return __and_<is_convertible<_UElements&&, _Elements>...>::value;
>                                                                  ^~~~~
> /usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/tuple:490:1: error: body of constexpr function 'static constexpr bool std::_TC<<anonymous>, _Elements>::_ImplicitlyMoveConvertibleTuple() [with _UElements = {const std::tuple<tensorflow::VariantBinaryOp, tensorflow::StringPiece, tensorflow::StringPiece>}; bool <anonymous> = true; _Elements = {tensorflow::VariantBinaryOp, tensorflow::StringPiece, tensorflow::StringPiece}]' not a return-statement
>      }
>  ^
> ERROR: /home/user/dev/git/tensorflow/tensorflow/core/kernels/BUILD:1884:1: output 'tensorflow/core/kernels/_objs/list_kernels_gpu/tensorflow/core/kernels/list_kernels.cu.pic.o' was not created
> ERROR: /home/user/dev/git/tensorflow/tensorflow/core/kernels/BUILD:1884:1: not all outputs were created or valid
> Target //tensorflow/tools/pip_package:build_pip_package failed to build
> Use --verbose_failures to see the command lines of failed build steps.
> INFO: Elapsed time: 29.727s, Critical Path: 28.35s
> FAILED: Build did NOT complete successfully
> ",1,,9,2018-01-19T17:10:53Z,2018-01-30T14:32:42Z,NONE,"### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 4.14.13-1-ARCH
- **TensorFlow installed from (source or binary)**: git
- **Python version**: 3.6.4
- **Bazel version (if compiling from source)**: 0.9.0-1
- **GCC/Compiler version (if compiling from source)**: 6.4.1
- **CUDA/cuDNN version**:  9.1.85-1 / 7.0.5-2
- **Exact command to reproduce**:
./configure
bazel build --config=opt --config=cuda --jobs 12 //tensorflow/tools/pip_package:build_pip_package


### Describe the problem
failed to build

### Source code / logs

> /usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/tuple:489:65: error: mismatched argument pack lengths while expanding 'std::is_convertible<_UElements&&, _Elements>'
>        return __and_<is_convertible<_UElements&&, _Elements>...>::value;
>                                                                  ^~~~~
> /usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/tuple:490:1: error: body of constexpr function 'static constexpr bool std::_TC<<anonymous>, _Elements>::_ImplicitlyMoveConvertibleTuple() [with _UElements = {const std::tuple<tensorflow::VariantBinaryOp, tensorflow::StringPiece, tensorflow::StringPiece>}; bool <anonymous> = true; _Elements = {tensorflow::VariantBinaryOp, tensorflow::StringPiece, tensorflow::StringPiece}]' not a return-statement
>      }
>  ^
> ERROR: /home/user/dev/git/tensorflow/tensorflow/core/kernels/BUILD:1884:1: output 'tensorflow/core/kernels/_objs/list_kernels_gpu/tensorflow/core/kernels/list_kernels.cu.pic.o' was not created
> ERROR: /home/user/dev/git/tensorflow/tensorflow/core/kernels/BUILD:1884:1: not all outputs were created or valid
> Target //tensorflow/tools/pip_package:build_pip_package failed to build
> Use --verbose_failures to see the command lines of failed build steps.
> INFO: Elapsed time: 29.727s, Critical Path: 28.35s
> FAILED: Build did NOT complete successfully
> ",2018-01-30T18:18:56Z,11,1,2,6.771085030681816
286,16245,Branch 182511847,cla: yes,,0,,2,2018-01-19T16:35:22Z,2018-01-19T18:50:20Z,MEMBER,,2018-01-19T17:55:52Z,0,3,0,4.271085030681817
287,16244,Benchmarking GPU ops in Tensorflow Graphs,,"I tried using the tool for benchmarking Tensorflow Graphs at
https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/benchmark

Seems it only gives the memory profile for RAM per ops. How can I get the GPU memory and utilization profile per ops using this tool?

",0,,1,2018-01-19T16:03:21Z,2018-01-19T21:20:25Z,NONE,"I tried using the tool for benchmarking Tensorflow Graphs at
https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/benchmark

Seems it only gives the memory profile for RAM per ops. How can I get the GPU memory and utilization profile per ops using this tool?

",2018-01-19T21:20:25Z,0,1,0,1.7710850306818169
288,16240,graph_metrics.py does not work well,,"I want to use function in graph_metrics.py, I run corresponding test file graph_metrics_test.py, but I get the following assertion error for ""weight_parameters"" metric
```

line 32, in testGraphMetrics
    self.assertEqual(expected[statistic_type], current_stats.value)
AssertionError: 100 != None
```
has anyone encounter this problem?",0,,2,2018-01-19T12:35:24Z,2018-01-20T01:50:26Z,NONE,"I want to use function in graph_metrics.py, I run corresponding test file graph_metrics_test.py, but I get the following assertion error for ""weight_parameters"" metric
```

line 32, in testGraphMetrics
    self.assertEqual(expected[statistic_type], current_stats.value)
AssertionError: 100 != None
```
has anyone encounter this problem?",2018-01-19T21:11:21Z,1,1,0,2.271085030681817
289,16238,//tensorflow/contrib/gan:losses_impl_test fails with AssertionError ,type:bug/performance,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04  s390x
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: v1.4.1
- **Python version**: 2.7.12
- **Bazel version (if compiling from source)**: 0.7.0
- **GCC/Compiler version (if compiling from source)**: 5.4.0
- **CUDA/cuDNN version**: No GPU
- **GPU model and memory**: NA
- **Exact command to reproduce**: bazel test -c opt //tensorflow/contrib/gan:losses_impl_test

### Describe the problem
One of the sub-test `test_stable_global_norm_unchanged` fails on s390x with 
`AssertionError: 110.709068 != 110.709084 +/- 0.000010`

Seems like a minor difference, so I tried changing the tolerance slightly as below:
```
-        self.assertNear(gnorm_np, precond_gnorm_np, 1e-5)
+        self.assertNear(gnorm_np, precond_gnorm_np, 2e-5)
```
with this the test is passing.

Is it ok to create a PR with this change? Could you please share your thoughts on this.

### Source code / logs
```
.......................F..................................................................................
======================================================================
FAIL: test_stable_global_norm_unchanged (__main__.CombineAdversarialLossTest)
Test that preconditioning doesn't change global norm value.
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/test/.cache/bazel/_bazel_test/774d974934abfb88e6e5d6a13042805c/execroot/org_tensorflow/bazel-out/local-opt/bin/tensorflow/contrib/gan/losses_impl_test.runfiles/org_tensorflow/tensorflow/contrib/gan/python/losses/python/losses_impl_test.py"", line 602, in test_stable_global_norm_unchanged
    self.assertNear(gnorm_np, precond_gnorm_np, 1e-5)
  File ""/home/test/.cache/bazel/_bazel_test/774d974934abfb88e6e5d6a13042805c/execroot/org_tensorflow/bazel-out/local-opt/bin/tensorflow/contrib/gan/losses_impl_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 879, in assertNear
    if msg is not None else """"))
AssertionError: 110.709068 != 110.709084 +/- 0.000010

----------------------------------------------------------------------
Ran 106 tests in 9.119s

FAILED (failures=1)
```

",2,,2,2018-01-19T10:25:54Z,2018-01-26T05:31:42Z,CONTRIBUTOR,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04  s390x
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: v1.4.1
- **Python version**: 2.7.12
- **Bazel version (if compiling from source)**: 0.7.0
- **GCC/Compiler version (if compiling from source)**: 5.4.0
- **CUDA/cuDNN version**: No GPU
- **GPU model and memory**: NA
- **Exact command to reproduce**: bazel test -c opt //tensorflow/contrib/gan:losses_impl_test

### Describe the problem
One of the sub-test `test_stable_global_norm_unchanged` fails on s390x with 
`AssertionError: 110.709068 != 110.709084 +/- 0.000010`

Seems like a minor difference, so I tried changing the tolerance slightly as below:
```
-        self.assertNear(gnorm_np, precond_gnorm_np, 1e-5)
+        self.assertNear(gnorm_np, precond_gnorm_np, 2e-5)
```
with this the test is passing.

Is it ok to create a PR with this change? Could you please share your thoughts on this.

### Source code / logs
```
.......................F..................................................................................
======================================================================
FAIL: test_stable_global_norm_unchanged (__main__.CombineAdversarialLossTest)
Test that preconditioning doesn't change global norm value.
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/test/.cache/bazel/_bazel_test/774d974934abfb88e6e5d6a13042805c/execroot/org_tensorflow/bazel-out/local-opt/bin/tensorflow/contrib/gan/losses_impl_test.runfiles/org_tensorflow/tensorflow/contrib/gan/python/losses/python/losses_impl_test.py"", line 602, in test_stable_global_norm_unchanged
    self.assertNear(gnorm_np, precond_gnorm_np, 1e-5)
  File ""/home/test/.cache/bazel/_bazel_test/774d974934abfb88e6e5d6a13042805c/execroot/org_tensorflow/bazel-out/local-opt/bin/tensorflow/contrib/gan/losses_impl_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 879, in assertNear
    if msg is not None else """"))
AssertionError: 110.709068 != 110.709084 +/- 0.000010

----------------------------------------------------------------------
Ran 106 tests in 9.119s

FAILED (failures=1)
```

",2018-01-19T21:00:09Z,7,2,1,5.271085030681817
290,16237,support preconditioner for `conjugated_gradient()` in `linear_equations.py`,"awaiting testing (then merge),cla: yes","1. support preconditioner for `conjugated_gradient()` in `tensorflow\tensorflow\contrib\solvers\python\ops\linear_equations.py`
2. add identity_operator() in `util.py` as default preconditioner
3. edit unit test files(`util_test.py`, `linear_equations_test.py`) to validate preconditioner",1,,4,2018-01-19T07:12:01Z,2018-01-24T18:39:00Z,CONTRIBUTOR,"1. support preconditioner for `conjugated_gradient()` in `tensorflow\tensorflow\contrib\solvers\python\ops\linear_equations.py`
2. add identity_operator() in `util.py` as default preconditioner
3. edit unit test files(`util_test.py`, `linear_equations_test.py`) to validate preconditioner",2018-01-19T07:13:03Z,5,2,1,5.271085030681817
291,16236,Branch 182474037,cla: yes,,0,,3,2018-01-19T06:47:58Z,2018-01-19T09:01:12Z,MEMBER,,2018-01-19T07:29:26Z,0,3,0,4.771085030681817
292,16235,Feature Request: Make NDLSTM use state_is_tuple=True,type:feature,"I have successfully used [NDLSTM](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/ndlstm) (specifically lstm2d.separable_lstm) in my own project but whenever I use it, I enocunter this warning: `""Using a concatenated state is slower and will soon be deprecated. use state_is_tuple=true.""`

The warning is caused by  `ndlstm_base_dynamic` in lstm1d.py. Specifically, this line: `lstm_cell = rnn_cell.BasicLSTMCell(noutput, state_is_tuple=False)`

I modified the code such that the deprecation warning won't appear:

```
with variable_scope.variable_scope(scope, ""SeqLstm"", [inputs]):
    lstm_cell = rnn_cell.BasicLSTMCell(noutput)
    if reverse:
      inputs = array_ops.reverse_v2(inputs, [0])
    outputs, _ = rnn.dynamic_rnn(
        lstm_cell, inputs, time_major=True, dtype=inputs.dtype)
    if reverse:
      outputs = array_ops.reverse_v2(outputs, [0])
    return outputs
```

Before I make any pull requests, is there a reason why the `state_is_tuple` argument is set to `False` in the code?",0,,5,2018-01-19T02:36:34Z,2018-01-25T19:27:43Z,CONTRIBUTOR,"I have successfully used [NDLSTM](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/ndlstm) (specifically lstm2d.separable_lstm) in my own project but whenever I use it, I enocunter this warning: `""Using a concatenated state is slower and will soon be deprecated. use state_is_tuple=true.""`

The warning is caused by  `ndlstm_base_dynamic` in lstm1d.py. Specifically, this line: `lstm_cell = rnn_cell.BasicLSTMCell(noutput, state_is_tuple=False)`

I modified the code such that the deprecation warning won't appear:

```
with variable_scope.variable_scope(scope, ""SeqLstm"", [inputs]):
    lstm_cell = rnn_cell.BasicLSTMCell(noutput)
    if reverse:
      inputs = array_ops.reverse_v2(inputs, [0])
    outputs, _ = rnn.dynamic_rnn(
        lstm_cell, inputs, time_major=True, dtype=inputs.dtype)
    if reverse:
      outputs = array_ops.reverse_v2(outputs, [0])
    return outputs
```

Before I make any pull requests, is there a reason why the `state_is_tuple` argument is set to `False` in the code?",2018-01-19T02:45:25Z,6,2,1,4.771085030681817
293,16234,The recognized result is not correct when converting the frozen graph to tflite for android device use ,comp:lite,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: mac High Sierra
- **TensorFlow installed from (source or binary)**:binary
- **TensorFlow version (use command below)**:1.5.0
- **Python version**: 2.7.10
- **Bazel version (if compiling from source)**:0.9.0
- **GCC/Compiler version (if compiling from source)**:c++/4.2.1
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

The detailed system information, you can check the url:
https://drive.google.com/file/d/19oKikJ0PcGHx9daauub28IYb8J3hA-rw/view?usp=sharing

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

Hi, I covert the frozen graph:mobilenet_v1_224 to tflite, and put it in the tflitecamerademo app, but the regonization result is not correct. If I use the tflite file which download from https://storage.googleapis.com/download.tensorflow.org/models/tflite/mobilenet_v1_224_android_quant_2017_11_08.zip

The regonization result is correct, I don't know what steps is not correct when I covert the frozeon graph to tflite file, could you help me to review what steps is the wrong?

I put the frozen graph, coverting tflite file and the regonized picture in the https://drive.google.com/drive/folders/12h9O2AtcnDuQZXogAdmexRSjxIQVc1Ej?usp=sharing

The correct result should be ""malamute"", but I use the my coverting tflite file, the result is ""shower curtain""

I use the command to do the covert
bazel run --config=opt //tensorflow/contrib/lite/toco:toco -- '--input_file=/tmp/mobilenet_frozen_graph.pb' '--output_file=/tmp/mobilenet_quant_20180117.tflite' '--input_format=TENSORFLOW_GRAPHDEF' '--output_format=TFLITE' '--inference_type=QUANTIZED_UINT8' '--inference_input_type=QUANTIZED_UINT8' '--input_shapes=1,224,224,3' '--input_arrays=input' '--output_arrays=MobilenetV1/Predictions/Reshape_1' '--mean_values=128' '--std_values=128' '--default_ranges_min=0' '--default_ranges_max=6'

Thanks

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",1,,4,2018-01-19T02:26:39Z,2018-01-31T20:13:48Z,NONE,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: mac High Sierra
- **TensorFlow installed from (source or binary)**:binary
- **TensorFlow version (use command below)**:1.5.0
- **Python version**: 2.7.10
- **Bazel version (if compiling from source)**:0.9.0
- **GCC/Compiler version (if compiling from source)**:c++/4.2.1
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

The detailed system information, you can check the url:
https://drive.google.com/file/d/19oKikJ0PcGHx9daauub28IYb8J3hA-rw/view?usp=sharing

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

Hi, I covert the frozen graph:mobilenet_v1_224 to tflite, and put it in the tflitecamerademo app, but the regonization result is not correct. If I use the tflite file which download from https://storage.googleapis.com/download.tensorflow.org/models/tflite/mobilenet_v1_224_android_quant_2017_11_08.zip

The regonization result is correct, I don't know what steps is not correct when I covert the frozeon graph to tflite file, could you help me to review what steps is the wrong?

I put the frozen graph, coverting tflite file and the regonized picture in the https://drive.google.com/drive/folders/12h9O2AtcnDuQZXogAdmexRSjxIQVc1Ej?usp=sharing

The correct result should be ""malamute"", but I use the my coverting tflite file, the result is ""shower curtain""

I use the command to do the covert
bazel run --config=opt //tensorflow/contrib/lite/toco:toco -- '--input_file=/tmp/mobilenet_frozen_graph.pb' '--output_file=/tmp/mobilenet_quant_20180117.tflite' '--input_format=TENSORFLOW_GRAPHDEF' '--output_format=TFLITE' '--inference_type=QUANTIZED_UINT8' '--inference_input_type=QUANTIZED_UINT8' '--input_shapes=1,224,224,3' '--input_arrays=input' '--output_arrays=MobilenetV1/Predictions/Reshape_1' '--mean_values=128' '--std_values=128' '--default_ranges_min=0' '--default_ranges_max=6'

Thanks

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",2018-01-31T20:13:48Z,12,1,2,4.271085030681817
294,16232,Singleton S3Client,"awaiting testing (then merge),cla: yes","Fixes #16230 .

This drastically speeds up performance of interactions with S3, and eliminates a lot of spurious log warnings when interacting with S3 files.

The filesystem unit tests went from taking ~40 seconds to taking ~4 seconds with this change, a 10X performance improvement.

Some items of note:
- I updated the delete test to work on a bucket that had tests run previously. Without this change, a manual wipe of the file in quest was required after each run.
- I moved the request timeout to a central location, instead of being local to the `Sync` operation.
- I eliminated the increased connection timeout for `Sync`, which shouldn't be needed.
- Configuration is no longer a static variable protected by a mutex, but instead created as-needed. This should be non-functional, given that config is only created once during normal operation now.",1,,7,2018-01-18T23:54:26Z,2018-01-23T22:53:51Z,CONTRIBUTOR,"Fixes #16230 .

This drastically speeds up performance of interactions with S3, and eliminates a lot of spurious log warnings when interacting with S3 files.

The filesystem unit tests went from taking ~40 seconds to taking ~4 seconds with this change, a 10X performance improvement.

Some items of note:
- I updated the delete test to work on a bucket that had tests run previously. Without this change, a manual wipe of the file in quest was required after each run.
- I moved the request timeout to a central location, instead of being local to the `Sync` operation.
- I eliminated the increased connection timeout for `Sync`, which shouldn't be needed.
- Configuration is no longer a static variable protected by a mutex, but instead created as-needed. This should be non-functional, given that config is only created once during normal operation now.",2018-01-22T21:26:47Z,5,2,1,6.769282508064391
295,16231,x86_64 compilation failed,"stat:awaiting tensorflower,type:build/install","### System information

- **MacOS High Sierra 10.13.2**:
- **Python 3.6.3**:
- **TensorFlow Latest Pull from 1/17/18**:

### Describe the problem
I am following Pete Warden's TensorFlow for Mobile Poets guide and seem to have a found an error. When I run ""tensorflow/contrib/makefile/build_all_ios.sh"" after about 20 minutes it returns an error. 

I have tried running lipo -info /Users/ryan/Downloads/tensorflow2/tensorflow/contrib/makefile/gen/protobuf_ios/lib/libprotobuf.a

and this returns: 

Architectures in the fat file:
/Users/ryan/Downloads/tensorflow2/tensorflow/contrib/makefile/gen/protobuf_ios/lib/libprotobuf.a are: i386 

I have the entire error script here:
https://drive.google.com/file/d/1JovTMGBJKbqzRPBzXy3cIQ-hbz76n0ab/view?usp=sharing

### Source code / logs
ld: symbol(s) not found for architecture x86_64
clang: error: linker command failed with exit code 1 (use -v to see 
invocation)
make: *** [/Users/ryan/Desktop/tensorflow-
master/tensorflow/contrib/makefile/gen/bin/ios_X86_64/benchmark] Error 1
+ '[' 2 -ne 0 ']'
+ echo 'x86_64 compilation failed.'
x86_64 compilation failed.
+ exit 1
",1,,2,2018-01-18T23:51:42Z,2018-01-31T19:42:50Z,NONE,"### System information

- **MacOS High Sierra 10.13.2**:
- **Python 3.6.3**:
- **TensorFlow Latest Pull from 1/17/18**:

### Describe the problem
I am following Pete Warden's TensorFlow for Mobile Poets guide and seem to have a found an error. When I run ""tensorflow/contrib/makefile/build_all_ios.sh"" after about 20 minutes it returns an error. 

I have tried running lipo -info /Users/ryan/Downloads/tensorflow2/tensorflow/contrib/makefile/gen/protobuf_ios/lib/libprotobuf.a

and this returns: 

Architectures in the fat file:
/Users/ryan/Downloads/tensorflow2/tensorflow/contrib/makefile/gen/protobuf_ios/lib/libprotobuf.a are: i386 

I have the entire error script here:
https://drive.google.com/file/d/1JovTMGBJKbqzRPBzXy3cIQ-hbz76n0ab/view?usp=sharing

### Source code / logs
ld: symbol(s) not found for architecture x86_64
clang: error: linker command failed with exit code 1 (use -v to see 
invocation)
make: *** [/Users/ryan/Desktop/tensorflow-
master/tensorflow/contrib/makefile/gen/bin/ios_X86_64/benchmark] Error 1
+ '[' 2 -ne 0 ']'
+ echo 'x86_64 compilation failed.'
x86_64 compilation failed.
+ exit 1
",2018-01-22T04:02:19Z,13,1,2,3.269282508064391
296,16230,A new S3Client is created with all file operations.,,"------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: OS X 10.12.6
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: commit 4595f1cff635ce024e875f0f3d480172731b0b22
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: 0.5.4-homebrew
- **GCC/Compiler version (if compiling from source)**: Apple LLVM version 9.0.0 (clang-900.0.39.2)
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A

### Describe the problem

The [S3 filesystem](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/s3/s3_file_system.cc) creates a new `Aws::S3::S3Client` object with all interactions with S3. This is a heavyweight object, and takes relatively large amount of time to create and destroy.

This should be a singleton associated with the filesystem object.

Fix shortly.",0,,4,2018-01-18T23:39:21Z,2018-01-23T22:53:51Z,CONTRIBUTOR,"------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: OS X 10.12.6
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: commit 4595f1cff635ce024e875f0f3d480172731b0b22
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: 0.5.4-homebrew
- **GCC/Compiler version (if compiling from source)**: Apple LLVM version 9.0.0 (clang-900.0.39.2)
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A

### Describe the problem

The [S3 filesystem](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/s3/s3_file_system.cc) creates a new `Aws::S3::S3Client` object with all interactions with S3. This is a heavyweight object, and takes relatively large amount of time to create and destroy.

This should be a singleton associated with the filesystem object.

Fix shortly.",2018-01-19T07:03:18Z,5,2,1,4.269282508064391
297,16225,maxout lose the number of features in the shape of its output,,"In tf.contrib.layers.maxout(), when the shape of ""inputs"" is not completely specified, the shape of its output will be completely unknown, such as [None, None, None] in the 3d case.
Since ""num_units"" has specified the final number of features in the maxout axis, the output should set its shape accordingly:
https://github.com/tensorflow/tensorflow/pull/16114",1,,3,2018-01-18T19:19:05Z,2018-02-01T05:58:39Z,NONE,"In tf.contrib.layers.maxout(), when the shape of ""inputs"" is not completely specified, the shape of its output will be completely unknown, such as [None, None, None] in the 3d case.
Since ""num_units"" has specified the final number of features in the maxout axis, the output should set its shape accordingly:
https://github.com/tensorflow/tensorflow/pull/16114",2018-01-19T07:03:04Z,13,1,2,3.769282508064391
298,16224,Suppress AWS curl init warning,"awaiting testing (then merge),cla: yes","This shows up each time we run the TensorBoard command, even if we're
not using anything AWS related.

```sh
jart@compy:~/tmp/aws-sdk-cpp-1.3.15$ grep -R ""Initializing Curl library"" .
./aws-cpp-sdk-core/source/http/curl/CurlHttpClient.cpp:        AWS_LOGSTREAM_INFO(CURL_HTTP_CLIENT_TAG, ""Initializing Curl library"");
```",1,,1,2018-01-18T18:56:07Z,2018-01-19T09:01:54Z,MEMBER,"This shows up each time we run the TensorBoard command, even if we're
not using anything AWS related.

```sh
jart@compy:~/tmp/aws-sdk-cpp-1.3.15$ grep -R ""Initializing Curl library"" .
./aws-cpp-sdk-core/source/http/curl/CurlHttpClient.cpp:        AWS_LOGSTREAM_INFO(CURL_HTTP_CLIENT_TAG, ""Initializing Curl library"");
```",2018-01-18T19:44:23Z,1,3,0,4.769282508064391
299,16221,Meaning of report_tensor_allocations_upon_oom output,"stat:awaiting response,type:support","Python: 3.6.2
OS: Ubuntu 16.04
Tensorflow: 1.5.0rc1

When running a session with `tf.RunOptions` and `report_tensor_allocations_upon_oom=True` I get the following output at the end of my log.

1. I am wondering why some entries occur multiple times? How can a single node have multiple allocations? Why are they not summed?
2. Does `Remaining 1252 nodes with 98.80MiB` mean that all 1252 nodes together use 98.80MiB or each single one uses that amount?
3. When summing up all values I get `10.607822265625GiB` but my free GPU space when starting my program is `11.92GiB` so shouldn't there still be enough space??

```
Current usage from device: /job:localhost/replica:0/task:0/device:GPU:0, allocator: GPU_0_bfc
  250.78MiB from network/convolutions/conv2d_5/Conv2D
  217.34MiB from network/convolutions/conv2d_5/Conv2D
  203.75MiB from network/convolutions/conv2d_5/Conv2D
  192.91MiB from network/convolutions/conv2d_11/Conv2D
  168.05MiB from network/convolutions/conv2d_3/Conv2D
  167.19MiB from network/convolutions/conv2d_2/Conv2D
  167.19MiB from network/convolutions/conv2d_2/Conv2D
  167.19MiB from network/convolutions/conv2d_3/Conv2D
  167.19MiB from network/convolutions/conv2d_2/Conv2D
  167.19MiB from network/convolutions/conv2d_3/Conv2D
  167.19MiB from network/convolutions/conv2d_4/Conv2D
  167.19MiB from network/convolutions/conv2d_4/Conv2D
  167.19MiB from network/convolutions/conv2d_4/Conv2D
  167.19MiB from network/convolutions/conv2d_2/Conv2D
  167.19MiB from network/convolutions/conv2d_3/Conv2D
  167.19MiB from network/convolutions/conv2d_4/Conv2D
  167.19MiB from network/convolutions/conv2d_2/Conv2D
  167.19MiB from network/convolutions/conv2d_2/Conv2D
  167.19MiB from network/convolutions/conv2d_5/Conv2D
  167.19MiB from network/convolutions/conv2d_3/Conv2D
  167.19MiB from network/convolutions/conv2d_3/Conv2D
  167.19MiB from network/convolutions/conv2d_4/Conv2D
  167.19MiB from network/convolutions/conv2d_4/Conv2D
  167.19MiB from network/convolutions/conv2d_2/Conv2D
  167.19MiB from network/convolutions/conv2d_5/Conv2D
  167.19MiB from network/convolutions/conv2d_5/Conv2D
  167.19MiB from network/convolutions/conv2d_5/Conv2D
  167.19MiB from network/convolutions/conv2d_3/Conv2D
  167.19MiB from network/convolutions/conv2d_4/Conv2D
  167.19MiB from network/convolutions/conv2d_2/Conv2D
  167.19MiB from network/convolutions/conv2d_3/Conv2D
  167.19MiB from network/convolutions/conv2d_4/Conv2D
  167.19MiB from network/convolutions/conv2d_5/Conv2D
  167.19MiB from network/convolutions/conv2d_11/Conv2D
  163.84MiB from network/convolutions/conv2d_7/Conv2D
  160.50MiB from network/convolutions/conv2d_7/Conv2D
  140.99MiB from network/convolutions/conv2d_12/Conv2D
  133.75MiB from network/convolutions/conv2d_6/Conv2D
  133.75MiB from network/convolutions/conv2d_7/Conv2D
  133.75MiB from network/convolutions/conv2d_11/Conv2D
  133.75MiB from network/convolutions/conv2d_11/Conv2D
  133.75MiB from network/convolutions/conv2d_11/Conv2D
  133.75MiB from network/convolutions/conv2d_12/Conv2D
  103.66MiB from network/convolutions/conv2d_8/Conv2D
  83.59MiB from network/convolutions/conv2d/Conv2D
  83.59MiB from network/convolutions/conv2d/Conv2D
  83.59MiB from network/convolutions/conv2d/Conv2D
  83.59MiB from network/convolutions/conv2d/Conv2D
  83.59MiB from network/convolutions/conv2d/Conv2D
  83.59MiB from network/convolutions/conv2d/Conv2D
  83.59MiB from network/convolutions/conv2d/Conv2D
  83.59MiB from network/convolutions/conv2d_6/Conv2D
  83.59MiB from network/convolutions/conv2d/Conv2D
  83.59MiB from network/convolutions/conv2d_6/Conv2D
  83.59MiB from network/convolutions/conv2d_6/Conv2D
  83.59MiB from network/convolutions/conv2d_6/Conv2D
  83.59MiB from network/convolutions/conv2d_6/Conv2D
  83.59MiB from network/convolutions/conv2d_6/Conv2D
  83.59MiB from network/convolutions/conv2d_7/Conv2D
  83.59MiB from network/convolutions/conv2d_6/Conv2D
  83.59MiB from network/convolutions/conv2d_7/Conv2D
  83.59MiB from network/convolutions/conv2d_7/Conv2D
  83.59MiB from network/convolutions/conv2d_8/Conv2D
  83.59MiB from network/convolutions/conv2d_7/Conv2D
  83.59MiB from network/convolutions/conv2d_8/Conv2D
  83.59MiB from network/convolutions/conv2d_7/Conv2D
  83.59MiB from network/convolutions/conv2d_8/Conv2D
  83.59MiB from network/convolutions/conv2d_8/Conv2D
  83.59MiB from network/convolutions/conv2d_8/Conv2D
  83.59MiB from network/convolutions/conv2d_9/Conv2D
  83.59MiB from network/convolutions/conv2d_9/Conv2D
  83.59MiB from network/convolutions/conv2d_8/Conv2D
  83.59MiB from network/convolutions/conv2d_8/Conv2D
  83.59MiB from network/convolutions/conv2d_9/Conv2D
  83.59MiB from network/convolutions/conv2d_9/Conv2D
  83.59MiB from network/convolutions/conv2d_10/Conv2D
  83.59MiB from network/convolutions/conv2d_10/Conv2D
  83.59MiB from network/convolutions/conv2d_9/Conv2D
  83.59MiB from network/convolutions/conv2d_10/Conv2D
  83.59MiB from network/convolutions/conv2d_10/Conv2D
  83.59MiB from network/convolutions/conv2d_9/Conv2D
  83.59MiB from network/convolutions/conv2d_9/Conv2D
  83.59MiB from network/convolutions/conv2d_10/Conv2D
  83.59MiB from network/convolutions/conv2d_9/Conv2D
  83.59MiB from network/convolutions/conv2d_10/Conv2D
  83.59MiB from network/convolutions/conv2d_10/Conv2D
  Remaining 1252 nodes with 98.80MiB
```",0,,2,2018-01-18T14:39:30Z,2018-01-19T08:41:16Z,CONTRIBUTOR,"Python: 3.6.2
OS: Ubuntu 16.04
Tensorflow: 1.5.0rc1

When running a session with `tf.RunOptions` and `report_tensor_allocations_upon_oom=True` I get the following output at the end of my log.

1. I am wondering why some entries occur multiple times? How can a single node have multiple allocations? Why are they not summed?
2. Does `Remaining 1252 nodes with 98.80MiB` mean that all 1252 nodes together use 98.80MiB or each single one uses that amount?
3. When summing up all values I get `10.607822265625GiB` but my free GPU space when starting my program is `11.92GiB` so shouldn't there still be enough space??

```
Current usage from device: /job:localhost/replica:0/task:0/device:GPU:0, allocator: GPU_0_bfc
  250.78MiB from network/convolutions/conv2d_5/Conv2D
  217.34MiB from network/convolutions/conv2d_5/Conv2D
  203.75MiB from network/convolutions/conv2d_5/Conv2D
  192.91MiB from network/convolutions/conv2d_11/Conv2D
  168.05MiB from network/convolutions/conv2d_3/Conv2D
  167.19MiB from network/convolutions/conv2d_2/Conv2D
  167.19MiB from network/convolutions/conv2d_2/Conv2D
  167.19MiB from network/convolutions/conv2d_3/Conv2D
  167.19MiB from network/convolutions/conv2d_2/Conv2D
  167.19MiB from network/convolutions/conv2d_3/Conv2D
  167.19MiB from network/convolutions/conv2d_4/Conv2D
  167.19MiB from network/convolutions/conv2d_4/Conv2D
  167.19MiB from network/convolutions/conv2d_4/Conv2D
  167.19MiB from network/convolutions/conv2d_2/Conv2D
  167.19MiB from network/convolutions/conv2d_3/Conv2D
  167.19MiB from network/convolutions/conv2d_4/Conv2D
  167.19MiB from network/convolutions/conv2d_2/Conv2D
  167.19MiB from network/convolutions/conv2d_2/Conv2D
  167.19MiB from network/convolutions/conv2d_5/Conv2D
  167.19MiB from network/convolutions/conv2d_3/Conv2D
  167.19MiB from network/convolutions/conv2d_3/Conv2D
  167.19MiB from network/convolutions/conv2d_4/Conv2D
  167.19MiB from network/convolutions/conv2d_4/Conv2D
  167.19MiB from network/convolutions/conv2d_2/Conv2D
  167.19MiB from network/convolutions/conv2d_5/Conv2D
  167.19MiB from network/convolutions/conv2d_5/Conv2D
  167.19MiB from network/convolutions/conv2d_5/Conv2D
  167.19MiB from network/convolutions/conv2d_3/Conv2D
  167.19MiB from network/convolutions/conv2d_4/Conv2D
  167.19MiB from network/convolutions/conv2d_2/Conv2D
  167.19MiB from network/convolutions/conv2d_3/Conv2D
  167.19MiB from network/convolutions/conv2d_4/Conv2D
  167.19MiB from network/convolutions/conv2d_5/Conv2D
  167.19MiB from network/convolutions/conv2d_11/Conv2D
  163.84MiB from network/convolutions/conv2d_7/Conv2D
  160.50MiB from network/convolutions/conv2d_7/Conv2D
  140.99MiB from network/convolutions/conv2d_12/Conv2D
  133.75MiB from network/convolutions/conv2d_6/Conv2D
  133.75MiB from network/convolutions/conv2d_7/Conv2D
  133.75MiB from network/convolutions/conv2d_11/Conv2D
  133.75MiB from network/convolutions/conv2d_11/Conv2D
  133.75MiB from network/convolutions/conv2d_11/Conv2D
  133.75MiB from network/convolutions/conv2d_12/Conv2D
  103.66MiB from network/convolutions/conv2d_8/Conv2D
  83.59MiB from network/convolutions/conv2d/Conv2D
  83.59MiB from network/convolutions/conv2d/Conv2D
  83.59MiB from network/convolutions/conv2d/Conv2D
  83.59MiB from network/convolutions/conv2d/Conv2D
  83.59MiB from network/convolutions/conv2d/Conv2D
  83.59MiB from network/convolutions/conv2d/Conv2D
  83.59MiB from network/convolutions/conv2d/Conv2D
  83.59MiB from network/convolutions/conv2d_6/Conv2D
  83.59MiB from network/convolutions/conv2d/Conv2D
  83.59MiB from network/convolutions/conv2d_6/Conv2D
  83.59MiB from network/convolutions/conv2d_6/Conv2D
  83.59MiB from network/convolutions/conv2d_6/Conv2D
  83.59MiB from network/convolutions/conv2d_6/Conv2D
  83.59MiB from network/convolutions/conv2d_6/Conv2D
  83.59MiB from network/convolutions/conv2d_7/Conv2D
  83.59MiB from network/convolutions/conv2d_6/Conv2D
  83.59MiB from network/convolutions/conv2d_7/Conv2D
  83.59MiB from network/convolutions/conv2d_7/Conv2D
  83.59MiB from network/convolutions/conv2d_8/Conv2D
  83.59MiB from network/convolutions/conv2d_7/Conv2D
  83.59MiB from network/convolutions/conv2d_8/Conv2D
  83.59MiB from network/convolutions/conv2d_7/Conv2D
  83.59MiB from network/convolutions/conv2d_8/Conv2D
  83.59MiB from network/convolutions/conv2d_8/Conv2D
  83.59MiB from network/convolutions/conv2d_8/Conv2D
  83.59MiB from network/convolutions/conv2d_9/Conv2D
  83.59MiB from network/convolutions/conv2d_9/Conv2D
  83.59MiB from network/convolutions/conv2d_8/Conv2D
  83.59MiB from network/convolutions/conv2d_8/Conv2D
  83.59MiB from network/convolutions/conv2d_9/Conv2D
  83.59MiB from network/convolutions/conv2d_9/Conv2D
  83.59MiB from network/convolutions/conv2d_10/Conv2D
  83.59MiB from network/convolutions/conv2d_10/Conv2D
  83.59MiB from network/convolutions/conv2d_9/Conv2D
  83.59MiB from network/convolutions/conv2d_10/Conv2D
  83.59MiB from network/convolutions/conv2d_10/Conv2D
  83.59MiB from network/convolutions/conv2d_9/Conv2D
  83.59MiB from network/convolutions/conv2d_9/Conv2D
  83.59MiB from network/convolutions/conv2d_10/Conv2D
  83.59MiB from network/convolutions/conv2d_9/Conv2D
  83.59MiB from network/convolutions/conv2d_10/Conv2D
  83.59MiB from network/convolutions/conv2d_10/Conv2D
  Remaining 1252 nodes with 98.80MiB
```",2018-01-18T17:01:36Z,1,2,0,3.269282508064391
300,16220,Fix result shape of tf.tensordot unknown when axes is an integer number,"awaiting testing (then merge),cla: yes","#8452 add the function ""Tensordot partial shape inference"", solves the problem #6682.
However, the shape of result of `tensordot` is still `<unknown>` when `axes` is an integer N, which is in common use.
For example, 
```
a = tf.placeholder('float32', shape=[None, 100])
b = tf.placeholder('float32', shape=[100, 300])
```
set `axes=1`,
```
result_tensordot = tf.tensordot(a, b, axes=1)
result_tensordot.get_shape()  # TensorShape(None)
result_tensordot.get_shape().as_list()  # Error
```
The equivalent `axes=[[1], [0]]` behaves correctly,
```
result_tensordot = tf.tensordot(a, b, axes=[[1], [0]])
result_tensordot.get_shape()  # TensorShape([Dimension(None), Dimension(300)])
result_tensordot.get_shape().as_list()  # [None, 300]
```
The simplified is more common and the partial shape should be inferred correctly.
This PR solves the problem.

",1,,5,2018-01-18T12:10:26Z,2018-01-24T18:29:16Z,CONTRIBUTOR,"#8452 add the function ""Tensordot partial shape inference"", solves the problem #6682.
However, the shape of result of `tensordot` is still `<unknown>` when `axes` is an integer N, which is in common use.
For example, 
```
a = tf.placeholder('float32', shape=[None, 100])
b = tf.placeholder('float32', shape=[100, 300])
```
set `axes=1`,
```
result_tensordot = tf.tensordot(a, b, axes=1)
result_tensordot.get_shape()  # TensorShape(None)
result_tensordot.get_shape().as_list()  # Error
```
The equivalent `axes=[[1], [0]]` behaves correctly,
```
result_tensordot = tf.tensordot(a, b, axes=[[1], [0]])
result_tensordot.get_shape()  # TensorShape([Dimension(None), Dimension(300)])
result_tensordot.get_shape().as_list()  # [None, 300]
```
The simplified is more common and the partial shape should be inferred correctly.
This PR solves the problem.

",2018-01-23T02:11:01Z,6,2,1,5.769282508064391
301,16217,Windows: Add missing dependencies in lib_proto_parsing,cla: yes,"Fix http://ci.tensorflow.org/job/tf-master-win-bzl/2275/console
Culprit: https://github.com/tensorflow/tensorflow/commit/ccbd14b741e6efbe51769f0f1b9cb3719c42c23b
@gunan @panyx0718 ",0,,2,2018-01-18T09:11:23Z,2018-01-19T09:07:14Z,MEMBER,"Fix http://ci.tensorflow.org/job/tf-master-win-bzl/2275/console
Culprit: https://github.com/tensorflow/tensorflow/commit/ccbd14b741e6efbe51769f0f1b9cb3719c42c23b
@gunan @panyx0718 ",2018-01-18T09:11:42Z,1,3,0,4.269282508064391
302,16215,Tensorflow doesn't delete previous checkpoints,"stat:awaiting response,type:support","### System information
- Linux Ubuntu 16.04:
- Tensorflow version 1.4.1*:
- Python 3.5.2: 

### Describe the problem

A brief summary is that, if I run multiple times my training script tensorflow doesn't delete the checkpoints created in previous runs of the script.

I am preparing a automatic script that every X days runs and train with the new data collected. But I am facing a problem, even that I have configured the saver to keep the 2 last checkpoints, it doesn't work as I expected. 

Example:
I configure to run 100.000 iterations and each 10.000 to save the checkpoint. The system works and starts saving 10.000, 20.000, ... And when get to 30.000 starts deleting the firsts checkpoints. When the script ends I have the 2 last checkpoints(90.000 and 100.000). 

Then when I train again the system starts from the last checkpoint, in this example the 100.000, and do the same as the previous, 110.000, 120.000,.. and when gets to the 130.000 starts to delete the 100.000 and so on. But the 2 checkpoints from the previous run(90.000 and 100.000) remain there even that in the checkpoint txt are not listed there.

This will be repeated in every run of the script, creating files that I don't need anymore and growing during the time.

This is an intended behavior(expecting to the user to delete or manage manually) or it is really a problem?
It exist any workaround?

Thank you for your time and amazing work. 
 ",0,,2,2018-01-18T08:40:20Z,2018-01-18T23:24:11Z,NONE,"### System information
- Linux Ubuntu 16.04:
- Tensorflow version 1.4.1*:
- Python 3.5.2: 

### Describe the problem

A brief summary is that, if I run multiple times my training script tensorflow doesn't delete the checkpoints created in previous runs of the script.

I am preparing a automatic script that every X days runs and train with the new data collected. But I am facing a problem, even that I have configured the saver to keep the 2 last checkpoints, it doesn't work as I expected. 

Example:
I configure to run 100.000 iterations and each 10.000 to save the checkpoint. The system works and starts saving 10.000, 20.000, ... And when get to 30.000 starts deleting the firsts checkpoints. When the script ends I have the 2 last checkpoints(90.000 and 100.000). 

Then when I train again the system starts from the last checkpoint, in this example the 100.000, and do the same as the previous, 110.000, 120.000,.. and when gets to the 130.000 starts to delete the 100.000 and so on. But the 2 checkpoints from the previous run(90.000 and 100.000) remain there even that in the checkpoint txt are not listed there.

This will be repeated in every run of the script, creating files that I don't need anymore and growing during the time.

This is an intended behavior(expecting to the user to delete or manage manually) or it is really a problem?
It exist any workaround?

Thank you for your time and amazing work. 
 ",2018-01-18T19:04:23Z,0,1,0,2.269282508064391
303,16208,using string_input_producer with train dataset and validate dataset,stat:awaiting response,"I have two datasets(files), for train and validate respectively. I can successfully load training set thru tf.train.string_input_producer, set num_epochs=5. Then I can iteratively get batch of data to optimize my model.
But, I got stuck when trying to load my validation set by the same way, the program keeps saying ""OutOfRange Error"" even I didn't set num_epochs in string_input_producer.
Can you supply an example that using string_input_producer  with two or more dataset?
same as the question on stackoverflow: [here](https://stackoverflow.com/questions/37068324/read-big-train-validation-test-datasets-in-tensorflow)
Please help me solve the problem. Thank you very much.
",0,,2,2018-01-18T03:21:26Z,2018-01-26T02:45:16Z,NONE,"I have two datasets(files), for train and validate respectively. I can successfully load training set thru tf.train.string_input_producer, set num_epochs=5. Then I can iteratively get batch of data to optimize my model.
But, I got stuck when trying to load my validation set by the same way, the program keeps saying ""OutOfRange Error"" even I didn't set num_epochs in string_input_producer.
Can you supply an example that using string_input_producer  with two or more dataset?
same as the question on stackoverflow: [here](https://stackoverflow.com/questions/37068324/read-big-train-validation-test-datasets-in-tensorflow)
Please help me solve the problem. Thank you very much.
",2018-01-18T13:27:03Z,8,1,2,2.269282508064391
304,16206,make label_image for tflite build again,"awaiting testing (then merge),cla: yes","1. add namespace to label_image.h to make label_image for tflite build again
2. add --config monolithic and mention NDK settings in label_image.md
3. fix a typo in display_usage()",1,,2,2018-01-18T02:33:26Z,2018-01-25T22:24:23Z,CONTRIBUTOR,"1. add namespace to label_image.h to make label_image for tflite build again
2. add --config monolithic and mention NDK settings in label_image.md
3. fix a typo in display_usage()",2018-01-25T22:24:20Z,7,2,1,4.269282508064391
305,16200,Accepts `PathLike` objects for `model_dir`,"awaiting testing (then merge),cla: yes","* Retrieves the file system path representation if `PathLike` object is passed to `Estimator` or `RunConfig` for `model_dir`, instead of `str`.
* Closes #15784",0,,6,2018-01-17T23:27:52Z,2018-01-20T23:36:15Z,CONTRIBUTOR,"* Retrieves the file system path representation if `PathLike` object is passed to `Estimator` or `RunConfig` for `model_dir`, instead of `str`.
* Closes #15784",2018-01-17T23:37:01Z,3,2,1,5.267546386419054
306,16198,Unable to build Tensorflow Benchmark model for Android,,"I've been trying to benchmark the model on mobile but I'm not able to build the model for Android. For desktop, I have been able to build and run the benchmark model.

The machine I'm using is a MacBook pro 15 inch with High Sierra and tensorflow v.1.4

I've been following the directions given at the following links:

(https://www.tensorflow.org/mobile/optimizing#how_to_profile_your_model
https://github.com/tensorflow/tensorflow/tree/r1.4/tensorflow/tools/benchmark)

Edit: Updated answer to the issue template

Have I written custom code

- No custom code was written

OS Platform and Distribution

- Mac OS High Sierra

TensorFlow installed from

- Tensorflow installed from source

TensorFlow version

- 1.4

Bazel version

- Bazel version 0.9.0

CUDA/cuDNN version

- N/A

GPU model and memory

- N/A

Exact command to reproduce

```
bazel build -c opt --cpu=armeabi-v7a \
  --crosstool_top=//external:android/crosstool \
  --host_crosstool_top=@bazel_tools//tools/cpp:toolchain \
  tensorflow/tools/benchmark:benchmark_model
```

Error message received

```
ERROR: /private/var/tmp/_bazel_abhisheksehgal/486c675b3107dc770b2281b905f670fe/external/highwayhash/BUILD:8:1: C++ compilation of rule '@highwayhash//:sip_hash' failed (Exit 1)
In file included from external/highwayhash/highwayhash/sip_hash.cc:15:
In file included from external/highwayhash/highwayhash/sip_hash.h:25:
external/highwayhash/highwayhash/state_helpers.h:76:3: error: use of undeclared identifier 'static_assert'; did you mean 'static_cast'?
  static_assert((kPacketSize & (kPacketSize - 1)) == 0, ""Size must be 2^i."");
  ^
In file included from external/highwayhash/highwayhash/sip_hash.cc:15:
external/highwayhash/highwayhash/sip_hash.h:33:15: warning: alias declarations are a C++11 extension [-Wc++11-extensions]
  using Key = HH_U64[2];
              ^
external/highwayhash/highwayhash/sip_hash.h:104:22: warning: alias declarations are a C++11 extension [-Wc++11-extensions]
using SipHashState = SipHashStateT<2, 4>;
                     ^
external/highwayhash/highwayhash/sip_hash.h:105:24: warning: alias declarations are a C++11 extension [-Wc++11-extensions]
using SipHash13State = SipHashStateT<1, 3>;
                       ^
external/highwayhash/highwayhash/sip_hash.cc:20:13: warning: alias declarations are a C++11 extension [-Wc++11-extensions]
using Key = highwayhash::SipHashState::Key;
            ^
external/highwayhash/highwayhash/sip_hash.cc:21:15: warning: alias declarations are a C++11 extension [-Wc++11-extensions]
using Key13 = highwayhash::SipHash13State::Key;
              ^
5 warnings and 1 error generated.
Target //tensorflow/tools/benchmark:benchmark_model failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 2.060s, Critical Path: 1.60s
FAILED: Build did NOT complete successfully
```",0,,5,2018-01-17T22:45:21Z,2018-01-19T20:32:12Z,NONE,"I've been trying to benchmark the model on mobile but I'm not able to build the model for Android. For desktop, I have been able to build and run the benchmark model.

The machine I'm using is a MacBook pro 15 inch with High Sierra and tensorflow v.1.4

I've been following the directions given at the following links:

(https://www.tensorflow.org/mobile/optimizing#how_to_profile_your_model
https://github.com/tensorflow/tensorflow/tree/r1.4/tensorflow/tools/benchmark)

Edit: Updated answer to the issue template

Have I written custom code

- No custom code was written

OS Platform and Distribution

- Mac OS High Sierra

TensorFlow installed from

- Tensorflow installed from source

TensorFlow version

- 1.4

Bazel version

- Bazel version 0.9.0

CUDA/cuDNN version

- N/A

GPU model and memory

- N/A

Exact command to reproduce

```
bazel build -c opt --cpu=armeabi-v7a \
  --crosstool_top=//external:android/crosstool \
  --host_crosstool_top=@bazel_tools//tools/cpp:toolchain \
  tensorflow/tools/benchmark:benchmark_model
```

Error message received

```
ERROR: /private/var/tmp/_bazel_abhisheksehgal/486c675b3107dc770b2281b905f670fe/external/highwayhash/BUILD:8:1: C++ compilation of rule '@highwayhash//:sip_hash' failed (Exit 1)
In file included from external/highwayhash/highwayhash/sip_hash.cc:15:
In file included from external/highwayhash/highwayhash/sip_hash.h:25:
external/highwayhash/highwayhash/state_helpers.h:76:3: error: use of undeclared identifier 'static_assert'; did you mean 'static_cast'?
  static_assert((kPacketSize & (kPacketSize - 1)) == 0, ""Size must be 2^i."");
  ^
In file included from external/highwayhash/highwayhash/sip_hash.cc:15:
external/highwayhash/highwayhash/sip_hash.h:33:15: warning: alias declarations are a C++11 extension [-Wc++11-extensions]
  using Key = HH_U64[2];
              ^
external/highwayhash/highwayhash/sip_hash.h:104:22: warning: alias declarations are a C++11 extension [-Wc++11-extensions]
using SipHashState = SipHashStateT<2, 4>;
                     ^
external/highwayhash/highwayhash/sip_hash.h:105:24: warning: alias declarations are a C++11 extension [-Wc++11-extensions]
using SipHash13State = SipHashStateT<1, 3>;
                       ^
external/highwayhash/highwayhash/sip_hash.cc:20:13: warning: alias declarations are a C++11 extension [-Wc++11-extensions]
using Key = highwayhash::SipHashState::Key;
            ^
external/highwayhash/highwayhash/sip_hash.cc:21:15: warning: alias declarations are a C++11 extension [-Wc++11-extensions]
using Key13 = highwayhash::SipHash13State::Key;
              ^
5 warnings and 1 error generated.
Target //tensorflow/tools/benchmark:benchmark_model failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 2.060s, Critical Path: 1.60s
FAILED: Build did NOT complete successfully
```",2018-01-18T07:03:03Z,2,1,0,3.767546386419054
307,16196,Fix issue of branch switching not working with bazel,"awaiting review,cla: yes","This fix tries to address the issue raised in #15957 where bazel stops working after switching git branch, and reconfigure with `./configure` will not work as well.

This fix adds a quick fix as was suggested, by having `export TF_CONFIG_TIME=""$(date)"" `in configure.py and add it to the environ list in git_configure.bzl.

This fix fixes #15957.

Signed-off-by: Yong Tang <yong.tang.github@outlook.com>",0,,2,2018-01-17T19:00:24Z,2018-01-19T21:40:07Z,MEMBER,"This fix tries to address the issue raised in #15957 where bazel stops working after switching git branch, and reconfigure with `./configure` will not work as well.

This fix adds a quick fix as was suggested, by having `export TF_CONFIG_TIME=""$(date)"" `in configure.py and add it to the environ list in git_configure.bzl.

This fix fixes #15957.

Signed-off-by: Yong Tang <yong.tang.github@outlook.com>",2018-01-19T18:27:46Z,2,3,0,4.267546386419054
308,16194,"Cannot interpret feed_dict key as Tensor: The name 'DecodeJpeg/contents:0' refers to a Tensor which does not exist. The operation, 'DecodeJpeg/contents', does not exist in the graph.",stat:awaiting response,"Hello,

I try to get the output of each layer of my CNN. Here is the full example:
```
`from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import argparse
import sys
import tempfile
import os
import DatasetReader as dr
import numpy as np
import matplotlib.pyplot as plt


import tensorflow as tf
import utils
FLAGS = None
PLOT_DIR = './output/plots'


def deepnn(x):

  with tf.name_scope('reshape'):

    x_image = tf.reshape(x, [-1, 100, 100, 1])#(x, [-1, 28, 28, 1])

  # First convolutional layer - maps one grayscale image to 32 feature maps.
  with tf.name_scope('conv1'):
    W_conv1 = weight_variable([5, 5, 1, 32])#([5, 5, 1, 32])
    b_conv1 = bias_variable([32])
    # conv1dis = conv2d(x_image, W_conv1) + b_conv1
    h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)
    tf.add_to_collection('conv_weights', conv2d(x_image, W_conv1))

  # Pooling layer - downsamples by 2X.
  with tf.name_scope('pool1'):
    h_pool1 = max_pool_2x2(h_conv1)

  # Second convolutional layer -- maps 32 feature maps to 64.
  with tf.name_scope('conv2'):
    W_conv2 = weight_variable([5, 5, 32, 64])
    b_conv2 = bias_variable([64])
    # conv2dis = conv2d(h_pool1, W_conv2) + b_conv2
    h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)
    # tf.add_to_collection('conv_weights', h_conv2)

  # Second pooling layer.
  with tf.name_scope('pool2'):
    h_pool2 = max_pool_2x2(h_conv2)

  # Fully connected layer 1 -- after 2 round of downsampling, our 28x28 image
  # is down to 7x7x64 feature maps -- maps this to 1024 features.
  with tf.name_scope('fc1'):
    W_fc1 = weight_variable([25 * 25 * 64, 1024])
    b_fc1 = bias_variable([1024])
    h_pool2_flat = tf.reshape(h_pool2, [-1, 25*25*64])
    h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)
    # tf.add_to_collection('conv_weights', W_fc1)

  # Dropout - controls the complexity of the model, prevents co-adaptation of
  # features.
  with tf.name_scope('dropout'):
    keep_prob = tf.placeholder(tf.float32)
    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)

  # Map the 1024 features to 10 classes, one for each digit
  with tf.name_scope('fc2'):
    W_fc2 = weight_variable([1024, 2])
    b_fc2 = bias_variable([2])

  y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2

  return y_conv, keep_prob


def conv2d(x, W):
  """"""conv2d returns a 2d convolution layer with full stride.""""""
  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')


def max_pool_2x2(x):
  """"""max_pool_2x2 downsamples a feature map by 2X.""""""
  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],
                        strides=[1, 2, 2, 1], padding='SAME')


def weight_variable(shape):
  """"""weight_variable generates a weight variable of a given shape.""""""
  initial = tf.truncated_normal(shape, stddev=0.1)
  return tf.Variable(initial)


def bias_variable(shape):
  """"""bias_variable generates a bias variable of a given shape.""""""
  initial = tf.constant(0.1, shape=shape)
  return tf.Variable(initial)


def main(_):
  # Import data
  V0Dataset = dr.read_data_sets(FLAGS.data_dir, one_hot=True)

  datasize = 10000
  # Create the model
  x = tf.placeholder(tf.float32, [None, datasize])#224*172])

  # Define loss and optimizer
  y_ = tf.placeholder(tf.float32, [None, 2])
  print(""logits shape {}"".format(y_))


  # # Build the graph for the deep net
  y_conv, keep_prob = deepnn(x)

  with tf.name_scope('loss'):
    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=y_,
                                                            logits=y_conv)
  cross_entropy = tf.reduce_mean(cross_entropy)

  with tf.name_scope('adam_optimizer'):
    train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)

  with tf.name_scope('accuracy'):
    correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))
    correct_prediction = tf.cast(correct_prediction, tf.float32)

  accuracy = tf.reduce_mean(correct_prediction)

  print('cross_entropy {}'.format(cross_entropy))
  print('accuracy {}'.format(accuracy))



  with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    for i in range(2):#500):
      batch = V0Dataset.train.next_batch(10)
      a = batch[1];
      a = a.reshape(10,2)
      train_step.run(feed_dict={x: batch[0], y_: a, keep_prob: 0.5})



    graph_location = tempfile.mkdtemp()
    print('Saving graph to: %s' % graph_location)
    train_writer = tf.summary.FileWriter(""/tmp/tensorflow/"")
    train_writer.add_graph(tf.get_default_graph())

    conv0 = sess.graph.get_tensor_by_name('conv1/Conv2D:0')
    print(""conv0 {}"".format(conv0))

    predictions0 = sess.run(conv0,
                           {'DecodeJpeg/contents:0': batch[0]}) # Error!!!!
    print(""predictions0 {}"".format(predictions0))
    print(""predictions0 {}"".format(predictions0.size))

```

Here are the errors I get:
```
`Traceback (most recent call last):
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1064, in _run
    allow_operation=False)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py"", line 3035, in as_graph_element
    return self._as_graph_element_locked(obj, allow_tensor, allow_operation)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py"", line 3077, in _as_graph_element_locked
    ""graph."" % (repr(name), repr(op_name)))
KeyError: ""The name 'DecodeJpeg/contents:0' refers to a Tensor which does not exist. The operation, 'DecodeJpeg/contents', does not exist in the graph.""

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""./deep_charging_station_train.py"", line 309, in <module>
    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/platform/app.py"", line 48, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""./deep_charging_station_train.py"", line 297, in main
    {'DecodeJpeg/contents:0': batch[0]})
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 889, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1067, in _run
    + e.args[0])
TypeError: Cannot interpret feed_dict key as Tensor: The name 'DecodeJpeg/contents:0' refers to a Tensor which does not exist. The operation, 'DecodeJpeg/contents', does not exist in the graph.
`



```

I don't understand why this appends. I looked with Tensorboard I don't know where should I get the DecodeJpeg informations of the layer

Edit:
Have I written custom code : I use deep mnist tutorial example and I modify the size of the input image
OS Platform and Distribution : Ubuntu 16.04
TensorFlow installed from
TensorFlow version 1.4.0
Bazel version N/A
CUDA/cuDNN version N/A
GPU model and memory N/A
Exact command to reproduce
",0,,7,2018-01-17T16:09:30Z,2018-02-15T00:07:39Z,NONE,"Hello,

I try to get the output of each layer of my CNN. Here is the full example:
```
`from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import argparse
import sys
import tempfile
import os
import DatasetReader as dr
import numpy as np
import matplotlib.pyplot as plt


import tensorflow as tf
import utils
FLAGS = None
PLOT_DIR = './output/plots'


def deepnn(x):

  with tf.name_scope('reshape'):

    x_image = tf.reshape(x, [-1, 100, 100, 1])#(x, [-1, 28, 28, 1])

  # First convolutional layer - maps one grayscale image to 32 feature maps.
  with tf.name_scope('conv1'):
    W_conv1 = weight_variable([5, 5, 1, 32])#([5, 5, 1, 32])
    b_conv1 = bias_variable([32])
    # conv1dis = conv2d(x_image, W_conv1) + b_conv1
    h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)
    tf.add_to_collection('conv_weights', conv2d(x_image, W_conv1))

  # Pooling layer - downsamples by 2X.
  with tf.name_scope('pool1'):
    h_pool1 = max_pool_2x2(h_conv1)

  # Second convolutional layer -- maps 32 feature maps to 64.
  with tf.name_scope('conv2'):
    W_conv2 = weight_variable([5, 5, 32, 64])
    b_conv2 = bias_variable([64])
    # conv2dis = conv2d(h_pool1, W_conv2) + b_conv2
    h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)
    # tf.add_to_collection('conv_weights', h_conv2)

  # Second pooling layer.
  with tf.name_scope('pool2'):
    h_pool2 = max_pool_2x2(h_conv2)

  # Fully connected layer 1 -- after 2 round of downsampling, our 28x28 image
  # is down to 7x7x64 feature maps -- maps this to 1024 features.
  with tf.name_scope('fc1'):
    W_fc1 = weight_variable([25 * 25 * 64, 1024])
    b_fc1 = bias_variable([1024])
    h_pool2_flat = tf.reshape(h_pool2, [-1, 25*25*64])
    h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)
    # tf.add_to_collection('conv_weights', W_fc1)

  # Dropout - controls the complexity of the model, prevents co-adaptation of
  # features.
  with tf.name_scope('dropout'):
    keep_prob = tf.placeholder(tf.float32)
    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)

  # Map the 1024 features to 10 classes, one for each digit
  with tf.name_scope('fc2'):
    W_fc2 = weight_variable([1024, 2])
    b_fc2 = bias_variable([2])

  y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2

  return y_conv, keep_prob


def conv2d(x, W):
  """"""conv2d returns a 2d convolution layer with full stride.""""""
  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')


def max_pool_2x2(x):
  """"""max_pool_2x2 downsamples a feature map by 2X.""""""
  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],
                        strides=[1, 2, 2, 1], padding='SAME')


def weight_variable(shape):
  """"""weight_variable generates a weight variable of a given shape.""""""
  initial = tf.truncated_normal(shape, stddev=0.1)
  return tf.Variable(initial)


def bias_variable(shape):
  """"""bias_variable generates a bias variable of a given shape.""""""
  initial = tf.constant(0.1, shape=shape)
  return tf.Variable(initial)


def main(_):
  # Import data
  V0Dataset = dr.read_data_sets(FLAGS.data_dir, one_hot=True)

  datasize = 10000
  # Create the model
  x = tf.placeholder(tf.float32, [None, datasize])#224*172])

  # Define loss and optimizer
  y_ = tf.placeholder(tf.float32, [None, 2])
  print(""logits shape {}"".format(y_))


  # # Build the graph for the deep net
  y_conv, keep_prob = deepnn(x)

  with tf.name_scope('loss'):
    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=y_,
                                                            logits=y_conv)
  cross_entropy = tf.reduce_mean(cross_entropy)

  with tf.name_scope('adam_optimizer'):
    train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)

  with tf.name_scope('accuracy'):
    correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))
    correct_prediction = tf.cast(correct_prediction, tf.float32)

  accuracy = tf.reduce_mean(correct_prediction)

  print('cross_entropy {}'.format(cross_entropy))
  print('accuracy {}'.format(accuracy))



  with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    for i in range(2):#500):
      batch = V0Dataset.train.next_batch(10)
      a = batch[1];
      a = a.reshape(10,2)
      train_step.run(feed_dict={x: batch[0], y_: a, keep_prob: 0.5})



    graph_location = tempfile.mkdtemp()
    print('Saving graph to: %s' % graph_location)
    train_writer = tf.summary.FileWriter(""/tmp/tensorflow/"")
    train_writer.add_graph(tf.get_default_graph())

    conv0 = sess.graph.get_tensor_by_name('conv1/Conv2D:0')
    print(""conv0 {}"".format(conv0))

    predictions0 = sess.run(conv0,
                           {'DecodeJpeg/contents:0': batch[0]}) # Error!!!!
    print(""predictions0 {}"".format(predictions0))
    print(""predictions0 {}"".format(predictions0.size))

```

Here are the errors I get:
```
`Traceback (most recent call last):
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1064, in _run
    allow_operation=False)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py"", line 3035, in as_graph_element
    return self._as_graph_element_locked(obj, allow_tensor, allow_operation)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py"", line 3077, in _as_graph_element_locked
    ""graph."" % (repr(name), repr(op_name)))
KeyError: ""The name 'DecodeJpeg/contents:0' refers to a Tensor which does not exist. The operation, 'DecodeJpeg/contents', does not exist in the graph.""

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""./deep_charging_station_train.py"", line 309, in <module>
    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/platform/app.py"", line 48, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""./deep_charging_station_train.py"", line 297, in main
    {'DecodeJpeg/contents:0': batch[0]})
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 889, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1067, in _run
    + e.args[0])
TypeError: Cannot interpret feed_dict key as Tensor: The name 'DecodeJpeg/contents:0' refers to a Tensor which does not exist. The operation, 'DecodeJpeg/contents', does not exist in the graph.
`



```

I don't understand why this appends. I looked with Tensorboard I don't know where should I get the DecodeJpeg informations of the layer

Edit:
Have I written custom code : I use deep mnist tutorial example and I modify the size of the input image
OS Platform and Distribution : Ubuntu 16.04
TensorFlow installed from
TensorFlow version 1.4.0
Bazel version N/A
CUDA/cuDNN version N/A
GPU model and memory N/A
Exact command to reproduce
",2018-01-18T01:24:38Z,28,1,4,4.767546386419054
309,16193,Performance issues with TF1.5 on CPU,type:bug/performance,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.5.0-rc1
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: 0.5.4
- **GCC/Compiler version (if compiling from source)**: 5.4.0
- **CUDA/cuDNN version**: NA
- **GPU model and memory**: NA
- **Exact command to reproduce**:

Hello,
I'm facing performance issues with the last releases of TF using a CPU.
I'm using the [benchmark tool](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/benchmark/benchmark_model.cc) to calculate mean inference time of a model.

For example, in order to evaluate mobilenet (trained on a custom dataset), I'm using this command :
`bazel-bin/tensorflow/tools/benchmark/benchmark_model --graph=""path to mobilenet graph"" --input_layer=""input"" --input_layer_shape=""1,224,224,3"" --input_layer_type=""float"" --output_layer=""MobilenetV1/Predictions/Reshape_1""`
After setting CUDA_VISIBLE_DEVICES to """" in order to run on CPU.

With TF 1.4.1, I obtain a mean inference time equals to 26ms (13ms if I compile with optimization flags).
Using tf 1.5.*, I obtain a mean inference time equals to 51ms (45ms if I compile with optimization flags).

The loss is very important, so I'm wondering if it's a known issue and how I can improve this.

I tried with tags/v1.5.0-rc0, tags/v1.5.0-rc1 and master, and the problem is the same.

Thank you",1,,8,2018-01-17T13:39:19Z,2018-01-22T09:01:17Z,NONE,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.5.0-rc1
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: 0.5.4
- **GCC/Compiler version (if compiling from source)**: 5.4.0
- **CUDA/cuDNN version**: NA
- **GPU model and memory**: NA
- **Exact command to reproduce**:

Hello,
I'm facing performance issues with the last releases of TF using a CPU.
I'm using the [benchmark tool](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/benchmark/benchmark_model.cc) to calculate mean inference time of a model.

For example, in order to evaluate mobilenet (trained on a custom dataset), I'm using this command :
`bazel-bin/tensorflow/tools/benchmark/benchmark_model --graph=""path to mobilenet graph"" --input_layer=""input"" --input_layer_shape=""1,224,224,3"" --input_layer_type=""float"" --output_layer=""MobilenetV1/Predictions/Reshape_1""`
After setting CUDA_VISIBLE_DEVICES to """" in order to run on CPU.

With TF 1.4.1, I obtain a mean inference time equals to 26ms (13ms if I compile with optimization flags).
Using tf 1.5.*, I obtain a mean inference time equals to 51ms (45ms if I compile with optimization flags).

The loss is very important, so I'm wondering if it's a known issue and how I can improve this.

I tried with tags/v1.5.0-rc0, tags/v1.5.0-rc1 and master, and the problem is the same.

Thank you",2018-01-17T21:15:04Z,5,1,1,6.267546386419054
310,16192,how to set ignore_label in tensorflow?,,"when i set the label=-1, there is an error: Received a label value of -1 which is outside the valid range of [0, 8)",0,,4,2018-01-17T13:31:38Z,2018-01-24T13:24:20Z,NONE,"when i set the label=-1, there is an error: Received a label value of -1 which is outside the valid range of [0, 8)",2018-01-18T01:24:34Z,7,1,1,3.267546386419054
311,16191,Fix docstring typo of losses_impl.py,cla: yes,"Add missing ""`"" to the docstring.",0,,3,2018-01-17T12:58:14Z,2018-01-17T14:58:11Z,CONTRIBUTOR,"Add missing ""`"" to the docstring.",2018-01-17T13:02:57Z,0,2,0,3.767546386419054
312,16190,ValueError: Inputs to `Dense` should have rank >= 2.,stat:awaiting response,"##error

Traceback (most recent call last):
  File ""firstGANtf.py"", line 90, in <module>
    G = Generator(input_size=g_input_size, hidden_size=g_hidden_size, output_size=g_output_size)
  File ""firstGANtf.py"", line 55, in __init__
    self.map1 = tf.contrib.layers.linear(inputs=input_size , num_outputs=hidden_size)
  File ""/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/contrib/framework/python/ops/arg_scope.py"", line 177, in func_with_args
    return func(*args, **current_args)
  File ""/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/contrib/layers/python/layers/layers.py"", line 1409, in fully_connected
    outputs = layer.apply(inputs)
  File ""/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/layers/base.py"", line 303, in apply
    return self.__call__(inputs, **kwargs)
  File ""/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/layers/base.py"", line 269, in __call__
    self.build(input_shapes[0])
  File ""/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/layers/core.py"", line 110, in build
    raise ValueError('Inputs to `Dense` should have rank >= 2.')
ValueError: Inputs to `Dense` should have rank >= 2.


##code

       self.map1 = tf.contrib.layers.linear(inputs=input_size , num_outputs=hidden_size)
        self.map2 = tf.contrib.layers.linear(inputs=hidden_size,  num_outputs=hidden_size)
        self.map3 = tf.contrib.layers.linear(inputs=hidden_size, num_outputs=output_size)",0,,5,2018-01-17T11:05:49Z,2018-02-07T19:58:12Z,NONE,"##error

Traceback (most recent call last):
  File ""firstGANtf.py"", line 90, in <module>
    G = Generator(input_size=g_input_size, hidden_size=g_hidden_size, output_size=g_output_size)
  File ""firstGANtf.py"", line 55, in __init__
    self.map1 = tf.contrib.layers.linear(inputs=input_size , num_outputs=hidden_size)
  File ""/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/contrib/framework/python/ops/arg_scope.py"", line 177, in func_with_args
    return func(*args, **current_args)
  File ""/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/contrib/layers/python/layers/layers.py"", line 1409, in fully_connected
    outputs = layer.apply(inputs)
  File ""/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/layers/base.py"", line 303, in apply
    return self.__call__(inputs, **kwargs)
  File ""/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/layers/base.py"", line 269, in __call__
    self.build(input_shapes[0])
  File ""/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/layers/core.py"", line 110, in build
    raise ValueError('Inputs to `Dense` should have rank >= 2.')
ValueError: Inputs to `Dense` should have rank >= 2.


##code

       self.map1 = tf.contrib.layers.linear(inputs=input_size , num_outputs=hidden_size)
        self.map2 = tf.contrib.layers.linear(inputs=hidden_size,  num_outputs=hidden_size)
        self.map3 = tf.contrib.layers.linear(inputs=hidden_size, num_outputs=output_size)",2018-01-18T01:27:14Z,20,1,3,3.767546386419054
313,16188,benchmark_model tool not build successfully for android version,,"Hello,

I try to build the benchmark_model for the android, but I encounter some errors.
Please help, is any setting not correct?

The configuration of the SDK and NDK in the WORKSPACE is
android_sdk_repository(
    name = ""androidsdk"",
    api_level = 23,
    build_tools_version = ""26.0.1"",
    path = ""/home/kk/android_sdk/android-sdk-linux"",
)

android_ndk_repository(
    name=""androidndk"",
    path=""/home/kk/android_sdk/ndk/android-ndk-r14"",
    api_level=14)

Use the command to build:
bazel build --cxxopt='--std=c++11' -c opt 
--crosstool_top=//external:android/crosstool --cpu=armeabi-v7a --host_crosstool_top=@bazel_tools//tools/cpp:toolchain tensorflow/tools/benchmark:benchmark_model

There are three errors
1. external/gif_archive/lib/openbsd-reallocarray.c:33:19: error: use of undeclared identifier 'SIZE_MAX'
2. tensorflow/core/common_runtime/gpu/gpu_debug_allocator.cc:172:53: error: no member named 'nanf' in namespace 'std'; did you mean simply 'nanf'?
3. external/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/lib/gcc/arm-linux-androideabi/4.9.x/../../../../arm-linux-androideabi/bin/ld: error: cannot find -lpthread

Thanks

Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 14.04.4 LTS
- **TensorFlow installed from (source or binary)**:use the pip install
- **TensorFlow version (use command below)**:1.4.0
- **Python version**: Python 2.7.6
- **Bazel version (if compiling from source)**:0.9.0
- **GCC/Compiler version (if compiling from source)**:(Ubuntu 4.8.4-2ubuntu1~14.04.3) 4.8.4
- **CUDA/cuDNN version**:NA
- **GPU model and memory**:NA
- **Exact command to reproduce**:NA

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",0,,4,2018-01-17T09:46:35Z,2018-01-19T07:00:17Z,NONE,"Hello,

I try to build the benchmark_model for the android, but I encounter some errors.
Please help, is any setting not correct?

The configuration of the SDK and NDK in the WORKSPACE is
android_sdk_repository(
    name = ""androidsdk"",
    api_level = 23,
    build_tools_version = ""26.0.1"",
    path = ""/home/kk/android_sdk/android-sdk-linux"",
)

android_ndk_repository(
    name=""androidndk"",
    path=""/home/kk/android_sdk/ndk/android-ndk-r14"",
    api_level=14)

Use the command to build:
bazel build --cxxopt='--std=c++11' -c opt 
--crosstool_top=//external:android/crosstool --cpu=armeabi-v7a --host_crosstool_top=@bazel_tools//tools/cpp:toolchain tensorflow/tools/benchmark:benchmark_model

There are three errors
1. external/gif_archive/lib/openbsd-reallocarray.c:33:19: error: use of undeclared identifier 'SIZE_MAX'
2. tensorflow/core/common_runtime/gpu/gpu_debug_allocator.cc:172:53: error: no member named 'nanf' in namespace 'std'; did you mean simply 'nanf'?
3. external/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/lib/gcc/arm-linux-androideabi/4.9.x/../../../../arm-linux-androideabi/bin/ld: error: cannot find -lpthread

Thanks

Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 14.04.4 LTS
- **TensorFlow installed from (source or binary)**:use the pip install
- **TensorFlow version (use command below)**:1.4.0
- **Python version**: Python 2.7.6
- **Bazel version (if compiling from source)**:0.9.0
- **GCC/Compiler version (if compiling from source)**:(Ubuntu 4.8.4-2ubuntu1~14.04.3) 4.8.4
- **CUDA/cuDNN version**:NA
- **GPU model and memory**:NA
- **Exact command to reproduce**:NA

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",2018-01-18T16:15:45Z,2,1,0,3.267546386419054
314,16186,A bug when applying MultiRNNCell?,,"
------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: This code is very similar to an official example
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **TensorFlow installed from (source or binary)**: pip install tensorflow
- **TensorFlow version (use command below)**: b'unknown' 1.4.0
- **Python version**: Python 3.5.2 :: Anaconda 4.2.0 (64-bit)
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
tf.nn.MultiRNNCell sometimes doesn't work.

It raises an issue like this:
ValueError: Dimensions must be equal, but are 64 and 96 for 'lstm/rnn/while/rnn/multi_rnn_cell/cell_0/cell_0/basic_lstm_cell/MatMul_1' (op: 'MatMul') with input shapes: [128,64], [96,128].

### Source code / logs
  import tensorflow as tf
  import numpy as np

  hidden_layer_size = 32
  embed = tf.zeros((128, 6, 64), dtype=tf.float32)

  num_LSTM_layers = 2
  with tf.variable_scope(""lstm""):
    
    lstm_cell = tf.contrib.rnn.BasicLSTMCell(hidden_layer_size, forget_bias=1.0)
    cell = tf.contrib.rnn.MultiRNNCell(cells=[lstm_cell]*num_LSTM_layers, state_is_tuple=True)
    outputs, states = tf.nn.dynamic_rnn(cell, embed, dtype=tf.float32)
   
Error:
ValueError: Dimensions must be equal, but are 64 and 96 for 'lstm/rnn/while/rnn/multi_rnn_cell/cell_0/cell_0/basic_lstm_cell/MatMul_1' (op: 'MatMul') with input shapes: [128,64], [96,128].

",0,,3,2018-01-17T09:23:15Z,2018-01-30T23:31:09Z,NONE,"
------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: This code is very similar to an official example
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **TensorFlow installed from (source or binary)**: pip install tensorflow
- **TensorFlow version (use command below)**: b'unknown' 1.4.0
- **Python version**: Python 3.5.2 :: Anaconda 4.2.0 (64-bit)
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
tf.nn.MultiRNNCell sometimes doesn't work.

It raises an issue like this:
ValueError: Dimensions must be equal, but are 64 and 96 for 'lstm/rnn/while/rnn/multi_rnn_cell/cell_0/cell_0/basic_lstm_cell/MatMul_1' (op: 'MatMul') with input shapes: [128,64], [96,128].

### Source code / logs
  import tensorflow as tf
  import numpy as np

  hidden_layer_size = 32
  embed = tf.zeros((128, 6, 64), dtype=tf.float32)

  num_LSTM_layers = 2
  with tf.variable_scope(""lstm""):
    
    lstm_cell = tf.contrib.rnn.BasicLSTMCell(hidden_layer_size, forget_bias=1.0)
    cell = tf.contrib.rnn.MultiRNNCell(cells=[lstm_cell]*num_LSTM_layers, state_is_tuple=True)
    outputs, states = tf.nn.dynamic_rnn(cell, embed, dtype=tf.float32)
   
Error:
ValueError: Dimensions must be equal, but are 64 and 96 for 'lstm/rnn/while/rnn/multi_rnn_cell/cell_0/cell_0/basic_lstm_cell/MatMul_1' (op: 'MatMul') with input shapes: [128,64], [96,128].

",2018-01-27T15:17:54Z,13,1,2,2.767546386419054
315,16183,`AssignVariableOp` supports `DT_BFLOAT16`,cla: yes,Fix #16103,0,,2,2018-01-17T08:06:26Z,2018-01-20T03:15:37Z,CONTRIBUTOR,Fix #16103,2018-01-20T03:15:37Z,3,2,1,3.267546386419054
316,16180,Tensorboard is down after upgrading the tensorflow?,stat:awaiting response,"Hello everyone:

I meet a issue about tensorboard after upgrading the tensorflow. It runs nicely before, but I want maintain some Python2.7 codes in Python3.4. That is why I install tensorflow .whl file of Python 3.4 and modify some grammer from Python2.7 to Python3.4. Then codes still run fine, but tensorboard is donw. The error message as following:

![image](https://user-images.githubusercontent.com/12611573/35029640-d1981942-fb96-11e7-9b89-c3c14ffaa54b.png)

OS Platform: Ubuntu 14.04
TensorFlow installed from: pip instll .whl file
TensorFlow version: tensorflow 1.2.1 for Python2, but can not check the version for Python 3

What should I do for this issue? degrade tensorflow or upgrade CUDA?
Can anybody give me any help? Thank you!
",0,,15,2018-01-17T07:00:41Z,2018-01-31T19:25:03Z,NONE,"Hello everyone:

I meet a issue about tensorboard after upgrading the tensorflow. It runs nicely before, but I want maintain some Python2.7 codes in Python3.4. That is why I install tensorflow .whl file of Python 3.4 and modify some grammer from Python2.7 to Python3.4. Then codes still run fine, but tensorboard is donw. The error message as following:

![image](https://user-images.githubusercontent.com/12611573/35029640-d1981942-fb96-11e7-9b89-c3c14ffaa54b.png)

OS Platform: Ubuntu 14.04
TensorFlow installed from: pip instll .whl file
TensorFlow version: tensorflow 1.2.1 for Python2, but can not check the version for Python 3

What should I do for this issue? degrade tensorflow or upgrade CUDA?
Can anybody give me any help? Thank you!
",2018-01-17T19:12:04Z,14,1,2,8.767546386419054
317,16178,Crash in TF lite demo android app when using preprocessing layer,,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04):** Linux Ubuntu 14.04
- **TensorFlow installed from (source or binary)**: pip
- **TensorFlow version (use command below)**: v1.4.0-rc0-21-g1e25994 1.4.0-rc1
- **Python version**: Python 3.6.2
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: 8/6
- **GPU model and memory**: Titan X (Pascal), 12 GB
- **Exact command to reproduce**:


### Describe the problem
I have a problem adding preprocessing layers to MobileNetV1 model that is quantized afterward. As preprocessing method I would like to use inception preprocessing, but TF lite does not support several operations (sub, div, broadcasting, ...), so I modified following preprocessing

```python
images = tf.divide(images, tf.constant(255.0))
images = tf.subtract(images, tf.constant(0.5))
images = tf.multiply(images, tf.constant(2.0))
```

to

```python
shape = images.get_shape()
c1 = tf.constant(1.0/255.0, shape=shape)
c1 = tf.fake_quant_with_min_max_args(c1, min=-1, max=1)
c2 = tf.constant(-0.5, shape=shape)
c2 = tf.fake_quant_with_min_max_args(c1, min=-1, max=1)
c3 = tf.constant(2.0, shape=shape)
c3 = tf.fake_quant_with_min_max_args(c1, min=-1, max=1)

images = tf.multiply(images, c1)
images = tf.fake_quant_with_min_max_args(images, min=0, max=1)
images = tf.add(images, c2)
images = tf.fake_quant_with_min_max_args(images, min=-0.5, max=0.5)
images = tf.multiply(images, c3)
images = tf.fake_quant_with_min_max_args(images, min=-1.0, max=1.0)
```

Quantization is performed with
```python
fold_batch_norms.FoldBatchNorms(graph)
quantize.Quantize(graph, is_training=is_training)
```
 and can be trained and evaluated.

Further, graph is frozen.
```bash
bazel-bin/tensorflow/python/tools/freeze_graph \
  --input_graph=MobileNetV1-4.pbtxt \
  --input_checkpoint=MobileNetV1-4.ckpt \
  --output_node_names=output/softmax \
  --output_graph=MobileNetV1-4-frozen.pb
```

Finally, frozen graph is converted to TF lite model using command.
```bash
bazel-bin/tensorflow/contrib/lite/toco/toco \
 --input_file=MobileNetV1-4-frozen.pb \
 --input_format=TENSORFLOW_GRAPHDEF \
 --output_format=TFLITE \
 --output_file=model.tflite \
 --inference_type=QUANTIZED_UINT8 \
 --inference_input_type=QUANTIZED_UINT8 \
 --input_array=input/image \
 --output_array=output/softmax \
 --input_shape=1,224,224,3
```

During conversion no error occurs.
```
2018-01-17 11:25:52.905034: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 604 operators, 896 arrays (0 quantized)
2018-01-17 11:25:53.301108: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 66 operators, 127 arrays (1 quantized)
2018-01-17 11:25:53.302502: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before pre-quantization graph transformations: 66 operators, 127 arrays (1 quantized)
2018-01-17 11:25:53.303020: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After pre-quantization graph transformations pass 1: 35 operators, 96 arrays (1 quantized)
2018-01-17 11:25:53.303601: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before quantization graph transformations: 35 operators, 96 arrays (1 quantized)
2018-01-17 11:25:53.326761: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After quantization graph transformations pass 1: 34 operators, 95 arrays (94 quantized)
2018-01-17 11:25:53.327269: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After quantization graph transformations pass 2: 34 operators, 95 arrays (94 quantized)
2018-01-17 11:25:53.327854: I tensorflow/contrib/lite/toco/allocate_transient_arrays.cc:313] Total transient array allocated size: 1756160 bytes, theoretical optimal value: 1204224 bytes.
2018-01-17 11:25:53.328080: I tensorflow/contrib/lite/toco/toco_tooling.cc:269] Estimated count of arithmetic ops: 1.14175 billion (note that a multiply-add is counted as 2 ops).
```

When I upload generated model to TF lite demo application, app crashes logcat prints this error.
```
01-17 11:56:52.190 10923-10923/? A/DEBUG: *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** ***
01-17 11:56:52.190 10923-10923/? A/DEBUG: Build fingerprint: 'samsung/dreamlteks/dreamlteks:7.0/NRD90M/G950NKSU1AQL3:user/release-keys'
01-17 11:56:52.191 10923-10923/? A/DEBUG: Revision: '11'
01-17 11:56:52.191 10923-10923/? A/DEBUG: ABI: 'arm64'
01-17 11:56:52.191 10923-10923/? A/DEBUG: pid: 10865, tid: 10881, name: CameraBackgroun  >>> android.example.com.tflitecamerademo <<<
01-17 11:56:52.191 10923-10923/? A/DEBUG: signal 6 (SIGABRT), code -6 (SI_TKILL), fault addr --------
01-17 11:56:52.191 10923-10923/? A/DEBUG:     x0   0000000000000000  x1   0000000000002a81  x2   0000000000000006  x3   0000000000000008
01-17 11:56:52.191 10923-10923/? A/DEBUG:     x4   0000007e81515040  x5   0000007e815166c0  x6   0000ffffffffffff  x7   ffffffffffffffff
01-17 11:56:52.191 10923-10923/? A/DEBUG:     x8   0000000000000083  x9   ffffffffffffffdf  x10  0000000000000000  x11  ffffffffffffffff
01-17 11:56:52.191 10923-10923/? A/DEBUG:     x12  0000000000000000  x13  ffffffffffff0000  x14  00000000000002e0  x15  000000000000044d
01-17 11:56:52.191 10923-10923/? A/DEBUG:     x16  0000007e9533aed0  x17  0000007e952e29f4  x18  0000000000000001  x19  0000007e817524f8
01-17 11:56:52.191 10923-10923/? A/DEBUG:     x20  0000000000000006  x21  0000007e81752450  x22  000000000000000b  x23  0000007e858340f0
01-17 11:56:52.191 10923-10923/? A/DEBUG:     x24  0000007e817524e8  x25  0000000000000000  x26  0000000000000080  x27  0000007e81516740
01-17 11:56:52.191 10923-10923/? A/DEBUG:     x28  0000000000000001  x29  0000007e81750730  x30  0000007e952dfd14
01-17 11:56:52.191 10923-10923/? A/DEBUG:     sp   0000007e81750710  pc   0000007e952e29fc  pstate 0000000060000000
01-17 11:56:52.198 10923-10923/? A/DEBUG: backtrace:
01-17 11:56:52.198 10923-10923/? A/DEBUG:     #00 pc 000000000006f9fc  /system/lib64/libc.so (tgkill+8)
01-17 11:56:52.198 10923-10923/? A/DEBUG:     #01 pc 000000000006cd10  /system/lib64/libc.so (pthread_kill+64)
01-17 11:56:52.198 10923-10923/? A/DEBUG:     #02 pc 0000000000025078  /system/lib64/libc.so (raise+24)
01-17 11:56:52.198 10923-10923/? A/DEBUG:     #03 pc 000000000001cc04  /system/lib64/libc.so (abort+52)
01-17 11:56:52.198 10923-10923/? A/DEBUG:     #04 pc 00000000000881a0  /data/app/android.example.com.tflitecamerademo-1/lib/arm64/libtensorflowlite_jni.so
01-17 11:56:52.199 10923-10923/? A/DEBUG:     #05 pc 0000000000071ce4  /data/app/android.example.com.tflitecamerademo-1/lib/arm64/libtensorflowlite_jni.so
01-17 11:56:52.199 10923-10923/? A/DEBUG:     #06 pc 00000000000707fc  /data/app/android.example.com.tflitecamerademo-1/lib/arm64/libtensorflowlite_jni.so
01-17 11:56:52.199 10923-10923/? A/DEBUG:     #07 pc 000000000007f99c  /data/app/android.example.com.tflitecamerademo-1/lib/arm64/libtensorflowlite_jni.so
01-17 11:56:52.199 10923-10923/? A/DEBUG:     #08 pc 0000000000011c5c  /data/app/android.example.com.tflitecamerademo-1/lib/arm64/libtensorflowlite_jni.so (Java_org_tensorflow_lite_NativeInterpreterWrapper_run+1628)
01-17 11:56:52.199 10923-10923/? A/DEBUG:     #09 pc 0000000000384abc  /data/app/android.example.com.tflitecamerademo-1/oat/arm64/base.odex (offset 0x329000)
```

This error doesn't seem to be related to added preprocessing layer, but without adding preprocessing layer, no error occurs and app can run.
",0,,2,2018-01-17T05:09:08Z,2018-01-17T08:15:46Z,NONE,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04):** Linux Ubuntu 14.04
- **TensorFlow installed from (source or binary)**: pip
- **TensorFlow version (use command below)**: v1.4.0-rc0-21-g1e25994 1.4.0-rc1
- **Python version**: Python 3.6.2
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: 8/6
- **GPU model and memory**: Titan X (Pascal), 12 GB
- **Exact command to reproduce**:


### Describe the problem
I have a problem adding preprocessing layers to MobileNetV1 model that is quantized afterward. As preprocessing method I would like to use inception preprocessing, but TF lite does not support several operations (sub, div, broadcasting, ...), so I modified following preprocessing

```python
images = tf.divide(images, tf.constant(255.0))
images = tf.subtract(images, tf.constant(0.5))
images = tf.multiply(images, tf.constant(2.0))
```

to

```python
shape = images.get_shape()
c1 = tf.constant(1.0/255.0, shape=shape)
c1 = tf.fake_quant_with_min_max_args(c1, min=-1, max=1)
c2 = tf.constant(-0.5, shape=shape)
c2 = tf.fake_quant_with_min_max_args(c1, min=-1, max=1)
c3 = tf.constant(2.0, shape=shape)
c3 = tf.fake_quant_with_min_max_args(c1, min=-1, max=1)

images = tf.multiply(images, c1)
images = tf.fake_quant_with_min_max_args(images, min=0, max=1)
images = tf.add(images, c2)
images = tf.fake_quant_with_min_max_args(images, min=-0.5, max=0.5)
images = tf.multiply(images, c3)
images = tf.fake_quant_with_min_max_args(images, min=-1.0, max=1.0)
```

Quantization is performed with
```python
fold_batch_norms.FoldBatchNorms(graph)
quantize.Quantize(graph, is_training=is_training)
```
 and can be trained and evaluated.

Further, graph is frozen.
```bash
bazel-bin/tensorflow/python/tools/freeze_graph \
  --input_graph=MobileNetV1-4.pbtxt \
  --input_checkpoint=MobileNetV1-4.ckpt \
  --output_node_names=output/softmax \
  --output_graph=MobileNetV1-4-frozen.pb
```

Finally, frozen graph is converted to TF lite model using command.
```bash
bazel-bin/tensorflow/contrib/lite/toco/toco \
 --input_file=MobileNetV1-4-frozen.pb \
 --input_format=TENSORFLOW_GRAPHDEF \
 --output_format=TFLITE \
 --output_file=model.tflite \
 --inference_type=QUANTIZED_UINT8 \
 --inference_input_type=QUANTIZED_UINT8 \
 --input_array=input/image \
 --output_array=output/softmax \
 --input_shape=1,224,224,3
```

During conversion no error occurs.
```
2018-01-17 11:25:52.905034: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 604 operators, 896 arrays (0 quantized)
2018-01-17 11:25:53.301108: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 66 operators, 127 arrays (1 quantized)
2018-01-17 11:25:53.302502: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before pre-quantization graph transformations: 66 operators, 127 arrays (1 quantized)
2018-01-17 11:25:53.303020: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After pre-quantization graph transformations pass 1: 35 operators, 96 arrays (1 quantized)
2018-01-17 11:25:53.303601: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before quantization graph transformations: 35 operators, 96 arrays (1 quantized)
2018-01-17 11:25:53.326761: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After quantization graph transformations pass 1: 34 operators, 95 arrays (94 quantized)
2018-01-17 11:25:53.327269: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After quantization graph transformations pass 2: 34 operators, 95 arrays (94 quantized)
2018-01-17 11:25:53.327854: I tensorflow/contrib/lite/toco/allocate_transient_arrays.cc:313] Total transient array allocated size: 1756160 bytes, theoretical optimal value: 1204224 bytes.
2018-01-17 11:25:53.328080: I tensorflow/contrib/lite/toco/toco_tooling.cc:269] Estimated count of arithmetic ops: 1.14175 billion (note that a multiply-add is counted as 2 ops).
```

When I upload generated model to TF lite demo application, app crashes logcat prints this error.
```
01-17 11:56:52.190 10923-10923/? A/DEBUG: *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** ***
01-17 11:56:52.190 10923-10923/? A/DEBUG: Build fingerprint: 'samsung/dreamlteks/dreamlteks:7.0/NRD90M/G950NKSU1AQL3:user/release-keys'
01-17 11:56:52.191 10923-10923/? A/DEBUG: Revision: '11'
01-17 11:56:52.191 10923-10923/? A/DEBUG: ABI: 'arm64'
01-17 11:56:52.191 10923-10923/? A/DEBUG: pid: 10865, tid: 10881, name: CameraBackgroun  >>> android.example.com.tflitecamerademo <<<
01-17 11:56:52.191 10923-10923/? A/DEBUG: signal 6 (SIGABRT), code -6 (SI_TKILL), fault addr --------
01-17 11:56:52.191 10923-10923/? A/DEBUG:     x0   0000000000000000  x1   0000000000002a81  x2   0000000000000006  x3   0000000000000008
01-17 11:56:52.191 10923-10923/? A/DEBUG:     x4   0000007e81515040  x5   0000007e815166c0  x6   0000ffffffffffff  x7   ffffffffffffffff
01-17 11:56:52.191 10923-10923/? A/DEBUG:     x8   0000000000000083  x9   ffffffffffffffdf  x10  0000000000000000  x11  ffffffffffffffff
01-17 11:56:52.191 10923-10923/? A/DEBUG:     x12  0000000000000000  x13  ffffffffffff0000  x14  00000000000002e0  x15  000000000000044d
01-17 11:56:52.191 10923-10923/? A/DEBUG:     x16  0000007e9533aed0  x17  0000007e952e29f4  x18  0000000000000001  x19  0000007e817524f8
01-17 11:56:52.191 10923-10923/? A/DEBUG:     x20  0000000000000006  x21  0000007e81752450  x22  000000000000000b  x23  0000007e858340f0
01-17 11:56:52.191 10923-10923/? A/DEBUG:     x24  0000007e817524e8  x25  0000000000000000  x26  0000000000000080  x27  0000007e81516740
01-17 11:56:52.191 10923-10923/? A/DEBUG:     x28  0000000000000001  x29  0000007e81750730  x30  0000007e952dfd14
01-17 11:56:52.191 10923-10923/? A/DEBUG:     sp   0000007e81750710  pc   0000007e952e29fc  pstate 0000000060000000
01-17 11:56:52.198 10923-10923/? A/DEBUG: backtrace:
01-17 11:56:52.198 10923-10923/? A/DEBUG:     #00 pc 000000000006f9fc  /system/lib64/libc.so (tgkill+8)
01-17 11:56:52.198 10923-10923/? A/DEBUG:     #01 pc 000000000006cd10  /system/lib64/libc.so (pthread_kill+64)
01-17 11:56:52.198 10923-10923/? A/DEBUG:     #02 pc 0000000000025078  /system/lib64/libc.so (raise+24)
01-17 11:56:52.198 10923-10923/? A/DEBUG:     #03 pc 000000000001cc04  /system/lib64/libc.so (abort+52)
01-17 11:56:52.198 10923-10923/? A/DEBUG:     #04 pc 00000000000881a0  /data/app/android.example.com.tflitecamerademo-1/lib/arm64/libtensorflowlite_jni.so
01-17 11:56:52.199 10923-10923/? A/DEBUG:     #05 pc 0000000000071ce4  /data/app/android.example.com.tflitecamerademo-1/lib/arm64/libtensorflowlite_jni.so
01-17 11:56:52.199 10923-10923/? A/DEBUG:     #06 pc 00000000000707fc  /data/app/android.example.com.tflitecamerademo-1/lib/arm64/libtensorflowlite_jni.so
01-17 11:56:52.199 10923-10923/? A/DEBUG:     #07 pc 000000000007f99c  /data/app/android.example.com.tflitecamerademo-1/lib/arm64/libtensorflowlite_jni.so
01-17 11:56:52.199 10923-10923/? A/DEBUG:     #08 pc 0000000000011c5c  /data/app/android.example.com.tflitecamerademo-1/lib/arm64/libtensorflowlite_jni.so (Java_org_tensorflow_lite_NativeInterpreterWrapper_run+1628)
01-17 11:56:52.199 10923-10923/? A/DEBUG:     #09 pc 0000000000384abc  /data/app/android.example.com.tflitecamerademo-1/oat/arm64/base.odex (offset 0x329000)
```

This error doesn't seem to be related to added preprocessing layer, but without adding preprocessing layer, no error occurs and app can run.
",2018-01-17T06:53:47Z,0,1,0,2.267546386419054
318,16177,"RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6   return f(*args, **kwds)",stat:awaiting response,"### System information
- **OS Platform and Distribution :**  Linux Ubuntu 17.10
- **TensorFlow installed from :**  Anaconda [followed this tutorial](https://www.tensorflow.org/install/install_linux#InstallingAnaconda)
- **TensorFlow version (use command below)**: v1.4.0-19-ga52c8d9 1.4.1
- **Python version**: Python 3.6.4 :: Anaconda, Inc.
- **CUDA/cuDNN version**: not using GPU version
- **GPU model and memory**: 2GB GT720

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**result :** 
/home/pankaja/anaconda3/envs/tensorflow/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6
  return f(*args, **kwds)
v1.4.0-19-ga52c8d9 1.4.1

### Describe the problem
Followed Official tensorflow documentation to install tensorflow on Ubuntu 17.10, python3 (python 3.6) and with CPU support. Used conda environment. Followed [this](https://www.tensorflow.org/install/install_linux#InstallingAnaconda) and in the 4th step this is the command I used `pip install --ignore-installed --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.4.1-cp36-cp36m-linux_x86_64.whl`
it installed successfully. But when I try to import tensorflow in python I'm getting this error.

```
/home/pankaja/anaconda3/envs/tensorflow/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6
  return f(*args, **kwds)

```

### Source code / logs
Activate Conda environment
`source activate tensorflow`

**command :** `python`
**log :** 
```
Python 3.6.4 |Anaconda, Inc.| (default, Jan 16 2018, 18:10:19) 
[GCC 7.2.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
```

**command :** `import tensorflow`
**log :**
```
/home/pankaja/anaconda3/envs/tensorflow/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6
  return f(*args, **kwds)
```


Why can't I use the tensorflow for python 3.6 in python 3.6 . How to fix this ?",0,,2,2018-01-17T05:05:21Z,2018-01-18T16:20:50Z,NONE,"### System information
- **OS Platform and Distribution :**  Linux Ubuntu 17.10
- **TensorFlow installed from :**  Anaconda [followed this tutorial](https://www.tensorflow.org/install/install_linux#InstallingAnaconda)
- **TensorFlow version (use command below)**: v1.4.0-19-ga52c8d9 1.4.1
- **Python version**: Python 3.6.4 :: Anaconda, Inc.
- **CUDA/cuDNN version**: not using GPU version
- **GPU model and memory**: 2GB GT720

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**result :** 
/home/pankaja/anaconda3/envs/tensorflow/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6
  return f(*args, **kwds)
v1.4.0-19-ga52c8d9 1.4.1

### Describe the problem
Followed Official tensorflow documentation to install tensorflow on Ubuntu 17.10, python3 (python 3.6) and with CPU support. Used conda environment. Followed [this](https://www.tensorflow.org/install/install_linux#InstallingAnaconda) and in the 4th step this is the command I used `pip install --ignore-installed --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.4.1-cp36-cp36m-linux_x86_64.whl`
it installed successfully. But when I try to import tensorflow in python I'm getting this error.

```
/home/pankaja/anaconda3/envs/tensorflow/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6
  return f(*args, **kwds)

```

### Source code / logs
Activate Conda environment
`source activate tensorflow`

**command :** `python`
**log :** 
```
Python 3.6.4 |Anaconda, Inc.| (default, Jan 16 2018, 18:10:19) 
[GCC 7.2.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
```

**command :** `import tensorflow`
**log :**
```
/home/pankaja/anaconda3/envs/tensorflow/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6
  return f(*args, **kwds)
```


Why can't I use the tensorflow for python 3.6 in python 3.6 . How to fix this ?",2018-01-17T12:59:46Z,1,1,0,2.267546386419054
319,16176,4d55397500 patch 1,"awaiting testing (then merge),cla: yes","Provide a practical meaning for the `pos_weights` parameter.

The current  weighted_cross_entropy_with_logits docs don't explain practically the relationship of
`pos_weights > 1`,  `pos_weights < 1` to precision, recall, and class imbalance.
",1,,2,2018-01-17T04:52:42Z,2018-01-22T21:30:33Z,CONTRIBUTOR,"Provide a practical meaning for the `pos_weights` parameter.

The current  weighted_cross_entropy_with_logits docs don't explain practically the relationship of
`pos_weights > 1`,  `pos_weights < 1` to precision, recall, and class imbalance.
",2018-01-22T21:30:49Z,5,2,1,4.267546386419054
320,16173,Add C++ toolchain for portable Linux builds,"awaiting review,cla: yes,kokoro:run",See #15777,1,,12,2018-01-17T02:10:01Z,2018-01-23T17:51:19Z,MEMBER,See #15777,2018-01-17T22:35:41Z,6,3,1,10.267546386419054
321,16172,Merge changes from r1.5 into master,"awaiting testing (then merge),cla: yes","This change picks up the commits exclusive to the r1.5 branch and puts them back into master.

There were a bunch of merge conflicts here. I favored master in most cases except those to do with obvious versioning differences.

I'm not sure if I did the merge correctly, considering there are a great many CLs presented here.",0,,3,2018-01-17T00:46:21Z,2018-01-19T09:07:54Z,MEMBER,"This change picks up the commits exclusive to the r1.5 branch and puts them back into master.

There were a bunch of merge conflicts here. I favored master in most cases except those to do with obvious versioning differences.

I'm not sure if I did the merge correctly, considering there are a great many CLs presented here.",2018-01-18T01:00:26Z,2,3,0,4.767546386419054
322,16167,Documentation Method Templates Improvement,type:docs,"### System information
N/A

### Describe the problem
The method/class templates in documentation should include a full, functioning path to the method instead of just truncating to the method's name.

I.e. this is what we have at present (bad): 
<img width=""399"" alt=""screen shot 2018-01-16 at 2 23 08 pm"" src=""https://user-images.githubusercontent.com/9597721/35007940-0511d55c-fac9-11e7-9d0c-4be2db021533.png"">

This is a more practical and copy/paste-friendly version:
<img width=""426"" alt=""screen shot 2018-01-16 at 2 22 49 pm"" src=""https://user-images.githubusercontent.com/9597721/35007976-2970cdc2-fac9-11e7-80b8-0ec1e2334734.png"">

I'm constantly just grabbing method templates, pasting to my text editor and then coming back to docs to copy/paste the package path which is now the header of the page; which is an awful workflow.

### Source code / logs
N/A",1,,5,2018-01-16T19:28:06Z,2018-02-02T02:58:59Z,NONE,"### System information
N/A

### Describe the problem
The method/class templates in documentation should include a full, functioning path to the method instead of just truncating to the method's name.

I.e. this is what we have at present (bad): 
<img width=""399"" alt=""screen shot 2018-01-16 at 2 23 08 pm"" src=""https://user-images.githubusercontent.com/9597721/35007940-0511d55c-fac9-11e7-9d0c-4be2db021533.png"">

This is a more practical and copy/paste-friendly version:
<img width=""426"" alt=""screen shot 2018-01-16 at 2 22 49 pm"" src=""https://user-images.githubusercontent.com/9597721/35007976-2970cdc2-fac9-11e7-80b8-0ec1e2334734.png"">

I'm constantly just grabbing method templates, pasting to my text editor and then coming back to docs to copy/paste the package path which is now the header of the page; which is an awful workflow.

### Source code / logs
N/A",2018-01-16T22:44:48Z,16,1,3,4.7658725856748525
323,16165,Error when building from source Fedora 27 CUDA 9.1,type:build/install,"### System information
- OS Platform and Distribution: Fedora 27
- TensorFlow installed from (source or binary): binary
- TensorFlow version: r1.4
- Python version: 3.6.3
- Bazel version: 0.8.1
- GCC/Compiler version: 7.2.1
- CUDA/cuDNN version: CUDA 9.1 cuDNN 7.0.5
- **GPU model and memory**: NVidia Geforce GTX 960 4GB
- Exact command to reproduce: bazel build -c opt --config=cuda --verbose_failures //tensorflow/tools/pip_package:build_pip_package

### Describe the problem
So I'm attempting to build tensorflow from source on fedora with the version of CUDA and cuDNN I already had installed to avoid have to also install an older version. The build however errors with the following message:

```
ERROR: .cache/bazel/_bazel_xd009642/f9f5dea1a139b69420e1045d339dda45/external/nccl_archive/BUILD:33:1: error while parsing .d file: /home/xd009642/.cache/bazel/_bazel_xd009642/f9f5dea1a139b69420e1045d339dda45/execroot/org_tensorflow/bazel-out/k8-py3-opt/bin/external/nccl_archive/_objs/nccl/external/nccl_archive/src/all_reduce.cu.pic.d (No such file or directory)
<command-line>:0:15: warning: ISO C++11 requires whitespace after the macro name
<command-line>:0:1: error: macro names must be identifiers
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 0.392s, Critical Path: 0.12s
FAILED: Build did NOT complete successfully
```
I also tried the command `bazel build --config=opt --config=cuda --incompatible_load_argument_is_label=false //tensorflow/tools/pip_package:build_pip_package` from [here](http://www.python36.com/install-tensorflow141-gpu/) with the same end result.

Any guidance is appreciated as this is my first time using bazel (and also trying to compile tensorflow).",0,,11,2018-01-16T18:21:58Z,2018-01-20T18:26:12Z,NONE,"### System information
- OS Platform and Distribution: Fedora 27
- TensorFlow installed from (source or binary): binary
- TensorFlow version: r1.4
- Python version: 3.6.3
- Bazel version: 0.8.1
- GCC/Compiler version: 7.2.1
- CUDA/cuDNN version: CUDA 9.1 cuDNN 7.0.5
- **GPU model and memory**: NVidia Geforce GTX 960 4GB
- Exact command to reproduce: bazel build -c opt --config=cuda --verbose_failures //tensorflow/tools/pip_package:build_pip_package

### Describe the problem
So I'm attempting to build tensorflow from source on fedora with the version of CUDA and cuDNN I already had installed to avoid have to also install an older version. The build however errors with the following message:

```
ERROR: .cache/bazel/_bazel_xd009642/f9f5dea1a139b69420e1045d339dda45/external/nccl_archive/BUILD:33:1: error while parsing .d file: /home/xd009642/.cache/bazel/_bazel_xd009642/f9f5dea1a139b69420e1045d339dda45/execroot/org_tensorflow/bazel-out/k8-py3-opt/bin/external/nccl_archive/_objs/nccl/external/nccl_archive/src/all_reduce.cu.pic.d (No such file or directory)
<command-line>:0:15: warning: ISO C++11 requires whitespace after the macro name
<command-line>:0:1: error: macro names must be identifiers
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 0.392s, Critical Path: 0.12s
FAILED: Build did NOT complete successfully
```
I also tried the command `bazel build --config=opt --config=cuda --incompatible_load_argument_is_label=false //tensorflow/tools/pip_package:build_pip_package` from [here](http://www.python36.com/install-tensorflow141-gpu/) with the same end result.

Any guidance is appreciated as this is my first time using bazel (and also trying to compile tensorflow).",2018-01-16T22:49:02Z,4,1,1,6.765872585674852
324,16164,"Accidentally cancelled inceptionV3 during install, now can't install at all",,"Hello,
i was setting up tensorflow for image classification, and after i ran : 

python -m scripts.retrain \
  --bottleneck_dir=tf_files/bottlenecks \
  --model_dir=tf_files/models/""${ARCHITECTURE}"" \
  --summaries_dir=tf_files/training_summaries/""${ARCHITECTURE}"" \
  --output_graph=tf_files/retrained_graph.pb \
  --output_labels=tf_files/retrained_labels.txt \
  --architecture=""${ARCHITECTURE}"" \
  --image_dir=tf_files/flower_photos

It automatically started installing inception, i realized that i needed to change some options so i cancelled the install of inception.
Now i believe that i have a half install that doesn't let me install the full package or use the half package.

I may be wrong, but any suggestions would be appreciated.
FYI: i've run :
pip install inception, to which i receive a ""python setup.py egg_info"" failed with error code 1 in {my local/temp dir}

I also just tried running the scripts.retrain again, to which i receive a ""EOFError: compressed file ended before the end-of-stream marker was reached""

Running on Windows 7",1,,5,2018-01-16T17:28:24Z,2018-01-31T19:38:29Z,NONE,"Hello,
i was setting up tensorflow for image classification, and after i ran : 

python -m scripts.retrain \
  --bottleneck_dir=tf_files/bottlenecks \
  --model_dir=tf_files/models/""${ARCHITECTURE}"" \
  --summaries_dir=tf_files/training_summaries/""${ARCHITECTURE}"" \
  --output_graph=tf_files/retrained_graph.pb \
  --output_labels=tf_files/retrained_labels.txt \
  --architecture=""${ARCHITECTURE}"" \
  --image_dir=tf_files/flower_photos

It automatically started installing inception, i realized that i needed to change some options so i cancelled the install of inception.
Now i believe that i have a half install that doesn't let me install the full package or use the half package.

I may be wrong, but any suggestions would be appreciated.
FYI: i've run :
pip install inception, to which i receive a ""python setup.py egg_info"" failed with error code 1 in {my local/temp dir}

I also just tried running the scripts.retrain again, to which i receive a ""EOFError: compressed file ended before the end-of-stream marker was reached""

Running on Windows 7",2018-01-17T01:01:27Z,15,1,3,4.7658725856748525
325,16161,tf.case raising IllegalArgumentError 'None of the conditions evaluated as True' when used with Dataset,"stat:awaiting tensorflower,type:bug/performance","### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Windows 10 64bit
- **TensorFlow installed from (source or binary)**:
via pip
- **TensorFlow version (use command below)**:
1.4.0
- **Python version**: 
3.5

When I use tf.case within a tf.data.Dataset, I get an IllegalArgumentError 'None of the conditions evaluated as True'. However, when I use the same code without the Dataset, it works fine. Furthermore, if I understand the error message correctly, it already tells me that one condition evaluated to true (see the end of the first line):

```
InvalidArgumentError (see above for traceback): assertion failed: [None of the conditions evaluated as True. Conditions: (Equal_3:0, Equal_4:0, Equal_5:0), Values:] [1 0 0]
	 [[Node: case/If_0/Assert_1/AssertGuard/Assert = Assert[T=[DT_STRING, DT_BOOL], summarize=3](case/If_0/Assert_1/AssertGuard/Assert/Switch, case/If_0/Assert_1/AssertGuard/Assert/data_0, case/If_0/Assert_1/AssertGuard/Assert/Switch_1)]]
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[]], output_types=[DT_INT32], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](OneShotIterator)]]
```

The following code reproduces the issue:
```
import tensorflow as tf


def random_map(i):
	random_int = tf.random_uniform([], minval=0, maxval=3, dtype=tf.int32)
	random_int = tf.Print(random_int, [random_int, tf.equal(random_int, 0), tf.equal(random_int, 1), tf.equal(random_int, 2)], 'random_int')

	result = tf.case([
		(tf.equal(random_int, 0), lambda: i * 10000),
		(tf.equal(random_int, 1), lambda: i * 20000),
		(tf.equal(random_int, 2), lambda: i * 30000)
	], exclusive=True)

	return result


print('working =========================================================================')
with tf.Session() as sess:
	input_pl = tf.placeholder(dtype=tf.int32)
	result = random_map(input_pl)
	for i in range(5):
		result_value = sess.run(result, feed_dict={input_pl: i})
		print(result_value)

print('not working =====================================================================')
with tf.Session() as sess:
	dataset = tf.data.Dataset.from_tensor_slices(tf.range(5))
	dataset = dataset.map(random_map)
	iterator = dataset.make_one_shot_iterator()
	next_result = iterator.get_next()

	for i in range(5):
		result_value = sess.run(next_result)
		print(result_value)
```

I also found [this question]( http://www.programfaqs.com/faq/tensorflow-case-error-invalid-argument-assertion-failed-none-of-the-conditions-evaluated-as-true/), which seems to be the same problem.",0,,3,2018-01-16T14:01:26Z,2018-01-17T19:01:54Z,CONTRIBUTOR,"### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Windows 10 64bit
- **TensorFlow installed from (source or binary)**:
via pip
- **TensorFlow version (use command below)**:
1.4.0
- **Python version**: 
3.5

When I use tf.case within a tf.data.Dataset, I get an IllegalArgumentError 'None of the conditions evaluated as True'. However, when I use the same code without the Dataset, it works fine. Furthermore, if I understand the error message correctly, it already tells me that one condition evaluated to true (see the end of the first line):

```
InvalidArgumentError (see above for traceback): assertion failed: [None of the conditions evaluated as True. Conditions: (Equal_3:0, Equal_4:0, Equal_5:0), Values:] [1 0 0]
	 [[Node: case/If_0/Assert_1/AssertGuard/Assert = Assert[T=[DT_STRING, DT_BOOL], summarize=3](case/If_0/Assert_1/AssertGuard/Assert/Switch, case/If_0/Assert_1/AssertGuard/Assert/data_0, case/If_0/Assert_1/AssertGuard/Assert/Switch_1)]]
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[]], output_types=[DT_INT32], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](OneShotIterator)]]
```

The following code reproduces the issue:
```
import tensorflow as tf


def random_map(i):
	random_int = tf.random_uniform([], minval=0, maxval=3, dtype=tf.int32)
	random_int = tf.Print(random_int, [random_int, tf.equal(random_int, 0), tf.equal(random_int, 1), tf.equal(random_int, 2)], 'random_int')

	result = tf.case([
		(tf.equal(random_int, 0), lambda: i * 10000),
		(tf.equal(random_int, 1), lambda: i * 20000),
		(tf.equal(random_int, 2), lambda: i * 30000)
	], exclusive=True)

	return result


print('working =========================================================================')
with tf.Session() as sess:
	input_pl = tf.placeholder(dtype=tf.int32)
	result = random_map(input_pl)
	for i in range(5):
		result_value = sess.run(result, feed_dict={input_pl: i})
		print(result_value)

print('not working =====================================================================')
with tf.Session() as sess:
	dataset = tf.data.Dataset.from_tensor_slices(tf.range(5))
	dataset = dataset.map(random_map)
	iterator = dataset.make_one_shot_iterator()
	next_result = iterator.get_next()

	for i in range(5):
		result_value = sess.run(next_result)
		print(result_value)
```

I also found [this question]( http://www.programfaqs.com/faq/tensorflow-case-error-invalid-argument-assertion-failed-none-of-the-conditions-evaluated-as-true/), which seems to be the same problem.",2018-01-16T22:56:02Z,1,2,0,3.765872585674852
326,16160,tf.contrib.lookup.HashTable(kv_initializer) does not work in eager mode.,"comp:eager,stat:awaiting tensorflower,type:bug/performance","Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------
```
== cat /etc/issue ===============================================
Darwin Xiaoyuns-MBP 15.4.0 Darwin Kernel Version 15.4.0: Fri Feb 26 22:08:05 PST 2016; root:xnu-3248.40.184~3/RELEASE_X86_64 x86_64
Mac OS X 10.11.4

== are we in docker =============================================
No

== compiler =====================================================
Apple LLVM version 7.3.0 (clang-703.0.31)
Target: x86_64-apple-darwin15.4.0
Thread model: posix
InstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin

== uname -a =====================================================
Darwin Xiaoyuns-MBP 15.4.0 Darwin Kernel Version 15.4.0: Fri Feb 26 22:08:05 PST 2016; root:xnu-3248.40.184~3/RELEASE_X86_64 x86_64

== check pips ===================================================
numpy (1.11.2)
protobuf (3.4.0)

== check for virtualenv =========================================
False

== tensorflow import ============================================
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
ImportError: No module named tensorflow

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH /usr/local/cuda/lib:/usr/local/cuda/lib:

== nvidia-smi ===================================================
/Users/xiaoyun/tf14py3/bin/tf_env_collect.sh: line 105: nvidia-smi: command not found

== cuda libs  ===================================================

== cat /etc/issue ===============================================
Darwin Xiaoyuns-MBP 15.4.0 Darwin Kernel Version 15.4.0: Fri Feb 26 22:08:05 PST 2016; root:xnu-3248.40.184~3/RELEASE_X86_64 x86_64
Mac OS X 10.11.4

== are we in docker =============================================
No

== compiler =====================================================
Apple LLVM version 7.3.0 (clang-703.0.31)
Target: x86_64-apple-darwin15.4.0
Thread model: posix
InstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin

== uname -a =====================================================
Darwin Xiaoyuns-MBP 15.4.0 Darwin Kernel Version 15.4.0: Fri Feb 26 22:08:05 PST 2016; root:xnu-3248.40.184~3/RELEASE_X86_64 x86_64

== check pips ===================================================
numpy (1.11.2)
protobuf (3.4.0)

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.5.0-rc1
tf.GIT_VERSION = v1.5.0-rc0-9-gf9472619f6
tf.COMPILER_VERSION = v1.5.0-rc0-9-gf9472619f6
Sanity check: array([1], dtype=int32)

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH /usr/local/cuda/lib:/usr/local/cuda/lib:

== nvidia-smi ===================================================
/Users/xiaoyun/tf14py3/bin/tf_env_collect.sh: line 105: nvidia-smi: command not found

== cuda libs  ===================================================
```
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
v1.5.0-rc0-9-gf9472619f6 1.5.0-rc1

### Describe the problem
Can not use hashtable in eager mode.

### Source code / logs
```
import tensorflow as tf
import tensorflow.contrib.eager as tfe

tfe.enable_eager_execution()

keyfile = ""./key_dict""
kv_initializer = tf.contrib.lookup.TextFileInitializer(
    keyfile, tf.string, 0, tf.int64, 1, delimiter=""\t"")
table = tf.contrib.lookup.HashTable(kv_initializer, 0)
table.init.run()

filenames = [""./data1""]
dataset = tf.data.TextLineDataset(filenames)
#dataset = dataset.map(lambda tkns:table.lookup(tkns))
for x in tfe.Iterator(dataset):
    print(x)

Traceback (most recent call last):
  File ""torch/textline.py"", line 13, in <module>
    table = tf.contrib.lookup.HashTable(kv_initializer, 0)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/ops/lookup_ops.py"", line 282, in __init__
    super(HashTable, self).__init__(table_ref, default_value, initializer)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/ops/lookup_ops.py"", line 168, in __init__
    self._init = initializer.initialize(self)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/ops/lookup_ops.py"", line 531, in initialize
    if constant_op.is_constant(filename):
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py"", line 224, in is_constant
    op = tensor_or_op.op
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 825, in op
    raise AttributeError(""op not supported for Eager Tensors."")
AttributeError: op not supported for Eager Tensors.
```",1,,5,2018-01-16T13:11:57Z,2018-01-17T23:40:09Z,NONE,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------
```
== cat /etc/issue ===============================================
Darwin Xiaoyuns-MBP 15.4.0 Darwin Kernel Version 15.4.0: Fri Feb 26 22:08:05 PST 2016; root:xnu-3248.40.184~3/RELEASE_X86_64 x86_64
Mac OS X 10.11.4

== are we in docker =============================================
No

== compiler =====================================================
Apple LLVM version 7.3.0 (clang-703.0.31)
Target: x86_64-apple-darwin15.4.0
Thread model: posix
InstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin

== uname -a =====================================================
Darwin Xiaoyuns-MBP 15.4.0 Darwin Kernel Version 15.4.0: Fri Feb 26 22:08:05 PST 2016; root:xnu-3248.40.184~3/RELEASE_X86_64 x86_64

== check pips ===================================================
numpy (1.11.2)
protobuf (3.4.0)

== check for virtualenv =========================================
False

== tensorflow import ============================================
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
ImportError: No module named tensorflow

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH /usr/local/cuda/lib:/usr/local/cuda/lib:

== nvidia-smi ===================================================
/Users/xiaoyun/tf14py3/bin/tf_env_collect.sh: line 105: nvidia-smi: command not found

== cuda libs  ===================================================

== cat /etc/issue ===============================================
Darwin Xiaoyuns-MBP 15.4.0 Darwin Kernel Version 15.4.0: Fri Feb 26 22:08:05 PST 2016; root:xnu-3248.40.184~3/RELEASE_X86_64 x86_64
Mac OS X 10.11.4

== are we in docker =============================================
No

== compiler =====================================================
Apple LLVM version 7.3.0 (clang-703.0.31)
Target: x86_64-apple-darwin15.4.0
Thread model: posix
InstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin

== uname -a =====================================================
Darwin Xiaoyuns-MBP 15.4.0 Darwin Kernel Version 15.4.0: Fri Feb 26 22:08:05 PST 2016; root:xnu-3248.40.184~3/RELEASE_X86_64 x86_64

== check pips ===================================================
numpy (1.11.2)
protobuf (3.4.0)

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.5.0-rc1
tf.GIT_VERSION = v1.5.0-rc0-9-gf9472619f6
tf.COMPILER_VERSION = v1.5.0-rc0-9-gf9472619f6
Sanity check: array([1], dtype=int32)

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH /usr/local/cuda/lib:/usr/local/cuda/lib:

== nvidia-smi ===================================================
/Users/xiaoyun/tf14py3/bin/tf_env_collect.sh: line 105: nvidia-smi: command not found

== cuda libs  ===================================================
```
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
v1.5.0-rc0-9-gf9472619f6 1.5.0-rc1

### Describe the problem
Can not use hashtable in eager mode.

### Source code / logs
```
import tensorflow as tf
import tensorflow.contrib.eager as tfe

tfe.enable_eager_execution()

keyfile = ""./key_dict""
kv_initializer = tf.contrib.lookup.TextFileInitializer(
    keyfile, tf.string, 0, tf.int64, 1, delimiter=""\t"")
table = tf.contrib.lookup.HashTable(kv_initializer, 0)
table.init.run()

filenames = [""./data1""]
dataset = tf.data.TextLineDataset(filenames)
#dataset = dataset.map(lambda tkns:table.lookup(tkns))
for x in tfe.Iterator(dataset):
    print(x)

Traceback (most recent call last):
  File ""torch/textline.py"", line 13, in <module>
    table = tf.contrib.lookup.HashTable(kv_initializer, 0)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/ops/lookup_ops.py"", line 282, in __init__
    super(HashTable, self).__init__(table_ref, default_value, initializer)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/ops/lookup_ops.py"", line 168, in __init__
    self._init = initializer.initialize(self)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/ops/lookup_ops.py"", line 531, in initialize
    if constant_op.is_constant(filename):
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py"", line 224, in is_constant
    op = tensor_or_op.op
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 825, in op
    raise AttributeError(""op not supported for Eager Tensors."")
AttributeError: op not supported for Eager Tensors.
```",2018-01-16T18:06:37Z,1,1,0,4.7658725856748525
327,16158,GAN model: move generated and real operations under discriminator namespace,"awaiting testing (then merge),cla: yes","Hi everybody,

`gan_model` runs the discriminator on both the generated and real data. This PR changes/fixes the namespace of the generated graph.

* Current: The network variables and operations on generated data are in the `Discriminator` namespace but the operations on real data are in the `Discriminator_1` namespace.
* PR: The network variables stay in the `Discriminator` namespace. Operations on generated data are in `Discriminator/generated` and operations on real data are in `Discriminator/real`.

`gan_model` only searches the `Discriminator` namespace for regularization. Presumably, if you were running activity regularization in your discriminator, only the part on generated data would be picked up. Plus, the graph looks much better visually this way and you can tell which discriminator is which.

Cheers",1,,2,2018-01-16T11:36:00Z,2018-01-22T21:31:35Z,CONTRIBUTOR,"Hi everybody,

`gan_model` runs the discriminator on both the generated and real data. This PR changes/fixes the namespace of the generated graph.

* Current: The network variables and operations on generated data are in the `Discriminator` namespace but the operations on real data are in the `Discriminator_1` namespace.
* PR: The network variables stay in the `Discriminator` namespace. Operations on generated data are in `Discriminator/generated` and operations on real data are in `Discriminator/real`.

`gan_model` only searches the `Discriminator` namespace for regularization. Presumably, if you were running activity regularization in your discriminator, only the part on generated data would be picked up. Plus, the graph looks much better visually this way and you can tell which discriminator is which.

Cheers",2018-01-16T11:40:07Z,6,2,1,4.2658725856748525
328,16157,Update rules_closure to fix bazel version check,cla: yes,Related https://github.com/bazelbuild/bazel/issues/4425#issuecomment-357681237,0,,2,2018-01-16T11:31:14Z,2018-01-17T04:47:51Z,MEMBER,Related https://github.com/bazelbuild/bazel/issues/4425#issuecomment-357681237,2018-01-16T11:43:16Z,1,3,0,4.2658725856748525
329,16153,New features: tf.alphas and tf.alphas_like - Related to #16128,"API review,awaiting review,cla: yes","This PR is related to the issue: #16128

#### I send my work here for peer reviewing and discussion. Please do not merge now.

### A few interrogations before merging

1. Are the names I have chosen fine with everyone or you would like it to be changed to something else ?
2. Do my implementations seem fine ?
3. What kind of tests should I implement ? Where shall I put them ?
4. Is it a good idea to replace the function body of tf.ones/tf.zeros and tf.ones_like/tf.zeros_like by a function call to tf.alphas and tf.alphas_like ? Not doing it would lead to code duplication, however I would understand that you might be reluctant, these functions are at the core of the library.

### Why I created these functions ?

I oftenly need to create similar tensors with a non-zero/one value. A simple example would be cost functions in GANs with *label smoothing* applied. 

As stated by @facaiy in #16128, I could use : 
```python
b1 = tf.ones_like(a, dtype=tf.float32) * 0.9 # Tensor full of 0.9
b2 = tf.ones_like(a, dtype=tf.int32) * 2 # Tensor full of 2
b4 = tf.ones_like(a, dtype=tf.bool) # Tensor full of True
```
 However, as shown in my later comments in the issue, the method implemented in this PR is almost twice as fast.

In a wider view, I think that using a single function more *generic* is always a good thing whenever it is possible.

### How does the function API work ?

In a very similar manner than the existing ones: 

```python
import tensorflow as tf

a = tf.constant([
    [
        [4, 5, 6],
        [1, 2, 3]
    ],
    [
        [4, 5, 6],
        [1, 2, 3]
    ]
])

b1 = tf.alphas_like(a, 0.5431)
b2 = tf.alphas_like(a, 5)
b3 = tf.alphas_like(a, -5)
b4 = tf.alphas_like(a, True)

with tf.Session() as sess:
    _b1, _b2, _b3, _b4 = sess.run([b1, b2, b3, b4])
    
print(""b1:"", _b1)
print(""b2:"", _b2)
print(""b3:"", _b3)
print(""b4:"", _b4)

############### OUTPUTS ###############

>>> b1: [
  [
    [ 0.5431  0.5431  0.5431]
    [ 0.5431  0.5431  0.5431]
  ]
  [
    [ 0.5431  0.5431  0.5431]
    [ 0.5431  0.5431  0.5431]
  ]
]

>>> b2: [
  [
    [5 5 5]
    [5 5 5]
  ]
  [
    [5 5 5]
    [5 5 5]
  ]
]

>>> b3: [
  [
    [-5 -5 -5]
    [-5 -5 -5]
  ]
  [
    [-5 -5 -5]
    [-5 -5 -5]
  ]
]

>>> b4: [
  [
    [ True  True  True]
    [ True  True  True]
  ]
  [
    [ True  True  True]
    [ True  True  True]
  ]
]
```

---------------------------

I'm of course free for discussion over video-calls. It's the first time I try to make a change at the core of TF, and I'm quite afraid of breaking everything ;) Thanks for your help btw.

All the best,

Jonathan DEKHTIAR",1,,6,2018-01-16T10:12:45Z,2018-01-22T22:47:28Z,CONTRIBUTOR,"This PR is related to the issue: #16128

#### I send my work here for peer reviewing and discussion. Please do not merge now.

### A few interrogations before merging

1. Are the names I have chosen fine with everyone or you would like it to be changed to something else ?
2. Do my implementations seem fine ?
3. What kind of tests should I implement ? Where shall I put them ?
4. Is it a good idea to replace the function body of tf.ones/tf.zeros and tf.ones_like/tf.zeros_like by a function call to tf.alphas and tf.alphas_like ? Not doing it would lead to code duplication, however I would understand that you might be reluctant, these functions are at the core of the library.

### Why I created these functions ?

I oftenly need to create similar tensors with a non-zero/one value. A simple example would be cost functions in GANs with *label smoothing* applied. 

As stated by @facaiy in #16128, I could use : 
```python
b1 = tf.ones_like(a, dtype=tf.float32) * 0.9 # Tensor full of 0.9
b2 = tf.ones_like(a, dtype=tf.int32) * 2 # Tensor full of 2
b4 = tf.ones_like(a, dtype=tf.bool) # Tensor full of True
```
 However, as shown in my later comments in the issue, the method implemented in this PR is almost twice as fast.

In a wider view, I think that using a single function more *generic* is always a good thing whenever it is possible.

### How does the function API work ?

In a very similar manner than the existing ones: 

```python
import tensorflow as tf

a = tf.constant([
    [
        [4, 5, 6],
        [1, 2, 3]
    ],
    [
        [4, 5, 6],
        [1, 2, 3]
    ]
])

b1 = tf.alphas_like(a, 0.5431)
b2 = tf.alphas_like(a, 5)
b3 = tf.alphas_like(a, -5)
b4 = tf.alphas_like(a, True)

with tf.Session() as sess:
    _b1, _b2, _b3, _b4 = sess.run([b1, b2, b3, b4])
    
print(""b1:"", _b1)
print(""b2:"", _b2)
print(""b3:"", _b3)
print(""b4:"", _b4)

############### OUTPUTS ###############

>>> b1: [
  [
    [ 0.5431  0.5431  0.5431]
    [ 0.5431  0.5431  0.5431]
  ]
  [
    [ 0.5431  0.5431  0.5431]
    [ 0.5431  0.5431  0.5431]
  ]
]

>>> b2: [
  [
    [5 5 5]
    [5 5 5]
  ]
  [
    [5 5 5]
    [5 5 5]
  ]
]

>>> b3: [
  [
    [-5 -5 -5]
    [-5 -5 -5]
  ]
  [
    [-5 -5 -5]
    [-5 -5 -5]
  ]
]

>>> b4: [
  [
    [ True  True  True]
    [ True  True  True]
  ]
  [
    [ True  True  True]
    [ True  True  True]
  ]
]
```

---------------------------

I'm of course free for discussion over video-calls. It's the first time I try to make a change at the core of TF, and I'm quite afraid of breaking everything ;) Thanks for your help btw.

All the best,

Jonathan DEKHTIAR",2018-01-22T09:52:58Z,6,2,1,6.2658725856748525
330,16151,ValueError: Labels are incompatible with given information. ,,"Have I written custom code: yes
OS: Windows 8.1
Tensorflow installed from: conda
Tensorflow version: 1.4

I am having problems in adding validation monitors to `Estimator.fit`. With this code I have:

```
def main(_):
    image_paths, labels = dataset_utils.read_dataset_list('../test/dummy_labels_file.txt')
    data_dir = ""../test/dummy_data/""
    images = dataset_utils.read_images(data_dir=data_dir, image_paths=image_paths, image_extension='png')
    print('Done reading images')
    images = dataset_utils.resize(images, (1596, 48))
    images = dataset_utils.transpose(images)
    labels = dataset_utils.encode(labels)
    x_train, x_test, y_train, y_test = dataset_utils.split(features=images, test_size=0.5, labels=labels)
    print(x_test)
    x_train_seq_lens = dataset_utils.get_seq_lens(x_train)
    x_test_seq_lens = dataset_utils.get_seq_lens(x_test)

    train_input_fn = tf.estimator.inputs.numpy_input_fn(
        x={""x"": np.array(x_train),
           ""seq_lens"": np.array(x_train_seq_lens)},
        y=np.array(y_train),
        num_epochs=1,
        shuffle=True,
        batch_size=1
    )

    validation_input_fn = tf.estimator.inputs.numpy_input_fn(
        x={""x"": np.array(x_test),
           ""seq_lens"": np.array(x_test_seq_lens)},
        y=np.array(y_test),
        shuffle=True
    )

    validation_monitor = learn.monitors.ValidationMonitor(
        input_fn=validation_input_fn,
        every_n_steps=1
    )

    model = GridRNNModelFn(num_time_steps=1596, num_features=48, num_hidden_units=128, num_classes=80,
                           learning_rate=0.001, optimizer=Optimizers.MOMENTUM)

    classifier = learn.Estimator(model_fn=model.model_fn, params=model.params, model_dir=""/tmp/grid_rnn_ocr_model"")
    classifier.fit(input_fn=train_input_fn, monitors=[validation_monitor])


if __name__ == '__main__':
    tf.app.run(main=main)
```

It throws this error:

`ValueError: Labels are incompatible with given information. Given labels: Tensor(""random_shuffle_queue_DequeueUpTo:3"", shape=(?, 37), dtype=int32), required signatures: TensorSignature(dtype=tf.int32, shape=TensorShape([Dimension(None), Dimension(33)]), is_sparse=False).`

Which leads me to think that the dynamic label lengths are not accepted. To reproduce this, simply clone this [repository](https://github.com/selcouthlyBlue/simplified_bi_lstm_ocr) and run the script specified in the readme.",0,,1,2018-01-16T08:43:19Z,2018-01-16T09:31:29Z,CONTRIBUTOR,"Have I written custom code: yes
OS: Windows 8.1
Tensorflow installed from: conda
Tensorflow version: 1.4

I am having problems in adding validation monitors to `Estimator.fit`. With this code I have:

```
def main(_):
    image_paths, labels = dataset_utils.read_dataset_list('../test/dummy_labels_file.txt')
    data_dir = ""../test/dummy_data/""
    images = dataset_utils.read_images(data_dir=data_dir, image_paths=image_paths, image_extension='png')
    print('Done reading images')
    images = dataset_utils.resize(images, (1596, 48))
    images = dataset_utils.transpose(images)
    labels = dataset_utils.encode(labels)
    x_train, x_test, y_train, y_test = dataset_utils.split(features=images, test_size=0.5, labels=labels)
    print(x_test)
    x_train_seq_lens = dataset_utils.get_seq_lens(x_train)
    x_test_seq_lens = dataset_utils.get_seq_lens(x_test)

    train_input_fn = tf.estimator.inputs.numpy_input_fn(
        x={""x"": np.array(x_train),
           ""seq_lens"": np.array(x_train_seq_lens)},
        y=np.array(y_train),
        num_epochs=1,
        shuffle=True,
        batch_size=1
    )

    validation_input_fn = tf.estimator.inputs.numpy_input_fn(
        x={""x"": np.array(x_test),
           ""seq_lens"": np.array(x_test_seq_lens)},
        y=np.array(y_test),
        shuffle=True
    )

    validation_monitor = learn.monitors.ValidationMonitor(
        input_fn=validation_input_fn,
        every_n_steps=1
    )

    model = GridRNNModelFn(num_time_steps=1596, num_features=48, num_hidden_units=128, num_classes=80,
                           learning_rate=0.001, optimizer=Optimizers.MOMENTUM)

    classifier = learn.Estimator(model_fn=model.model_fn, params=model.params, model_dir=""/tmp/grid_rnn_ocr_model"")
    classifier.fit(input_fn=train_input_fn, monitors=[validation_monitor])


if __name__ == '__main__':
    tf.app.run(main=main)
```

It throws this error:

`ValueError: Labels are incompatible with given information. Given labels: Tensor(""random_shuffle_queue_DequeueUpTo:3"", shape=(?, 37), dtype=int32), required signatures: TensorSignature(dtype=tf.int32, shape=TensorShape([Dimension(None), Dimension(33)]), is_sparse=False).`

Which leads me to think that the dynamic label lengths are not accepted. To reproduce this, simply clone this [repository](https://github.com/selcouthlyBlue/simplified_bi_lstm_ocr) and run the script specified in the readme.",2018-01-16T09:31:29Z,0,2,0,2.765872585674852
331,16148,non_max_suppression is on CPU?,,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
     Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
      Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:
      binary(By pip)
- **TensorFlow version (use command below)**:
      1.4.1
- **Python version**: 
      3.5.2
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
    8.0.61/6.0.21
- **GPU model and memory**:
    GTX 1080 Ti, 11172MiB
- **Exact command to reproduce**:
     python main.py

### Describe the problem
    
I train my RFCN by tensorflow. My project need very high speed. So I use the profile and I find that non_max_suppression is on CPU? Is there a GPU version?I think if you calculate all pairs of boxes IOU first, then just for-loop once will ultimately boost speed, there have some trick in it, just see the source code in [https://github.com/rbgirshick/py-faster-rcnn/tree/master/lib/nms](https://github.com/rbgirshick/py-faster-rcnn/tree/master/lib/nms). I think cuda version of NMS is faster than CPU version.
",0,,2,2018-01-16T08:21:02Z,2018-01-16T19:33:57Z,NONE,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
     Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
      Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:
      binary(By pip)
- **TensorFlow version (use command below)**:
      1.4.1
- **Python version**: 
      3.5.2
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
    8.0.61/6.0.21
- **GPU model and memory**:
    GTX 1080 Ti, 11172MiB
- **Exact command to reproduce**:
     python main.py

### Describe the problem
    
I train my RFCN by tensorflow. My project need very high speed. So I use the profile and I find that non_max_suppression is on CPU? Is there a GPU version?I think if you calculate all pairs of boxes IOU first, then just for-loop once will ultimately boost speed, there have some trick in it, just see the source code in [https://github.com/rbgirshick/py-faster-rcnn/tree/master/lib/nms](https://github.com/rbgirshick/py-faster-rcnn/tree/master/lib/nms). I think cuda version of NMS is faster than CPU version.
",2018-01-16T13:30:51Z,0,1,0,2.265872585674852
332,16145,Decoding contents of BMP file on big endian,"awaiting testing (then merge),cla: yes","As the BMP file contents are encoded in little endian format, added byte swapping for reading the various header components correctly on big endian.",1,,2,2018-01-16T06:29:25Z,2018-01-26T16:40:05Z,CONTRIBUTOR,"As the BMP file contents are encoded in little endian format, added byte swapping for reading the various header components correctly on big endian.",2018-01-25T19:04:02Z,10,2,2,4.2658725856748525
333,16144,Is it possible to train CNN model by using tensorflow JAVA API?,,"Hello, TF.
I have plane to train my CNN model by using tensorflow JAVA API.
I got success on simple model( with a simple matmul operation between weights and bias)
BUT I failed to train CNN model.
",0,,2,2018-01-16T06:15:29Z,2018-01-16T07:20:47Z,NONE,"Hello, TF.
I have plane to train my CNN model by using tensorflow JAVA API.
I got success on simple model( with a simple matmul operation between weights and bias)
BUT I failed to train CNN model.
",2018-01-16T07:20:46Z,0,1,0,2.265872585674852
334,16143,"Undefined symbol ""_ZN3Aws8Security14SecureMemClearEPhj""",stat:awaiting response,"compiled tensorflow r.15 from source , when import tensorflow in python got following error:
>>> import tensorflow as tf
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/lib/python2.7/site-packages/tensorflow-1.5.0rc1-py2.7-freebsd-11.0-RELEASE-p1-i386.egg/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""/usr/local/lib/python2.7/site-packages/tensorflow-1.5.0rc1-py2.7-freebsd-11.0-RELEASE-p1-i386.egg/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/usr/local/lib/python2.7/site-packages/tensorflow-1.5.0rc1-py2.7-freebsd-11.0-RELEASE-p1-i386.egg/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/site-packages/tensorflow-1.5.0rc1-py2.7-freebsd-11.0-RELEASE-p1-i386.egg/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/usr/local/lib/python2.7/site-packages/tensorflow-1.5.0rc1-py2.7-freebsd-11.0-RELEASE-p1-i386.egg/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/usr/local/lib/python2.7/site-packages/tensorflow-1.5.0rc1-py2.7-freebsd-11.0-RELEASE-p1-i386.egg/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
ImportError: /usr/local/lib/python2.7/site-packages/tensorflow-1.5.0rc1-py2.7-freebsd-11.0-RELEASE-p1-i386.egg/tensorflow/python/_pywrap_tensorflow_internal.so: Undefined symbol ""_ZN3Aws8Security14SecureMemClearEPhj""


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace

thanks in advance !!!",0,,2,2018-01-16T06:04:50Z,2018-01-28T13:49:48Z,NONE,"compiled tensorflow r.15 from source , when import tensorflow in python got following error:
>>> import tensorflow as tf
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/lib/python2.7/site-packages/tensorflow-1.5.0rc1-py2.7-freebsd-11.0-RELEASE-p1-i386.egg/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""/usr/local/lib/python2.7/site-packages/tensorflow-1.5.0rc1-py2.7-freebsd-11.0-RELEASE-p1-i386.egg/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/usr/local/lib/python2.7/site-packages/tensorflow-1.5.0rc1-py2.7-freebsd-11.0-RELEASE-p1-i386.egg/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/site-packages/tensorflow-1.5.0rc1-py2.7-freebsd-11.0-RELEASE-p1-i386.egg/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/usr/local/lib/python2.7/site-packages/tensorflow-1.5.0rc1-py2.7-freebsd-11.0-RELEASE-p1-i386.egg/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/usr/local/lib/python2.7/site-packages/tensorflow-1.5.0rc1-py2.7-freebsd-11.0-RELEASE-p1-i386.egg/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
ImportError: /usr/local/lib/python2.7/site-packages/tensorflow-1.5.0rc1-py2.7-freebsd-11.0-RELEASE-p1-i386.egg/tensorflow/python/_pywrap_tensorflow_internal.so: Undefined symbol ""_ZN3Aws8Security14SecureMemClearEPhj""


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace

thanks in advance !!!",2018-01-16T19:01:24Z,12,1,2,2.265872585674852
335,16142,fix typo,"awaiting testing (then merge),cla: yes",,2,,4,2018-01-16T05:36:10Z,2018-01-23T17:35:39Z,CONTRIBUTOR,,2018-01-16T05:41:07Z,7,2,1,6.2658725856748525
336,16135,Distributed Tensorflow  using MPI,,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

I have tried stackflow and Google group discussion forum but could  get any reply or comment

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Redhat 7.4

- **TensorFlow installed from (source or binary)**:
from source with MPI
- **TensorFlow version (use command below)**:
1.41
- **Python version**: 
2.7.14
- **Bazel version (if compiling from source)**:

- **GCC/Compiler version (if compiling from source)**:
GCC 6.0
- **CUDA/cuDNN version**:
8.0/6.5
- **GPU model and memory**:
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 384.81                 Driver Version: 384.81                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla K20Xm         Off  | 00000000:08:00.0 Off |                    0 |
| N/A   34C    P0    61W / 235W |      0MiB /  5699MiB |     72%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
I am using the following  script to launch distributed computing.


#! /bin/bash

module load openmpi/3.0.0-gnu

host=$(hostname -s)
if [[ $host == ""node06"" ]]; then
        echo ""statring Node 6""
        python tf_dis_2.py --job_name=""ps"" --task_index=0
elif [[ $host == ""node07"" ]]; then
        echo ""starting Node 7 as worker""
        python tf_dis_2.py --job_name=""worker"" --task_index=0
elif [[ $host == ""node08"" ]]; then
        echo ""starting Node 8 as worker""
        python tf_dis_2.py --job_name=""worker"" --task_index=1
fi

-----

I am running it on slurm  with three nodes.

srun -N 3 -n 3 --gres=gpu:1 -w node[06-08] test.sh

I am using MPI instead of GPRC.

I am getting the following message:

---------------------------------------------------
srun -N 3 -n 3 --gres=gpu:1 -w node[06-08] test.sh
statring Node 6
starting Node 8 as worker
starting Node 7 as worker
2018-01-15 11:34:59.961617: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: 
name: Tesla K20Xm major: 3 minor: 5 memoryClockRate(GHz): 0.732
pciBusID: 0000:08:00.0
totalMemory: 5.57GiB freeMemory: 5.49GiB
2018-01-15 11:34:59.961674: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K20Xm, pci bus id: 0000:08:00.0, compute capability: 3.5)
E0115 11:35:00.020327488   36133 ev_epoll1_linux.c:1051]     grpc epoll fd: 22
2018-01-15 11:35:00.026716: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> node06:2222}
2018-01-15 11:35:00.026760: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> node07:2223, 1 -> localhost:2224}
2018-01-15 11:35:00.029261: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:324] Started server with target: grpc://localhost:2224
2018-01-15 11:35:00.439045: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: 
name: Tesla K20Xm major: 3 minor: 5 memoryClockRate(GHz): 0.732
pciBusID: 0000:08:00.0
totalMemory: 5.57GiB freeMemory: 5.49GiB
2018-01-15 11:35:00.439124: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K20Xm, pci bus id: 0000:08:00.0, compute capability: 3.5)
E0115 11:35:00.497022377   13701 ev_epoll1_linux.c:1051]     grpc epoll fd: 22
2018-01-15 11:35:00.503585: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> localhost:2222}
2018-01-15 11:35:00.503622: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> node07:2223, 1 -> node08:2224}
2018-01-15 11:35:00.505803: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:324] Started server with target: grpc://localhost:2222
2018-01-15 11:33:39.681311: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: 
name: Tesla K20Xm major: 3 minor: 5 memoryClockRate(GHz): 0.732
pciBusID: 0000:08:00.0
totalMemory: 5.57GiB freeMemory: 5.49GiB
2018-01-15 11:33:39.681375: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K20Xm, pci bus id: 0000:08:00.0, compute capability: 3.5)
E0115 11:33:39.739196190   46236 ev_epoll1_linux.c:1051]     grpc epoll fd: 22
2018-01-15 11:33:39.745655: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> node06:2222}
2018-01-15 11:33:39.745697: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> localhost:2223, 1 -> node08:2224}
2018-01-15 11:33:39.747692: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:324] Started server with target: grpc://localhost:2223
Abid Malik
Extracting MNIST_data/train-images-idx3-ubyte.gz
Extracting MNIST_data/train-labels-idx1-ubyte.gz
Extracting MNIST_data/t10k-images-idx3-ubyte.gz
Extracting MNIST_data/t10k-labels-idx1-ubyte.gz
Variables initialized ...
Traceback (most recent call last):
  File ""tf_dis_2.py"", line 102, in <module>
    sv = tf.train.Supervisor(is_chief=(FLAGS.task_index == 0),logdir=""/tmp/train_logs"",global_step=global_step,init_op=init_op)
  File ""/home/amalik/.local/lib/python2.7/site-packages/tensorflow/python/training/supervisor.py"", line 336, in __init__
    self._verify_setup()
  File ""/home/amalik/.local/lib/python2.7/site-packages/tensorflow/python/training/supervisor.py"", line 885, in _verify_setup
    ""their device set: %s"" % op)
ValueError: When using replicas, all Variables must have their device set: name: ""weights/Variable""
op: ""VariableV2""
attr {
  key: ""container""
  value {
    s: """"
  }
}
attr {
  key: ""dtype""
  value {
    type: DT_FLOAT
  }
}
attr {
  key: ""shape""
  value {
    shape {
      dim {
        size: 784
      }
      dim {
        size: 100
      }
    }
  }
}
attr {
  key: ""shared_name""
  value {
    s: """"
  }
}

2018-01-15 11:33:41.719083: E tensorflow/core/distributed_runtime/master.cc:269] Master init: Unavailable: Endpoint read failed
Extracting MNIST_data/train-images-idx3-ubyte.gz
Extracting MNIST_data/train-labels-idx1-ubyte.gz
Extracting MNIST_data/t10k-images-idx3-ubyte.gz
Extracting MNIST_data/t10k-labels-idx1-ubyte.gz
Variables initialized ...
Traceback (most recent call last):
  File ""tf_dis_2.py"", line 114, in <module>
    with sv.prepare_or_wait_for_session(server.target) as sess:
  File ""/home/amalik/.local/lib/python2.7/site-packages/tensorflow/python/training/supervisor.py"", line 708, in prepare_or_wait_for_session
    init_feed_dict=self._init_feed_dict, init_fn=self._init_fn)
  File ""/home/amalik/.local/lib/python2.7/site-packages/tensorflow/python/training/session_manager.py"", line 273, in prepare_session
    config=config)
  File ""/home/amalik/.local/lib/python2.7/site-packages/tensorflow/python/training/session_manager.py"", line 205, in _restore_checkpoint
    saver.restore(sess, ckpt.model_checkpoint_path)
  File ""/home/amalik/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1666, in restore
    {self.saver_def.filename_tensor_name: save_path})
  File ""/home/amalik/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 889, in run
    run_metadata_ptr)
  File ""/home/amalik/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1120, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/home/amalik/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1317, in _do_run
    options, run_metadata)
  File ""/home/amalik/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1336, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.UnavailableError: Endpoint read failed
srun: error: node08: task 2: Exited with exit code 1
srun: error: node07: task 1: Exited with exit code 1
---------------------------------------------------------------------------------

Why is it crashing? I have been trying to solve this for the last three weeks by putting it on different forums and groups. However, could not get any reply. I would be grateful if someone can guide me. I apologize in advance if this is not the right forum.





### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:

``
from __future__ import print_function

import tensorflow as tf
import sys
import time


print(""Abid Malik"")


parameter_servers = [""node06:2222""]
workers = [""node07:2223"",""node08:2224""]
cluster = tf.train.ClusterSpec({""ps"":parameter_servers, ""worker"":workers})



tf.app.flags.DEFINE_string(""job_name"", """", ""Either 'ps' or 'worker'"")
tf.app.flags.DEFINE_integer(""task_index"", 0, ""Index of task within the job"")
FLAGS = tf.app.flags.FLAGS





server = tf.train.Server(
    cluster,
    job_name=FLAGS.job_name,
    task_index=FLAGS.task_index)


batch_size = 100
learning_rate = 0.0005
training_epochs = 20
logs_path = ""/tmp/mnist/1""


from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets('MNIST_data', one_hot=True)

if FLAGS.job_name == ""ps"":
    server.join()
elif FLAGS.job_name == ""worker"":

        with tf.device(tf.train.replica_device_setter(worker_device=""/job:worker/task:%d"" % FLAGS.task_index,cluster=cluster)):
              
                global_step = tf.get_variable('global_step',[],initializer = tf.constant_initializer(0), trainable = False)

              
        with tf.name_scope('input'):
              
                  x = tf.placeholder(tf.float32, shape=[None, 784], name=""x-input"")
               
                  y_ = tf.placeholder(tf.float32, shape=[None, 10], name=""y-input"")

                
        tf.set_random_seed(1)
        with tf.name_scope(""weights""):
                        W1 = tf.Variable(tf.random_normal([784, 100]))
                        W2 = tf.Variable(tf.random_normal([100, 10]))

               
        with tf.name_scope(""biases""):
                        b1 = tf.Variable(tf.zeros([100]))
                        b2 = tf.Variable(tf.zeros([10]))

               
        with tf.name_scope(""softmax""):
                        # y is our prediction
                        z2 = tf.add(tf.matmul(x,W1),b1)
                        a2 = tf.nn.sigmoid(z2)
                        z3 = tf.add(tf.matmul(a2,W2),b2)
                        y  = tf.nn.softmax(z3)

               
        with tf.name_scope('cross_entropy'):
                        # this is our cost
                        cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))

             
        with tf.name_scope('train'):
                       
                                                                                                                                                                                                                                                                

grad_op = tf.train.GradientDescentOptimizer(learning_rate)
                        train_op = grad_op.minimize(cross_entropy, global_step=global_step)


        with tf.name_scope('Accuracy'):
                        # accuracy
                        correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))
                        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

   
        tf.summary.scalar(""cost"", cross_entropy)
        tf.summary.scalar(""accuracy"", accuracy)

        saver = tf.train.Saver()
       
        summary_op = tf.summary.merge_all()
        init_op = tf.global_variables_initializer()
        print(""Variables initialized ..."")

        sv = tf.train.Supervisor(is_chief=(FLAGS.task_index == 0),logdir=""/tmp/train_logs"",global_step=global_step,init_op=init_op)


        begin_time = time.time()
        frequency = 100
        with sv.prepare_or_wait_for_session(server.target) as sess:
                # create log writer object (this will log on every machine)
                writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())

                # perform training cycles
                start_time = time.time()
                for epoch in range(training_epochs):

                        # number of batches in one epoch
                        batch_count = int(mnist.train.num_examples/batch_size)

                        count = 0
                        for i in range(batch_count):
                                batch_x, batch_y = mnist.train.next_batch(batch_size)

                                # perform the operations we defined earlier on batch
                                _, cost, summary, step = sess.run([train_op, cross_entropy, summary_op, global_step], feed_dict={x: batch_x, y_: batch_y})
                                writer.add_summary(summary, step)

                                count += 1
                                if count % frequency == 0 or i+1 == batch_count:
                                        elapsed_time = time.time() - start_time
                                        start_time = time.time()
                                        print(""Step: %d,"" % (step+1),
                                                                "" Epoch: %2d,"" % (epoch+1),
                                                                "" Batch: %3d of %3d,"" % (i+1, batch_count),
                                                                "" Cost: %.4f,"" % cost,
                                                                "" AvgTime: %3.2fms"" % float(elapsed_time*1000/frequency))
                                        count = 0


                print(""Test-Accuracy: %2.2f"" % sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))
                print(""Total Time: %3.2fs"" % float(time.time() - begin_time))
                print(""Final Cost: %.4f"" % cost)

        sv.stop()
        print(""done"")
                                                                                                                                                                                                                                                                 

``",0,,4,2018-01-15T18:05:52Z,2018-01-23T20:26:01Z,NONE,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

I have tried stackflow and Google group discussion forum but could  get any reply or comment

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Redhat 7.4

- **TensorFlow installed from (source or binary)**:
from source with MPI
- **TensorFlow version (use command below)**:
1.41
- **Python version**: 
2.7.14
- **Bazel version (if compiling from source)**:

- **GCC/Compiler version (if compiling from source)**:
GCC 6.0
- **CUDA/cuDNN version**:
8.0/6.5
- **GPU model and memory**:
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 384.81                 Driver Version: 384.81                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla K20Xm         Off  | 00000000:08:00.0 Off |                    0 |
| N/A   34C    P0    61W / 235W |      0MiB /  5699MiB |     72%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
I am using the following  script to launch distributed computing.


#! /bin/bash

module load openmpi/3.0.0-gnu

host=$(hostname -s)
if [[ $host == ""node06"" ]]; then
        echo ""statring Node 6""
        python tf_dis_2.py --job_name=""ps"" --task_index=0
elif [[ $host == ""node07"" ]]; then
        echo ""starting Node 7 as worker""
        python tf_dis_2.py --job_name=""worker"" --task_index=0
elif [[ $host == ""node08"" ]]; then
        echo ""starting Node 8 as worker""
        python tf_dis_2.py --job_name=""worker"" --task_index=1
fi

-----

I am running it on slurm  with three nodes.

srun -N 3 -n 3 --gres=gpu:1 -w node[06-08] test.sh

I am using MPI instead of GPRC.

I am getting the following message:

---------------------------------------------------
srun -N 3 -n 3 --gres=gpu:1 -w node[06-08] test.sh
statring Node 6
starting Node 8 as worker
starting Node 7 as worker
2018-01-15 11:34:59.961617: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: 
name: Tesla K20Xm major: 3 minor: 5 memoryClockRate(GHz): 0.732
pciBusID: 0000:08:00.0
totalMemory: 5.57GiB freeMemory: 5.49GiB
2018-01-15 11:34:59.961674: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K20Xm, pci bus id: 0000:08:00.0, compute capability: 3.5)
E0115 11:35:00.020327488   36133 ev_epoll1_linux.c:1051]     grpc epoll fd: 22
2018-01-15 11:35:00.026716: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> node06:2222}
2018-01-15 11:35:00.026760: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> node07:2223, 1 -> localhost:2224}
2018-01-15 11:35:00.029261: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:324] Started server with target: grpc://localhost:2224
2018-01-15 11:35:00.439045: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: 
name: Tesla K20Xm major: 3 minor: 5 memoryClockRate(GHz): 0.732
pciBusID: 0000:08:00.0
totalMemory: 5.57GiB freeMemory: 5.49GiB
2018-01-15 11:35:00.439124: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K20Xm, pci bus id: 0000:08:00.0, compute capability: 3.5)
E0115 11:35:00.497022377   13701 ev_epoll1_linux.c:1051]     grpc epoll fd: 22
2018-01-15 11:35:00.503585: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> localhost:2222}
2018-01-15 11:35:00.503622: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> node07:2223, 1 -> node08:2224}
2018-01-15 11:35:00.505803: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:324] Started server with target: grpc://localhost:2222
2018-01-15 11:33:39.681311: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: 
name: Tesla K20Xm major: 3 minor: 5 memoryClockRate(GHz): 0.732
pciBusID: 0000:08:00.0
totalMemory: 5.57GiB freeMemory: 5.49GiB
2018-01-15 11:33:39.681375: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K20Xm, pci bus id: 0000:08:00.0, compute capability: 3.5)
E0115 11:33:39.739196190   46236 ev_epoll1_linux.c:1051]     grpc epoll fd: 22
2018-01-15 11:33:39.745655: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> node06:2222}
2018-01-15 11:33:39.745697: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> localhost:2223, 1 -> node08:2224}
2018-01-15 11:33:39.747692: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:324] Started server with target: grpc://localhost:2223
Abid Malik
Extracting MNIST_data/train-images-idx3-ubyte.gz
Extracting MNIST_data/train-labels-idx1-ubyte.gz
Extracting MNIST_data/t10k-images-idx3-ubyte.gz
Extracting MNIST_data/t10k-labels-idx1-ubyte.gz
Variables initialized ...
Traceback (most recent call last):
  File ""tf_dis_2.py"", line 102, in <module>
    sv = tf.train.Supervisor(is_chief=(FLAGS.task_index == 0),logdir=""/tmp/train_logs"",global_step=global_step,init_op=init_op)
  File ""/home/amalik/.local/lib/python2.7/site-packages/tensorflow/python/training/supervisor.py"", line 336, in __init__
    self._verify_setup()
  File ""/home/amalik/.local/lib/python2.7/site-packages/tensorflow/python/training/supervisor.py"", line 885, in _verify_setup
    ""their device set: %s"" % op)
ValueError: When using replicas, all Variables must have their device set: name: ""weights/Variable""
op: ""VariableV2""
attr {
  key: ""container""
  value {
    s: """"
  }
}
attr {
  key: ""dtype""
  value {
    type: DT_FLOAT
  }
}
attr {
  key: ""shape""
  value {
    shape {
      dim {
        size: 784
      }
      dim {
        size: 100
      }
    }
  }
}
attr {
  key: ""shared_name""
  value {
    s: """"
  }
}

2018-01-15 11:33:41.719083: E tensorflow/core/distributed_runtime/master.cc:269] Master init: Unavailable: Endpoint read failed
Extracting MNIST_data/train-images-idx3-ubyte.gz
Extracting MNIST_data/train-labels-idx1-ubyte.gz
Extracting MNIST_data/t10k-images-idx3-ubyte.gz
Extracting MNIST_data/t10k-labels-idx1-ubyte.gz
Variables initialized ...
Traceback (most recent call last):
  File ""tf_dis_2.py"", line 114, in <module>
    with sv.prepare_or_wait_for_session(server.target) as sess:
  File ""/home/amalik/.local/lib/python2.7/site-packages/tensorflow/python/training/supervisor.py"", line 708, in prepare_or_wait_for_session
    init_feed_dict=self._init_feed_dict, init_fn=self._init_fn)
  File ""/home/amalik/.local/lib/python2.7/site-packages/tensorflow/python/training/session_manager.py"", line 273, in prepare_session
    config=config)
  File ""/home/amalik/.local/lib/python2.7/site-packages/tensorflow/python/training/session_manager.py"", line 205, in _restore_checkpoint
    saver.restore(sess, ckpt.model_checkpoint_path)
  File ""/home/amalik/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1666, in restore
    {self.saver_def.filename_tensor_name: save_path})
  File ""/home/amalik/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 889, in run
    run_metadata_ptr)
  File ""/home/amalik/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1120, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/home/amalik/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1317, in _do_run
    options, run_metadata)
  File ""/home/amalik/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1336, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.UnavailableError: Endpoint read failed
srun: error: node08: task 2: Exited with exit code 1
srun: error: node07: task 1: Exited with exit code 1
---------------------------------------------------------------------------------

Why is it crashing? I have been trying to solve this for the last three weeks by putting it on different forums and groups. However, could not get any reply. I would be grateful if someone can guide me. I apologize in advance if this is not the right forum.





### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:

``
from __future__ import print_function

import tensorflow as tf
import sys
import time


print(""Abid Malik"")


parameter_servers = [""node06:2222""]
workers = [""node07:2223"",""node08:2224""]
cluster = tf.train.ClusterSpec({""ps"":parameter_servers, ""worker"":workers})



tf.app.flags.DEFINE_string(""job_name"", """", ""Either 'ps' or 'worker'"")
tf.app.flags.DEFINE_integer(""task_index"", 0, ""Index of task within the job"")
FLAGS = tf.app.flags.FLAGS





server = tf.train.Server(
    cluster,
    job_name=FLAGS.job_name,
    task_index=FLAGS.task_index)


batch_size = 100
learning_rate = 0.0005
training_epochs = 20
logs_path = ""/tmp/mnist/1""


from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets('MNIST_data', one_hot=True)

if FLAGS.job_name == ""ps"":
    server.join()
elif FLAGS.job_name == ""worker"":

        with tf.device(tf.train.replica_device_setter(worker_device=""/job:worker/task:%d"" % FLAGS.task_index,cluster=cluster)):
              
                global_step = tf.get_variable('global_step',[],initializer = tf.constant_initializer(0), trainable = False)

              
        with tf.name_scope('input'):
              
                  x = tf.placeholder(tf.float32, shape=[None, 784], name=""x-input"")
               
                  y_ = tf.placeholder(tf.float32, shape=[None, 10], name=""y-input"")

                
        tf.set_random_seed(1)
        with tf.name_scope(""weights""):
                        W1 = tf.Variable(tf.random_normal([784, 100]))
                        W2 = tf.Variable(tf.random_normal([100, 10]))

               
        with tf.name_scope(""biases""):
                        b1 = tf.Variable(tf.zeros([100]))
                        b2 = tf.Variable(tf.zeros([10]))

               
        with tf.name_scope(""softmax""):
                        # y is our prediction
                        z2 = tf.add(tf.matmul(x,W1),b1)
                        a2 = tf.nn.sigmoid(z2)
                        z3 = tf.add(tf.matmul(a2,W2),b2)
                        y  = tf.nn.softmax(z3)

               
        with tf.name_scope('cross_entropy'):
                        # this is our cost
                        cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))

             
        with tf.name_scope('train'):
                       
                                                                                                                                                                                                                                                                

grad_op = tf.train.GradientDescentOptimizer(learning_rate)
                        train_op = grad_op.minimize(cross_entropy, global_step=global_step)


        with tf.name_scope('Accuracy'):
                        # accuracy
                        correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))
                        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

   
        tf.summary.scalar(""cost"", cross_entropy)
        tf.summary.scalar(""accuracy"", accuracy)

        saver = tf.train.Saver()
       
        summary_op = tf.summary.merge_all()
        init_op = tf.global_variables_initializer()
        print(""Variables initialized ..."")

        sv = tf.train.Supervisor(is_chief=(FLAGS.task_index == 0),logdir=""/tmp/train_logs"",global_step=global_step,init_op=init_op)


        begin_time = time.time()
        frequency = 100
        with sv.prepare_or_wait_for_session(server.target) as sess:
                # create log writer object (this will log on every machine)
                writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())

                # perform training cycles
                start_time = time.time()
                for epoch in range(training_epochs):

                        # number of batches in one epoch
                        batch_count = int(mnist.train.num_examples/batch_size)

                        count = 0
                        for i in range(batch_count):
                                batch_x, batch_y = mnist.train.next_batch(batch_size)

                                # perform the operations we defined earlier on batch
                                _, cost, summary, step = sess.run([train_op, cross_entropy, summary_op, global_step], feed_dict={x: batch_x, y_: batch_y})
                                writer.add_summary(summary, step)

                                count += 1
                                if count % frequency == 0 or i+1 == batch_count:
                                        elapsed_time = time.time() - start_time
                                        start_time = time.time()
                                        print(""Step: %d,"" % (step+1),
                                                                "" Epoch: %2d,"" % (epoch+1),
                                                                "" Batch: %3d of %3d,"" % (i+1, batch_count),
                                                                "" Cost: %.4f,"" % cost,
                                                                "" AvgTime: %3.2fms"" % float(elapsed_time*1000/frequency))
                                        count = 0


                print(""Test-Accuracy: %2.2f"" % sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))
                print(""Total Time: %3.2fs"" % float(time.time() - begin_time))
                print(""Final Cost: %.4f"" % cost)

        sv.stop()
        print(""done"")
                                                                                                                                                                                                                                                                 

``",2018-01-21T00:55:39Z,8,1,2,3.2642573699364457
337,16132,Bug while printing parameters and gradients,type:support,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary (anaconda)
- **TensorFlow version (use command below)**: v1.4.0-19-ga52c8d9 1.4.1
- **Python version**: 3.6.4
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: using CPU
- **GPU model and memory**: using CPU
- **Exact command to reproduce**: see below

### Describe the problem
The model is very simple, I do digits classification with MNIST. There is only one parameter matrix W, no bias and no non-linearities. The model show convergence since the loss is decreasing. I checked predictions and accuracy but I do not copy paste useless code here. If I print the parameters before and after training they are the same, however, it shouldn't be the case. Moreover, the gradient of the loss w.r.t. parameters are zero but again it shouldn't be the case since the model converges so there should be a non-zero gradient. I cannot explain why and my implementation seems correct, that's why I am posting my code here.

### Source code / logs
```
import numpy as np
import tensorflow as tf

tf.set_random_seed(42)

from tensorflow.examples.tutorials.mnist import input_data

mnist = input_data.read_data_sets('data/', one_hot=True)

x = tf.placeholder(tf.float32, shape=(None, 784))
y = tf.placeholder(tf.float32, shape=(None, 10))

W = tf.get_variable('W0', (784, 10))
pred = tf.matmul(x, W)
loss = tf.reduce_sum((y - pred) ** 2)
grads = tf.gradients(loss, W)
train_step = tf.train.AdamOptimizer().minimize(loss)

sess = tf.Session()
sess.run(tf.global_variables_initializer())

print(sess.run(W))

>>> [[-0.0823722  -0.01139299 -0.04053238 ... -0.03432762 -0.05707605
  -0.01042821]
 [ 0.06725802  0.07879441  0.05811419 ... -0.05443887 -0.03835129
  -0.0796528 ]
 [-0.06725079 -0.00356448  0.0823487  ...  0.0006832  -0.01058736
  -0.04312544]
 ...
 [ 0.04159895  0.01873457  0.05547244 ... -0.04325137 -0.00306174
   0.06578781]
 [ 0.05061891 -0.07273331  0.06083969 ...  0.0548989  -0.01343339
  -0.02337921]
 [ 0.02918045 -0.05145956  0.0042838  ...  0.05564766 -0.04886324
  -0.02436799]]

for _ in range(1000):
    x_mb, y_mb = mnist.train.next_batch(32)
    loss_, _ = sess.run([loss, train_step], {x: x_mb, y: y_mb})
    print('loss: {:2.5}'.format(loss_))

>>> I won't print uselss log here but the loss is decreasing

print(sess.run(W))

>>> [[-0.0823722  -0.01139299 -0.04053238 ... -0.03432762 -0.05707605
  -0.01042821]
 [ 0.06725802  0.07879441  0.05811419 ... -0.05443887 -0.03835129
  -0.0796528 ]
 [-0.06725079 -0.00356448  0.0823487  ...  0.0006832  -0.01058736
  -0.04312544]
 ...
 [ 0.04159895  0.01873457  0.05547244 ... -0.04325137 -0.00306174
   0.06578781]
 [ 0.05061891 -0.07273331  0.06083969 ...  0.0548989  -0.01343339
  -0.02337921]
 [ 0.02918045 -0.05145956  0.0042838  ...  0.05564766 -0.04886324
  -0.02436799]]

print(sess.run(grads, {x: x_mb, y: y_mb}))

>>> [array([[0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.],
       ...,
       [0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)]

```
",0,,1,2018-01-15T16:08:51Z,2018-01-19T22:26:56Z,NONE,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary (anaconda)
- **TensorFlow version (use command below)**: v1.4.0-19-ga52c8d9 1.4.1
- **Python version**: 3.6.4
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: using CPU
- **GPU model and memory**: using CPU
- **Exact command to reproduce**: see below

### Describe the problem
The model is very simple, I do digits classification with MNIST. There is only one parameter matrix W, no bias and no non-linearities. The model show convergence since the loss is decreasing. I checked predictions and accuracy but I do not copy paste useless code here. If I print the parameters before and after training they are the same, however, it shouldn't be the case. Moreover, the gradient of the loss w.r.t. parameters are zero but again it shouldn't be the case since the model converges so there should be a non-zero gradient. I cannot explain why and my implementation seems correct, that's why I am posting my code here.

### Source code / logs
```
import numpy as np
import tensorflow as tf

tf.set_random_seed(42)

from tensorflow.examples.tutorials.mnist import input_data

mnist = input_data.read_data_sets('data/', one_hot=True)

x = tf.placeholder(tf.float32, shape=(None, 784))
y = tf.placeholder(tf.float32, shape=(None, 10))

W = tf.get_variable('W0', (784, 10))
pred = tf.matmul(x, W)
loss = tf.reduce_sum((y - pred) ** 2)
grads = tf.gradients(loss, W)
train_step = tf.train.AdamOptimizer().minimize(loss)

sess = tf.Session()
sess.run(tf.global_variables_initializer())

print(sess.run(W))

>>> [[-0.0823722  -0.01139299 -0.04053238 ... -0.03432762 -0.05707605
  -0.01042821]
 [ 0.06725802  0.07879441  0.05811419 ... -0.05443887 -0.03835129
  -0.0796528 ]
 [-0.06725079 -0.00356448  0.0823487  ...  0.0006832  -0.01058736
  -0.04312544]
 ...
 [ 0.04159895  0.01873457  0.05547244 ... -0.04325137 -0.00306174
   0.06578781]
 [ 0.05061891 -0.07273331  0.06083969 ...  0.0548989  -0.01343339
  -0.02337921]
 [ 0.02918045 -0.05145956  0.0042838  ...  0.05564766 -0.04886324
  -0.02436799]]

for _ in range(1000):
    x_mb, y_mb = mnist.train.next_batch(32)
    loss_, _ = sess.run([loss, train_step], {x: x_mb, y: y_mb})
    print('loss: {:2.5}'.format(loss_))

>>> I won't print uselss log here but the loss is decreasing

print(sess.run(W))

>>> [[-0.0823722  -0.01139299 -0.04053238 ... -0.03432762 -0.05707605
  -0.01042821]
 [ 0.06725802  0.07879441  0.05811419 ... -0.05443887 -0.03835129
  -0.0796528 ]
 [-0.06725079 -0.00356448  0.0823487  ...  0.0006832  -0.01058736
  -0.04312544]
 ...
 [ 0.04159895  0.01873457  0.05547244 ... -0.04325137 -0.00306174
   0.06578781]
 [ 0.05061891 -0.07273331  0.06083969 ...  0.0548989  -0.01343339
  -0.02337921]
 [ 0.02918045 -0.05145956  0.0042838  ...  0.05564766 -0.04886324
  -0.02436799]]

print(sess.run(grads, {x: x_mb, y: y_mb}))

>>> [array([[0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.],
       ...,
       [0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)]

```
",2018-01-19T22:26:56Z,4,1,1,1.7642573699364454
338,16131,Update contrib/HVX readme,"awaiting testing (then merge),cla: yes","I updated the README because of some imprecisions and to add clarifications of what I think will guide the users more appropriately.

First, the very simple ""quick start guide"" doesn't work, there's no ""-X"" option (at least publicly) and so you always need to have the SDK installed manually.

Apart from that, some clarifications and rewording were done to help the users understand what's happening.

/cc @satok16 ",1,,2,2018-01-15T15:59:50Z,2018-01-26T16:39:08Z,CONTRIBUTOR,"I updated the README because of some imprecisions and to add clarifications of what I think will guide the users more appropriately.

First, the very simple ""quick start guide"" doesn't work, there's no ""-X"" option (at least publicly) and so you always need to have the SDK installed manually.

Apart from that, some clarifications and rewording were done to help the users understand what's happening.

/cc @satok16 ",2018-01-23T18:04:35Z,11,2,2,4.264257369936446
339,16130,Fix broken python3 build,"awaiting testing (then merge),cla: yes","Currently building tensorflow master branch with python3 fails with following error message.
```
ERROR: ${BAZEL_CACHE}/external/astor_archive/BUILD:8:1: Converting to Python 3: external/astor_archive/astor/code_gen.py failed (Exit 1).
```
It seems that the 3 newly added `third_party/*.BUILD` scripts from https://github.com/tensorflow/tensorflow/pull/15955/commits/4080654c8f03ec34f2822c14db5fd8b75f63d569 are missing `srcs_version = ""PY2AND3""` part, which all the other py_library modules have.

I'm using bazel 0.5.4 on linux ubuntu 16.04 to build the current master branch.",0,,2,2018-01-15T11:00:22Z,2018-01-15T17:30:36Z,CONTRIBUTOR,"Currently building tensorflow master branch with python3 fails with following error message.
```
ERROR: ${BAZEL_CACHE}/external/astor_archive/BUILD:8:1: Converting to Python 3: external/astor_archive/astor/code_gen.py failed (Exit 1).
```
It seems that the 3 newly added `third_party/*.BUILD` scripts from https://github.com/tensorflow/tensorflow/pull/15955/commits/4080654c8f03ec34f2822c14db5fd8b75f63d569 are missing `srcs_version = ""PY2AND3""` part, which all the other py_library modules have.

I'm using bazel 0.5.4 on linux ubuntu 16.04 to build the current master branch.",2018-01-15T16:09:15Z,0,2,0,3.2642573699364457
340,16129,allow 'None' as batch size for TimeFreqLSTMCell,"awaiting testing (then merge),cla: yes","Currently it is not allowed to have a variable batch size in TimeFreqLSTMCell, as the size is casted to an int internally.
This patch fixes this by omitting the int cast.

Tested it in an audio event detection framework without problems.",1,,10,2018-01-15T10:31:31Z,2018-02-15T01:16:57Z,CONTRIBUTOR,"Currently it is not allowed to have a variable batch size in TimeFreqLSTMCell, as the size is casted to an int internally.
This patch fixes this by omitting the int cast.

Tested it in an audio event detection framework without problems.",2018-02-06T14:56:42Z,30,2,4,8.264257369936445
341,16127,"fix default parameters for TimeFreqLSTMCell, fixes #16100",cla: yes,"Resolve #16100 

The default parameters for TimeFreqLSTMCell lead to a division by `None`, which throws an exception.",0,,5,2018-01-15T10:24:06Z,2018-01-22T20:27:14Z,CONTRIBUTOR,"Resolve #16100 

The default parameters for TimeFreqLSTMCell lead to a division by `None`, which throws an exception.",2018-01-15T10:26:15Z,7,2,1,4.764257369936446
342,16125,Disable stacktrace_handler_test becase stack trace isn't generated on Windows,cla: yes,Fix http://ci.tensorflow.org/job/tf-master-win-bzl/2259/console,0,,3,2018-01-15T08:16:44Z,2018-01-15T08:57:31Z,MEMBER,Fix http://ci.tensorflow.org/job/tf-master-win-bzl/2259/console,2018-01-15T08:18:41Z,0,3,0,4.764257369936446
343,16124,How can I batch images of arbitrary sizes in tensorflow?,stat:awaiting response,I want to realize arbitrary inputs that I can batch them in one batch.,0,,4,2018-01-15T07:02:24Z,2018-01-16T15:22:27Z,NONE,I want to realize arbitrary inputs that I can batch them in one batch.,2018-01-15T18:59:25Z,1,1,0,3.2642573699364457
344,16121,Enable some passes for graph_transform on Windows,"awaiting testing (then merge),cla: yes","Don't know why but the following passes are disabled on Windows:

* quantize_weights
* quantize_nodes
* round_weights

This patch re-enabled them. This should fix #11351.

Regarding the original commit disabled the passes on Windows, `git blame` gives the commit:

```
commit d1ba01f81d8fa1d0171ba9ce871599063d5c7eb9
Author: A. Unique TensorFlower <gardener@tensorflow.org>
Date:   Wed Feb 1 18:13:33 2017 -0800

    Merge changes from github.
    Change: 146316196
```

Does anyone know what the commit message means?

I built it on Windows 7 x64 and ran it for my tiny MNIST model. Looks fine...
**So I am abusing CI to test it.**
",1,,5,2018-01-15T06:15:55Z,2018-02-08T16:58:37Z,CONTRIBUTOR,"Don't know why but the following passes are disabled on Windows:

* quantize_weights
* quantize_nodes
* round_weights

This patch re-enabled them. This should fix #11351.

Regarding the original commit disabled the passes on Windows, `git blame` gives the commit:

```
commit d1ba01f81d8fa1d0171ba9ce871599063d5c7eb9
Author: A. Unique TensorFlower <gardener@tensorflow.org>
Date:   Wed Feb 1 18:13:33 2017 -0800

    Merge changes from github.
    Change: 146316196
```

Does anyone know what the commit message means?

I built it on Windows 7 x64 and ran it for my tiny MNIST model. Looks fine...
**So I am abusing CI to test it.**
",2018-01-20T06:31:05Z,23,2,3,5.764257369936446
345,16119,Created dense_to_sparse in contrib.layers,"awaiting testing (then merge),cla: yes",Added `dense_to_sparse`. This does the conversion of dense labels into sparse ones to be passed into the core ctc_loss function. Addresses feature request https://github.com/tensorflow/tensorflow/issues/15985,1,,7,2018-01-15T04:41:50Z,2018-01-25T01:21:52Z,CONTRIBUTOR,Added `dense_to_sparse`. This does the conversion of dense labels into sparse ones to be passed into the core ctc_loss function. Addresses feature request https://github.com/tensorflow/tensorflow/issues/15985,2018-01-23T00:14:50Z,10,2,2,6.764257369936446
346,16118,Minor improvements to TFRecord format docs,"awaiting testing (then merge),cla: yes","The TFRecord format documentation mentions that hashes are computed using a CRC32, but doesn't mention the polynomial used. I added that detail, so the documentation is now sufficient for a developer trying to write a parser / writer for (uncompressed) TFRecord files.",1,,3,2018-01-15T01:26:53Z,2018-01-23T18:29:39Z,CONTRIBUTOR,"The TFRecord format documentation mentions that hashes are computed using a CRC32, but doesn't mention the polynomial used. I added that detail, so the documentation is now sufficient for a developer trying to write a parser / writer for (uncompressed) TFRecord files.",2018-01-22T23:12:57Z,8,2,2,4.764257369936446
347,16114,Update maxout.py,"awaiting testing (then merge),cla: yes,stat:awaiting response",Specify the final number of features in the maxout axis,1,,7,2018-01-14T22:37:39Z,2018-02-01T05:56:45Z,NONE,Specify the final number of features in the maxout axis,2018-01-29T21:51:45Z,17,1,3,5.762697311013863
348,16113,Propagate the error string of GIF processing for decode_gif,"cla: yes,stat:awaiting response","This fix tries to improve the error thrown by `decode_gif` to include the error string generated by GIF processing.

Previously, the error was not very indicative as the error string
returned by GIF processing was hidden:
```
..........
InvalidArgumentError (see above for traceback): Invalid GIF data, size 2091369
	 [[Node: DecodeGif = DecodeGif[_device=""/job:localhost/replica:0/task:0/device:CPU:0""](ReadFile)]]
```

This fix propagate the error string (`can't process optimized gif`) to be part of the `InvalidArgumentError`:
```
InvalidArgumentError (see above for traceback): Invalid GIF data (size 2091369), can't process optimized gif
         [[Node: DecodeGif = DecodeGif[_device=""/job:localhost/replica:0/task:0/device:CPU:0""](ReadFile)]]
```

This fix fixes #15838.

Signed-off-by: Yong Tang <yong.tang.github@outlook.com>",1,,1,2018-01-14T18:39:01Z,2018-01-23T18:01:19Z,MEMBER,"This fix tries to improve the error thrown by `decode_gif` to include the error string generated by GIF processing.

Previously, the error was not very indicative as the error string
returned by GIF processing was hidden:
```
..........
InvalidArgumentError (see above for traceback): Invalid GIF data, size 2091369
	 [[Node: DecodeGif = DecodeGif[_device=""/job:localhost/replica:0/task:0/device:CPU:0""](ReadFile)]]
```

This fix propagate the error string (`can't process optimized gif`) to be part of the `InvalidArgumentError`:
```
InvalidArgumentError (see above for traceback): Invalid GIF data (size 2091369), can't process optimized gif
         [[Node: DecodeGif = DecodeGif[_device=""/job:localhost/replica:0/task:0/device:CPU:0""](ReadFile)]]
```

This fix fixes #15838.

Signed-off-by: Yong Tang <yong.tang.github@outlook.com>",2018-01-23T00:20:48Z,9,3,2,4.762697311013863
349,16112,"Define gradient for tf.linspace and make it work with higher-rank tensors, not just scalars.",,"I needed to feed-forward through `tf.linspace`, but it seems that it does not have gradient defined.

I don't know how to define gradient for existing op, but I've implemented my own version of `tf.linspace` in python using tensorflow with so that automatically defined gradient.
```python
            def linspace(start, end, num):
                range = end - start
                num_steps = num - 1
                h = range / num_steps

                def cond(ta, x, k):
                    return tf.less(x, end)

                def body(ta, x, k):
                    x = x + h
                    ta = ta.write(k, x)
                    return ta, x, k+1

                k = tf.constant(0)
                ta = tf.TensorArray(dtype=tf.float32, size=num)
                ta = ta.write(k, start)
                ta = tf.while_loop(cond, body, [ta, start, k+1])[0]
                return ta.stack()
```

One more feature I can suggest adding is improve to `linspace` so it would work with higher-rank tensors.
The function I wrote is also very short and simple, but is interesting as a generalization of `linspace`.
```python
            def linspace_vectors(start, end, num):
                cnct = tf.concat([start, end], 1)
                seq = tf.map_fn(
                    lambda row_i: linspace(row_i[0], row_i[1], num), cnct)
                splits = tf.split(seq, num, 1)
                return tf.stack(splits)
```
The function is taken from my project and returns 3-rank tensor with shapes [num, r, 1]. Inputs are 2-rank tensors with shapes [r, 1].
So that I linspaced vectors-columns, not just scalars as `tf.linspace` do.
What do you think? Is it worth adding?

Have I written custom code: Yes
OS Platform and Distribution: Ubuntu 16.04
TensorFlow installed from: pip nightly build
TensorFlow version: 1.4.1
Bazel version: N/A
CUDA/cuDNN version: N/A
GPU model and memory: N/A
Exact command to reproduce: N/A",0,,4,2018-01-14T13:10:47Z,2018-02-07T22:52:40Z,NONE,"I needed to feed-forward through `tf.linspace`, but it seems that it does not have gradient defined.

I don't know how to define gradient for existing op, but I've implemented my own version of `tf.linspace` in python using tensorflow with so that automatically defined gradient.
```python
            def linspace(start, end, num):
                range = end - start
                num_steps = num - 1
                h = range / num_steps

                def cond(ta, x, k):
                    return tf.less(x, end)

                def body(ta, x, k):
                    x = x + h
                    ta = ta.write(k, x)
                    return ta, x, k+1

                k = tf.constant(0)
                ta = tf.TensorArray(dtype=tf.float32, size=num)
                ta = ta.write(k, start)
                ta = tf.while_loop(cond, body, [ta, start, k+1])[0]
                return ta.stack()
```

One more feature I can suggest adding is improve to `linspace` so it would work with higher-rank tensors.
The function I wrote is also very short and simple, but is interesting as a generalization of `linspace`.
```python
            def linspace_vectors(start, end, num):
                cnct = tf.concat([start, end], 1)
                seq = tf.map_fn(
                    lambda row_i: linspace(row_i[0], row_i[1], num), cnct)
                splits = tf.split(seq, num, 1)
                return tf.stack(splits)
```
The function is taken from my project and returns 3-rank tensor with shapes [num, r, 1]. Inputs are 2-rank tensors with shapes [r, 1].
So that I linspaced vectors-columns, not just scalars as `tf.linspace` do.
What do you think? Is it worth adding?

Have I written custom code: Yes
OS Platform and Distribution: Ubuntu 16.04
TensorFlow installed from: pip nightly build
TensorFlow version: 1.4.1
Bazel version: N/A
CUDA/cuDNN version: N/A
GPU model and memory: N/A
Exact command to reproduce: N/A",2018-01-15T00:56:24Z,23,1,3,3.262697311013863
350,16108,No tf.metrics.true_negatives,,"**TensorFlow version**: 1.4.1

Is there any particular reason for why there is no `tf.metrics.true_negatives` method? I know it's simple to calculate from other confusion metrics that are available, but I was wondering why the developers chose to let this one method out.",1,,5,2018-01-14T09:47:59Z,2018-01-31T19:02:00Z,NONE,"**TensorFlow version**: 1.4.1

Is there any particular reason for why there is no `tf.metrics.true_negatives` method? I know it's simple to calculate from other confusion metrics that are available, but I was wondering why the developers chose to let this one method out.",2018-01-14T18:55:05Z,17,1,3,4.762697311013863
351,16106,Eager: Invalid placement of vars/consts depending on their types and not the tf.device,,"Hi,
I'm currently testing eager execution on TF 1.5.0-rc1 (built it with XLA and CUDA enabled) and observe strange behavior: variables and constants get created either on GPU or CPU depending on their types, and not `with tf.device(...):` block. Moreover, on creation of int32 variable it fails completely. For example, when I run the following code

```
import tensorflow as tf
import tensorflow.contrib.eager as tfe

tfe.enable_eager_execution()

print('TensorFlow version:', tf.__version__)

with tf.device('/gpu:0'):
    A = tf.constant([1.0, 2.0, 3.0], dtype=tf.float32)
    print('Const A is placed on:', A.device)

    B = tf.constant([1, 2, 3], dtype=tf.int32)
    print('Const B is placed on:', B.device)

    C = tfe.Variable([1.0, 2.0, 3.0], dtype=tf.float32)
    print('Variable C is placed on:', C.device)

    D = tfe.Variable([1, 2, 3], dtype=tf.int32)
    print('Variable D is placed on:', D.device)
```

I get the following output:

```
TensorFlow version: 1.5.0-rc1
2018-01-14 01:16:06.385929: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:895] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-01-14 01:16:06.386198: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties:
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:01:00.0
totalMemory: 10.91GiB freeMemory: 363.06MiB
2018-01-14 01:16:06.386223: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
Const A is placed on: /job:localhost/replica:0/task:0/device:GPU:0
Const B is placed on: CPU:0
Variable C is placed on: /job:localhost/replica:0/task:0/device:GPU:0
Traceback (most recent call last):
  File ""tf_bug.py"", line 18, in <module>
    D = tfe.Variable([1, 2, 3], dtype=tf.int32)
  File ""/home/kpot/pyves/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py"", line 277, in __init__
    constraint=constraint)
  File ""/home/kpot/pyves/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py"", line 422, in _init_from_args
    graph_mode=self._in_graph_mode)
  File ""/home/kpot/pyves/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py"", line 53, in _eager_safe_variable_handle
    container=container)
  File ""/home/kpot/pyves/lib/python3.6/site-packages/tensorflow/python/ops/gen_resource_variable_ops.py"", line 396, in var_handle_op
    attrs=_attrs, ctx=_ctx, name=name)
  File ""/home/kpot/pyves/lib/python3.6/site-packages/tensorflow/python/eager/execute.py"", line 66, in quick_execute
    six.raise_from(core._status_to_exception(e.code, message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.NotFoundError: No registered 'VarHandleOp' OpKernel for GPU devices compatible with node VarHandleOp = VarHandleOp[container=""eager-execution-2/"", dtype=DT_INT32, shape=[3], shared_name=""11""]()
	 (OpKernel was found, but attributes didn't match)
	.  Registered:  device='GPU'; dtype in [DT_VARIANT]
  device='GPU'; dtype in [DT_COMPLEX128]
  device='GPU'; dtype in [DT_COMPLEX64]
  device='GPU'; dtype in [DT_BOOL]
  device='GPU'; dtype in [DT_DOUBLE]
  device='GPU'; dtype in [DT_FLOAT]
  device='GPU'; dtype in [DT_HALF]
  device='CPU'
  device='XLA_GPU'
  device='XLA_CPU'
 [Op:VarHandleOp] name: Variable/
```

As you can see, the constants and variables get placed either on GPU:0 or CPU:0 despite all of them gathered inside the same `tf.device('/gpu:0')` block.",0,,14,2018-01-13T20:28:29Z,2018-01-25T00:10:50Z,NONE,"Hi,
I'm currently testing eager execution on TF 1.5.0-rc1 (built it with XLA and CUDA enabled) and observe strange behavior: variables and constants get created either on GPU or CPU depending on their types, and not `with tf.device(...):` block. Moreover, on creation of int32 variable it fails completely. For example, when I run the following code

```
import tensorflow as tf
import tensorflow.contrib.eager as tfe

tfe.enable_eager_execution()

print('TensorFlow version:', tf.__version__)

with tf.device('/gpu:0'):
    A = tf.constant([1.0, 2.0, 3.0], dtype=tf.float32)
    print('Const A is placed on:', A.device)

    B = tf.constant([1, 2, 3], dtype=tf.int32)
    print('Const B is placed on:', B.device)

    C = tfe.Variable([1.0, 2.0, 3.0], dtype=tf.float32)
    print('Variable C is placed on:', C.device)

    D = tfe.Variable([1, 2, 3], dtype=tf.int32)
    print('Variable D is placed on:', D.device)
```

I get the following output:

```
TensorFlow version: 1.5.0-rc1
2018-01-14 01:16:06.385929: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:895] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-01-14 01:16:06.386198: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties:
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:01:00.0
totalMemory: 10.91GiB freeMemory: 363.06MiB
2018-01-14 01:16:06.386223: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
Const A is placed on: /job:localhost/replica:0/task:0/device:GPU:0
Const B is placed on: CPU:0
Variable C is placed on: /job:localhost/replica:0/task:0/device:GPU:0
Traceback (most recent call last):
  File ""tf_bug.py"", line 18, in <module>
    D = tfe.Variable([1, 2, 3], dtype=tf.int32)
  File ""/home/kpot/pyves/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py"", line 277, in __init__
    constraint=constraint)
  File ""/home/kpot/pyves/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py"", line 422, in _init_from_args
    graph_mode=self._in_graph_mode)
  File ""/home/kpot/pyves/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py"", line 53, in _eager_safe_variable_handle
    container=container)
  File ""/home/kpot/pyves/lib/python3.6/site-packages/tensorflow/python/ops/gen_resource_variable_ops.py"", line 396, in var_handle_op
    attrs=_attrs, ctx=_ctx, name=name)
  File ""/home/kpot/pyves/lib/python3.6/site-packages/tensorflow/python/eager/execute.py"", line 66, in quick_execute
    six.raise_from(core._status_to_exception(e.code, message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.NotFoundError: No registered 'VarHandleOp' OpKernel for GPU devices compatible with node VarHandleOp = VarHandleOp[container=""eager-execution-2/"", dtype=DT_INT32, shape=[3], shared_name=""11""]()
	 (OpKernel was found, but attributes didn't match)
	.  Registered:  device='GPU'; dtype in [DT_VARIANT]
  device='GPU'; dtype in [DT_COMPLEX128]
  device='GPU'; dtype in [DT_COMPLEX64]
  device='GPU'; dtype in [DT_BOOL]
  device='GPU'; dtype in [DT_DOUBLE]
  device='GPU'; dtype in [DT_FLOAT]
  device='GPU'; dtype in [DT_HALF]
  device='CPU'
  device='XLA_GPU'
  device='XLA_CPU'
 [Op:VarHandleOp] name: Variable/
```

As you can see, the constants and variables get placed either on GPU:0 or CPU:0 despite all of them gathered inside the same `tf.device('/gpu:0')` block.",2018-01-14T06:54:09Z,12,1,2,8.261189256564224
352,16103,No OpKernel was registered to support Op 'AssignVariableOp' with DT_BFLOAT16,"stat:awaiting response,type:support","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Arch linux
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: 1.5.0-rc1
- **Python version**: NA (go bindings)
- **Bazel version (if compiling from source)**: 0.9.0
- **GCC/Compiler version (if compiling from source)**: 7.2.1
- **CUDA/cuDNN version**: na (CPU)
- **GPU model and memory**: na
- **Exact command to reproduce**: See below

### Describe the problem
`AssignVariableOp` does not appear to appear to have a kernel for `DT_BFLOAT16`.


### Source code / logs
```
package main

import (
	tf ""github.com/tensorflow/tensorflow/tensorflow/go""
	""github.com/tensorflow/tensorflow/tensorflow/go/op""
)

func main() {
	s := op.NewScope()
	bfloat := op.Cast(s, op.Const(s, float32(0.1234)), tf.Bfloat16)
	variable := op.VarHandleOp(s, tf.Bfloat16, tf.ScalarShape())
	init := op.AssignVariableOp(s, variable, bfloat)

	graph, err := s.Finalize()
	if err != nil {
		panic(err)
	}
	sess, err := tf.NewSession(graph, nil)
	if err != nil {
		panic(err)
	}
	_, err = sess.Run(nil, nil, []*tf.Operation{init})
	if err != nil {
		panic(err)
	}
}
```
```
go run bfloat_demo.go 
panic: No OpKernel was registered to support Op 'AssignVariableOp' with these attrs.  Registered devices: [CPU], Registered kernels:
  device='CPU'; dtype in [DT_VARIANT]
  device='CPU'; dtype in [DT_QINT32]
  device='CPU'; dtype in [DT_QUINT8]
  device='CPU'; dtype in [DT_QINT8]
  device='CPU'; dtype in [DT_RESOURCE]
  device='CPU'; dtype in [DT_STRING]
  device='CPU'; dtype in [DT_BOOL]
  device='CPU'; dtype in [DT_COMPLEX128]
  device='CPU'; dtype in [DT_COMPLEX64]
  device='CPU'; dtype in [DT_DOUBLE]
  device='CPU'; dtype in [DT_FLOAT]
  device='CPU'; dtype in [DT_HALF]
  device='CPU'; dtype in [DT_INT8]
  device='CPU'; dtype in [DT_UINT8]
  device='CPU'; dtype in [DT_INT16]
  device='CPU'; dtype in [DT_UINT16]
  device='CPU'; dtype in [DT_INT32]
  device='CPU'; dtype in [DT_INT64]

	 [[Node: AssignVariableOp = AssignVariableOp[dtype=DT_BFLOAT16](VarHandleOp, Cast)]]

goroutine 1 [running]:
main.main()
	/home/isaac/go/src/github.com/is8ac/gotf/bfloat_demo.go:24 +0x250
exit status 2
```",0,,2,2018-01-13T17:45:32Z,2018-01-20T00:39:43Z,NONE,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Arch linux
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: 1.5.0-rc1
- **Python version**: NA (go bindings)
- **Bazel version (if compiling from source)**: 0.9.0
- **GCC/Compiler version (if compiling from source)**: 7.2.1
- **CUDA/cuDNN version**: na (CPU)
- **GPU model and memory**: na
- **Exact command to reproduce**: See below

### Describe the problem
`AssignVariableOp` does not appear to appear to have a kernel for `DT_BFLOAT16`.


### Source code / logs
```
package main

import (
	tf ""github.com/tensorflow/tensorflow/tensorflow/go""
	""github.com/tensorflow/tensorflow/tensorflow/go/op""
)

func main() {
	s := op.NewScope()
	bfloat := op.Cast(s, op.Const(s, float32(0.1234)), tf.Bfloat16)
	variable := op.VarHandleOp(s, tf.Bfloat16, tf.ScalarShape())
	init := op.AssignVariableOp(s, variable, bfloat)

	graph, err := s.Finalize()
	if err != nil {
		panic(err)
	}
	sess, err := tf.NewSession(graph, nil)
	if err != nil {
		panic(err)
	}
	_, err = sess.Run(nil, nil, []*tf.Operation{init})
	if err != nil {
		panic(err)
	}
}
```
```
go run bfloat_demo.go 
panic: No OpKernel was registered to support Op 'AssignVariableOp' with these attrs.  Registered devices: [CPU], Registered kernels:
  device='CPU'; dtype in [DT_VARIANT]
  device='CPU'; dtype in [DT_QINT32]
  device='CPU'; dtype in [DT_QUINT8]
  device='CPU'; dtype in [DT_QINT8]
  device='CPU'; dtype in [DT_RESOURCE]
  device='CPU'; dtype in [DT_STRING]
  device='CPU'; dtype in [DT_BOOL]
  device='CPU'; dtype in [DT_COMPLEX128]
  device='CPU'; dtype in [DT_COMPLEX64]
  device='CPU'; dtype in [DT_DOUBLE]
  device='CPU'; dtype in [DT_FLOAT]
  device='CPU'; dtype in [DT_HALF]
  device='CPU'; dtype in [DT_INT8]
  device='CPU'; dtype in [DT_UINT8]
  device='CPU'; dtype in [DT_INT16]
  device='CPU'; dtype in [DT_UINT16]
  device='CPU'; dtype in [DT_INT32]
  device='CPU'; dtype in [DT_INT64]

	 [[Node: AssignVariableOp = AssignVariableOp[dtype=DT_BFLOAT16](VarHandleOp, Cast)]]

goroutine 1 [running]:
main.main()
	/home/isaac/go/src/github.com/is8ac/gotf/bfloat_demo.go:24 +0x250
exit status 2
```",2018-01-19T22:34:16Z,7,1,1,2.261189256564224
353,16101,Add stream selection support for `tf.contrib.ffmpeg.decode_audio`,"awaiting review,cla: yes,stat:awaiting tensorflower","This fix tries to address the issue raised in #16073 where it was not possible to selectively decode a perticular stream with `tf.contrib.ffmpeg.decode_audio`.
This fix adds an additional attribute `stream` which could be used to specify the stream to decode (e.g., `stream=""1""`). By default `stream=""""` which leaves the decision to ffmpeg.

This fix fixes #16073.

Signed-off-by: Yong Tang <yong.tang.github@outlook.com>",1,,12,2018-01-13T15:50:52Z,2018-01-23T22:21:40Z,MEMBER,"This fix tries to address the issue raised in #16073 where it was not possible to selectively decode a perticular stream with `tf.contrib.ffmpeg.decode_audio`.
This fix adds an additional attribute `stream` which could be used to specify the stream to decode (e.g., `stream=""1""`). By default `stream=""""` which leaves the decision to ffmpeg.

This fix fixes #16073.

Signed-off-by: Yong Tang <yong.tang.github@outlook.com>",2018-01-13T16:34:12Z,10,3,2,10.261189256564224
354,16100,Exception when not providing optional parameter frequency_skip in TimeFreqLSTMCell,"stat:contributions welcome,type:bug/performance","### System information
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes, see below
- OS Platform and Distribution: 
- TensorFlow installed from: `pip3 install --user tensorflow-gpu`
- TensorFlow version: 1.4.1
- Python version: 3.5.2
- CUDA: 8.0
- GPU: NVidia Titan X

### Describe the problem

Using a `TimeFreqLSTMCell` in a `dynamic_rnn` or `static_rcnn` without providing the optional parameter `frequency_skip` results in an exception:

```
TypeError: unsupported operand type(s) for /: 'int' and 'NoneType'
```

The line which throws this exception is https://github.com/tensorflow/tensorflow/blob/8b78c23c161c9d0bec462d5f4c73f0fca413bc8b/tensorflow/contrib/rnn/python/ops/rnn_cell.py#L474-L475
`frequency_skip` has it's default value `None` here.

Maybe the default should be changed to `1`?

### Source code / logs

Sadly I am not allowed to share my full source code. However, this is how I create the RNN layers:

```
lstmcell = tf.contrib.rnn.TimeFreqLSTMCell(lstm_input.shape.as_list()[2], forget_bias = self.lstm_forget_bias, feature_size = lstm_input_rev.shape.as_list()[2])
                
stacked_lstm = tf.nn.rnn_cell.MultiRNNCell([lstmcell] * self.layers_lstm)
                
lstm_output, lstm_state = tf.nn.dynamic_rnn(stacked_lstm, lstm_input_rev, dtype=""float32"", time_major=True)
```",1,,1,2018-01-13T12:20:41Z,2018-01-22T20:27:14Z,CONTRIBUTOR,"### System information
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes, see below
- OS Platform and Distribution: 
- TensorFlow installed from: `pip3 install --user tensorflow-gpu`
- TensorFlow version: 1.4.1
- Python version: 3.5.2
- CUDA: 8.0
- GPU: NVidia Titan X

### Describe the problem

Using a `TimeFreqLSTMCell` in a `dynamic_rnn` or `static_rcnn` without providing the optional parameter `frequency_skip` results in an exception:

```
TypeError: unsupported operand type(s) for /: 'int' and 'NoneType'
```

The line which throws this exception is https://github.com/tensorflow/tensorflow/blob/8b78c23c161c9d0bec462d5f4c73f0fca413bc8b/tensorflow/contrib/rnn/python/ops/rnn_cell.py#L474-L475
`frequency_skip` has it's default value `None` here.

Maybe the default should be changed to `1`?

### Source code / logs

Sadly I am not allowed to share my full source code. However, this is how I create the RNN layers:

```
lstmcell = tf.contrib.rnn.TimeFreqLSTMCell(lstm_input.shape.as_list()[2], forget_bias = self.lstm_forget_bias, feature_size = lstm_input_rev.shape.as_list()[2])
                
stacked_lstm = tf.nn.rnn_cell.MultiRNNCell([lstmcell] * self.layers_lstm)
                
lstm_output, lstm_state = tf.nn.dynamic_rnn(stacked_lstm, lstm_input_rev, dtype=""float32"", time_major=True)
```",2018-01-13T15:02:35Z,9,2,2,3.761189256564224
355,16099,Make srcd in variable,"awaiting testing (then merge),cla: yes",,0,,3,2018-01-13T10:00:11Z,2018-01-17T01:52:16Z,CONTRIBUTOR,,2018-01-16T21:06:32Z,4,2,1,3.761189256564224
356,16096,Address bad merge in Java install instructions,cla: yes,,0,,2,2018-01-13T07:25:31Z,2018-01-13T18:36:04Z,MEMBER,,2018-01-13T08:07:42Z,0,3,0,4.261189256564224
357,16094,Shape must be rank 1 but is rank 0 for 'CTCLoss' (op: 'CTCLoss'),stat:awaiting response,"Have I written custom code: yes
OS: Windows 8.1
Tensorflow installed from: conda
Tensorflow version: 1.4


I've successfully converted a Tensor into a SparseTensor with this code:

```
def dense_to_sparse(dense_tensor, out_type):
    indices = tf.where(tf.not_equal(dense_tensor, tf.constant(0, dense_tensor.dtype)
    values = tf.gather_nd(dense_tensor, indices)
    shape = tf.shape(dense_tensor, out_type=out_type)
    return tf.SparseTensor(indices, values, shape)
```

I want to try out using a `SparseTensor` converted from a dense one: 

```
input_layer = tf.placeholder(tf.float32, [None, 1596, 48])
dense_labels = tf.placeholder(tf.int32)
sparse_from_dense = dense_to_sparse(dense_lables, out_type=tf.int64)
cell_fw = grid_rnn.Grid2LSTMCell(num_units=128)
cell_bw = grid_rnn.Grid2LSTMCell(num_units=128)
bidirectional_grid_rnn = tf.nn.bidirectional_dynamic_rnn(cell_fw, cell_bw, input_layer, dtype=tf.float32)
outputs = tf.reshape(bidirectional_grid_rnn[0], [-1, 256])

W = tf.Variable(tf.truncated_normal([256, 80], stddev=0.1, dtype=tf.float32), name='W')
b = tf.Variable(tf.constant(0., dtype=tf.float32, shape=[80], name='b'))

logits = tf.matmul(outputs, W) + b
logits = tf.reshape(logits, [tf.shape(input_layer)[0], -1, 80])
logits = tf.transpose(logits, (1, 0, 2))

loss = tf.nn.ctc_loss(inputs=logits, labels=sparse, sequence_length=320)
```

Unfortunately, when I do this, I encounter this error:

`Shape must be rank 1 but is rank 0 for 'CTCLoss' (op: 'CTCLoss') with input shapes: [?,?,80], [?,1], [?], [].`

",0,,3,2018-01-13T04:41:46Z,2018-01-13T23:20:07Z,CONTRIBUTOR,"Have I written custom code: yes
OS: Windows 8.1
Tensorflow installed from: conda
Tensorflow version: 1.4


I've successfully converted a Tensor into a SparseTensor with this code:

```
def dense_to_sparse(dense_tensor, out_type):
    indices = tf.where(tf.not_equal(dense_tensor, tf.constant(0, dense_tensor.dtype)
    values = tf.gather_nd(dense_tensor, indices)
    shape = tf.shape(dense_tensor, out_type=out_type)
    return tf.SparseTensor(indices, values, shape)
```

I want to try out using a `SparseTensor` converted from a dense one: 

```
input_layer = tf.placeholder(tf.float32, [None, 1596, 48])
dense_labels = tf.placeholder(tf.int32)
sparse_from_dense = dense_to_sparse(dense_lables, out_type=tf.int64)
cell_fw = grid_rnn.Grid2LSTMCell(num_units=128)
cell_bw = grid_rnn.Grid2LSTMCell(num_units=128)
bidirectional_grid_rnn = tf.nn.bidirectional_dynamic_rnn(cell_fw, cell_bw, input_layer, dtype=tf.float32)
outputs = tf.reshape(bidirectional_grid_rnn[0], [-1, 256])

W = tf.Variable(tf.truncated_normal([256, 80], stddev=0.1, dtype=tf.float32), name='W')
b = tf.Variable(tf.constant(0., dtype=tf.float32, shape=[80], name='b'))

logits = tf.matmul(outputs, W) + b
logits = tf.reshape(logits, [tf.shape(input_layer)[0], -1, 80])
logits = tf.transpose(logits, (1, 0, 2))

loss = tf.nn.ctc_loss(inputs=logits, labels=sparse, sequence_length=320)
```

Unfortunately, when I do this, I encounter this error:

`Shape must be rank 1 but is rank 0 for 'CTCLoss' (op: 'CTCLoss') with input shapes: [?,?,80], [?,1], [?], [].`

",2018-01-13T12:53:28Z,0,2,0,3.761189256564224
358,16093,when will we have multi gpu support under eager mode? Pytorch has it.,type:feature,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",1,,4,2018-01-13T03:22:07Z,2018-01-17T18:51:42Z,NONE,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",2018-01-17T18:50:12Z,4,1,1,4.261189256564224
359,16090,MKL-DNN:  fix batchnorm unit test failures,"awaiting testing (then merge),cla: yes","Fix failures of all 9 fuse batchnorm test cases.

handle corner case (empty input tensors)
handle inference case properly - bwd bug related to fwd primitive creation (as a hint)
refactor - moving output tensor allocation to separate methods - to avoid duplicated code",0,,2,2018-01-13T01:02:12Z,2018-01-17T05:49:23Z,CONTRIBUTOR,"Fix failures of all 9 fuse batchnorm test cases.

handle corner case (empty input tensors)
handle inference case properly - bwd bug related to fwd primitive creation (as a hint)
refactor - moving output tensor allocation to separate methods - to avoid duplicated code",2018-01-16T16:50:19Z,4,2,1,3.261189256564224
360,16088,Disable keras data_utils_test as it's flaky.,"awaiting review,cla: yes",,1,,5,2018-01-12T23:44:10Z,2018-01-23T07:01:30Z,MEMBER,,2018-01-12T23:49:57Z,11,3,2,6.759730302172271
361,16086,[Intel MKL-DNN] fixes for several MKLDNN unit tests.,"awaiting testing (then merge),cla: yes",Current MKLDNN element wise (add) results in several unit test failure. A temporary workaround is provided by comment out the MKLDNN element wise (add) optimization. ,0,,2,2018-01-12T22:49:26Z,2018-01-17T05:47:39Z,CONTRIBUTOR,Current MKLDNN element wise (add) results in several unit test failure. A temporary workaround is provided by comment out the MKLDNN element wise (add) optimization. ,2018-01-16T16:48:04Z,5,2,1,3.2597303021722714
362,16085,Add unspecified dimensions (-1) support for noise_shape with tf.nn.dropout,"awaiting testing (then merge),cla: yes","This fix tries to address the issue raised in #16034 where it was not possible to have unspecified dimensions for `noise_shape` with `tf.nn.dropout`.

This fix adds the support so that it is possible to specify `noise_shape = [-1, 1, 1, -1]` instead of `noise_shape = [k, 1, 1, n]`.

This fix fixes #16034.

Signed-off-by: Yong Tang <yong.tang.github@outlook.com>",1,,11,2018-01-12T22:21:34Z,2018-02-15T23:21:28Z,MEMBER,"This fix tries to address the issue raised in #16034 where it was not possible to have unspecified dimensions for `noise_shape` with `tf.nn.dropout`.

This fix adds the support so that it is possible to specify `noise_shape = [-1, 1, 1, -1]` instead of `noise_shape = [k, 1, 1, n]`.

This fix fixes #16034.

Signed-off-by: Yong Tang <yong.tang.github@outlook.com>",2018-01-22T23:38:44Z,33,3,4,9.759730302172272
363,16084,Update download_dependencies.sh to prevent crash from 403,"awaiting testing (then merge),cla: yes","The eigen bitbucket seems to have changed causing the scrip to crash with a unrecognized archive error.
Changing to grep -v mirror.bazel seems to fix this because otherwise we get a 403 forbidden error.",1,,2,2018-01-12T22:20:03Z,2018-01-23T18:28:28Z,CONTRIBUTOR,"The eigen bitbucket seems to have changed causing the scrip to crash with a unrecognized archive error.
Changing to grep -v mirror.bazel seems to fix this because otherwise we get a 403 forbidden error.",2018-01-23T18:28:46Z,11,2,2,4.259730302172271
364,16082,Connect Apache Beam/Spark to TensorFlow (MonitoredTrainingSession) in a streaming manner?,"stat:awaiting response,stat:community support,type:feature","### Describe the problem
I have a lengthy question on [SO](https://stackoverflow.com/questions/47986410/optimal-data-streaming-and-processing-solution-for-enormous-datasets-into-tf-dat) about this. But in short, is there a way (or a best practice) to pipe big training datasets directly into a distributed setting (e.g. GKE),  especially if they are subjected to a heavy preprocessing? 
I'm basically reaching the limit of what can be sanely stored in TFRecords (they are verbose and heavy).
The closest issue was this one (https://github.com/tensorflow/tensorflow/issues/12903) and this guide (https://github.com/GoogleCloudPlatform/dataflow-prediction-example) but I do not see a healthy way to implement it (last one with a `@singleton` looks like a hack and not usable with the `tf.Dataset` or `MonitoredTrainingSession`).

I believe this is a useful issue/feature request for a decent amount of Tensorflow users. ",0,,5,2018-01-12T20:49:42Z,2018-02-06T23:10:59Z,CONTRIBUTOR,"### Describe the problem
I have a lengthy question on [SO](https://stackoverflow.com/questions/47986410/optimal-data-streaming-and-processing-solution-for-enormous-datasets-into-tf-dat) about this. But in short, is there a way (or a best practice) to pipe big training datasets directly into a distributed setting (e.g. GKE),  especially if they are subjected to a heavy preprocessing? 
I'm basically reaching the limit of what can be sanely stored in TFRecords (they are verbose and heavy).
The closest issue was this one (https://github.com/tensorflow/tensorflow/issues/12903) and this guide (https://github.com/GoogleCloudPlatform/dataflow-prediction-example) but I do not see a healthy way to implement it (last one with a `@singleton` looks like a hack and not usable with the `tf.Dataset` or `MonitoredTrainingSession`).

I believe this is a useful issue/feature request for a decent amount of Tensorflow users. ",2018-01-12T20:56:29Z,24,2,3,4.759730302172271
365,16081,MKL-DNN: fix concat issue related to negative input concat_dim,"awaiting testing (then merge),cla: yes","For a negative concat_dim input, the actual concat_dim should be N + concat_dim with N
being the dims of input tensors.

This PR fixes an issue of setting N properly.",0,,2,2018-01-12T20:40:28Z,2018-01-17T06:50:27Z,CONTRIBUTOR,"For a negative concat_dim input, the actual concat_dim should be N + concat_dim with N
being the dims of input tensors.

This PR fixes an issue of setting N properly.",2018-01-16T16:51:11Z,5,2,1,3.2597303021722714
366,16079,Branch 181765083,cla: yes,,0,,2,2018-01-12T19:16:43Z,2018-01-12T21:26:35Z,MEMBER,,2018-01-12T19:17:35Z,0,3,0,4.259730302172271
367,16075,optimize_for_inference_lib.fold_batch_norms() preserves data_format,"awaiting testing (then merge),cla: yes","`fold_batch_norms()` currently breaks graphs containing convolutions using NCHW data format. The function replaces a BiasAdd operation with a new one, while not preserving the data format of the original operation. As a result, the new operation always has NHWC data format, and the execution of the resulting graph fails because of mismatching dimensions.

The proposed resolution is to copy the `data_format` property from the original operation.

The patch fixes https://github.com/tensorflow/tensorflow/issues/15034.",1,,10,2018-01-12T17:07:04Z,2018-01-31T23:02:26Z,CONTRIBUTOR,"`fold_batch_norms()` currently breaks graphs containing convolutions using NCHW data format. The function replaces a BiasAdd operation with a new one, while not preserving the data format of the original operation. As a result, the new operation always has NHWC data format, and the execution of the resulting graph fails because of mismatching dimensions.

The proposed resolution is to copy the `data_format` property from the original operation.

The patch fixes https://github.com/tensorflow/tensorflow/issues/15034.",2018-01-14T12:10:31Z,19,2,3,8.259730302172272
368,16073,Feature request: (optionally) return all audio streams in tf.contrib.ffmpeg.decode_audio,type:feature,"I'm trying to read [musdb18](https://sigsep.github.io/musdb.html) with `tf.data` and it comes in the form of mp4 files with multiple audio streams so `ffmpeg -map` is needed.

`tf.contrib.ffmpeg.decode_audio` cannot be configured to choose the audio stream. I wonder if we could have a `tf.contrib.ffmpeg.decode_audios` that returns every audio stream in the file, or if we could have a new argument in `tf.contrib.ffmpeg.decode_audio` for choosing streams.

Being able to choose the audio stream is also important for getting the right language in a movie file's audio, and similarly `tf.contrib.ffmpeg.decode_video` could need the same extension (though multiple video streams is not as common AFAIK).",0,,6,2018-01-12T16:28:55Z,2018-01-23T22:21:40Z,CONTRIBUTOR,"I'm trying to read [musdb18](https://sigsep.github.io/musdb.html) with `tf.data` and it comes in the form of mp4 files with multiple audio streams so `ffmpeg -map` is needed.

`tf.contrib.ffmpeg.decode_audio` cannot be configured to choose the audio stream. I wonder if we could have a `tf.contrib.ffmpeg.decode_audios` that returns every audio stream in the file, or if we could have a new argument in `tf.contrib.ffmpeg.decode_audio` for choosing streams.

Being able to choose the audio stream is also important for getting the right language in a movie file's audio, and similarly `tf.contrib.ffmpeg.decode_video` could need the same extension (though multiple video streams is not as common AFAIK).",2018-01-12T16:30:59Z,11,2,2,5.259730302172271
369,16072,Dynamic Bi-directional RNN vs Dynamic RNN.- Not working as expected.,stat:awaiting response,"I am trying to use Bidirectional RNN and pass the output through a CNN for text classification. However, I am getting all sorts of shape errors with bidirectional RNN. Although, If I use two dynamic rnn with reverse op in the second layer, it appears to work fine:

Here is bidirectional RNN code that DOES NOT work for me:

```
    # Bidirectional LSTM layer
    with tf.name_scope(""bidirectional-lstm""):
        lstm_fw_cell = tf.nn.rnn_cell.BasicLSTMCell(hidden_size, forget_bias=1.0)
        lstm_bw_cell = tf.nn.rnn_cell.BasicLSTMCell(hidden_size, forget_bias=1.0)

        self.lstm_outputs, _ = tf.nn.bidirectional_dynamic_rnn(
            lstm_fw_cell, 
            lstm_bw_cell, 
            self.embedded_chars, 
            sequence_length=self.seqlen, 
            dtype=tf.float32)
        self.lstm_outputs = tf.concat(self.lstm_outputs, axis=2)
```


Here is the two layer dynamic rnn that DOES work for me:


```
  # Bidirectional LSTM layer
    with tf.name_scope(""bidirectional-lstm""):
        lstm_fw_cell = tf.nn.rnn_cell.BasicLSTMCell(hidden_size, forget_bias=1.0)
        lstm_bw_cell = tf.nn.rnn_cell.BasicLSTMCell(hidden_size, forget_bias=1.0)
    with tf.variable_scope(""lstm-output-fw""):
        self.lstm_outputs_fw, _ = tf.nn.dynamic_rnn(
            lstm_fw_cell, 
            self.embedded_chars, 
            sequence_length=self.seqlen, 
            dtype=tf.float32)

    with tf.variable_scope(""lstm-output-bw""):
        self.embedded_chars_rev = array_ops.reverse_sequence(self.embedded_chars, seq_lengths=self.seqlen, seq_dim=1)
        tmp, _ = tf.nn.dynamic_rnn(
            lstm_bw_cell, 
            self.embedded_chars_rev, 
            sequence_length=self.seqlen, 
            dtype=tf.float32)
        self.lstm_outputs_bw = array_ops.reverse_sequence(tmp, seq_lengths=self.seqlen, seq_dim=1)

    Concatenate outputs
    self.lstm_outputs = tf.add(self.lstm_outputs_fw, self.lstm_outputs_bw, name=""lstm_outputs"")
```

I am passing the output of this to CNN and error occurs when computing the

Here is the rest of the code:

# Convolution + maxpool layer for each filter size
        pooled_outputs = []
        for i, filter_size in enumerate(filter_sizes):
            with tf.name_scope(""conv-maxpool-%s"" % filter_size):
                # Convolution Layer
                filter_shape = [filter_size, hidden_size, 1, num_filters]
                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=""W"")
                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=""b"")

                conv = tf.nn.conv2d(
                    self.lstm_outputs_expanded, 
                    W,
                    strides=[1, 1, 1, 1], 
                    padding=""VALID"",
                    name=""conv"")

                # Apply nonlinearity
                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=""relu"")

                # Maxpooling over the outputs
                pooled = tf.nn.max_pool(
                    h, 
                    ksize=[1, sequence_length - filter_size + 1, 1, 1],
                    strides=[1, 1, 1, 1], 
                    padding='VALID',
                    name=""pool"")
                pooled_outputs.append(pooled)

        # Combine all the pooled features
        num_filters_total = num_filters * len(filter_sizes)
        self.h_pool = tf.concat(axis=3, values=pooled_outputs)
        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])


        # Dropout layer
        with tf.name_scope(""dropout""):
            self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)


```
        # Final (unnormalized) scores and predictions
        with tf.name_scope(""output""):
            # Standard output weights initialization
            W = tf.get_variable(
                ""W"", 
                shape=[num_filters_total, num_classes], 
                initializer=tf.contrib.layers.xavier_initializer())
            b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=""b"")

            # # Initialized output weights to 0.0, might improve accuracy
            # W = tf.Variable(tf.constant(0.0, shape=[num_filters_total, num_classes]), name=""W"")
            # b = tf.Variable(tf.constant(0.0, shape=[num_classes]), name=""b"")

            l2_loss += tf.nn.l2_loss(W)
            l2_loss += tf.nn.l2_loss(b)

            self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=""scores"")

            self.predictions = tf.argmax(self.scores, 1, name=""predictions"")

        # Calculate mean cross-entropy loss
        with tf.name_scope(""loss""):
            losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.input_y)
            self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss

        # Accuracy
        with tf.name_scope(""accuracy""):
            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))
            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, ""float""), name=""accuracy"")
```


here are the errors I am getting.

```
During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""train_upgraded.py"", line 209, in <module>
    train_step(x_batch, seqlen_batch, y_batch)
  File ""train_upgraded.py"", line 177, in train_step
    feed_dict)
  File ""/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 889, in run
    run_metadata_ptr)
  File ""/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1120, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1317, in _do_run
    options, run_metadata)
  File ""/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1336, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: logits and labels must be same size: logits_size=[7550,2] labels_size=[50,2]
         [[Node: loss/SoftmaxCrossEntropyWithLogits = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](loss/Reshape, loss/Reshape_1)]]

Caused by op 'loss/SoftmaxCrossEntropyWithLogits', defined at:
  File ""train_upgraded.py"", line 87, in <module>
    l2_reg_lambda=FLAGS.l2_reg_lambda)
  File ""/media/hemant/MVV/MyValueVest-local/learning/Initial Embeddings/STEP 2 lstm-context-embeddings-master/model_upgraded.py"", line 138, in __init__
    losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.input_y)
  File ""/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py"", line 1783, in softmax_cross_entropy_with_logits
    precise_logits, labels, name=name)
  File ""/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py"", line 4364, in _softmax_cross_entropy_with_logits
    name=name)
  File ""/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 2956, in create_op
    op_def=op_def)
  File ""/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1470, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

InvalidArgumentError (see above for traceback): logits and labels must be same size: logits_size=[7550,2] labels_size=[50,2]
         [[Node: loss/SoftmaxCrossEntropyWithLogits = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](loss/Reshape, loss/Reshape_1)]]
During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""train_upgraded.py"", line 209, in <module>
    train_step(x_batch, seqlen_batch, y_batch)
  File ""train_upgraded.py"", line 177, in train_step
    feed_dict)
  File ""/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 889, in run
    run_metadata_ptr)
  File ""/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1120, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1317, in _do_run
    options, run_metadata)
  File ""/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1336, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: logits and labels must be same size: logits_size=[7550,2] labels_size=[50,2]
         [[Node: loss/SoftmaxCrossEntropyWithLogits = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](loss/Reshape, loss/Reshape_1)]]

Caused by op 'loss/SoftmaxCrossEntropyWithLogits', defined at:
  File ""train_upgraded.py"", line 87, in <module>
    l2_reg_lambda=FLAGS.l2_reg_lambda)
  File ""/media/hemant/MVV/MyValueVest-local/learning/Initial Embeddings/STEP 2 lstm-context-embeddings-master/model_upgraded.py"", line 138, in __init__
    losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.input_y)
  File ""/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py"", line 1783, in softmax_cross_entropy_with_logits
    precise_logits, labels, name=name)
  File ""/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py"", line 4364, in _softmax_cross_entropy_with_logits
    name=name)
  File ""/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 2956, in create_op
    op_def=op_def)
  File ""/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1470, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

InvalidArgumentError (see above for traceback): logits and labels must be same size: logits_size=[7550,2] labels_size=[50,2]
         [[Node: loss/SoftmaxCrossEntropyWithLogits = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](loss/Reshape, loss/Reshape_1)]]
```",0,,3,2018-01-12T16:12:57Z,2018-02-06T12:55:04Z,NONE,"I am trying to use Bidirectional RNN and pass the output through a CNN for text classification. However, I am getting all sorts of shape errors with bidirectional RNN. Although, If I use two dynamic rnn with reverse op in the second layer, it appears to work fine:

Here is bidirectional RNN code that DOES NOT work for me:

```
    # Bidirectional LSTM layer
    with tf.name_scope(""bidirectional-lstm""):
        lstm_fw_cell = tf.nn.rnn_cell.BasicLSTMCell(hidden_size, forget_bias=1.0)
        lstm_bw_cell = tf.nn.rnn_cell.BasicLSTMCell(hidden_size, forget_bias=1.0)

        self.lstm_outputs, _ = tf.nn.bidirectional_dynamic_rnn(
            lstm_fw_cell, 
            lstm_bw_cell, 
            self.embedded_chars, 
            sequence_length=self.seqlen, 
            dtype=tf.float32)
        self.lstm_outputs = tf.concat(self.lstm_outputs, axis=2)
```


Here is the two layer dynamic rnn that DOES work for me:


```
  # Bidirectional LSTM layer
    with tf.name_scope(""bidirectional-lstm""):
        lstm_fw_cell = tf.nn.rnn_cell.BasicLSTMCell(hidden_size, forget_bias=1.0)
        lstm_bw_cell = tf.nn.rnn_cell.BasicLSTMCell(hidden_size, forget_bias=1.0)
    with tf.variable_scope(""lstm-output-fw""):
        self.lstm_outputs_fw, _ = tf.nn.dynamic_rnn(
            lstm_fw_cell, 
            self.embedded_chars, 
            sequence_length=self.seqlen, 
            dtype=tf.float32)

    with tf.variable_scope(""lstm-output-bw""):
        self.embedded_chars_rev = array_ops.reverse_sequence(self.embedded_chars, seq_lengths=self.seqlen, seq_dim=1)
        tmp, _ = tf.nn.dynamic_rnn(
            lstm_bw_cell, 
            self.embedded_chars_rev, 
            sequence_length=self.seqlen, 
            dtype=tf.float32)
        self.lstm_outputs_bw = array_ops.reverse_sequence(tmp, seq_lengths=self.seqlen, seq_dim=1)

    Concatenate outputs
    self.lstm_outputs = tf.add(self.lstm_outputs_fw, self.lstm_outputs_bw, name=""lstm_outputs"")
```

I am passing the output of this to CNN and error occurs when computing the

Here is the rest of the code:

# Convolution + maxpool layer for each filter size
        pooled_outputs = []
        for i, filter_size in enumerate(filter_sizes):
            with tf.name_scope(""conv-maxpool-%s"" % filter_size):
                # Convolution Layer
                filter_shape = [filter_size, hidden_size, 1, num_filters]
                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=""W"")
                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=""b"")

                conv = tf.nn.conv2d(
                    self.lstm_outputs_expanded, 
                    W,
                    strides=[1, 1, 1, 1], 
                    padding=""VALID"",
                    name=""conv"")

                # Apply nonlinearity
                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=""relu"")

                # Maxpooling over the outputs
                pooled = tf.nn.max_pool(
                    h, 
                    ksize=[1, sequence_length - filter_size + 1, 1, 1],
                    strides=[1, 1, 1, 1], 
                    padding='VALID',
                    name=""pool"")
                pooled_outputs.append(pooled)

        # Combine all the pooled features
        num_filters_total = num_filters * len(filter_sizes)
        self.h_pool = tf.concat(axis=3, values=pooled_outputs)
        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])


        # Dropout layer
        with tf.name_scope(""dropout""):
            self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)


```
        # Final (unnormalized) scores and predictions
        with tf.name_scope(""output""):
            # Standard output weights initialization
            W = tf.get_variable(
                ""W"", 
                shape=[num_filters_total, num_classes], 
                initializer=tf.contrib.layers.xavier_initializer())
            b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=""b"")

            # # Initialized output weights to 0.0, might improve accuracy
            # W = tf.Variable(tf.constant(0.0, shape=[num_filters_total, num_classes]), name=""W"")
            # b = tf.Variable(tf.constant(0.0, shape=[num_classes]), name=""b"")

            l2_loss += tf.nn.l2_loss(W)
            l2_loss += tf.nn.l2_loss(b)

            self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=""scores"")

            self.predictions = tf.argmax(self.scores, 1, name=""predictions"")

        # Calculate mean cross-entropy loss
        with tf.name_scope(""loss""):
            losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.input_y)
            self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss

        # Accuracy
        with tf.name_scope(""accuracy""):
            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))
            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, ""float""), name=""accuracy"")
```


here are the errors I am getting.

```
During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""train_upgraded.py"", line 209, in <module>
    train_step(x_batch, seqlen_batch, y_batch)
  File ""train_upgraded.py"", line 177, in train_step
    feed_dict)
  File ""/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 889, in run
    run_metadata_ptr)
  File ""/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1120, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1317, in _do_run
    options, run_metadata)
  File ""/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1336, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: logits and labels must be same size: logits_size=[7550,2] labels_size=[50,2]
         [[Node: loss/SoftmaxCrossEntropyWithLogits = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](loss/Reshape, loss/Reshape_1)]]

Caused by op 'loss/SoftmaxCrossEntropyWithLogits', defined at:
  File ""train_upgraded.py"", line 87, in <module>
    l2_reg_lambda=FLAGS.l2_reg_lambda)
  File ""/media/hemant/MVV/MyValueVest-local/learning/Initial Embeddings/STEP 2 lstm-context-embeddings-master/model_upgraded.py"", line 138, in __init__
    losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.input_y)
  File ""/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py"", line 1783, in softmax_cross_entropy_with_logits
    precise_logits, labels, name=name)
  File ""/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py"", line 4364, in _softmax_cross_entropy_with_logits
    name=name)
  File ""/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 2956, in create_op
    op_def=op_def)
  File ""/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1470, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

InvalidArgumentError (see above for traceback): logits and labels must be same size: logits_size=[7550,2] labels_size=[50,2]
         [[Node: loss/SoftmaxCrossEntropyWithLogits = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](loss/Reshape, loss/Reshape_1)]]
During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""train_upgraded.py"", line 209, in <module>
    train_step(x_batch, seqlen_batch, y_batch)
  File ""train_upgraded.py"", line 177, in train_step
    feed_dict)
  File ""/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 889, in run
    run_metadata_ptr)
  File ""/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1120, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1317, in _do_run
    options, run_metadata)
  File ""/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1336, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: logits and labels must be same size: logits_size=[7550,2] labels_size=[50,2]
         [[Node: loss/SoftmaxCrossEntropyWithLogits = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](loss/Reshape, loss/Reshape_1)]]

Caused by op 'loss/SoftmaxCrossEntropyWithLogits', defined at:
  File ""train_upgraded.py"", line 87, in <module>
    l2_reg_lambda=FLAGS.l2_reg_lambda)
  File ""/media/hemant/MVV/MyValueVest-local/learning/Initial Embeddings/STEP 2 lstm-context-embeddings-master/model_upgraded.py"", line 138, in __init__
    losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.input_y)
  File ""/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py"", line 1783, in softmax_cross_entropy_with_logits
    precise_logits, labels, name=name)
  File ""/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py"", line 4364, in _softmax_cross_entropy_with_logits
    name=name)
  File ""/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 2956, in create_op
    op_def=op_def)
  File ""/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1470, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

InvalidArgumentError (see above for traceback): logits and labels must be same size: logits_size=[7550,2] labels_size=[50,2]
         [[Node: loss/SoftmaxCrossEntropyWithLogits = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](loss/Reshape, loss/Reshape_1)]]
```",2018-02-06T07:35:28Z,24,1,3,2.7597303021722714
370,16071,fix comments and code matches,"awaiting review,cla: yes",,0,,2,2018-01-12T13:39:17Z,2018-01-15T16:06:45Z,CONTRIBUTOR,,2018-01-14T02:23:16Z,3,2,1,3.2597303021722714
371,16069,Key generator/encoder_8/conv/filter not found in checkpoint,type:support,"I'm using python 3.6.3 win 10 64bit and tensorflow 1.2.1 and now I'm working on https://github.com/datitran/face2face-demo project and **part 4.export model** I'm taking this error:NotFoundError (see above for traceback): Key generator/encoder_8/conv/filter not found in checkpoint 

how can I solve this problem ?

what I run
`C:\Users\hajum>python C:\Users\hajum\Desktop\face2face-demo-master\reduce_model.py --model-input C:\Users\hajum\Desktop\face2face-model --model-output C:\Users\hajum\Desktop\face2face-reduced-model`

same folder names with project but I have my own models

what it shows
```
2018-01-12 13:24:05.337267: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE instructions, but these are available on your machine and could speed up CPU computations.
2018-01-12 13:24:05.337407: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE2 instructions, but these are available on your machine and could speed up CPU computations.
2018-01-12 13:24:05.338476: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
2018-01-12 13:24:05.338779: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-01-12 13:24:05.339070: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-01-12 13:24:05.339369: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-01-12 13:24:05.339659: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-01-12 13:24:05.339962: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2018-01-12 13:24:05.779044: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/encoder_8/conv/filter not found in checkpoint
2018-01-12 13:24:05.779482: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/decoder_2/deconv/filter not found in checkpoint
2018-01-12 13:24:05.779562: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/decoder_1/deconv/filter not found in checkpoint
2018-01-12 13:24:05.780339: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/decoder_3/deconv/filter not found in checkpoint
2018-01-12 13:24:05.780354: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/decoder_3/batchnorm/offset not found in checkpoint
2018-01-12 13:24:05.781063: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/decoder_2/batchnorm/scale not found in checkpoint
2018-01-12 13:24:05.781066: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/decoder_3/batchnorm/scale not found in checkpoint
2018-01-12 13:24:05.781015: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/decoder_2/batchnorm/offset not found in checkpoint
2018-01-12 13:24:05.784971: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/encoder_8/batchnorm/scale not found in checkpoint
2018-01-12 13:24:05.785703: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/decoder_4/batchnorm/offset not found in checkpoint
2018-01-12 13:24:05.785849: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/decoder_4/batchnorm/scale not found in checkpoint
2018-01-12 13:24:05.785928: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/decoder_4/deconv/filter not found in checkpoint
2018-01-12 13:24:05.787052: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/decoder_5/batchnorm/offset not found in checkpoint
2018-01-12 13:24:05.787160: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/decoder_6/batchnorm/offset not found in checkpoint
2018-01-12 13:24:05.787346: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/decoder_5/deconv/filter not found in checkpoint
2018-01-12 13:24:05.787687: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/decoder_5/batchnorm/scale not found in checkpoint
2018-01-12 13:24:05.791739: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/encoder_8/batchnorm/offset not found in checkpoint
2018-01-12 13:24:05.793255: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/decoder_6/batchnorm/scale not found in checkpoint
2018-01-12 13:24:05.793508: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/decoder_6/deconv/filter not found in checkpoint
2018-01-12 13:24:05.794303: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/decoder_7/batchnorm/offset not found in checkpoint
2018-01-12 13:24:05.795123: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/decoder_7/batchnorm/scale not found in checkpoint
2018-01-12 13:24:05.795823: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/decoder_8/batchnorm/offset not found in checkpoint
2018-01-12 13:24:05.796067: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/decoder_7/deconv/filter not found in checkpoint
2018-01-12 13:24:05.797352: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/decoder_8/batchnorm/scale not found in checkpoint
2018-01-12 13:24:05.798112: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/encoder_7/conv/filter not found in checkpoint
2018-01-12 13:24:05.800556: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/encoder_1/conv/filter not found in checkpoint
2018-01-12 13:24:05.801703: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/encoder_2/batchnorm/offset not found in checkpoint
2018-01-12 13:24:05.801868: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/decoder_8/deconv/filter not found in checkpoint
2018-01-12 13:24:05.801974: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/encoder_2/batchnorm/scale not found in checkpoint
2018-01-12 13:24:05.801977: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/encoder_2/conv/filter not found in checkpoint
2018-01-12 13:24:05.804154: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/encoder_3/batchnorm/offset not found in checkpoint
2018-01-12 13:24:05.805983: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/encoder_3/batchnorm/scale not found in checkpoint
2018-01-12 13:24:05.806160: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/encoder_7/batchnorm/scale not found in checkpoint
2018-01-12 13:24:05.807834: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/encoder_4/batchnorm/offset not found in checkpoint
2018-01-12 13:24:05.808628: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/encoder_3/conv/filter not found in checkpoint
2018-01-12 13:24:05.808808: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/encoder_5/batchnorm/offset not found in checkpoint
2018-01-12 13:24:05.809721: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/encoder_4/batchnorm/scale not found in checkpoint
2018-01-12 13:24:05.809836: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/encoder_4/conv/filter not found in checkpoint
2018-01-12 13:24:05.810842: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/encoder_5/batchnorm/scale not found in checkpoint
2018-01-12 13:24:05.811998: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/encoder_5/conv/filter not found in checkpoint
2018-01-12 13:24:05.812062: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/encoder_7/batchnorm/offset not found in checkpoint
2018-01-12 13:24:05.812846: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/encoder_6/batchnorm/offset not found in checkpoint
2018-01-12 13:24:05.812889: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/encoder_6/batchnorm/scale not found in checkpoint
2018-01-12 13:24:05.813195: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/encoder_6/conv/filter not found in checkpoint
Traceback (most recent call last):
  File ""C:\Users\hajum\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 1139, in _do_call
    return fn(*args)
  File ""C:\Users\hajum\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 1121, in _run_fn
    status, run_metadata)
  File ""C:\Users\hajum\Anaconda3\lib\contextlib.py"", line 88, in __exit__
    next(self.gen)
  File ""C:\Users\hajum\Anaconda3\lib\site-packages\tensorflow\python\framework\errors_impl.py"", line 466, in raise_exception_on_not_ok_status
    pywrap_tensorflow.TF_GetCode(status))
tensorflow.python.framework.errors_impl.NotFoundError: Key generator/encoder_8/conv/filter not found in checkpoint
         [[Node: save/RestoreV2_43 = RestoreV2[dtypes=[DT_FLOAT], _device=""/job:localhost/replica:0/task:0/cpu:0""](_arg_save/Const_0_0, save/RestoreV2_43/tensor_names, save/RestoreV2_43/shape_and_slices)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\hajum\Desktop\face2face-demo-master\reduce_model.py"", line 215, in <module>
    saver.restore(sess, checkpoint)
  File ""C:\Users\hajum\Anaconda3\lib\site-packages\tensorflow\python\training\saver.py"", line 1548, in restore
    {self.saver_def.filename_tensor_name: save_path})
  File ""C:\Users\hajum\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 789, in run
    run_metadata_ptr)
  File ""C:\Users\hajum\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 997, in _run
    feed_dict_string, options, run_metadata)
  File ""C:\Users\hajum\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 1132, in _do_run
    target_list, options, run_metadata)
  File ""C:\Users\hajum\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 1152, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.NotFoundError: Key generator/encoder_8/conv/filter not found in checkpoint
         [[Node: save/RestoreV2_43 = RestoreV2[dtypes=[DT_FLOAT], _device=""/job:localhost/replica:0/task:0/cpu:0""](_arg_save/Const_0_0, save/RestoreV2_43/tensor_names, save/RestoreV2_43/shape_and_slices)]]

Caused by op 'save/RestoreV2_43', defined at:
  File ""C:\Users\hajum\Desktop\face2face-demo-master\reduce_model.py"", line 213, in <module>
    saver = tf.train.Saver()
  File ""C:\Users\hajum\Anaconda3\lib\site-packages\tensorflow\python\training\saver.py"", line 1139, in __init__
    self.build()
  File ""C:\Users\hajum\Anaconda3\lib\site-packages\tensorflow\python\training\saver.py"", line 1170, in build
    restore_sequentially=self._restore_sequentially)
  File ""C:\Users\hajum\Anaconda3\lib\site-packages\tensorflow\python\training\saver.py"", line 691, in build
    restore_sequentially, reshape)
  File ""C:\Users\hajum\Anaconda3\lib\site-packages\tensorflow\python\training\saver.py"", line 407, in _AddRestoreOps
    tensors = self.restore_op(filename_tensor, saveable, preferred_shard)
  File ""C:\Users\hajum\Anaconda3\lib\site-packages\tensorflow\python\training\saver.py"", line 247, in restore_op
    [spec.tensor.dtype])[0])
  File ""C:\Users\hajum\Anaconda3\lib\site-packages\tensorflow\python\ops\gen_io_ops.py"", line 640, in restore_v2
    dtypes=dtypes, name=name)
  File ""C:\Users\hajum\Anaconda3\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 767, in apply_op
    op_def=op_def)
  File ""C:\Users\hajum\Anaconda3\lib\site-packages\tensorflow\python\framework\ops.py"", line 2506, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""C:\Users\hajum\Anaconda3\lib\site-packages\tensorflow\python\framework\ops.py"", line 1269, in __init__
    self._traceback = _extract_stack()

NotFoundError (see above for traceback): Key generator/encoder_8/conv/filter not found in checkpoint
         [[Node: save/RestoreV2_43 = RestoreV2[dtypes=[DT_FLOAT], _device=""/job:localhost/replica:0/task:0/cpu:0""](_arg_save/Const_0_0, save/RestoreV2_43/tensor_names, save/RestoreV2_43/shape_and_slices)]]
```",0,,4,2018-01-12T11:30:28Z,2018-01-16T23:01:50Z,NONE,"I'm using python 3.6.3 win 10 64bit and tensorflow 1.2.1 and now I'm working on https://github.com/datitran/face2face-demo project and **part 4.export model** I'm taking this error:NotFoundError (see above for traceback): Key generator/encoder_8/conv/filter not found in checkpoint 

how can I solve this problem ?

what I run
`C:\Users\hajum>python C:\Users\hajum\Desktop\face2face-demo-master\reduce_model.py --model-input C:\Users\hajum\Desktop\face2face-model --model-output C:\Users\hajum\Desktop\face2face-reduced-model`

same folder names with project but I have my own models

what it shows
```
2018-01-12 13:24:05.337267: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE instructions, but these are available on your machine and could speed up CPU computations.
2018-01-12 13:24:05.337407: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE2 instructions, but these are available on your machine and could speed up CPU computations.
2018-01-12 13:24:05.338476: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
2018-01-12 13:24:05.338779: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-01-12 13:24:05.339070: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-01-12 13:24:05.339369: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-01-12 13:24:05.339659: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-01-12 13:24:05.339962: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2018-01-12 13:24:05.779044: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/encoder_8/conv/filter not found in checkpoint
2018-01-12 13:24:05.779482: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/decoder_2/deconv/filter not found in checkpoint
2018-01-12 13:24:05.779562: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/decoder_1/deconv/filter not found in checkpoint
2018-01-12 13:24:05.780339: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/decoder_3/deconv/filter not found in checkpoint
2018-01-12 13:24:05.780354: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/decoder_3/batchnorm/offset not found in checkpoint
2018-01-12 13:24:05.781063: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/decoder_2/batchnorm/scale not found in checkpoint
2018-01-12 13:24:05.781066: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/decoder_3/batchnorm/scale not found in checkpoint
2018-01-12 13:24:05.781015: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/decoder_2/batchnorm/offset not found in checkpoint
2018-01-12 13:24:05.784971: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/encoder_8/batchnorm/scale not found in checkpoint
2018-01-12 13:24:05.785703: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/decoder_4/batchnorm/offset not found in checkpoint
2018-01-12 13:24:05.785849: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/decoder_4/batchnorm/scale not found in checkpoint
2018-01-12 13:24:05.785928: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/decoder_4/deconv/filter not found in checkpoint
2018-01-12 13:24:05.787052: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/decoder_5/batchnorm/offset not found in checkpoint
2018-01-12 13:24:05.787160: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/decoder_6/batchnorm/offset not found in checkpoint
2018-01-12 13:24:05.787346: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/decoder_5/deconv/filter not found in checkpoint
2018-01-12 13:24:05.787687: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/decoder_5/batchnorm/scale not found in checkpoint
2018-01-12 13:24:05.791739: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/encoder_8/batchnorm/offset not found in checkpoint
2018-01-12 13:24:05.793255: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/decoder_6/batchnorm/scale not found in checkpoint
2018-01-12 13:24:05.793508: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/decoder_6/deconv/filter not found in checkpoint
2018-01-12 13:24:05.794303: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/decoder_7/batchnorm/offset not found in checkpoint
2018-01-12 13:24:05.795123: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/decoder_7/batchnorm/scale not found in checkpoint
2018-01-12 13:24:05.795823: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/decoder_8/batchnorm/offset not found in checkpoint
2018-01-12 13:24:05.796067: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/decoder_7/deconv/filter not found in checkpoint
2018-01-12 13:24:05.797352: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/decoder_8/batchnorm/scale not found in checkpoint
2018-01-12 13:24:05.798112: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/encoder_7/conv/filter not found in checkpoint
2018-01-12 13:24:05.800556: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/encoder_1/conv/filter not found in checkpoint
2018-01-12 13:24:05.801703: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/encoder_2/batchnorm/offset not found in checkpoint
2018-01-12 13:24:05.801868: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/decoder_8/deconv/filter not found in checkpoint
2018-01-12 13:24:05.801974: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/encoder_2/batchnorm/scale not found in checkpoint
2018-01-12 13:24:05.801977: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/encoder_2/conv/filter not found in checkpoint
2018-01-12 13:24:05.804154: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/encoder_3/batchnorm/offset not found in checkpoint
2018-01-12 13:24:05.805983: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/encoder_3/batchnorm/scale not found in checkpoint
2018-01-12 13:24:05.806160: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/encoder_7/batchnorm/scale not found in checkpoint
2018-01-12 13:24:05.807834: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/encoder_4/batchnorm/offset not found in checkpoint
2018-01-12 13:24:05.808628: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/encoder_3/conv/filter not found in checkpoint
2018-01-12 13:24:05.808808: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/encoder_5/batchnorm/offset not found in checkpoint
2018-01-12 13:24:05.809721: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/encoder_4/batchnorm/scale not found in checkpoint
2018-01-12 13:24:05.809836: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/encoder_4/conv/filter not found in checkpoint
2018-01-12 13:24:05.810842: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/encoder_5/batchnorm/scale not found in checkpoint
2018-01-12 13:24:05.811998: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/encoder_5/conv/filter not found in checkpoint
2018-01-12 13:24:05.812062: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/encoder_7/batchnorm/offset not found in checkpoint
2018-01-12 13:24:05.812846: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/encoder_6/batchnorm/offset not found in checkpoint
2018-01-12 13:24:05.812889: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/encoder_6/batchnorm/scale not found in checkpoint
2018-01-12 13:24:05.813195: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/encoder_6/conv/filter not found in checkpoint
Traceback (most recent call last):
  File ""C:\Users\hajum\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 1139, in _do_call
    return fn(*args)
  File ""C:\Users\hajum\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 1121, in _run_fn
    status, run_metadata)
  File ""C:\Users\hajum\Anaconda3\lib\contextlib.py"", line 88, in __exit__
    next(self.gen)
  File ""C:\Users\hajum\Anaconda3\lib\site-packages\tensorflow\python\framework\errors_impl.py"", line 466, in raise_exception_on_not_ok_status
    pywrap_tensorflow.TF_GetCode(status))
tensorflow.python.framework.errors_impl.NotFoundError: Key generator/encoder_8/conv/filter not found in checkpoint
         [[Node: save/RestoreV2_43 = RestoreV2[dtypes=[DT_FLOAT], _device=""/job:localhost/replica:0/task:0/cpu:0""](_arg_save/Const_0_0, save/RestoreV2_43/tensor_names, save/RestoreV2_43/shape_and_slices)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\hajum\Desktop\face2face-demo-master\reduce_model.py"", line 215, in <module>
    saver.restore(sess, checkpoint)
  File ""C:\Users\hajum\Anaconda3\lib\site-packages\tensorflow\python\training\saver.py"", line 1548, in restore
    {self.saver_def.filename_tensor_name: save_path})
  File ""C:\Users\hajum\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 789, in run
    run_metadata_ptr)
  File ""C:\Users\hajum\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 997, in _run
    feed_dict_string, options, run_metadata)
  File ""C:\Users\hajum\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 1132, in _do_run
    target_list, options, run_metadata)
  File ""C:\Users\hajum\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 1152, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.NotFoundError: Key generator/encoder_8/conv/filter not found in checkpoint
         [[Node: save/RestoreV2_43 = RestoreV2[dtypes=[DT_FLOAT], _device=""/job:localhost/replica:0/task:0/cpu:0""](_arg_save/Const_0_0, save/RestoreV2_43/tensor_names, save/RestoreV2_43/shape_and_slices)]]

Caused by op 'save/RestoreV2_43', defined at:
  File ""C:\Users\hajum\Desktop\face2face-demo-master\reduce_model.py"", line 213, in <module>
    saver = tf.train.Saver()
  File ""C:\Users\hajum\Anaconda3\lib\site-packages\tensorflow\python\training\saver.py"", line 1139, in __init__
    self.build()
  File ""C:\Users\hajum\Anaconda3\lib\site-packages\tensorflow\python\training\saver.py"", line 1170, in build
    restore_sequentially=self._restore_sequentially)
  File ""C:\Users\hajum\Anaconda3\lib\site-packages\tensorflow\python\training\saver.py"", line 691, in build
    restore_sequentially, reshape)
  File ""C:\Users\hajum\Anaconda3\lib\site-packages\tensorflow\python\training\saver.py"", line 407, in _AddRestoreOps
    tensors = self.restore_op(filename_tensor, saveable, preferred_shard)
  File ""C:\Users\hajum\Anaconda3\lib\site-packages\tensorflow\python\training\saver.py"", line 247, in restore_op
    [spec.tensor.dtype])[0])
  File ""C:\Users\hajum\Anaconda3\lib\site-packages\tensorflow\python\ops\gen_io_ops.py"", line 640, in restore_v2
    dtypes=dtypes, name=name)
  File ""C:\Users\hajum\Anaconda3\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 767, in apply_op
    op_def=op_def)
  File ""C:\Users\hajum\Anaconda3\lib\site-packages\tensorflow\python\framework\ops.py"", line 2506, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""C:\Users\hajum\Anaconda3\lib\site-packages\tensorflow\python\framework\ops.py"", line 1269, in __init__
    self._traceback = _extract_stack()

NotFoundError (see above for traceback): Key generator/encoder_8/conv/filter not found in checkpoint
         [[Node: save/RestoreV2_43 = RestoreV2[dtypes=[DT_FLOAT], _device=""/job:localhost/replica:0/task:0/cpu:0""](_arg_save/Const_0_0, save/RestoreV2_43/tensor_names, save/RestoreV2_43/shape_and_slices)]]
```",2018-01-16T23:01:49Z,4,1,1,3.2597303021722714
372,16067,[XLA] Separate out the dynamic slice wrapping tests,"awaiting testing (then merge),cla: yes","This is a set of changes to allow disabling of bfloat16 tests, for backends which don't support bfloat16.  Originally it was a change to the same set of tests to allow the wrapping behaviour tests to be disabled.  That change was made obsolete by some parallel work.

---
The original text was:

The XLA documentation says that the behaviour of dynamic slice and dynamic update slice is undefined when the indices wrap.

This separates out the tests which check for wrapping behaviour, so that they can be ignored for backends which don't exhibit the test's expected results.
",0,,5,2018-01-12T09:26:37Z,2018-01-20T22:46:05Z,CONTRIBUTOR,"This is a set of changes to allow disabling of bfloat16 tests, for backends which don't support bfloat16.  Originally it was a change to the same set of tests to allow the wrapping behaviour tests to be disabled.  That change was made obsolete by some parallel work.

---
The original text was:

The XLA documentation says that the behaviour of dynamic slice and dynamic update slice is undefined when the indices wrap.

This separates out the tests which check for wrapping behaviour, so that they can be ignored for backends which don't exhibit the test's expected results.
",2018-01-15T13:50:52Z,8,2,2,4.759730302172271
373,16066,the loss is nan,,"when i training the facenet(build by myself) the loss is normal on the first iteration, but on the second and following iteration ,the loss became nan, i don't know what happened, please help me, Thanks!!!",0,,2,2018-01-12T05:50:20Z,2018-01-13T03:50:46Z,NONE,"when i training the facenet(build by myself) the loss is normal on the first iteration, but on the second and following iteration ,the loss became nan, i don't know what happened, please help me, Thanks!!!",2018-01-13T03:50:46Z,1,1,0,2.2597303021722714
374,16060,Branch 181679271,cla: yes,Merging internal changes,0,,1,2018-01-12T01:17:21Z,2018-01-12T04:00:51Z,CONTRIBUTOR,Merging internal changes,2018-01-12T03:18:38Z,0,2,0,2.7597303021722714
375,16059,[Intel MKL] Fixes for various MKLDNN unit test failures,"awaiting testing (then merge),cla: yes","1. MklLayout pass changes

   Making workspace type uint8 for MaxPool; Handling duplicate control edge insertion

   1) Handles case of inserting duplicate control edge (fixing Mkl layout graph
   pass unit test)
   2) Enables uint8 as workspace tensor type (makes consistent with LRN workspace
   handling)

   Workspace tensor type change is also performed in MaxPool and MaxPoolGrad
   operators.

2. Handling MklReshape failing case

   MklReshape was failing on a unit test when Mkl layout and Tensorflow layout for
   input tensors were same, but shape of input tensor and output tensor was
   different. No reorder is required in such case, but reshape is needed. Before
   this fix, we were asserting that reorder is performed.

3. Adding support for empty input/filter tensors in Convolution backprop operators",0,,2,2018-01-12T00:47:26Z,2018-01-17T05:48:52Z,CONTRIBUTOR,"1. MklLayout pass changes

   Making workspace type uint8 for MaxPool; Handling duplicate control edge insertion

   1) Handles case of inserting duplicate control edge (fixing Mkl layout graph
   pass unit test)
   2) Enables uint8 as workspace tensor type (makes consistent with LRN workspace
   handling)

   Workspace tensor type change is also performed in MaxPool and MaxPoolGrad
   operators.

2. Handling MklReshape failing case

   MklReshape was failing on a unit test when Mkl layout and Tensorflow layout for
   input tensors were same, but shape of input tensor and output tensor was
   different. No reorder is required in such case, but reshape is needed. Before
   this fix, we were asserting that reorder is performed.

3. Adding support for empty input/filter tensors in Convolution backprop operators",2018-01-12T05:09:32Z,5,2,1,3.2597303021722714
376,16058,How to initialize embeddings layer within Estimator API?,"stat:awaiting response,type:bug/performance","I'm trying to use existing embeddings within tensorflow model, the size of embedding is greater than 2Gb and this makes my original try of doing this unsuccessful:

```
embedding_var = tf.get_variable(
        ""embeddings"", 
        shape=GLOVE_MATRIX.shape, 
        initializer=tf.constant_initializer(np.array(GLOVE_MATRIX))
)
```
Which gave me this error:

` Cannot create a tensor proto whose content is larger than 2GB.`

I'm using AWS SageMaker, which based on the Estimator API, and the actual running of the graph in session happens behind the scene, so I'm not sure how to initialize some placeholders for embedding given that. Would be helpful if someone will be able to share the way how to do such initialization in term of EstimatorAPI.

--------------------------

Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",1,,3,2018-01-12T00:22:23Z,2018-01-17T21:04:57Z,CONTRIBUTOR,"I'm trying to use existing embeddings within tensorflow model, the size of embedding is greater than 2Gb and this makes my original try of doing this unsuccessful:

```
embedding_var = tf.get_variable(
        ""embeddings"", 
        shape=GLOVE_MATRIX.shape, 
        initializer=tf.constant_initializer(np.array(GLOVE_MATRIX))
)
```
Which gave me this error:

` Cannot create a tensor proto whose content is larger than 2GB.`

I'm using AWS SageMaker, which based on the Estimator API, and the actual running of the graph in session happens behind the scene, so I'm not sure how to initialize some placeholders for embedding given that. Would be helpful if someone will be able to share the way how to do such initialization in term of EstimatorAPI.

--------------------------

Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",2018-01-12T21:24:07Z,5,2,1,4.759730302172271
377,16057,Make platform a proper module,cla: yes,"This fixes an issue where the nice error about importing tensorflow from the TF source directory is not displayed in Python 2.7

Fixes #16019 ",1,,6,2018-01-11T22:58:56Z,2018-01-23T20:00:35Z,NONE,"This fixes an issue where the nice error about importing tensorflow from the TF source directory is not displayed in Python 2.7

Fixes #16019 ",2018-01-22T23:41:35Z,12,1,2,5.258317766807329
378,16056,Apply 1.5-rc1 cherry-picks.,cla: yes,,0,,4,2018-01-11T22:49:23Z,2018-01-12T00:38:14Z,MEMBER,,2018-01-11T22:58:56Z,1,3,0,5.258317766807329
379,16055,MKL: Fixed 3 bugs picked up by the unit tests,cla: yes,"- There were 2 kinds of registrations for MatMul - with and without the 'eigen' label. Re-added the registrations with the 'eigen' label when MKL is used. 
- Removed the ifdef that removed the check for the label when MKL was used. The eigen op should be called when the eigen label is used.
- In the selective registration header test, unicode strings aren't handled correctly, so there's a ""u"" before the kernel class string that is compared to the hardcoded string. This has been fixed.
```
- [('BiasAdd', 'BiasOp<CPUDevice, float>'), 
+ [('BiasAdd', u'BiasOp<CPUDevice, float>'), 
```",0,,2,2018-01-11T22:10:37Z,2018-01-12T20:52:14Z,CONTRIBUTOR,"- There were 2 kinds of registrations for MatMul - with and without the 'eigen' label. Re-added the registrations with the 'eigen' label when MKL is used. 
- Removed the ifdef that removed the check for the label when MKL was used. The eigen op should be called when the eigen label is used.
- In the selective registration header test, unicode strings aren't handled correctly, so there's a ""u"" before the kernel class string that is compared to the hardcoded string. This has been fixed.
```
- [('BiasAdd', 'BiasOp<CPUDevice, float>'), 
+ [('BiasAdd', u'BiasOp<CPUDevice, float>'), 
```",2018-01-12T05:07:01Z,1,2,0,3.2583177668073287
380,16047,Branch 181629980,cla: yes,,0,,2,2018-01-11T19:10:21Z,2018-01-12T02:45:09Z,MEMBER,,2018-01-11T19:10:49Z,1,3,0,4.258317766807329
381,16046,Feature Request: clarify supported environments for official binaries.,,"As it stands now, binary release of TensorFlow 1.5 is set to drop compatibility with Ubuntu 14.04 ( https://github.com/tensorflow/tensorflow/issues/15777), and compatibility with Debian Linux distros, such as Amazon Linux AMI (`ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found`).

To avoid surprise, TensorFlow should either:
1. Follow other open-source projects like Ray/PyTorch and provide official binaries for these systems
or
2. Document that support is dropped, to encourage other players (ie, AWS) to take over the job of providing these binaries

@martinwicke",0,,11,2018-01-11T18:13:57Z,2018-01-31T04:22:09Z,CONTRIBUTOR,"As it stands now, binary release of TensorFlow 1.5 is set to drop compatibility with Ubuntu 14.04 ( https://github.com/tensorflow/tensorflow/issues/15777), and compatibility with Debian Linux distros, such as Amazon Linux AMI (`ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found`).

To avoid surprise, TensorFlow should either:
1. Follow other open-source projects like Ray/PyTorch and provide official binaries for these systems
or
2. Document that support is dropped, to encourage other players (ie, AWS) to take over the job of providing these binaries

@martinwicke",2018-01-11T18:58:25Z,20,2,3,7.758317766807329
382,16041,Code documentation for `confusion_matrix.py` misleading,stat:awaiting tensorflower,"### Describe the problem

The documentation for `confusion_matrix.py` says:

```
  Args:
    labels: 1-D `Tensor` of real labels for the classification task.
    predictions: 1-D `Tensor` of predictions for a given classification.
```

, however I found that those two arguments are simply python arrays and not Tensors. The following trial test code demonstrates this. As a TF/Python newbie, I'm wondering if this is actually a real issue, and if so I'll create a PR to correct it to prevent confusion to future programmers.

### Source code / logs

```
import tensorflow as tf

y_ = [0, 2, 2, 2]
y = [2, 1, 2, 2]

with tf.Session() as sess:
    confusion_matrix = tf.confusion_matrix(labels=y_, predictions=y, num_classes=4)
    confusion_matrix_to_Print = sess.run(confusion_matrix)
    print(confusion_matrix_to_Print)

```",0,,2,2018-01-11T14:25:22Z,2018-01-12T02:11:58Z,NONE,"### Describe the problem

The documentation for `confusion_matrix.py` says:

```
  Args:
    labels: 1-D `Tensor` of real labels for the classification task.
    predictions: 1-D `Tensor` of predictions for a given classification.
```

, however I found that those two arguments are simply python arrays and not Tensors. The following trial test code demonstrates this. As a TF/Python newbie, I'm wondering if this is actually a real issue, and if so I'll create a PR to correct it to prevent confusion to future programmers.

### Source code / logs

```
import tensorflow as tf

y_ = [0, 2, 2, 2]
y = [2, 1, 2, 2]

with tf.Session() as sess:
    confusion_matrix = tf.confusion_matrix(labels=y_, predictions=y, num_classes=4)
    confusion_matrix_to_Print = sess.run(confusion_matrix)
    print(confusion_matrix_to_Print)

```",2018-01-12T02:04:25Z,1,1,0,2.2583177668073287
383,16039,How TF-Detect draw a rectangular?,,"How TF-Detect draw a rectangular?
I can't find the corresponding code?
Is it calling OpenGL to draw a rectangular?",0,,4,2018-01-11T13:12:20Z,2018-01-29T22:16:53Z,NONE,"How TF-Detect draw a rectangular?
I can't find the corresponding code?
Is it calling OpenGL to draw a rectangular?",2018-01-12T01:03:25Z,18,1,3,3.2583177668073287
384,16036,"raise PiCameraMMALError(status, prefix) picamera.exc.PiCameraMMALError: Failed to enable connection: Out of resources",stat:awaiting response,,0,,4,2018-01-11T11:14:55Z,2018-01-12T06:55:18Z,NONE,,2018-01-11T19:01:23Z,1,1,0,3.2583177668073287
385,16034,Feature request: tf.nn.dropout noise_shape should support unspecified dimensions,"stat:awaiting response,type:feature","It would be nice if the noise_shape in [tf.nn.dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) would support unspecified dimensions, and just use the shape of the input tensor, e.g. `-1` or `None`. This way it could be specified as `noise_shape = [-1, 1, 1, -1]` instead of `noise_shape = [k, 1, 1, n]`.",0,,3,2018-01-11T11:03:11Z,2018-02-15T23:21:28Z,NONE,"It would be nice if the noise_shape in [tf.nn.dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) would support unspecified dimensions, and just use the shape of the input tensor, e.g. `-1` or `None`. This way it could be specified as `noise_shape = [-1, 1, 1, -1]` instead of `noise_shape = [k, 1, 1, n]`.",2018-01-11T19:01:19Z,34,1,4,2.7583177668073287
386,16031,tf.data.Dataset.padded_batch() doesn't work with dataset.map using tf.py_func,stat:awaiting response,"
------------------------

### System information
- **Have I written custom code**: yes
- **OS Platform and Distribution**: CentOS Linux release 7.2.1511
- **TensorFlow installed from**: pip
- **TensorFlow version (use command below)**: 1.4.1
- **Python version**: 2.7.5

### Describe the problem
It's quite common for NLP tasks to read variable-length sentences from text files, to map them and to padd them. But Dataset.padded_batch() doesn't work with tf.dataset which uses map (tf.py_func)

### Source code
```
import tensorflow as tf                                                                           
import numpy as np                                                                                
                                                                                                  
def convert(line):                                                                                
    tokens = line.split()                                                                 
    return np.array(tokens, dtype=np.int32)

# Simulating reading variable-length sentences from a file. Using TextLineDataset will have the same problem
dataset = tf.data.Dataset.from_tensor_slices([""1 2 3"", ""4 5""])         
                           
# Tokenize each sentence and convert it to list of int
dataset = dataset.map(lambda line: tf.py_func(convert, [line], [tf.int32]))     
                  
dataset = dataset.padded_batch(1, [None]) # This line doesn't work whatever the batch_size is
# dataset = dataset.batch(1) # This line works well                                              
 
iterator = dataset.make_one_shot_iterator()                                                       
batch_data = iterator.get_next()                                                                  
                                                                                                  
with tf.Session() as sess:                                                                        
    print sess.run(batch_data)                                                                                                 
```

### Log
```
    dataset = dataset.padded_batch(1, [None])
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 695, in padded_batch
    return PaddedBatchDataset(self, batch_size, padded_shapes, padding_values)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 1292, in __init__
    input_dataset.output_shapes, _partial_shape_to_tensor, padded_shapes)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/data/util/nest.py"", line 512, in map_structure_up_to
    assert_shallow_structure(shallow_tree, input_tree)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/data/util/nest.py"", line 356, in assert_shallow_structure
    ""Input has type: %s."" % type(input_tree))
TypeError: If shallow structure is a sequence, input must also be a sequence. Input has type: <type 'list'>.
```",0,,3,2018-01-11T05:44:19Z,2018-01-11T17:56:46Z,NONE,"
------------------------

### System information
- **Have I written custom code**: yes
- **OS Platform and Distribution**: CentOS Linux release 7.2.1511
- **TensorFlow installed from**: pip
- **TensorFlow version (use command below)**: 1.4.1
- **Python version**: 2.7.5

### Describe the problem
It's quite common for NLP tasks to read variable-length sentences from text files, to map them and to padd them. But Dataset.padded_batch() doesn't work with tf.dataset which uses map (tf.py_func)

### Source code
```
import tensorflow as tf                                                                           
import numpy as np                                                                                
                                                                                                  
def convert(line):                                                                                
    tokens = line.split()                                                                 
    return np.array(tokens, dtype=np.int32)

# Simulating reading variable-length sentences from a file. Using TextLineDataset will have the same problem
dataset = tf.data.Dataset.from_tensor_slices([""1 2 3"", ""4 5""])         
                           
# Tokenize each sentence and convert it to list of int
dataset = dataset.map(lambda line: tf.py_func(convert, [line], [tf.int32]))     
                  
dataset = dataset.padded_batch(1, [None]) # This line doesn't work whatever the batch_size is
# dataset = dataset.batch(1) # This line works well                                              
 
iterator = dataset.make_one_shot_iterator()                                                       
batch_data = iterator.get_next()                                                                  
                                                                                                  
with tf.Session() as sess:                                                                        
    print sess.run(batch_data)                                                                                                 
```

### Log
```
    dataset = dataset.padded_batch(1, [None])
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 695, in padded_batch
    return PaddedBatchDataset(self, batch_size, padded_shapes, padding_values)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 1292, in __init__
    input_dataset.output_shapes, _partial_shape_to_tensor, padded_shapes)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/data/util/nest.py"", line 512, in map_structure_up_to
    assert_shallow_structure(shallow_tree, input_tree)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/data/util/nest.py"", line 356, in assert_shallow_structure
    ""Input has type: %s."" % type(input_tree))
TypeError: If shallow structure is a sequence, input must also be a sequence. Input has type: <type 'list'>.
```",2018-01-11T12:57:18Z,0,1,0,2.7583177668073287
387,16027,py2tf: add py2tf_internal BUILD rule to pip package,cla: yes,* to make pip tests pass,0,,1,2018-01-11T03:11:27Z,2018-01-11T04:56:14Z,CONTRIBUTOR,* to make pip tests pass,2018-01-11T03:38:01Z,0,2,0,2.7583177668073287
388,16024,R1.4,cla: no,,0,,3,2018-01-11T00:17:30Z,2018-01-11T00:18:57Z,NONE,,2018-01-11T00:18:57Z,0,1,0,2.7583177668073287
389,16021,Update version strings.,cla: yes,,0,,1,2018-01-10T22:01:39Z,2018-01-10T22:02:37Z,MEMBER,,2018-01-10T22:02:37Z,0,3,0,3.7569491711848753
390,16019,tf-nightly and master - cannot import tensorflow,stat:contributions welcome,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: 
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: tf-nightly-gpu-1.6.0.dev20180110
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: 9.0/7
- **GPU model and memory**: V100 16GB
- **Exact command to reproduce**: ```import tensorflow```

### Describe the problem
On the current tf-nightly-gpu I cannot import tensorflow, the following error is produced. I am also seeing the same behavior on tf-nightly and also a build from source of master (SHA: 82b1e8eee8847730026379e3a5762c0e09d6fd36):
``` python
In [1]: import tensorflow
---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
<ipython-input-1-64156d691fe5> in <module>()
----> 1 import tensorflow as tf

/home/ubuntu/tensorflow/tensorflow/__init__.py in <module>()
     22
     23 # pylint: disable=wildcard-import
---> 24 from tensorflow.python import *
     25 # pylint: enable=wildcard-import
     26

/home/ubuntu/tensorflow/tensorflow/python/__init__.py in <module>()
     47 import numpy as np
     48
---> 49 from tensorflow.python import pywrap_tensorflow
     50
     51 # Protocol buffers

/home/ubuntu/tensorflow/tensorflow/python/pywrap_tensorflow.py in <module>()
     23 import traceback
     24
---> 25 from tensorflow.python.platform import self_check
     26
     27

ImportError: No module named platform
```

### Source code / logs
N/A",0,,3,2018-01-10T20:50:09Z,2018-01-23T20:01:03Z,NONE,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: 
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: tf-nightly-gpu-1.6.0.dev20180110
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: 9.0/7
- **GPU model and memory**: V100 16GB
- **Exact command to reproduce**: ```import tensorflow```

### Describe the problem
On the current tf-nightly-gpu I cannot import tensorflow, the following error is produced. I am also seeing the same behavior on tf-nightly and also a build from source of master (SHA: 82b1e8eee8847730026379e3a5762c0e09d6fd36):
``` python
In [1]: import tensorflow
---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
<ipython-input-1-64156d691fe5> in <module>()
----> 1 import tensorflow as tf

/home/ubuntu/tensorflow/tensorflow/__init__.py in <module>()
     22
     23 # pylint: disable=wildcard-import
---> 24 from tensorflow.python import *
     25 # pylint: enable=wildcard-import
     26

/home/ubuntu/tensorflow/tensorflow/python/__init__.py in <module>()
     47 import numpy as np
     48
---> 49 from tensorflow.python import pywrap_tensorflow
     50
     51 # Protocol buffers

/home/ubuntu/tensorflow/tensorflow/python/pywrap_tensorflow.py in <module>()
     23 import traceback
     24
---> 25 from tensorflow.python.platform import self_check
     26
     27

ImportError: No module named platform
```

### Source code / logs
N/A",2018-01-11T01:44:18Z,13,1,2,2.7569491711848753
391,16018,Branch 181499300,cla: yes,,0,,5,2018-01-10T20:38:55Z,2018-01-11T00:11:47Z,MEMBER,,2018-01-10T20:39:22Z,1,3,0,5.756949171184875
392,16015,Modify `_parse_bazel_version` to return a tuple of ints,cla: yes,"Bazel is updating its version to 0.10.0, and this will break the version check. Applying suggested fix in https://github.com/bazelbuild/bazel/issues/4425.",0,,1,2018-01-10T19:17:41Z,2018-01-10T20:28:08Z,MEMBER,"Bazel is updating its version to 0.10.0, and this will break the version check. Applying suggested fix in https://github.com/bazelbuild/bazel/issues/4425.",2018-01-10T19:17:59Z,0,3,0,3.7569491711848753
393,16013,Disabling the interleave_op_test for now.,cla: yes,,0,,2,2018-01-10T18:43:39Z,2018-01-10T20:11:17Z,MEMBER,,2018-01-10T20:02:05Z,0,3,0,4.256949171184875
394,16012,Fix a bug in ResolveConstantConcat,"awaiting testing (then merge),cla: yes",Changes to fix a bug in ResolveConstantConcat whereby shared tensors are removed without checking if they are used in other operators in the graph.,1,,14,2018-01-10T18:10:58Z,2018-01-25T01:23:11Z,CONTRIBUTOR,Changes to fix a bug in ResolveConstantConcat whereby shared tensors are removed without checking if they are used in other operators in the graph.,2018-01-10T23:23:08Z,15,2,3,10.256949171184875
395,16011,Tensorboard issue with the official docker image - 1.5.0-rc0-gpu-py3,"stat:awaiting tensorflower,type:build/install","Hello everyone,

I have the exact same issue as stated here: https://github.com/tensorflow/tensorflow/issues/14855
And on the official tensorboard repository: https://github.com/tensorflow/tensorboard/issues/812

The fact that I am using the official Docker Image and I didn't build anything from scratch.

`tensorflow/tensorflow   1.5.0-rc0-gpu-py3   9e770d59b136        6 days ago          2.85GB`

What I get when I try to launch Tensorboard as usual:

```
# tensorboard --logdir=log_directory
/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Traceback (most recent call last):
  File ""/usr/local/bin/tensorboard"", line 7, in <module>
    from tensorboard.main import run_main
ImportError: cannot import name 'run_main'
```

**Resolution Idea:**
I noticed that by simply running the command: `pip install tensorboard` inside the container the problem is solved and I am able to normally launch Tensorboard.

Thanks a lot for the help,

All the best,

Jonathan
",1,,17,2018-01-10T16:29:58Z,2018-01-19T03:21:53Z,CONTRIBUTOR,"Hello everyone,

I have the exact same issue as stated here: https://github.com/tensorflow/tensorflow/issues/14855
And on the official tensorboard repository: https://github.com/tensorflow/tensorboard/issues/812

The fact that I am using the official Docker Image and I didn't build anything from scratch.

`tensorflow/tensorflow   1.5.0-rc0-gpu-py3   9e770d59b136        6 days ago          2.85GB`

What I get when I try to launch Tensorboard as usual:

```
# tensorboard --logdir=log_directory
/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Traceback (most recent call last):
  File ""/usr/local/bin/tensorboard"", line 7, in <module>
    from tensorboard.main import run_main
ImportError: cannot import name 'run_main'
```

**Resolution Idea:**
I noticed that by simply running the command: `pip install tensorboard` inside the container the problem is solved and I am able to normally launch Tensorboard.

Thanks a lot for the help,

All the best,

Jonathan
",2018-01-10T17:51:10Z,9,2,2,11.756949171184875
396,16010,lib_package does not bundle MKL-DNN,stat:awaiting response,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.4.1
- **Python version**: 2.7.12
- **Bazel version (if compiling from source)**: 0.8.1
- **GCC/Compiler version (if compiling from source)**: 5.4.0
- **CUDA/cuDNN version**: -
- **GPU model and memory**: -
- **Exact command to reproduce**: 

### Describe the problem
I wan't to build the tensorflow C-API from source with MKL-DNN support in order to use it in another project. The easiest solution (if not the only convenient one) I found for building the C-API is using the lib_package tool:

```bash
bazel build --config=mkl -c opt //tensorflow/tools/lib_package:libtensorflow
```
The build succeeds. However, the packaged library does not contain `libmklml_intel.so` and `libiomp5.so`. 

```bash
$ ldd libtensorflow_framework.so
	linux-vdso.so.1 =>  (0x00007ffec0f8a000)
	libmklml_intel.so => not found
	libiomp5.so => not found
	libdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x00007feb07b2f000)
	libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007feb07826000)
	libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007feb07609000)
	libstdc++.so.6 => /usr/lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007feb07287000)
	libgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007feb07071000)
	libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007feb06ca7000)

```
Is there a way to fix the Bazel build such that it outputs all necessary libs?
",0,,3,2018-01-10T14:47:46Z,2018-01-11T18:49:00Z,NONE,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.4.1
- **Python version**: 2.7.12
- **Bazel version (if compiling from source)**: 0.8.1
- **GCC/Compiler version (if compiling from source)**: 5.4.0
- **CUDA/cuDNN version**: -
- **GPU model and memory**: -
- **Exact command to reproduce**: 

### Describe the problem
I wan't to build the tensorflow C-API from source with MKL-DNN support in order to use it in another project. The easiest solution (if not the only convenient one) I found for building the C-API is using the lib_package tool:

```bash
bazel build --config=mkl -c opt //tensorflow/tools/lib_package:libtensorflow
```
The build succeeds. However, the packaged library does not contain `libmklml_intel.so` and `libiomp5.so`. 

```bash
$ ldd libtensorflow_framework.so
	linux-vdso.so.1 =>  (0x00007ffec0f8a000)
	libmklml_intel.so => not found
	libiomp5.so => not found
	libdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x00007feb07b2f000)
	libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007feb07826000)
	libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007feb07609000)
	libstdc++.so.6 => /usr/lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007feb07287000)
	libgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007feb07071000)
	libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007feb06ca7000)

```
Is there a way to fix the Bazel build such that it outputs all necessary libs?
",2018-01-11T06:15:12Z,1,1,0,2.7569491711848753
397,16009,"bazel build ask for ANDROID_NDK_HOME, ANDROID_SDK_HOME -- no way to disable it",type:build/install,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:
source
- **TensorFlow version (use command below)**:
('v1.5.0-rc0-1-g793280a', '1.5.0-rc0')
- **Python version**: 
2.7
- **Bazel version (if compiling from source)**:
```
Build label: 0.9.0
Build target: bazel-out/k8-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Tue Dec 19 09:31:58 2017 (1513675918)
Build timestamp: 1513675918
Build timestamp as int: 1513675918
```
- **GCC/Compiler version (if compiling from source)**:
g++ (Ubuntu 5.4.0-6ubuntu1~16.04.5) 5.4.0 20160609
- **CUDA/cuDNN version**:
toolkit_9.0 and cudnn 7.0.5_for_9.0
- **GPU model and memory**:
different machines (irrelevant)
- **Exact command to reproduce**:
see [this gist](https://gist.github.com/PatWie/aef90e72dbeaf2f79fbcaa031d74baad) which is mainly

```bash
export TF_NEED_GCP=0
export TF_NEED_CUDA=1
export TF_CUDA_VERSION=""$($CUDA_TOOLKIT_PATH/bin/nvcc --version | sed -n 's/^.*release \(.*\),.*/\1/p')""
export TF_CUDA_COMPUTE_CAPABILITIES=6.1,5.2,3.5
export TF_NEED_HDFS=0
export TF_NEED_OPENCL=0
export TF_NEED_JEMALLOC=1
export TF_ENABLE_XLA=0
export TF_NEED_VERBS=0
export TF_CUDA_CLANG=0
export TF_CUDNN_VERSION=7
export TF_NEED_MKL=0
export TF_DOWNLOAD_MKL=0
export TF_NEED_MPI=0
export TF_NEED_GDR=0
export TF_NEED_S3=0
export TF_NEED_OPENCL_SYCL=0
export TF_NEED_COMPUTECPP=0
export GCC_HOST_COMPILER_PATH=$(which gcc)
export CC_OPT_FLAGS=""-march=native""

./configure

bazel build --config=opt --copt=-mfpmath=both --copt=-msse4.2 --copt=-O3 --cxxopt=-D_GLIBCXX_USE_CXX11_ABI=1 ....
```
### Describe the problem
In the past, using exactly this scripted worked. However, there are now a few issues:
The build uses `AVX2` even I haven't specified it as `--copt` (which worked in the past)

### Source code / logs
depending on the machine it gives

```
Python 2.7.12 (default, Nov 20 2017, 18:23:56) 
[GCC 5.4.0 20160609] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow
2018-01-10 15:06:19.070740: F tensorflow/core/platform/cpu_feature_guard.cc:36] The TensorFlow library was compiled to use AVX2 instructions, but these aren't available on your machine.
zsh: abort      python
```
or
```
Python 2.7.12 (default, Nov 20 2017, 18:23:56) 
[GCC 5.4.0 20160609] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow
zsh: illegal hardware instruction  python
```

On machines with AVX2 everything is fine. Further, there is no way to skip to setup ANDROID_NDK_HOME, ANDROID_SDK_HOME (I manually uncommented this in `configure.py`).

*edit*
I am willing to provide a pull-request for `configure.py`, adding something like `TF_NEED_ANDROID`.",0,,5,2018-01-10T14:10:14Z,2018-01-30T19:31:57Z,NONE,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:
source
- **TensorFlow version (use command below)**:
('v1.5.0-rc0-1-g793280a', '1.5.0-rc0')
- **Python version**: 
2.7
- **Bazel version (if compiling from source)**:
```
Build label: 0.9.0
Build target: bazel-out/k8-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Tue Dec 19 09:31:58 2017 (1513675918)
Build timestamp: 1513675918
Build timestamp as int: 1513675918
```
- **GCC/Compiler version (if compiling from source)**:
g++ (Ubuntu 5.4.0-6ubuntu1~16.04.5) 5.4.0 20160609
- **CUDA/cuDNN version**:
toolkit_9.0 and cudnn 7.0.5_for_9.0
- **GPU model and memory**:
different machines (irrelevant)
- **Exact command to reproduce**:
see [this gist](https://gist.github.com/PatWie/aef90e72dbeaf2f79fbcaa031d74baad) which is mainly

```bash
export TF_NEED_GCP=0
export TF_NEED_CUDA=1
export TF_CUDA_VERSION=""$($CUDA_TOOLKIT_PATH/bin/nvcc --version | sed -n 's/^.*release \(.*\),.*/\1/p')""
export TF_CUDA_COMPUTE_CAPABILITIES=6.1,5.2,3.5
export TF_NEED_HDFS=0
export TF_NEED_OPENCL=0
export TF_NEED_JEMALLOC=1
export TF_ENABLE_XLA=0
export TF_NEED_VERBS=0
export TF_CUDA_CLANG=0
export TF_CUDNN_VERSION=7
export TF_NEED_MKL=0
export TF_DOWNLOAD_MKL=0
export TF_NEED_MPI=0
export TF_NEED_GDR=0
export TF_NEED_S3=0
export TF_NEED_OPENCL_SYCL=0
export TF_NEED_COMPUTECPP=0
export GCC_HOST_COMPILER_PATH=$(which gcc)
export CC_OPT_FLAGS=""-march=native""

./configure

bazel build --config=opt --copt=-mfpmath=both --copt=-msse4.2 --copt=-O3 --cxxopt=-D_GLIBCXX_USE_CXX11_ABI=1 ....
```
### Describe the problem
In the past, using exactly this scripted worked. However, there are now a few issues:
The build uses `AVX2` even I haven't specified it as `--copt` (which worked in the past)

### Source code / logs
depending on the machine it gives

```
Python 2.7.12 (default, Nov 20 2017, 18:23:56) 
[GCC 5.4.0 20160609] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow
2018-01-10 15:06:19.070740: F tensorflow/core/platform/cpu_feature_guard.cc:36] The TensorFlow library was compiled to use AVX2 instructions, but these aren't available on your machine.
zsh: abort      python
```
or
```
Python 2.7.12 (default, Nov 20 2017, 18:23:56) 
[GCC 5.4.0 20160609] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow
zsh: illegal hardware instruction  python
```

On machines with AVX2 everything is fine. Further, there is no way to skip to setup ANDROID_NDK_HOME, ANDROID_SDK_HOME (I manually uncommented this in `configure.py`).

*edit*
I am willing to provide a pull-request for `configure.py`, adding something like `TF_NEED_ANDROID`.",2018-01-10T21:49:22Z,20,1,3,3.7569491711848753
398,16008,"Java/JNI , Object Detection: Not big Difference with GPU or CPU? (Insignificant difference) ~300ms with and without GPU",,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: 

**binary** by instructions https://www.tensorflow.org/versions/master/install/install_java
> 
> Install on Linux
> 
> Take the following steps to install TensorFlow for Java on Linux or macOS:
> 
> 1 Download libtensorflow.jar, which is the TensorFlow Java Archive (JAR).
> 2 Decide whether you will run TensorFlow for Java on CPU(s) only or with the help of GPU(s). To help you decide, read the section entitled ""Determine which TensorFlow to install"" in one of the following guides:
>  - Installing TensorFlow on Linux
>
> 3 Download and extract the appropriate Java Native Interface (JNI) file for your operating system and processor support by running the following shell commands:
> 
>  TF_TYPE=""gpu""
>  OS=$(uname -s | tr '[:upper:]' '[:lower:]')
>  mkdir -p ./jni
>  curl -L \
>    ""https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow_jni-${TF_TYPE}-${OS}-x86_64-1.4.0.tar.gz"" |
>    tar -xz -C ./jni

- **TensorFlow version (use command below)**: 1.4.0
- **Python version**: n/a, not used here (Java instead)
- **Bazel version (if compiling from source)**: n/a, not used here
- **GCC/Compiler version (if compiling from source)**: n/a, not used here
- **CUDA/cuDNN version**: Cuda compilation tools, release 8.0, V8.0.61, cuDNN 6
- **GPU model and memory**: GeForce 940MX

### Source code / logs
```
Checking to see if TensorFlow native methods are already loaded
TensorFlow native methods not found, attempting to load via tensorflow_inference
Successfully loaded TensorFlow native methods (RunStats error may be ignored)
2018-01-10 15:51:41.115224: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2018-01-10 15:51:41.254497: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-01-10 15:51:41.255183: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: 
name: GeForce 940MX major: 5 minor: 0 memoryClockRate(GHz): 1.2415
pciBusID: 0000:01:00.0
totalMemory: 1,96GiB freeMemory: 1,51GiB
2018-01-10 15:51:41.255217: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce 940MX, pci bus id: 0000:01:00.0, compute capability: 5.0)
Model load took 313ms, TensorFlow version: 1.4.0
```
",0,,5,2018-01-10T14:06:41Z,2018-01-11T06:11:54Z,NONE,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: 

**binary** by instructions https://www.tensorflow.org/versions/master/install/install_java
> 
> Install on Linux
> 
> Take the following steps to install TensorFlow for Java on Linux or macOS:
> 
> 1 Download libtensorflow.jar, which is the TensorFlow Java Archive (JAR).
> 2 Decide whether you will run TensorFlow for Java on CPU(s) only or with the help of GPU(s). To help you decide, read the section entitled ""Determine which TensorFlow to install"" in one of the following guides:
>  - Installing TensorFlow on Linux
>
> 3 Download and extract the appropriate Java Native Interface (JNI) file for your operating system and processor support by running the following shell commands:
> 
>  TF_TYPE=""gpu""
>  OS=$(uname -s | tr '[:upper:]' '[:lower:]')
>  mkdir -p ./jni
>  curl -L \
>    ""https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow_jni-${TF_TYPE}-${OS}-x86_64-1.4.0.tar.gz"" |
>    tar -xz -C ./jni

- **TensorFlow version (use command below)**: 1.4.0
- **Python version**: n/a, not used here (Java instead)
- **Bazel version (if compiling from source)**: n/a, not used here
- **GCC/Compiler version (if compiling from source)**: n/a, not used here
- **CUDA/cuDNN version**: Cuda compilation tools, release 8.0, V8.0.61, cuDNN 6
- **GPU model and memory**: GeForce 940MX

### Source code / logs
```
Checking to see if TensorFlow native methods are already loaded
TensorFlow native methods not found, attempting to load via tensorflow_inference
Successfully loaded TensorFlow native methods (RunStats error may be ignored)
2018-01-10 15:51:41.115224: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2018-01-10 15:51:41.254497: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-01-10 15:51:41.255183: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: 
name: GeForce 940MX major: 5 minor: 0 memoryClockRate(GHz): 1.2415
pciBusID: 0000:01:00.0
totalMemory: 1,96GiB freeMemory: 1,51GiB
2018-01-10 15:51:41.255217: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce 940MX, pci bus id: 0000:01:00.0, compute capability: 5.0)
Model load took 313ms, TensorFlow version: 1.4.0
```
",2018-01-11T06:11:54Z,1,1,0,3.7569491711848753
399,16007,Fix inline if/else statement in CMAKE_CACHE_ARGS,"awaiting testing (then merge),cla: yes","An if/else statement was given inline as an argument to CMAKE_CACHE_ARGS for some CMake external projects as discussed in #15209. This resulted in the following init cache entries on some systems:

```
set(CMAKE_POSITION_INDEPENDENT_CODE ""ON;if(;tensorflow_ENABLE_POSITION_INDEPENDENT_CODE;);else;(;)""CACHE BOOL ""Initial cache"" FORCE)
set(CMAKE_POSITION_INDEPENDENT_CODE ""OFF;endif;(;)"" CACHEBOOL ""Initial cache"" FORCE)
```
This commit changes the inline if/else arguments to -DCMAKE_POSITION_INDEPENDENT_CODE:BOOL=${tensorflow_ENABLE_POSITION_INDEPENDENT_CODE} which is functionality equivalent.",0,,5,2018-01-10T13:05:09Z,2018-01-11T14:21:40Z,CONTRIBUTOR,"An if/else statement was given inline as an argument to CMAKE_CACHE_ARGS for some CMake external projects as discussed in #15209. This resulted in the following init cache entries on some systems:

```
set(CMAKE_POSITION_INDEPENDENT_CODE ""ON;if(;tensorflow_ENABLE_POSITION_INDEPENDENT_CODE;);else;(;)""CACHE BOOL ""Initial cache"" FORCE)
set(CMAKE_POSITION_INDEPENDENT_CODE ""OFF;endif;(;)"" CACHEBOOL ""Initial cache"" FORCE)
```
This commit changes the inline if/else arguments to -DCMAKE_POSITION_INDEPENDENT_CODE:BOOL=${tensorflow_ENABLE_POSITION_INDEPENDENT_CODE} which is functionality equivalent.",2018-01-11T02:33:39Z,1,2,0,4.756949171184875
400,16006,Add property to get cell wrapped by DropoutWrapper ,"awaiting testing (then merge),cla: yes",Adding wrapped cell property as discussed in #15810.,1,,7,2018-01-10T12:20:19Z,2018-01-23T21:46:26Z,CONTRIBUTOR,Adding wrapped cell property as discussed in #15810.,2018-01-14T03:25:19Z,13,2,2,6.756949171184875
401,16005,Verbs w 0 copies,"awaiting testing (then merge),cla: yes","## Verbs implementation to use direct tensor writes (0 copies)

### Motivation:

Following HKUST research on the use of GPU direct, and their [GDR implementation](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/gdr/README.md), we wish to adopt the 0 copies approach and apply it to the current verbs implementation, while keeping the current implementation advantages, such as configurability and the use of RDMA for control messages.

### Performance:

Compared with the current GRPC, verbs and GDR implementation, the result implementation gave the best performance for every model, with any number of nodes. For VGG16 on 8 nodes with 4 P100 GPUs each, the prototype beat the second place by over 15%.

### Implementation requirements:

1. Tensor writes need to be done directly from the source Tensor to the destination Tensor, with no memory copies in between. This should be done for all DMAble tensors which are located either on CPU or on a RDMA compatible GPU device (GPU direct). 
2. Non DMAble tensors (CanMemCopy == false) will be serialized to proto on the sender side, RDMA written to a registered buffer on the receiver side, and then deserialized by the receiver.
3. Tensors which are located on a non-RDMA-compatible GPU, will be RDMA written to a registered CPU proxy buffer on the receiver side, and then copied to GPU by the receiver.

### Implementation constrains:

For best stability and proof of correctness, we will divide the implementation to two stages:
1. At first stage we will keep changes to the current implementation to the minimum possible. The expense will be that we may have unused or unnecessary code leftovers, which may also affect performance. 
2. At second stage, we will re-iterate over the code and remove irrelevant code parts.
The design of the solution aims that we will achieve both stages with relative ease. 

### Design guidelines:

1. Since we do not want to do any unnecessary memory copying, we will no longer allocate a fixed CPU buffer as the destination for the RDMA write. Instead we will do the writing directly to the result tensor, or if the result tensor is on a device which does not support RDMA, we will do the writing to a proxy CPU tensor and then copy its content to the result tensor.
2. The address of the destination Tensor needs to be sent to the sender side for writing, meaning that the result/proxy tensor should be pre-allocated on the receiver side, prior to sending the tensor request. In order to do that, we need to know its meta-data, i.e. shape and data-type for DMAble tensors, and proto-size for serialized tensors. Unfortunately, this information is only available on the sender side which complicates manners. In order to avoid sending extra messages for querying the meta-data on each step, we store a local meta-data cache per tensor. Based on the assumption that the meta-data of a tensor rarely changes between steps, we expect that on most times the cache will only be updated once. When the sender receives a request for a tensor, if it is the first time this tensor is requested, or in the rare case that the meta-data did change, the sender will first send a meta-data response, on which the receiver will update the local cache, and reallocate the result/proxy tensors if required. When the receiver sends the tensor request, it will contain also the meta-data currently stored in its local cache, so the sender can compare it to see if there was a change.
3. When the sender writes the tensor content to the result tensor, no additional data is being written with it. That means we need to reside on ibverbs immediate (uint32_t) to indicate which request we are responding to (in order to trigger the receive callback). The easiest and most elegant way is to key the recv callback with a unique request_index (uint32_t), instead of the current key_with_step_id (string). 
4. Since the sender no longer writes the tensor from/to fixed buffers, we no longer need to schedule the writes using the local/remote status. In addition we no longer rely on the RmdaTensorBuffer members as the source/destination addresses and rkey/lkey. Instead, each RdmaTensorBuffer will hold multiple ""Response"" objects (one per step-id), from which we derive destination address and rkey. The source address and lkey are always the ones of the source Tensor.
5. With the addition of tensor pre-allocation, we noticed there is a large code similarity between sending the first tensor request and re-sending the request in case of meta-data changes. After implementing a common method for tensor pre-allocation, it turned out that implementation becomes much simpler by encapsulating the process of request sending/re-sending, meta-data response callback and content response callback, all in a single ""Request"" class. The request class holds all the relevant request information, which reduces excessive parameter passing and lambda capturing. This decision is purely for elegance and code simplicity, and we decided to implement it in first stage because it makes the implementation much easier.
6. At phase 2, we adopt that approach for the sender side as well, by encapsulate all the send and resend logic in the ""Response"" class (and remove the RdmaTensorBuffer class completely). This should make our design easier to understand, and also hold common notions with the rest of the distributed implementations.

### New types/classes:

* **enum RdmaImmDataType** - Immediate types to distinguish between different RDMA writes on the remote side. Ack writes and control-message writes have a fixed immediate value. The rest of the writes are tensor writes and the immediate value is the relevant request index.
* **enum  RdmaWriteIDType**    - Types to distinguish between different RDMA write-complete events: Ack, control message, tensor DMA write and tensor proto write.
* **class RdmaWriteID**        - Context for RDMA write complete events. Holds the RdmaWriteIDType and additional data.
* **class RemoteAddressContext** - Remote address information (address + mr). Will be passed as write context for tensor proto writes.
* **class RdmaTensorMetaData** - Meta-data for a tensor (type, shape, is_dead, proto_size).
* **class RdmaMemoryMgr**      - Manages the meta-data cache, and the registered memory regions.
* **class RdmaTensorRequest**    - Holds and manages information for a single tensor request throughout the entire receive cycle. API:
	* **Start()**                - Start the request sequence.
		* Allocate the result tensor (and proxy tensor if required).
		* Send RDMA_MESSAGE_TENSOR_REQUEST to the remote side.
	* **RecvTensorMetaData()**   - Receive meta-data from the remote side.
		* Update the local meta-data cache.
		* Reallocate the result tensor (and proxy tensor if required).
		* Re-send the request to the remote side.
	* **RecvTensorContent()**    - Receive tensor content from the remote side (RDMA write was completed).
		* Decode proto if required and/or move to GPU if the content was not written to it directly (GPU direct is not avaliable).
		* Invoke the done callback.
* **class RdmaTensorResponse**   - Holds and manages information for a single tensor response throughout the entire send cycle. API:
	* **Start()**                - Start the response sequence. 
		* Find the tensor in the local tag-match table.
		* Compare the tensor's meta-data to the meta-data in the message (taken from the requester's local cache). 
			* If meta-data changed:
				* Clone the tensor to be sent later.
				* Send a meta-data update message and wait for re-request.
			* Else:
				* Send the tensor's content (using direct RDMA write).
	* **Resume()**               - Resume the response sequence after a re-request. Send the tensor's content that was cloned earlier.
	* **Destroy()**              - Destroy the response's resources and remove it form the pending list.

### Protocol changes:

The protocol messages themselves will remain mostly unchanged at the first stage, but will be used differently, as described below. The current messages structures already have most of the required fields for the new implementation. The only change is the ""buffer_size"" field which is no longer used since we are no longer sending additional information with the tensor, and thus it is now always equal to the ""tensor_bytes"" field. Instead, we use that field to pass the ""request_index"".

### Message structure:

| type | name_size | name | step_id | request_index | remote_addr | rkey | is_dead | data_type | tensor_shape | tensor_bytes |
|------|---------- |------|---------|---------------|-------------|------|---------|-----------|--------------|--------------|
|  1B  |    2B     | 512  |  8B     |      8B       |         8B  |   4B |      1B |     XB    |    XB        |    8B        |

* **RDMA_MESSAGE_TENSOR_REQUEST**  - (receiver ==> sender) The original tensor request. 
	* type - The message type.
	* name (name_size) - Name of the requested tensor.
	* step_id - Step ID.
	* request_index - Request index.
	* remote_addr/rkey - Address/rkey of the result/proxy tensor. Irrelevant for first-time request.
	* is_dead/data_type/tensor_shape/tensor_bytes - The current meta-data as stored in the receiver local cache. The sender will use that information to know if the receiver's cache requires updating.
* **RDMA_MESSAGE_BUFFER_REQUEST**  - (sender ==> receiver) The meta-data update message in case meta-data had changed (or if it is the first time the tensor is requested).
	* type - The message type.
	* request_index - Request index.
	* is_dead/data_type/tensor_shape/tensor_bytes - The up-to-date meta-data.

  **Note:** At phase 2 this message is renamed to - **RDMA_MESSAGE_META_DATA_UPDATE**.
* **RDMA_MESSAGE_BUFFER_RESPONSE** - (receiver ==> sender) Tensor re-requset after meta-data update and reallocation of result/proxy tensors.
	* type - The message type.
	* name (name_size) - Name of the requested tensor.
	* step_id - Step ID.
	* request_index - Request index.
	* remote_addr/rkey - Address/rkey of the reallocated result/proxy tensor.
	* is_dead/data_type/tensor_shape/tensor_bytes - The new meta-data. Will be removed in the next phase.

  **Note:** At phase 2 this message is renamed to - **RDMA_MESSAGE_TENSOR_RE_REQUEST**.
* **RDMA_MESSAGE_TENSOR_WRITE**    - (sender ==> receiver) No longer sent. There is only a direct write of the tensor content to the result/proxy tensor. Request index passed as the immediate value of the write.
* **RDMA_MESSAGE_TENSOR_IDLE**     - (receiver ==> sender) No longer sent.

### Phase 1:
![alt text](https://raw.githubusercontent.com/Mellanox/tensorflow/eladw_verbs_w_0_copies/tensorflow/contrib/verbs/verbs_with_0_copies_phase1_protocol.jpg ""Phase 1 transport protocol"")

### Phase 2:
![alt text](https://raw.githubusercontent.com/Mellanox/tensorflow/eladw_verbs_w_0_copies/tensorflow/contrib/verbs/verbs_with_0_copies.png ""Phase 2 transport protocol"")

### Second stage optimizations:
1. Remove unused code leftovers - Done.
2. Remove the ACK buffer completely, since we can rely completely on its immediate value - Done.

### Future optimizations:
1. Map the tensor names to indexes, to significantly reduce the request message size.
2. Understand the purpose of empty tensors and if we can skip remote fetching for them.
3. Consider concatenating multiple requests and/or using multiple message buffers.
4. Consider a no-request architecture.
  
 ",0,,14,2018-01-10T12:15:07Z,2018-01-20T22:41:29Z,CONTRIBUTOR,"## Verbs implementation to use direct tensor writes (0 copies)

### Motivation:

Following HKUST research on the use of GPU direct, and their [GDR implementation](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/gdr/README.md), we wish to adopt the 0 copies approach and apply it to the current verbs implementation, while keeping the current implementation advantages, such as configurability and the use of RDMA for control messages.

### Performance:

Compared with the current GRPC, verbs and GDR implementation, the result implementation gave the best performance for every model, with any number of nodes. For VGG16 on 8 nodes with 4 P100 GPUs each, the prototype beat the second place by over 15%.

### Implementation requirements:

1. Tensor writes need to be done directly from the source Tensor to the destination Tensor, with no memory copies in between. This should be done for all DMAble tensors which are located either on CPU or on a RDMA compatible GPU device (GPU direct). 
2. Non DMAble tensors (CanMemCopy == false) will be serialized to proto on the sender side, RDMA written to a registered buffer on the receiver side, and then deserialized by the receiver.
3. Tensors which are located on a non-RDMA-compatible GPU, will be RDMA written to a registered CPU proxy buffer on the receiver side, and then copied to GPU by the receiver.

### Implementation constrains:

For best stability and proof of correctness, we will divide the implementation to two stages:
1. At first stage we will keep changes to the current implementation to the minimum possible. The expense will be that we may have unused or unnecessary code leftovers, which may also affect performance. 
2. At second stage, we will re-iterate over the code and remove irrelevant code parts.
The design of the solution aims that we will achieve both stages with relative ease. 

### Design guidelines:

1. Since we do not want to do any unnecessary memory copying, we will no longer allocate a fixed CPU buffer as the destination for the RDMA write. Instead we will do the writing directly to the result tensor, or if the result tensor is on a device which does not support RDMA, we will do the writing to a proxy CPU tensor and then copy its content to the result tensor.
2. The address of the destination Tensor needs to be sent to the sender side for writing, meaning that the result/proxy tensor should be pre-allocated on the receiver side, prior to sending the tensor request. In order to do that, we need to know its meta-data, i.e. shape and data-type for DMAble tensors, and proto-size for serialized tensors. Unfortunately, this information is only available on the sender side which complicates manners. In order to avoid sending extra messages for querying the meta-data on each step, we store a local meta-data cache per tensor. Based on the assumption that the meta-data of a tensor rarely changes between steps, we expect that on most times the cache will only be updated once. When the sender receives a request for a tensor, if it is the first time this tensor is requested, or in the rare case that the meta-data did change, the sender will first send a meta-data response, on which the receiver will update the local cache, and reallocate the result/proxy tensors if required. When the receiver sends the tensor request, it will contain also the meta-data currently stored in its local cache, so the sender can compare it to see if there was a change.
3. When the sender writes the tensor content to the result tensor, no additional data is being written with it. That means we need to reside on ibverbs immediate (uint32_t) to indicate which request we are responding to (in order to trigger the receive callback). The easiest and most elegant way is to key the recv callback with a unique request_index (uint32_t), instead of the current key_with_step_id (string). 
4. Since the sender no longer writes the tensor from/to fixed buffers, we no longer need to schedule the writes using the local/remote status. In addition we no longer rely on the RmdaTensorBuffer members as the source/destination addresses and rkey/lkey. Instead, each RdmaTensorBuffer will hold multiple ""Response"" objects (one per step-id), from which we derive destination address and rkey. The source address and lkey are always the ones of the source Tensor.
5. With the addition of tensor pre-allocation, we noticed there is a large code similarity between sending the first tensor request and re-sending the request in case of meta-data changes. After implementing a common method for tensor pre-allocation, it turned out that implementation becomes much simpler by encapsulating the process of request sending/re-sending, meta-data response callback and content response callback, all in a single ""Request"" class. The request class holds all the relevant request information, which reduces excessive parameter passing and lambda capturing. This decision is purely for elegance and code simplicity, and we decided to implement it in first stage because it makes the implementation much easier.
6. At phase 2, we adopt that approach for the sender side as well, by encapsulate all the send and resend logic in the ""Response"" class (and remove the RdmaTensorBuffer class completely). This should make our design easier to understand, and also hold common notions with the rest of the distributed implementations.

### New types/classes:

* **enum RdmaImmDataType** - Immediate types to distinguish between different RDMA writes on the remote side. Ack writes and control-message writes have a fixed immediate value. The rest of the writes are tensor writes and the immediate value is the relevant request index.
* **enum  RdmaWriteIDType**    - Types to distinguish between different RDMA write-complete events: Ack, control message, tensor DMA write and tensor proto write.
* **class RdmaWriteID**        - Context for RDMA write complete events. Holds the RdmaWriteIDType and additional data.
* **class RemoteAddressContext** - Remote address information (address + mr). Will be passed as write context for tensor proto writes.
* **class RdmaTensorMetaData** - Meta-data for a tensor (type, shape, is_dead, proto_size).
* **class RdmaMemoryMgr**      - Manages the meta-data cache, and the registered memory regions.
* **class RdmaTensorRequest**    - Holds and manages information for a single tensor request throughout the entire receive cycle. API:
	* **Start()**                - Start the request sequence.
		* Allocate the result tensor (and proxy tensor if required).
		* Send RDMA_MESSAGE_TENSOR_REQUEST to the remote side.
	* **RecvTensorMetaData()**   - Receive meta-data from the remote side.
		* Update the local meta-data cache.
		* Reallocate the result tensor (and proxy tensor if required).
		* Re-send the request to the remote side.
	* **RecvTensorContent()**    - Receive tensor content from the remote side (RDMA write was completed).
		* Decode proto if required and/or move to GPU if the content was not written to it directly (GPU direct is not avaliable).
		* Invoke the done callback.
* **class RdmaTensorResponse**   - Holds and manages information for a single tensor response throughout the entire send cycle. API:
	* **Start()**                - Start the response sequence. 
		* Find the tensor in the local tag-match table.
		* Compare the tensor's meta-data to the meta-data in the message (taken from the requester's local cache). 
			* If meta-data changed:
				* Clone the tensor to be sent later.
				* Send a meta-data update message and wait for re-request.
			* Else:
				* Send the tensor's content (using direct RDMA write).
	* **Resume()**               - Resume the response sequence after a re-request. Send the tensor's content that was cloned earlier.
	* **Destroy()**              - Destroy the response's resources and remove it form the pending list.

### Protocol changes:

The protocol messages themselves will remain mostly unchanged at the first stage, but will be used differently, as described below. The current messages structures already have most of the required fields for the new implementation. The only change is the ""buffer_size"" field which is no longer used since we are no longer sending additional information with the tensor, and thus it is now always equal to the ""tensor_bytes"" field. Instead, we use that field to pass the ""request_index"".

### Message structure:

| type | name_size | name | step_id | request_index | remote_addr | rkey | is_dead | data_type | tensor_shape | tensor_bytes |
|------|---------- |------|---------|---------------|-------------|------|---------|-----------|--------------|--------------|
|  1B  |    2B     | 512  |  8B     |      8B       |         8B  |   4B |      1B |     XB    |    XB        |    8B        |

* **RDMA_MESSAGE_TENSOR_REQUEST**  - (receiver ==> sender) The original tensor request. 
	* type - The message type.
	* name (name_size) - Name of the requested tensor.
	* step_id - Step ID.
	* request_index - Request index.
	* remote_addr/rkey - Address/rkey of the result/proxy tensor. Irrelevant for first-time request.
	* is_dead/data_type/tensor_shape/tensor_bytes - The current meta-data as stored in the receiver local cache. The sender will use that information to know if the receiver's cache requires updating.
* **RDMA_MESSAGE_BUFFER_REQUEST**  - (sender ==> receiver) The meta-data update message in case meta-data had changed (or if it is the first time the tensor is requested).
	* type - The message type.
	* request_index - Request index.
	* is_dead/data_type/tensor_shape/tensor_bytes - The up-to-date meta-data.

  **Note:** At phase 2 this message is renamed to - **RDMA_MESSAGE_META_DATA_UPDATE**.
* **RDMA_MESSAGE_BUFFER_RESPONSE** - (receiver ==> sender) Tensor re-requset after meta-data update and reallocation of result/proxy tensors.
	* type - The message type.
	* name (name_size) - Name of the requested tensor.
	* step_id - Step ID.
	* request_index - Request index.
	* remote_addr/rkey - Address/rkey of the reallocated result/proxy tensor.
	* is_dead/data_type/tensor_shape/tensor_bytes - The new meta-data. Will be removed in the next phase.

  **Note:** At phase 2 this message is renamed to - **RDMA_MESSAGE_TENSOR_RE_REQUEST**.
* **RDMA_MESSAGE_TENSOR_WRITE**    - (sender ==> receiver) No longer sent. There is only a direct write of the tensor content to the result/proxy tensor. Request index passed as the immediate value of the write.
* **RDMA_MESSAGE_TENSOR_IDLE**     - (receiver ==> sender) No longer sent.

### Phase 1:
![alt text](https://raw.githubusercontent.com/Mellanox/tensorflow/eladw_verbs_w_0_copies/tensorflow/contrib/verbs/verbs_with_0_copies_phase1_protocol.jpg ""Phase 1 transport protocol"")

### Phase 2:
![alt text](https://raw.githubusercontent.com/Mellanox/tensorflow/eladw_verbs_w_0_copies/tensorflow/contrib/verbs/verbs_with_0_copies.png ""Phase 2 transport protocol"")

### Second stage optimizations:
1. Remove unused code leftovers - Done.
2. Remove the ACK buffer completely, since we can rely completely on its immediate value - Done.

### Future optimizations:
1. Map the tensor names to indexes, to significantly reduce the request message size.
2. Understand the purpose of empty tensors and if we can skip remote fetching for them.
3. Consider concatenating multiple requests and/or using multiple message buffers.
4. Consider a no-request architecture.
  
 ",2018-01-10T12:18:11Z,10,2,2,9.256949171184875
402,16003,Adding meta_graph_be.pb testdata for big endian for framework_meta_graph_test,cla: yes,,1,,9,2018-01-10T09:11:05Z,2018-01-23T20:02:57Z,CONTRIBUTOR,,2018-01-16T06:34:03Z,13,2,2,7.756949171184876
403,16002,fix a_ to allocator_,cla: yes,"```bash
[07:05:58]	[Step 1/1] In file included from tensorflow/core/common_runtime/threadpool_device.cc:32:0:
[07:05:58]	[Step 1/1] ./tensorflow/core/common_runtime/mkl_cpu_allocator.h: In member function 'virtual void tensorflow::MklCPUAllocator::ClearStats()':
[07:05:58]	[Step 1/1] ./tensorflow/core/common_runtime/mkl_cpu_allocator.h:120:32: error: 'a_' was not declared in this scope
[07:05:58]	[Step 1/1]    void ClearStats() override { a_->ClearStats(); }
[07:05:58]	[Step 1/1]                                 ^
```

Please review this PR ASAP... @yuefengz ",0,,5,2018-01-10T07:55:45Z,2018-01-11T05:18:01Z,NONE,"```bash
[07:05:58]	[Step 1/1] In file included from tensorflow/core/common_runtime/threadpool_device.cc:32:0:
[07:05:58]	[Step 1/1] ./tensorflow/core/common_runtime/mkl_cpu_allocator.h: In member function 'virtual void tensorflow::MklCPUAllocator::ClearStats()':
[07:05:58]	[Step 1/1] ./tensorflow/core/common_runtime/mkl_cpu_allocator.h:120:32: error: 'a_' was not declared in this scope
[07:05:58]	[Step 1/1]    void ClearStats() override { a_->ClearStats(); }
[07:05:58]	[Step 1/1]                                 ^
```

Please review this PR ASAP... @yuefengz ",2018-01-11T05:18:01Z,1,1,0,3.7569491711848753
404,15998,tensorflow input/output tensor reshape c++,stat:awaiting response,"currently , I am working on loading and testing a tensorflow model on android using c++, and the trained model is a full convolutional model, so the input need to be dynamically reshaped according to input image size.

I can make this done easily using python. but when turn to c++ , I can hardly find much examples and experience on this.

the trained model is converted to *.pb file , and the input and output tensor shape has been specified before conversion in python. and now I want to reshape the input and output in c++ before using the model.",0,,2,2018-01-10T05:53:51Z,2018-01-11T02:22:41Z,NONE,"currently , I am working on loading and testing a tensorflow model on android using c++, and the trained model is a full convolutional model, so the input need to be dynamically reshaped according to input image size.

I can make this done easily using python. but when turn to c++ , I can hardly find much examples and experience on this.

the trained model is converted to *.pb file , and the input and output tensor shape has been specified before conversion in python. and now I want to reshape the input and output in c++ before using the model.",2018-01-10T13:17:17Z,1,1,0,2.2569491711848753
405,15995,/home/hp/.cache/bazel/_bazel_hp/a7dace51e7355e610cd60a2c24b75c6d/external/astor_archive/BUILD:8:1: Converting to Python 3: external/astor_archive/astor/source_repr.py failed (Exit 1). ,type:support,"

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:source
- **TensorFlow version (use command below)**:clone from git
- **Python version**: 3.6.3
- **Bazel version (if compiling from source)**:0.5.4
- **GCC/Compiler version (if compiling from source)**:5.4.0
- **CUDA/cuDNN version**:9.0/7.0
- **GPU model and memory**:GTX1070ti  8GB
- **Exact command to reproduce**:


### Describe the problem
build error.when I finished ./configure and run bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package, met an error.

### Source code / logsExtracting Bazel installation...
..............
WARNING: /home/hp/Downloads/tensorflow/tensorflow/core/BUILD:1825:1: in includes attribute of cc_library rule //tensorflow/core:framework_headers_lib: '../../external/nsync/public' resolves to 'external/nsync/public' not below the relative path of its package 'tensorflow/core'. This will be an error in the future. Since this rule was created by the macro 'cc_header_only_library', the error might have been caused by the macro implementation in /home/hp/Downloads/tensorflow/tensorflow/tensorflow.bzl:1152:30.
WARNING: /home/hp/.cache/bazel/_bazel_hp/a7dace51e7355e610cd60a2c24b75c6d/external/grpc/WORKSPACE:1: Workspace name in /home/hp/.cache/bazel/_bazel_hp/a7dace51e7355e610cd60a2c24b75c6d/external/grpc/WORKSPACE (@com_github_grpc_grpc) does not match the name given in the repository's definition (@grpc); this will cause a build error in future versions.
WARNING: /home/hp/Downloads/tensorflow/tensorflow/contrib/learn/BUILD:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': No longer supported. Switch to SavedModel immediately.
WARNING: /home/hp/Downloads/tensorflow/tensorflow/contrib/learn/BUILD:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': No longer supported. Switch to SavedModel immediately.
INFO: Found 1 target...
ERROR: /home/hp/.cache/bazel/_bazel_hp/a7dace51e7355e610cd60a2c24b75c6d/external/astor_archive/BUILD:8:1: Converting to Python 3: external/astor_archive/astor/source_repr.py failed (Exit 1).

  ",0,,1,2018-01-10T03:41:02Z,2018-01-11T00:22:57Z,NONE,"

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:source
- **TensorFlow version (use command below)**:clone from git
- **Python version**: 3.6.3
- **Bazel version (if compiling from source)**:0.5.4
- **GCC/Compiler version (if compiling from source)**:5.4.0
- **CUDA/cuDNN version**:9.0/7.0
- **GPU model and memory**:GTX1070ti  8GB
- **Exact command to reproduce**:


### Describe the problem
build error.when I finished ./configure and run bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package, met an error.

### Source code / logsExtracting Bazel installation...
..............
WARNING: /home/hp/Downloads/tensorflow/tensorflow/core/BUILD:1825:1: in includes attribute of cc_library rule //tensorflow/core:framework_headers_lib: '../../external/nsync/public' resolves to 'external/nsync/public' not below the relative path of its package 'tensorflow/core'. This will be an error in the future. Since this rule was created by the macro 'cc_header_only_library', the error might have been caused by the macro implementation in /home/hp/Downloads/tensorflow/tensorflow/tensorflow.bzl:1152:30.
WARNING: /home/hp/.cache/bazel/_bazel_hp/a7dace51e7355e610cd60a2c24b75c6d/external/grpc/WORKSPACE:1: Workspace name in /home/hp/.cache/bazel/_bazel_hp/a7dace51e7355e610cd60a2c24b75c6d/external/grpc/WORKSPACE (@com_github_grpc_grpc) does not match the name given in the repository's definition (@grpc); this will cause a build error in future versions.
WARNING: /home/hp/Downloads/tensorflow/tensorflow/contrib/learn/BUILD:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': No longer supported. Switch to SavedModel immediately.
WARNING: /home/hp/Downloads/tensorflow/tensorflow/contrib/learn/BUILD:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': No longer supported. Switch to SavedModel immediately.
INFO: Found 1 target...
ERROR: /home/hp/.cache/bazel/_bazel_hp/a7dace51e7355e610cd60a2c24b75c6d/external/astor_archive/BUILD:8:1: Converting to Python 3: external/astor_archive/astor/source_repr.py failed (Exit 1).

  ",2018-01-11T00:22:55Z,1,1,0,1.7569491711848753
406,15994,Feature request: (documentation) operation complexity / performance chart,,"* Have I written custom code: NA
* OS Platform and Distribution: Any
* TensorFlow installed from: NA
* TensorFlow version: NA
* Bazel version: NA
* CUDA/cuDNN version: NA
* GPU model and memory: NA
* Exact command to reproduce: NA

It would be interesting to have a complexity/performance chart for different operations. For example, to know that `tf.reshape` is computationally cheaper than `tf.transpose`. 

I did see the [Performance Guide](https://www.tensorflow.org/performance/performance_guide), but that's not what I mean.",1,,6,2018-01-10T03:08:58Z,2018-02-15T14:22:10Z,NONE,"* Have I written custom code: NA
* OS Platform and Distribution: Any
* TensorFlow installed from: NA
* TensorFlow version: NA
* Bazel version: NA
* CUDA/cuDNN version: NA
* GPU model and memory: NA
* Exact command to reproduce: NA

It would be interesting to have a complexity/performance chart for different operations. For example, to know that `tf.reshape` is computationally cheaper than `tf.transpose`. 

I did see the [Performance Guide](https://www.tensorflow.org/performance/performance_guide), but that's not what I mean.",2018-01-10T13:17:05Z,35,1,4,5.256949171184875
407,15993,Fix typos,"awaiting testing (then merge),cla: yes","This PR fixes some typos: `refered`, `ouptuts`, `from from`, `suport`, `whithin`, `posibility`, and `then then`.",1,,2,2018-01-10T02:59:48Z,2018-01-11T03:07:31Z,CONTRIBUTOR,"This PR fixes some typos: `refered`, `ouptuts`, `from from`, `suport`, `whithin`, `posibility`, and `then then`.",2018-01-11T02:15:41Z,1,2,0,4.256949171184875
408,15991,Hide MSVC workaround from Clang on Windows,"awaiting testing (then merge),cla: yes",#15990,0,,2,2018-01-10T01:51:56Z,2018-01-11T20:58:23Z,CONTRIBUTOR,#15990,2018-01-11T20:19:47Z,1,2,0,3.2569491711848753
409,15989,Fix freeze_graph command line argument error.,"awaiting review,cla: yes",Fix TypeError: main() missing 1 required positional argument: 'unused_args' when using freeze_graph command line tool (pip console script entry point),1,,7,2018-01-10T00:12:56Z,2018-01-30T00:05:57Z,NONE,Fix TypeError: main() missing 1 required positional argument: 'unused_args' when using freeze_graph command line tool (pip console script entry point),2018-01-10T00:27:42Z,20,1,3,5.756949171184875
410,15988,Add internal release notes that were previously missing.,cla: yes,"I wasn't sure about some of the Important/Other changes, so please double-check
that I haven't missed anything actually critical.",0,,1,2018-01-09T23:45:17Z,2018-01-10T19:29:28Z,MEMBER,"I wasn't sure about some of the Important/Other changes, so please double-check
that I haven't missed anything actually critical.",2018-01-10T00:28:45Z,1,3,0,3.7556222186353314
411,15987,"Documentation for placeholder does not explain when shape is (), [] or [None]",type:docs,"### System information

Not necessary.

### Describe the problem

The documentation for `placeholder` does not explain the case when its shape is `()`, `[]` or `[None]`.

### Possible solution

Add the explanations in [this SO  answer](https://stackoverflow.com/a/46941087/3924118) to the documentation of `placeholder`, including the example !!

  ",0,,2,2018-01-09T23:28:05Z,2018-01-10T01:01:55Z,NONE,"### System information

Not necessary.

### Describe the problem

The documentation for `placeholder` does not explain the case when its shape is `()`, `[]` or `[None]`.

### Possible solution

Add the explanations in [this SO  answer](https://stackoverflow.com/a/46941087/3924118) to the documentation of `placeholder`, including the example !!

  ",2018-01-10T01:01:55Z,1,1,0,2.2556222186353314
412,15986,Add new internal release notes that were missed in the previous iteration.,cla: no,,0,,2,2018-01-09T23:22:48Z,2018-01-09T23:23:49Z,MEMBER,,2018-01-09T23:23:48Z,0,3,0,4.255622218635331
413,15985,Feature Request: Dense to Sparse and Dense to Sparse Tensor Ops,"stat:contributions welcome,type:feature","I think it would be helpful if there is a dense_to_sparse op in Tensorflow for ops like `ctc_loss` that requires sparse labels. I'm not really sure where else it can be used aside from that but in case only `ctc_loss` uses it, I think it would help if dense labels can be passed into `ctc_loss` and do the conversion within.",0,,24,2018-01-09T23:19:33Z,2018-01-15T04:43:23Z,CONTRIBUTOR,"I think it would be helpful if there is a dense_to_sparse op in Tensorflow for ops like `ctc_loss` that requires sparse labels. I'm not really sure where else it can be used aside from that but in case only `ctc_loss` uses it, I think it would help if dense labels can be passed into `ctc_loss` and do the conversion within.",2018-01-10T00:56:54Z,6,2,1,14.255622218635331
414,15983,Feature request: Reduce learning rate on plateau,"stat:awaiting tensorflower,type:feature","Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes. But applies to stock examples as well.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 16.04.
- **TensorFlow installed from (source or binary)**:
Binary.
- **TensorFlow version (use command below)**:
v1.4.0-19-ga52c8d9 1.4.1
- **Python version**: 
Python 3.5.4
- **Bazel version (if compiling from source)**:
N/A
- **GCC/Compiler version (if compiling from source)**:
N/A
- **CUDA/cuDNN version**:
CUDA: V8.0.61
cuDNN: 6.0.21
- **GPU model and memory**:
GTX 1080Ti 11GB running driver version 384.98
- **Exact command to reproduce**:
N/A. However, I am using the Experiment, Estimator and Dataset APIs in order to do training.

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
I would like to reduce the learning rate during training. However, I do not want to treat this as another tunable hyperparameter, so I would like this to be based on performance plateauing. In Keras, it is easy to implement learning rate reduction by monitoring the validation loss using the ReduceLROnPlateau callback function, but in TensorFlow this does not seem to be the case/easy. I certainly haven't found any implementation of this, so I propose this as a feature request.

Feel free to close this if this is not the correct forum.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",0,,4,2018-01-09T22:49:36Z,2018-01-11T17:51:34Z,NONE,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes. But applies to stock examples as well.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 16.04.
- **TensorFlow installed from (source or binary)**:
Binary.
- **TensorFlow version (use command below)**:
v1.4.0-19-ga52c8d9 1.4.1
- **Python version**: 
Python 3.5.4
- **Bazel version (if compiling from source)**:
N/A
- **GCC/Compiler version (if compiling from source)**:
N/A
- **CUDA/cuDNN version**:
CUDA: V8.0.61
cuDNN: 6.0.21
- **GPU model and memory**:
GTX 1080Ti 11GB running driver version 384.98
- **Exact command to reproduce**:
N/A. However, I am using the Experiment, Estimator and Dataset APIs in order to do training.

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
I would like to reduce the learning rate during training. However, I do not want to treat this as another tunable hyperparameter, so I would like this to be based on performance plateauing. In Keras, it is easy to implement learning rate reduction by monitoring the validation loss using the ReduceLROnPlateau callback function, but in TensorFlow this does not seem to be the case/easy. I certainly haven't found any implementation of this, so I propose this as a feature request.

Feel free to close this if this is not the correct forum.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",2018-01-11T00:48:57Z,2,1,0,3.2556222186353314
415,15981,tf.Estimator creates loss and loss_1 for eval/train,type:support,"- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Ubuntu 14.12
- **TensorFlow installed from (source or binary)**:
binary
- **TensorFlow version (use command below)**:
1.4
- **Python version**: 
2.7

### Describe the problem

When using the tf.Estimator, the summary files save out summaries for the loss variable evaluated every checkpoint.  The summary for the training, is saved as 'loss_1' .  I got this tensorboard by running the ciphar10_estimator code located: https://github.com/tensorflow/models/tree/master/tutorials/image/cifar10_estimator/

This makes it difficult to compare the eval/train loss on the same graph in tensorboard.  What causes this naming issue and what can be done to fix it?

Thanks!

![screen shot 2018-01-09 at 12 02 53 pm](https://user-images.githubusercontent.com/22623388/34740703-311a2fc6-f535-11e7-9f88-ab16f65052ee.png)

  ",0,,4,2018-01-09T20:04:17Z,2018-01-11T01:24:05Z,NONE,"- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Ubuntu 14.12
- **TensorFlow installed from (source or binary)**:
binary
- **TensorFlow version (use command below)**:
1.4
- **Python version**: 
2.7

### Describe the problem

When using the tf.Estimator, the summary files save out summaries for the loss variable evaluated every checkpoint.  The summary for the training, is saved as 'loss_1' .  I got this tensorboard by running the ciphar10_estimator code located: https://github.com/tensorflow/models/tree/master/tutorials/image/cifar10_estimator/

This makes it difficult to compare the eval/train loss on the same graph in tensorboard.  What causes this naming issue and what can be done to fix it?

Thanks!

![screen shot 2018-01-09 at 12 02 53 pm](https://user-images.githubusercontent.com/22623388/34740703-311a2fc6-f535-11e7-9f88-ab16f65052ee.png)

  ",2018-01-10T00:07:50Z,2,1,0,3.2556222186353314
416,15980,While loop randomly doesn't evaluate tensors,stat:awaiting tensorflower,"Hello!
I believe to have found a bug in Tensorflow when running the code below. I am currently trying to build a neural transducer, and have stumbled across TF sometimes not returning any values for a tensor. I have not had the chance yet to test this out on another machine (no GPU, TF 1.4.1, Ubuntu 17.10). The code is redacted a bit to highlight only the parts that fail. [I've also posted to StackOverflow](https://stackoverflow.com/questions/48081063/tensorflow-non-deterministic-behaviour-with-large-model-using-while-loop) but didn't get any response there.

Notes:

- I believe the bug occurs around line 160, in the body of the while loop in the function run_full_transducer
- The session is returning [encoder_outputs, transducer_outputs]
- I do not use random functions
- As far as I can tell, if I remove the Print OP in line 164, the output is always 0

Example of a correct return value (more or less):
```
array([[[ 0.00811536, -0.00200322, -0.01177037,  0.03676344, -0.01909475,
             -0.03157664,  0.026092  ,  0.02367685, -0.01894805,  0.02832799,
              0.0377345 , -0.02583589, -0.02908566,  0.0299024 ,  0.00518877,
             -0.00064737,  0.01431572, -0.01053502, -0.01783628, -0.00382657,
              0.00076749, -0.02705991,  0.00112415, -0.0193013 ,  0.02346764,
              0.03014467,  0.02663364,  0.02503882,  0.03362656, -0.01877708,
              0.01859642,  0.02460729, -0.01395229, -0.03033791,  0.01177907,
             -0.03049169, -0.00389978,  0.02221515, -0.00073605,  0.01248251,
              0.00424051,  0.01070387,  0.02818898,  0.0321721 , -0.02462685,
              0.03495178, -0.02408989, -0.02742486,  0.00331823, -0.02311424,
             -0.01327039,  0.01095297,  0.02584363,  0.02083527, -0.01588045,
              0.02837921,  0.02100117,  0.00918638,  0.00109535, -0.02965789,
              0.01040822, -0.03240473,  0.00453057, -0.00603903]],
    
           [[ 0.01053647, -0.00457577, -0.01939731,  0.06317309, -0.03113565,
             -0.05525927,  0.04647589,  0.04213476, -0.03498235,  0.04962765,
              0.05989208, -0.04340284, -0.04777668,  0.05346756,  0.00395604,
             -0.0005207 ,  0.02079381, -0.01424338, -0.02584206, -0.00530154,
             -0.00031365, -0.04966826, -0.00091683, -0.03025239,  0.04526306,
              0.0595435 ,  0.0463665 ,  0.04578522,  0.05916505, -0.031725  ,
              0.03164144,  0.04257958, -0.02865831, -0.04795898,  0.01856991,
             -0.05512668, -0.00730711,  0.03953242,  0.00017992,  0.01710426,
              0.00754557,  0.01975578,  0.0469296 ,  0.05237873, -0.04435374,
              0.05924731, -0.04474678, -0.04605344,  0.00947831, -0.04284734,
             -0.01979787,  0.02003288,  0.04196753,  0.03900779, -0.02887472,
              0.05130195,  0.03419674,  0.0105699 ,  0.001114  , -0.0524303 ,
              0.01738651, -0.06084244,  0.01364262, -0.01153531]]], dtype=float32), array([], shape=(0, 1, 3), dtype=float32)]
```
Incorrect:
```
 [array([[[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
              0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
              0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
              0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
              0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]],
    
           [[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
              0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
              0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
              0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
              0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]]], dtype=float32), array([], shape=(0, 1, 3), dtype=float32)]
```

Code:
``` python
 import tensorflow as tf
    from tensorflow.contrib.rnn import LSTMCell, LSTMStateTuple
    from tensorflow.python.layers import core as layers_core
    import numpy as np
    # NOTE: Time major
    
    # Constants
    input_dimensions = 1
    vocab_size = 3
    input_embedding_size = 20
    encoder_hidden_units = 64
    inputs_embedded = True
    transducer_hidden_units = 64
    batch_size = 1
    GO_SYMBOL = vocab_size - 1  # TODO: Make these constants correct
    END_SYMBOL = vocab_size
    input_block_size = 2
    log_prob_init_value = 0
    
    
    # ---------------- Helper classes -----------------------
    
    
    # ----------------- Model -------------------------------
    embeddings = tf.Variable(tf.random_uniform([vocab_size, input_embedding_size], -1.0, 1.0), dtype=tf.float32)
    
    
    class Model(object):
        def __init__(self):
            self.encoder_inputs, self.encoder_inputs_length, self.encoder_hidden_state, \
            self.encoder_outputs, self.encoder_hidden_state_new = self.build_encoder_model()
            self.encoder_raw_outputs, self.trans_hidden_state, self.transducer_amount_outputs, \
            self.transducer_hidden_state_new, self.logits, self.decoder_prediction = self.build_transducer_model()
    
        def build_encoder_model(self):
            encoder_inputs = tf.Variable(tf.zeros(shape=(input_block_size, batch_size, input_dimensions)),
                                         dtype=tf.float32, name='encoder_inputs', trainable=False)
            encoder_inputs_length = tf.Variable([tf.shape(encoder_inputs)[0]], dtype=tf.int32,
                                                name='encoder_inputs_length', trainable=False)
            encoder_hidden_state = tf.Variable(tf.zeros(shape=(2, 1, encoder_hidden_units)), dtype=tf.float32,
                                               name='encoder_hidden_state')  # Save the state as one tensor
    
            if inputs_embedded is True:
                encoder_inputs_embedded = encoder_inputs
            else:
                encoder_inputs_embedded = tf.nn.embedding_lookup(embeddings, encoder_inputs)
    
            # Build model
            encoder_cell = tf.contrib.rnn.LSTMCell(encoder_hidden_units)
    
            # Build previous state
            encoder_hidden_c, encoder_hidden_h = tf.split(encoder_hidden_state, num_or_size_splits=2, axis=0)
            encoder_hidden_c = tf.reshape(encoder_hidden_c, shape=[-1, encoder_hidden_units])
            encoder_hidden_h = tf.reshape(encoder_hidden_h, shape=[-1, encoder_hidden_units])
            encoder_hidden_state_t = LSTMStateTuple(encoder_hidden_c, encoder_hidden_h)
    
            #   encoder_outputs: [max_time, batch_size, num_units]
            encoder_outputs, encoder_hidden_state_new = tf.nn.dynamic_rnn(
                encoder_cell, encoder_inputs_embedded,
                sequence_length=encoder_inputs_length, time_major=True,
                dtype=tf.float32, initial_state=encoder_hidden_state_t)
    
            # Modify output of encoder_hidden_state_new so that it can be fed back in again without problems.
            encoder_hidden_state_new = tf.concat([encoder_hidden_state_new.c, encoder_hidden_state_new.h], axis=0)
            encoder_hidden_state_new = tf.reshape(encoder_hidden_state_new, shape=[2, -1, encoder_hidden_units])
    
            return encoder_inputs, encoder_inputs_length, encoder_hidden_state, encoder_outputs, encoder_hidden_state_new
    
        def build_transducer_model(self):
            encoder_raw_outputs = tf.Variable(tf.zeros(shape=(input_block_size, 1, encoder_hidden_units)),
                                              dtype=tf.float32,
                                              name='encoder_raw_outputs')
            trans_hidden_state = tf.Variable(tf.zeros(shape=(2, 1, transducer_hidden_units)),
                                             dtype=tf.float32,
                                             name='trans_hidden_state')  # Save the state as one tensor
            transducer_amount_outputs = tf.Variable(0, dtype=tf.int32, name='transducer_amount_outputs',
                                                    trainable=False)
    
            # Model building
            helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(
                embedding=embeddings,
                start_tokens=tf.tile([GO_SYMBOL], [batch_size]),
                end_token=END_SYMBOL)
    
            attention_states = tf.transpose(encoder_raw_outputs,
                                            [1, 0, 2])  # attention_states: [batch_size, max_time, num_units]
    
            attention_mechanism = tf.contrib.seq2seq.LuongAttention(
                encoder_hidden_units, attention_states)
    
            decoder_cell = tf.contrib.seq2seq.AttentionWrapper(
                tf.contrib.rnn.LSTMCell(transducer_hidden_units),
                attention_mechanism,
                attention_layer_size=transducer_hidden_units)
    
            projection_layer = layers_core.Dense(vocab_size, use_bias=False)
    
            # Build previous state
            trans_hidden_c, trans_hidden_h = tf.split(trans_hidden_state, num_or_size_splits=2, axis=0)
            trans_hidden_c = tf.reshape(trans_hidden_c, shape=[-1, transducer_hidden_units])
            trans_hidden_h = tf.reshape(trans_hidden_h, shape=[-1, transducer_hidden_units])
            trans_hidden_state_t = LSTMStateTuple(trans_hidden_c, trans_hidden_h)
    
            decoder = tf.contrib.seq2seq.BasicDecoder(
                decoder_cell, helper,
                decoder_cell.zero_state(1, tf.float32).clone(cell_state=trans_hidden_state_t),
                output_layer=projection_layer)
    
            outputs, transducer_hidden_state_new, _ = tf.contrib.seq2seq.dynamic_decode(decoder,
                                                                                        output_time_major=True,
                                                                                        maximum_iterations=transducer_amount_outputs)
            logits = outputs.rnn_output  # logits of shape [max_time,batch_size,vocab_size]
            decoder_prediction = outputs.sample_id  # For debugging
    
            # Modify output of transducer_hidden_state_new so that it can be fed back in again without problems.
            transducer_hidden_state_new = tf.concat(
                [transducer_hidden_state_new[0].c, transducer_hidden_state_new[0].h],
                axis=0)
            transducer_hidden_state_new = tf.reshape(transducer_hidden_state_new,
                                                     shape=[2, -1, transducer_hidden_units])
    
            return encoder_raw_outputs, trans_hidden_state, transducer_amount_outputs, transducer_hidden_state_new, \
                   logits, decoder_prediction
    
    
    model = Model()
    
    
    # ----------------- Alignment -------------------------
    
    # ----------------- Training --------------------------
    
    def run_full_transducer():
        # Inputs
        max_blocks = tf.placeholder(dtype=tf.int32, name='max_blocks')
        inputs_full_raw = tf.placeholder(shape=(None, batch_size, input_dimensions), dtype=tf.float32,
                                         name='inputs_full_raw')
        transducer_list_outputs = tf.placeholder(shape=(None,), dtype=tf.int32,
                                                 name='transducer_list_outputs')  # amount to output per block
    
        # Turn inputs into tensor which is easily readable
        inputs_full = tf.reshape(inputs_full_raw, shape=[max_blocks, input_block_size, batch_size, input_dimensions])
    
        # Outputs
        outputs_ta = tf.TensorArray(dtype=tf.float32, size=max_blocks)
    
        # Hidden states
        # TODO: make these correct
        encoder_hidden_init = tf.ones(shape=(2, 1, encoder_hidden_units))
        trans_hidden_init = tf.ones(shape=(2, 1, transducer_hidden_units))
    
        init_state = (0, outputs_ta, encoder_hidden_init, trans_hidden_init)
    
        def cond(current_block, outputs_int, encoder_hidden, trans_hidden):
            return current_block < max_blocks
    
        def body(current_block, outputs_int, encoder_hidden, trans_hidden):
            # Process encoder
            model.encoder_inputs = model.encoder_inputs.assign(inputs_full[current_block])
            model.encoder_inputs_length = model.encoder_inputs_length.assign([tf.shape(model.encoder_inputs)[0]])
            model.encoder_hidden_state = model.encoder_hidden_state.assign(encoder_hidden)
    
            # TODO: Error is SOMETIMES gone when using tf.Print
            current_block = tf.Print(current_block, [model.encoder_inputs], message='Enc in: ')
            #current_block = tf.Print(current_block, [model.encoder_outputs], message='Enc out: ')
    
            # Flow data from encoder to transducer
            model.encoder_raw_outputs = model.encoder_raw_outputs.assign(model.encoder_outputs)
            model.trans_hidden_state = model.trans_hidden_state.assign(trans_hidden)
            model.transducer_amount_outputs = model.transducer_amount_outputs.assign(transducer_list_outputs[current_block])
    
            # Note the outputs
            outputs_int = outputs_int.write(current_block, model.logits)
    
            return current_block + 1, outputs_int, model.encoder_hidden_state_new, model.transducer_hidden_state_new
    
        _, outputs_final, _, _ = tf.while_loop(cond, body, init_state)
    
        # Process outputs
        outputs = outputs_final.stack()  # Now the outputs are of shape [block, amount_of_trans_out, batch_size, vocab]
        outputs = tf.reshape(outputs, shape=(-1, 1, vocab_size))  # And now its [amount_outputs, batch_size, vocab]
    
        model.encoder_outputs = tf.Print(model.encoder_outputs, [model.encoder_outputs], message='Current block enc out: ')
    
        return max_blocks, inputs_full_raw, transducer_list_outputs, outputs, model.encoder_outputs
    
    # ---------------------- Testing -----------------------------
    
    
    # ---------------------- Management -----------------------------
    
    init = tf.global_variables_initializer()
    
    with tf.Session() as sess:
        sess.run(init)
    
        inp_max_blocks, inp_inputs_full_raw, inp_trans_list_out, out_outputs, enc_out = run_full_transducer()
    
        print sess.run([enc_out, out_outputs], feed_dict={
            inp_max_blocks: 3,
            inp_inputs_full_raw: np.ones(shape=(3 * input_block_size, 1, input_dimensions)),
            inp_trans_list_out: [1, 3, 2]
        })
```
System information:
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 17.10 (Artful Aardvark)
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**:  1.4.1
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: Execute the code block as a python file a few times

Thanks!
Nikita
  
  
  ",0,,6,2018-01-09T19:58:14Z,2018-01-16T10:11:51Z,NONE,"Hello!
I believe to have found a bug in Tensorflow when running the code below. I am currently trying to build a neural transducer, and have stumbled across TF sometimes not returning any values for a tensor. I have not had the chance yet to test this out on another machine (no GPU, TF 1.4.1, Ubuntu 17.10). The code is redacted a bit to highlight only the parts that fail. [I've also posted to StackOverflow](https://stackoverflow.com/questions/48081063/tensorflow-non-deterministic-behaviour-with-large-model-using-while-loop) but didn't get any response there.

Notes:

- I believe the bug occurs around line 160, in the body of the while loop in the function run_full_transducer
- The session is returning [encoder_outputs, transducer_outputs]
- I do not use random functions
- As far as I can tell, if I remove the Print OP in line 164, the output is always 0

Example of a correct return value (more or less):
```
array([[[ 0.00811536, -0.00200322, -0.01177037,  0.03676344, -0.01909475,
             -0.03157664,  0.026092  ,  0.02367685, -0.01894805,  0.02832799,
              0.0377345 , -0.02583589, -0.02908566,  0.0299024 ,  0.00518877,
             -0.00064737,  0.01431572, -0.01053502, -0.01783628, -0.00382657,
              0.00076749, -0.02705991,  0.00112415, -0.0193013 ,  0.02346764,
              0.03014467,  0.02663364,  0.02503882,  0.03362656, -0.01877708,
              0.01859642,  0.02460729, -0.01395229, -0.03033791,  0.01177907,
             -0.03049169, -0.00389978,  0.02221515, -0.00073605,  0.01248251,
              0.00424051,  0.01070387,  0.02818898,  0.0321721 , -0.02462685,
              0.03495178, -0.02408989, -0.02742486,  0.00331823, -0.02311424,
             -0.01327039,  0.01095297,  0.02584363,  0.02083527, -0.01588045,
              0.02837921,  0.02100117,  0.00918638,  0.00109535, -0.02965789,
              0.01040822, -0.03240473,  0.00453057, -0.00603903]],
    
           [[ 0.01053647, -0.00457577, -0.01939731,  0.06317309, -0.03113565,
             -0.05525927,  0.04647589,  0.04213476, -0.03498235,  0.04962765,
              0.05989208, -0.04340284, -0.04777668,  0.05346756,  0.00395604,
             -0.0005207 ,  0.02079381, -0.01424338, -0.02584206, -0.00530154,
             -0.00031365, -0.04966826, -0.00091683, -0.03025239,  0.04526306,
              0.0595435 ,  0.0463665 ,  0.04578522,  0.05916505, -0.031725  ,
              0.03164144,  0.04257958, -0.02865831, -0.04795898,  0.01856991,
             -0.05512668, -0.00730711,  0.03953242,  0.00017992,  0.01710426,
              0.00754557,  0.01975578,  0.0469296 ,  0.05237873, -0.04435374,
              0.05924731, -0.04474678, -0.04605344,  0.00947831, -0.04284734,
             -0.01979787,  0.02003288,  0.04196753,  0.03900779, -0.02887472,
              0.05130195,  0.03419674,  0.0105699 ,  0.001114  , -0.0524303 ,
              0.01738651, -0.06084244,  0.01364262, -0.01153531]]], dtype=float32), array([], shape=(0, 1, 3), dtype=float32)]
```
Incorrect:
```
 [array([[[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
              0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
              0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
              0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
              0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]],
    
           [[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
              0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
              0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
              0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
              0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]]], dtype=float32), array([], shape=(0, 1, 3), dtype=float32)]
```

Code:
``` python
 import tensorflow as tf
    from tensorflow.contrib.rnn import LSTMCell, LSTMStateTuple
    from tensorflow.python.layers import core as layers_core
    import numpy as np
    # NOTE: Time major
    
    # Constants
    input_dimensions = 1
    vocab_size = 3
    input_embedding_size = 20
    encoder_hidden_units = 64
    inputs_embedded = True
    transducer_hidden_units = 64
    batch_size = 1
    GO_SYMBOL = vocab_size - 1  # TODO: Make these constants correct
    END_SYMBOL = vocab_size
    input_block_size = 2
    log_prob_init_value = 0
    
    
    # ---------------- Helper classes -----------------------
    
    
    # ----------------- Model -------------------------------
    embeddings = tf.Variable(tf.random_uniform([vocab_size, input_embedding_size], -1.0, 1.0), dtype=tf.float32)
    
    
    class Model(object):
        def __init__(self):
            self.encoder_inputs, self.encoder_inputs_length, self.encoder_hidden_state, \
            self.encoder_outputs, self.encoder_hidden_state_new = self.build_encoder_model()
            self.encoder_raw_outputs, self.trans_hidden_state, self.transducer_amount_outputs, \
            self.transducer_hidden_state_new, self.logits, self.decoder_prediction = self.build_transducer_model()
    
        def build_encoder_model(self):
            encoder_inputs = tf.Variable(tf.zeros(shape=(input_block_size, batch_size, input_dimensions)),
                                         dtype=tf.float32, name='encoder_inputs', trainable=False)
            encoder_inputs_length = tf.Variable([tf.shape(encoder_inputs)[0]], dtype=tf.int32,
                                                name='encoder_inputs_length', trainable=False)
            encoder_hidden_state = tf.Variable(tf.zeros(shape=(2, 1, encoder_hidden_units)), dtype=tf.float32,
                                               name='encoder_hidden_state')  # Save the state as one tensor
    
            if inputs_embedded is True:
                encoder_inputs_embedded = encoder_inputs
            else:
                encoder_inputs_embedded = tf.nn.embedding_lookup(embeddings, encoder_inputs)
    
            # Build model
            encoder_cell = tf.contrib.rnn.LSTMCell(encoder_hidden_units)
    
            # Build previous state
            encoder_hidden_c, encoder_hidden_h = tf.split(encoder_hidden_state, num_or_size_splits=2, axis=0)
            encoder_hidden_c = tf.reshape(encoder_hidden_c, shape=[-1, encoder_hidden_units])
            encoder_hidden_h = tf.reshape(encoder_hidden_h, shape=[-1, encoder_hidden_units])
            encoder_hidden_state_t = LSTMStateTuple(encoder_hidden_c, encoder_hidden_h)
    
            #   encoder_outputs: [max_time, batch_size, num_units]
            encoder_outputs, encoder_hidden_state_new = tf.nn.dynamic_rnn(
                encoder_cell, encoder_inputs_embedded,
                sequence_length=encoder_inputs_length, time_major=True,
                dtype=tf.float32, initial_state=encoder_hidden_state_t)
    
            # Modify output of encoder_hidden_state_new so that it can be fed back in again without problems.
            encoder_hidden_state_new = tf.concat([encoder_hidden_state_new.c, encoder_hidden_state_new.h], axis=0)
            encoder_hidden_state_new = tf.reshape(encoder_hidden_state_new, shape=[2, -1, encoder_hidden_units])
    
            return encoder_inputs, encoder_inputs_length, encoder_hidden_state, encoder_outputs, encoder_hidden_state_new
    
        def build_transducer_model(self):
            encoder_raw_outputs = tf.Variable(tf.zeros(shape=(input_block_size, 1, encoder_hidden_units)),
                                              dtype=tf.float32,
                                              name='encoder_raw_outputs')
            trans_hidden_state = tf.Variable(tf.zeros(shape=(2, 1, transducer_hidden_units)),
                                             dtype=tf.float32,
                                             name='trans_hidden_state')  # Save the state as one tensor
            transducer_amount_outputs = tf.Variable(0, dtype=tf.int32, name='transducer_amount_outputs',
                                                    trainable=False)
    
            # Model building
            helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(
                embedding=embeddings,
                start_tokens=tf.tile([GO_SYMBOL], [batch_size]),
                end_token=END_SYMBOL)
    
            attention_states = tf.transpose(encoder_raw_outputs,
                                            [1, 0, 2])  # attention_states: [batch_size, max_time, num_units]
    
            attention_mechanism = tf.contrib.seq2seq.LuongAttention(
                encoder_hidden_units, attention_states)
    
            decoder_cell = tf.contrib.seq2seq.AttentionWrapper(
                tf.contrib.rnn.LSTMCell(transducer_hidden_units),
                attention_mechanism,
                attention_layer_size=transducer_hidden_units)
    
            projection_layer = layers_core.Dense(vocab_size, use_bias=False)
    
            # Build previous state
            trans_hidden_c, trans_hidden_h = tf.split(trans_hidden_state, num_or_size_splits=2, axis=0)
            trans_hidden_c = tf.reshape(trans_hidden_c, shape=[-1, transducer_hidden_units])
            trans_hidden_h = tf.reshape(trans_hidden_h, shape=[-1, transducer_hidden_units])
            trans_hidden_state_t = LSTMStateTuple(trans_hidden_c, trans_hidden_h)
    
            decoder = tf.contrib.seq2seq.BasicDecoder(
                decoder_cell, helper,
                decoder_cell.zero_state(1, tf.float32).clone(cell_state=trans_hidden_state_t),
                output_layer=projection_layer)
    
            outputs, transducer_hidden_state_new, _ = tf.contrib.seq2seq.dynamic_decode(decoder,
                                                                                        output_time_major=True,
                                                                                        maximum_iterations=transducer_amount_outputs)
            logits = outputs.rnn_output  # logits of shape [max_time,batch_size,vocab_size]
            decoder_prediction = outputs.sample_id  # For debugging
    
            # Modify output of transducer_hidden_state_new so that it can be fed back in again without problems.
            transducer_hidden_state_new = tf.concat(
                [transducer_hidden_state_new[0].c, transducer_hidden_state_new[0].h],
                axis=0)
            transducer_hidden_state_new = tf.reshape(transducer_hidden_state_new,
                                                     shape=[2, -1, transducer_hidden_units])
    
            return encoder_raw_outputs, trans_hidden_state, transducer_amount_outputs, transducer_hidden_state_new, \
                   logits, decoder_prediction
    
    
    model = Model()
    
    
    # ----------------- Alignment -------------------------
    
    # ----------------- Training --------------------------
    
    def run_full_transducer():
        # Inputs
        max_blocks = tf.placeholder(dtype=tf.int32, name='max_blocks')
        inputs_full_raw = tf.placeholder(shape=(None, batch_size, input_dimensions), dtype=tf.float32,
                                         name='inputs_full_raw')
        transducer_list_outputs = tf.placeholder(shape=(None,), dtype=tf.int32,
                                                 name='transducer_list_outputs')  # amount to output per block
    
        # Turn inputs into tensor which is easily readable
        inputs_full = tf.reshape(inputs_full_raw, shape=[max_blocks, input_block_size, batch_size, input_dimensions])
    
        # Outputs
        outputs_ta = tf.TensorArray(dtype=tf.float32, size=max_blocks)
    
        # Hidden states
        # TODO: make these correct
        encoder_hidden_init = tf.ones(shape=(2, 1, encoder_hidden_units))
        trans_hidden_init = tf.ones(shape=(2, 1, transducer_hidden_units))
    
        init_state = (0, outputs_ta, encoder_hidden_init, trans_hidden_init)
    
        def cond(current_block, outputs_int, encoder_hidden, trans_hidden):
            return current_block < max_blocks
    
        def body(current_block, outputs_int, encoder_hidden, trans_hidden):
            # Process encoder
            model.encoder_inputs = model.encoder_inputs.assign(inputs_full[current_block])
            model.encoder_inputs_length = model.encoder_inputs_length.assign([tf.shape(model.encoder_inputs)[0]])
            model.encoder_hidden_state = model.encoder_hidden_state.assign(encoder_hidden)
    
            # TODO: Error is SOMETIMES gone when using tf.Print
            current_block = tf.Print(current_block, [model.encoder_inputs], message='Enc in: ')
            #current_block = tf.Print(current_block, [model.encoder_outputs], message='Enc out: ')
    
            # Flow data from encoder to transducer
            model.encoder_raw_outputs = model.encoder_raw_outputs.assign(model.encoder_outputs)
            model.trans_hidden_state = model.trans_hidden_state.assign(trans_hidden)
            model.transducer_amount_outputs = model.transducer_amount_outputs.assign(transducer_list_outputs[current_block])
    
            # Note the outputs
            outputs_int = outputs_int.write(current_block, model.logits)
    
            return current_block + 1, outputs_int, model.encoder_hidden_state_new, model.transducer_hidden_state_new
    
        _, outputs_final, _, _ = tf.while_loop(cond, body, init_state)
    
        # Process outputs
        outputs = outputs_final.stack()  # Now the outputs are of shape [block, amount_of_trans_out, batch_size, vocab]
        outputs = tf.reshape(outputs, shape=(-1, 1, vocab_size))  # And now its [amount_outputs, batch_size, vocab]
    
        model.encoder_outputs = tf.Print(model.encoder_outputs, [model.encoder_outputs], message='Current block enc out: ')
    
        return max_blocks, inputs_full_raw, transducer_list_outputs, outputs, model.encoder_outputs
    
    # ---------------------- Testing -----------------------------
    
    
    # ---------------------- Management -----------------------------
    
    init = tf.global_variables_initializer()
    
    with tf.Session() as sess:
        sess.run(init)
    
        inp_max_blocks, inp_inputs_full_raw, inp_trans_list_out, out_outputs, enc_out = run_full_transducer()
    
        print sess.run([enc_out, out_outputs], feed_dict={
            inp_max_blocks: 3,
            inp_inputs_full_raw: np.ones(shape=(3 * input_block_size, 1, input_dimensions)),
            inp_trans_list_out: [1, 3, 2]
        })
```
System information:
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 17.10 (Artful Aardvark)
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**:  1.4.1
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: Execute the code block as a python file a few times

Thanks!
Nikita
  
  
  ",2018-01-11T03:02:39Z,7,1,1,4.255622218635331
417,15979,Branch 181341793,cla: yes,,0,,2,2018-01-09T19:16:46Z,2018-01-09T21:47:59Z,MEMBER,,2018-01-09T19:16:57Z,0,3,0,4.255622218635331
418,15975,MKL: Fix for a compilation error caused by a previous commit,"awaiting testing (then merge),cla: yes",,1,,5,2018-01-09T13:05:01Z,2018-01-17T05:27:30Z,CONTRIBUTOR,,2018-01-11T03:11:25Z,8,2,2,5.755622218635331
419,15974,Estimator.predict always loads model checkpoint preventing partially loading checkpoints,stat:awaiting tensorflower,"### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Windows 10
- **TensorFlow installed from (source or binary)**:
pip
- **TensorFlow version (use command below)**:
1.4
- **Python version**: 
3.5

### Describe the problem
When using Tensorflow's Estimator to do predictions, the Estimator always loads the checkpoint in the model_dir. As this is done after the model_fn is called, there is no way to partially load a checkpoint for predictions. For training, I partially load the initial checkpoint in the model_fn which works fine.

I also tried not specifying the model_dir for the Estimator. As the [documentation states](https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator#__init__), this results in the Estimator using a temporary folder. However, as the temporary folder does not contain a checkpoint, I get the error `Could not find trained model in model_dir`.

It looks like there is no way to only partially load a checkpoint for prediction. If so, please provide a way to do this. For me, this is important, because I have a large model with several outputs. For different datasets, some of the outputs have different sizes. However, some of them are the same for all datasets. That's why I want to load them with the same code and only partially, because I don't need to load and run the whole network for this prediction. ",0,,2,2018-01-09T12:56:34Z,2018-01-10T00:21:25Z,CONTRIBUTOR,"### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Windows 10
- **TensorFlow installed from (source or binary)**:
pip
- **TensorFlow version (use command below)**:
1.4
- **Python version**: 
3.5

### Describe the problem
When using Tensorflow's Estimator to do predictions, the Estimator always loads the checkpoint in the model_dir. As this is done after the model_fn is called, there is no way to partially load a checkpoint for predictions. For training, I partially load the initial checkpoint in the model_fn which works fine.

I also tried not specifying the model_dir for the Estimator. As the [documentation states](https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator#__init__), this results in the Estimator using a temporary folder. However, as the temporary folder does not contain a checkpoint, I get the error `Could not find trained model in model_dir`.

It looks like there is no way to only partially load a checkpoint for prediction. If so, please provide a way to do this. For me, this is important, because I have a large model with several outputs. For different datasets, some of the outputs have different sizes. However, some of them are the same for all datasets. That's why I want to load them with the same code and only partially, because I don't need to load and run the whole network for this prediction. ",2018-01-10T00:21:20Z,1,2,0,3.2556222186353314
420,15973,"How to change the model, without any change into android APK file",,"Hello,

I want to make an android app in this way, like we can change model file anytime in future, and it will not require any change into application code, means no need to generate new APK file of application, on any change into model.
In short I want to know, is there anyway to place model file other then assets folder. So that I can refer updated model file anytime from app.

Thanks,
Sumeet Guha.",0,,4,2018-01-09T10:46:45Z,2018-01-29T23:07:03Z,NONE,"Hello,

I want to make an android app in this way, like we can change model file anytime in future, and it will not require any change into application code, means no need to generate new APK file of application, on any change into model.
In short I want to know, is there anyway to place model file other then assets folder. So that I can refer updated model file anytime from app.

Thanks,
Sumeet Guha.",2018-01-09T19:01:48Z,20,1,3,3.2556222186353314
421,15972,Maven Version of tensorflow Java API jar wrongly updated in Documentation,,"### System information
Have I written custom code : N/A
OS Platform and Distribution : N/A
TensorFlow installed from : N/A
TensorFlow version : N/A
Bazel version : N/A
CUDA/cuDNN version : N/A
GPU model and memory : N/A
Exact command to reproduce : N/A

### Describe the problem
https://www.tensorflow.org/install/install_java shows maven version as 1.4.1 

```
<dependency>
  <groupId>org.tensorflow</groupId>
  <artifactId>tensorflow</artifactId>
  <version>1.4.1</version>
</dependency>
```
However, this version is not available in public maven Repositories.
https://mvnrepository.com/artifact/org.tensorflow/tensorflow
Only versions  1.3.0 , 1.4.0, 1.4.0-rc0 and 1.5.0-rc0 are available.
Please correct documentation or release 1.4.1 Versions.

### Source code / logs
N/A
  ",1,,5,2018-01-09T09:44:50Z,2018-01-31T02:51:10Z,NONE,"### System information
Have I written custom code : N/A
OS Platform and Distribution : N/A
TensorFlow installed from : N/A
TensorFlow version : N/A
Bazel version : N/A
CUDA/cuDNN version : N/A
GPU model and memory : N/A
Exact command to reproduce : N/A

### Describe the problem
https://www.tensorflow.org/install/install_java shows maven version as 1.4.1 

```
<dependency>
  <groupId>org.tensorflow</groupId>
  <artifactId>tensorflow</artifactId>
  <version>1.4.1</version>
</dependency>
```
However, this version is not available in public maven Repositories.
https://mvnrepository.com/artifact/org.tensorflow/tensorflow
Only versions  1.3.0 , 1.4.0, 1.4.0-rc0 and 1.5.0-rc0 are available.
Please correct documentation or release 1.4.1 Versions.

### Source code / logs
N/A
  ",2018-01-09T19:01:44Z,22,1,3,4.755622218635331
422,15971,Fix local path for hexagon_graph_execution in sample script,"awaiting review,cla: yes","As Android arch is supported since r1.5, the local path must also be changed.
If not, and error occurs that the file can not be found.

Signed-off-by: MyungSung Kwak <yesmung@gmail.com>",1,,6,2018-01-09T09:17:34Z,2018-02-13T06:46:54Z,CONTRIBUTOR,"As Android arch is supported since r1.5, the local path must also be changed.
If not, and error occurs that the file can not be found.

Signed-off-by: MyungSung Kwak <yesmung@gmail.com>",2018-01-10T08:21:27Z,34,2,4,6.255622218635331
423,15968,Imperfect implementation of tf.losses.mean_pairwise_squared_error,stat:awaiting response,"### System information
- **TensorFlow version**: 1.4.0, 1.4.1, and 1.5.0-rc0 (checked)
- **Have I written custom code**: N/A
- **OS Platform and Distribution**: N/A
- **TensorFlow installed from**: N/A
- **Bazel version**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A

### Describe the problem
The implementation of `tf.losses.mean_pairwise_squared_error` looks imperfect.
For example, as explained in [the API reference of the function](https://www.tensorflow.org/api_docs/python/tf/losses/mean_pairwise_squared_error)
> For example, if `labels`=[a, b, c] and `predictions`=[x, y, z], there are three pairs of differences are summed to compute the loss: loss = [ ((a-b) - (x-y)).^2 + ((a-c) - (x-z)).^2 + ((b-c) - (y-z)).^2 ] / 3

let me put the following data as `labels` and `predictions`:
```
labels = tf.constant([[0., 0.5, 1.]])
predictions = tf.constant([[1., 1., 1.]])
tf.losses.mean_pairwise_squared_error(labels, predictions)
```
In this case, the result should be `[(0-0.5)^2+(0-1)^2+(0.5-1)^2]/3=0.5`, but tensorflow returns different value 0.3333333134651184.

### Suggestion to fix the source code
[tensorflow/python/ops/losses/losses_impl.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/losses/losses_impl.py)

If the loss function `mean_pairwise_squared_error` measures the differences between pairs of corresponding elements of `predictions` and `labels` as explained in [the API reference of the function](https://www.tensorflow.org/api_docs/python/tf/losses/mean_pairwise_squared_error), here is a simple patch:
> (lines 520-521 need to be changed as)
> `term1 = 2.0 * _safe_div(sum_squares_diff_per_batch, num_present_per_batch-1)`
and
> (lines 525-526 need to be changed as)
> `term2 = 2.0 * _safe_div(math_ops.square(sum_diff), math_ops.multiply(num_present_per_batch, num_present_per_batch-1))`
  ",1,,2,2018-01-09T05:51:16Z,2018-02-02T13:37:01Z,CONTRIBUTOR,"### System information
- **TensorFlow version**: 1.4.0, 1.4.1, and 1.5.0-rc0 (checked)
- **Have I written custom code**: N/A
- **OS Platform and Distribution**: N/A
- **TensorFlow installed from**: N/A
- **Bazel version**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A

### Describe the problem
The implementation of `tf.losses.mean_pairwise_squared_error` looks imperfect.
For example, as explained in [the API reference of the function](https://www.tensorflow.org/api_docs/python/tf/losses/mean_pairwise_squared_error)
> For example, if `labels`=[a, b, c] and `predictions`=[x, y, z], there are three pairs of differences are summed to compute the loss: loss = [ ((a-b) - (x-y)).^2 + ((a-c) - (x-z)).^2 + ((b-c) - (y-z)).^2 ] / 3

let me put the following data as `labels` and `predictions`:
```
labels = tf.constant([[0., 0.5, 1.]])
predictions = tf.constant([[1., 1., 1.]])
tf.losses.mean_pairwise_squared_error(labels, predictions)
```
In this case, the result should be `[(0-0.5)^2+(0-1)^2+(0.5-1)^2]/3=0.5`, but tensorflow returns different value 0.3333333134651184.

### Suggestion to fix the source code
[tensorflow/python/ops/losses/losses_impl.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/losses/losses_impl.py)

If the loss function `mean_pairwise_squared_error` measures the differences between pairs of corresponding elements of `predictions` and `labels` as explained in [the API reference of the function](https://www.tensorflow.org/api_docs/python/tf/losses/mean_pairwise_squared_error), here is a simple patch:
> (lines 520-521 need to be changed as)
> `term1 = 2.0 * _safe_div(sum_squares_diff_per_batch, num_present_per_batch-1)`
and
> (lines 525-526 need to be changed as)
> `term2 = 2.0 * _safe_div(math_ops.square(sum_diff), math_ops.multiply(num_present_per_batch, num_present_per_batch-1))`
  ",2018-01-09T13:00:03Z,23,2,3,4.255622218635331
424,15967,Make graph transform tool accessible via command line for pip install.,"awaiting review,cla: yes","RELNOTE: Make graph transform tool available from command line as
`transform_graph` for pip package.
Fix  #13287.",1,,1,2018-01-09T05:23:56Z,2018-01-24T07:43:36Z,MEMBER,"RELNOTE: Make graph transform tool available from command line as
`transform_graph` for pip package.
Fix  #13287.",2018-01-23T21:41:49Z,15,3,3,4.755622218635331
425,15966,remove write_version=saver_pb2.SaverDef.V1,"awaiting review,cla: yes",This PR fixes the failed testAdditionalHooks and testRestoredModelPerformance test for PR #14341,1,,3,2018-01-09T04:36:20Z,2018-02-06T18:44:49Z,CONTRIBUTOR,This PR fixes the failed testAdditionalHooks and testRestoredModelPerformance test for PR #14341,2018-02-06T14:56:13Z,27,2,3,4.755622218635331
426,15964,DownloadfileTask Failed,stat:awaiting response,"try projrct as https://www.tensorflow.org/mobile/android_build#android_sample_apps,but downloadtask failed,  then solve it ,may be you shoule change the 

> download-models.gradle  classpath 'de.undercouch:gradle-download-task:3.2.0' to 3.3.0",0,,2,2018-01-09T03:37:07Z,2018-01-30T10:07:20Z,NONE,"try projrct as https://www.tensorflow.org/mobile/android_build#android_sample_apps,but downloadtask failed,  then solve it ,may be you shoule change the 

> download-models.gradle  classpath 'de.undercouch:gradle-download-task:3.2.0' to 3.3.0",2018-01-09T12:59:53Z,21,1,3,2.2556222186353314
427,15961,Adding cuda_config.h to the pip package.,cla: yes,,0,,2,2018-01-09T01:13:19Z,2018-01-11T07:14:13Z,MEMBER,,2018-01-11T01:24:09Z,2,3,0,4.255622218635331
428,15960,Branch 181239691,cla: yes,,0,,2,2018-01-09T01:09:42Z,2018-01-09T19:10:06Z,MEMBER,,2018-01-09T01:10:13Z,0,3,0,4.255622218635331
429,15959,Adding an install sources line for 1.5.0-rc0. Earlier we only updated,cla: yes, this for official.,0,,3,2018-01-08T22:21:15Z,2018-01-10T20:15:12Z,MEMBER, this for official.,2018-01-08T23:36:06Z,2,3,0,4.754334778144042
430,15957,Switching branch and run ./configure does not regenerate spec.json,type:build/install,"When building from source with TensorFlow and switch to another branch, error returned even if I rerun `./configure`:

```
ubuntu@ubuntu:~/tensorflow$ bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package
..........
ubuntu@ubuntu:~/tensorflow$ git checkout -b test
ubuntu@ubuntu:~/tensorflow$ ./configure
..........
ubuntu@ubuntu:~/tensorflow$ bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package
..........
INFO: Analysed target //tensorflow/tools/pip_package:build_pip_package (252 packages loaded).
INFO: Found 1 target...
ERROR: /home/ubuntu/tensorflow/tensorflow/core/BUILD:1671:1: Executing genrule //tensorflow/core:version_info_gen failed (Exit 1)
Traceback (most recent call last):
  File ""tensorflow/tools/git/gen_git_source.py"", line 284, in <module>
    generate(args.generate)
  File ""tensorflow/tools/git/gen_git_source.py"", line 229, in generate
    (old_branch, new_branch))
RuntimeError: Run ./configure again, branch was 'refs/heads/master' but is now 'refs/heads/test'
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 9.025s, Critical Path: 0.30s
FAILED: Build did NOT complete successfully
```


I think the issue is that `spec.json` is not updated when running `./configure`


```
ubuntu@ubuntu:~/tensorflow$ cat /home/ubuntu/.cache/bazel/_bazel_ubuntu/ad1e09741bb4109fbc70ef8216b59ee2/external/local_config_git/gen/spec.json
{
  ""path"": ""/home/ubuntu/.cache/bazel/_bazel_ubuntu/ad1e09741bb4109fbc70ef8216b59ee2/external/org_tensorflow/"", 
  ""git"": true, 
  ""branch"": ""refs/heads/master""
}
ubuntu@ubuntu:~/tensorflow$ 
```

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: master
- **Python version**:  2.7.12
- **Bazel version (if compiling from source)**: 0.9.0
- **GCC/Compiler version (if compiling from source)**: gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.5) 
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**:

```sh
git checkout -b test
./configure
bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package
```

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

  ",1,,3,2018-01-08T21:07:43Z,2018-01-19T21:40:07Z,MEMBER,"When building from source with TensorFlow and switch to another branch, error returned even if I rerun `./configure`:

```
ubuntu@ubuntu:~/tensorflow$ bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package
..........
ubuntu@ubuntu:~/tensorflow$ git checkout -b test
ubuntu@ubuntu:~/tensorflow$ ./configure
..........
ubuntu@ubuntu:~/tensorflow$ bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package
..........
INFO: Analysed target //tensorflow/tools/pip_package:build_pip_package (252 packages loaded).
INFO: Found 1 target...
ERROR: /home/ubuntu/tensorflow/tensorflow/core/BUILD:1671:1: Executing genrule //tensorflow/core:version_info_gen failed (Exit 1)
Traceback (most recent call last):
  File ""tensorflow/tools/git/gen_git_source.py"", line 284, in <module>
    generate(args.generate)
  File ""tensorflow/tools/git/gen_git_source.py"", line 229, in generate
    (old_branch, new_branch))
RuntimeError: Run ./configure again, branch was 'refs/heads/master' but is now 'refs/heads/test'
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 9.025s, Critical Path: 0.30s
FAILED: Build did NOT complete successfully
```


I think the issue is that `spec.json` is not updated when running `./configure`


```
ubuntu@ubuntu:~/tensorflow$ cat /home/ubuntu/.cache/bazel/_bazel_ubuntu/ad1e09741bb4109fbc70ef8216b59ee2/external/local_config_git/gen/spec.json
{
  ""path"": ""/home/ubuntu/.cache/bazel/_bazel_ubuntu/ad1e09741bb4109fbc70ef8216b59ee2/external/org_tensorflow/"", 
  ""git"": true, 
  ""branch"": ""refs/heads/master""
}
ubuntu@ubuntu:~/tensorflow$ 
```

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: master
- **Python version**:  2.7.12
- **Bazel version (if compiling from source)**: 0.9.0
- **GCC/Compiler version (if compiling from source)**: gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.5) 
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**:

```sh
git checkout -b test
./configure
bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package
```

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

  ",2018-01-11T01:39:12Z,11,3,2,5.754334778144042
431,15955,Branch 181174976,cla: yes,,0,,4,2018-01-08T18:28:33Z,2018-01-09T01:00:25Z,MEMBER,,2018-01-08T18:28:49Z,1,3,0,5.254334778144042
432,15951,[Build] Source build at HEAD generating XLA erros on Mac OS,,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OS High Sierra
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: HEAD@a770968
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: 0.9.0
- **GCC/Compiler version (if compiling from source)**: 9.0.0
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: Build procedures on doc optimized for native arch and XLA enabled.

### Describe the problem
Building TensorFlow on Mac OS with XLA enabled and configuration given above optimized for native arch and CPU only yields the following errors:
```
ERROR: /Users/adriano/MachineLearning/tensorflow/tensorflow/compiler/xla/service/cpu/BUILD:522:1: C++ compilation of rule '//tensorflow/compiler/xla/service/cpu:runtime_fft' failed (Exit 1)
In file included from tensorflow/compiler/xla/service/cpu/runtime_fft.cc:21:
./tensorflow/compiler/xla/service/cpu/runtime_fft_impl.h:42:30: error: implicit instantiation of undefined template 'std::__1::array<long long, 3>'
  const std::array<int64, 3> fft_shape = {
                             ^
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/__tuple:222:64: note: template is declared here
template <class _Tp, size_t _Size> struct _LIBCPP_TEMPLATE_VIS array;
                                                               ^
In file included from tensorflow/compiler/xla/service/cpu/runtime_fft.cc:21:
./tensorflow/compiler/xla/service/cpu/runtime_fft_impl.h:65:30: error: implicit instantiation of undefined template 'std::__1::array<long long, 3>'
  const std::array<int64, 3> fft_shape = {
                             ^
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/__tuple:222:64: note: template is declared here
template <class _Tp, size_t _Size> struct _LIBCPP_TEMPLATE_VIS array;
                                                               ^
In file included from tensorflow/compiler/xla/service/cpu/runtime_fft.cc:21:
./tensorflow/compiler/xla/service/cpu/runtime_fft_impl.h:106:30: error: implicit instantiation of undefined template 'std::__1::array<long long, 3>'
  const std::array<int64, 3> fft_shape = {
                             ^
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/__tuple:222:64: note: template is declared here
template <class _Tp, size_t _Size> struct _LIBCPP_TEMPLATE_VIS array;
                                                               ^
3 errors generated.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 1997.286s, Critical Path: 88.18s
FAILED: Build did NOT complete successfully

```
",1,,2,2018-01-08T17:04:44Z,2018-01-10T21:10:23Z,CONTRIBUTOR,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OS High Sierra
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: HEAD@a770968
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: 0.9.0
- **GCC/Compiler version (if compiling from source)**: 9.0.0
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: Build procedures on doc optimized for native arch and XLA enabled.

### Describe the problem
Building TensorFlow on Mac OS with XLA enabled and configuration given above optimized for native arch and CPU only yields the following errors:
```
ERROR: /Users/adriano/MachineLearning/tensorflow/tensorflow/compiler/xla/service/cpu/BUILD:522:1: C++ compilation of rule '//tensorflow/compiler/xla/service/cpu:runtime_fft' failed (Exit 1)
In file included from tensorflow/compiler/xla/service/cpu/runtime_fft.cc:21:
./tensorflow/compiler/xla/service/cpu/runtime_fft_impl.h:42:30: error: implicit instantiation of undefined template 'std::__1::array<long long, 3>'
  const std::array<int64, 3> fft_shape = {
                             ^
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/__tuple:222:64: note: template is declared here
template <class _Tp, size_t _Size> struct _LIBCPP_TEMPLATE_VIS array;
                                                               ^
In file included from tensorflow/compiler/xla/service/cpu/runtime_fft.cc:21:
./tensorflow/compiler/xla/service/cpu/runtime_fft_impl.h:65:30: error: implicit instantiation of undefined template 'std::__1::array<long long, 3>'
  const std::array<int64, 3> fft_shape = {
                             ^
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/__tuple:222:64: note: template is declared here
template <class _Tp, size_t _Size> struct _LIBCPP_TEMPLATE_VIS array;
                                                               ^
In file included from tensorflow/compiler/xla/service/cpu/runtime_fft.cc:21:
./tensorflow/compiler/xla/service/cpu/runtime_fft_impl.h:106:30: error: implicit instantiation of undefined template 'std::__1::array<long long, 3>'
  const std::array<int64, 3> fft_shape = {
                             ^
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/__tuple:222:64: note: template is declared here
template <class _Tp, size_t _Size> struct _LIBCPP_TEMPLATE_VIS array;
                                                               ^
3 errors generated.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 1997.286s, Critical Path: 88.18s
FAILED: Build did NOT complete successfully

```
",2018-01-10T21:10:23Z,2,2,0,4.254334778144042
433,15949,Building TensorFlow on Windows: patch and rm,type:build/install,"### System information
- **Have I written custom code**: Yes, provided below.
- **OS Platform and Distribution**: Windows 10 1709, Build 16299.192
- **TensorFlow installed from (source or binary)**: Binary, Attempting source build of master
- **TensorFlow version**: 1.4.0
- **Python version**: 3.6
- **Bazel version**:  0.9.0
- **GCC/Compiler version**:  MSYS2 Shell, GCC unknown.
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A, CPU is i7-8550U, 8 GB memory
- **Exact command to reproduce**: Any `bazel build` on Windows. Please see Description.

### Describe the problem
Building TensorFlow on Windows has been a struggle with compatibility due to the fact that for many, MSYS will not run `patch` when installed from the MSYS2 shell. I have found a reliable way to resolve the issue: using Choco to install `patch`, moving patch.exe to a folder FOLDERNAME within its default directory, and then running %FOLDERNAME%/patch.exe with the flag `--binary` (to use CR LF line breaks) with a custom batch script compiled into a executable.

`bazel build` now completes `patch` commands without issue. But as it often is, another hurdle exists to the finish line. Bazel now attempts to recursively force remove a file using `rm -rf`, which obviously does not exist as a package in Choco as a bash command. MSYS will run it, but not from the command line.

Is there any way to get around the use of `rm`, or make a compatible solution for Windows using `del`?

I have ensured that #15829 has been installed. Still fails

If this is better left to the Bazel developers, please close this issue. 

### Source code

#### patch.bat
``` sh
start C:\ProgramData\chocolatey\lib\patch\tools\bin\%FOLDERNAME%\patch.exe --binary
exit
```

### Logs
``` sh
C:\tensorflow>bazel build --config=mkl --config=monolithic -c opt --copt=-march=native --copt=-mmmx --copt=-msse --copt=-msse2 --copt=-msse3 --copt=-mssse3 --copt=-msse4.1 --copt=-msse4.2 --copt=-mavx --copt=-mavx2 --copt=-maes --copt=-mfpmath=both //tensorflow/tools/pip_package:build_pip_package
#ERROR: C:/tensorflow/tensorflow/python/BUILD:4646:1: no such package '@cython//': Traceback (most recent call last):
        File ""C:/tensorflow/third_party/repo.bzl"", line 86
                _apply_delete(ctx, ctx.attr.delete)
        File ""C:/tensorflow/third_party/repo.bzl"", line 68, in _apply_delete
                _execute_and_check_ret_code(ctx, cmd)
        File ""C:/tensorflow/third_party/repo.bzl"", line 44, in _execute_and_check_ret_code
                fail(""Non-zero return code({1}) when ...))
Non-zero return code(127) when executing 'C:\msys64\usr\bin\bash.exe -c rm -rf C:/users/eric/appdata/local/temp/_bazel_eric/x1e5egqw/external/cython/BUILD.bazel':
Stdout:
Stderr: /usr/bin/bash: rm: command not found
 and referenced by '//tensorflow/python:framework/fast_tensor_util.pyx_cython_translation'
ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: Loading failed
INFO: Elapsed time: 61.324s
FAILED: Build did NOT complete successfully (92 packages loaded)
```

  ",1,,3,2018-01-08T15:32:59Z,2018-01-12T01:43:52Z,NONE,"### System information
- **Have I written custom code**: Yes, provided below.
- **OS Platform and Distribution**: Windows 10 1709, Build 16299.192
- **TensorFlow installed from (source or binary)**: Binary, Attempting source build of master
- **TensorFlow version**: 1.4.0
- **Python version**: 3.6
- **Bazel version**:  0.9.0
- **GCC/Compiler version**:  MSYS2 Shell, GCC unknown.
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A, CPU is i7-8550U, 8 GB memory
- **Exact command to reproduce**: Any `bazel build` on Windows. Please see Description.

### Describe the problem
Building TensorFlow on Windows has been a struggle with compatibility due to the fact that for many, MSYS will not run `patch` when installed from the MSYS2 shell. I have found a reliable way to resolve the issue: using Choco to install `patch`, moving patch.exe to a folder FOLDERNAME within its default directory, and then running %FOLDERNAME%/patch.exe with the flag `--binary` (to use CR LF line breaks) with a custom batch script compiled into a executable.

`bazel build` now completes `patch` commands without issue. But as it often is, another hurdle exists to the finish line. Bazel now attempts to recursively force remove a file using `rm -rf`, which obviously does not exist as a package in Choco as a bash command. MSYS will run it, but not from the command line.

Is there any way to get around the use of `rm`, or make a compatible solution for Windows using `del`?

I have ensured that #15829 has been installed. Still fails

If this is better left to the Bazel developers, please close this issue. 

### Source code

#### patch.bat
``` sh
start C:\ProgramData\chocolatey\lib\patch\tools\bin\%FOLDERNAME%\patch.exe --binary
exit
```

### Logs
``` sh
C:\tensorflow>bazel build --config=mkl --config=monolithic -c opt --copt=-march=native --copt=-mmmx --copt=-msse --copt=-msse2 --copt=-msse3 --copt=-mssse3 --copt=-msse4.1 --copt=-msse4.2 --copt=-mavx --copt=-mavx2 --copt=-maes --copt=-mfpmath=both //tensorflow/tools/pip_package:build_pip_package
#ERROR: C:/tensorflow/tensorflow/python/BUILD:4646:1: no such package '@cython//': Traceback (most recent call last):
        File ""C:/tensorflow/third_party/repo.bzl"", line 86
                _apply_delete(ctx, ctx.attr.delete)
        File ""C:/tensorflow/third_party/repo.bzl"", line 68, in _apply_delete
                _execute_and_check_ret_code(ctx, cmd)
        File ""C:/tensorflow/third_party/repo.bzl"", line 44, in _execute_and_check_ret_code
                fail(""Non-zero return code({1}) when ...))
Non-zero return code(127) when executing 'C:\msys64\usr\bin\bash.exe -c rm -rf C:/users/eric/appdata/local/temp/_bazel_eric/x1e5egqw/external/cython/BUILD.bazel':
Stdout:
Stderr: /usr/bin/bash: rm: command not found
 and referenced by '//tensorflow/python:framework/fast_tensor_util.pyx_cython_translation'
ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: Loading failed
INFO: Elapsed time: 61.324s
FAILED: Build did NOT complete successfully (92 packages loaded)
```

  ",2018-01-10T19:08:57Z,4,1,1,3.754334778144042
434,15947,Windows: Override /DEIGEN_STRONG_INLINE=inline for //tensorflow/core/kernels:conv_ops,"awaiting testing (then merge),cla: yes","This change reduces the Windows building time by more than 15 minutes

Fix #10521",0,,7,2018-01-08T15:22:06Z,2018-01-13T04:52:14Z,MEMBER,"This change reduces the Windows building time by more than 15 minutes

Fix #10521",2018-01-09T10:20:33Z,5,3,1,6.754334778144042
435,15946,Support for large number of classes when using tf.metrics.mean_per_class_accuracy(),"awaiting testing (then merge),cla: yes,kokoro:run","`tf.metrics.mean_per_class_accuracy()` uses a `num_classes x num_classes` matrix to keep track of accuracies for each class. This wastes a lot of memory and doesn't work well for large number of classes (e.g. matrix size for 500k classes is 500000^2*4 = 1 terabyte).

By switching to two 1-D variables of size `num_classes` instead, memory usage is reduced considerably. One variable keeps track of correct predictions for each class, while the other variable keeps track of the total number of predictions for each class.",1,,7,2018-01-08T14:49:46Z,2018-01-23T18:22:08Z,CONTRIBUTOR,"`tf.metrics.mean_per_class_accuracy()` uses a `num_classes x num_classes` matrix to keep track of accuracies for each class. This wastes a lot of memory and doesn't work well for large number of classes (e.g. matrix size for 500k classes is 500000^2*4 = 1 terabyte).

By switching to two 1-D variables of size `num_classes` instead, memory usage is reduced considerably. One variable keeps track of correct predictions for each class, while the other variable keeps track of the total number of predictions for each class.",2018-01-22T23:48:42Z,15,2,3,6.754334778144042
436,15941,Import Error: No module named '_pywrap_tensorflow',type:build/install,"On running the following command: import tensorflow I get an error:

`C:\Users\Neerav>python
Python 3.5.0 (v3.5.0:374f501f4567, Sep 13 2015, 02:27:37) [MSC v.1900 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow
Traceback (most recent call last):
  File ""C:\Users\Neerav\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 18, in swig_import_helper
    fp, pathname, description = imp.find_module('_pywrap_tensorflow', [dirname(__file__)])
  File ""C:\Users\Neerav\AppData\Local\Programs\Python\Python35\lib\imp.py"", line 296, in find_module
    raise ImportError(_ERR_MSG.format(name), name=name)
ImportError: No module named '_pywrap_tensorflow'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\Neerav\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\__init__.py"", line 54, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Neerav\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 28, in <module>
    _pywrap_tensorflow = swig_import_helper()
  File ""C:\Users\Neerav\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 20, in swig_import_helper
    import _pywrap_tensorflow
ImportError: No module named '_pywrap_tensorflow'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\Neerav\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""C:\Users\Neerav\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\__init__.py"", line 60, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\Neerav\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 18, in swig_import_helper
    fp, pathname, description = imp.find_module('_pywrap_tensorflow', [dirname(__file__)])
  File ""C:\Users\Neerav\AppData\Local\Programs\Python\Python35\lib\imp.py"", line 296, in find_module
    raise ImportError(_ERR_MSG.format(name), name=name)
ImportError: No module named '_pywrap_tensorflow'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\Neerav\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\__init__.py"", line 54, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Neerav\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 28, in <module>
    _pywrap_tensorflow = swig_import_helper()
  File ""C:\Users\Neerav\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 20, in swig_import_helper
    import _pywrap_tensorflow
ImportError: No module named '_pywrap_tensorflow'


Error importing tensorflow.  Unless you are using bazel,
you should not try to import tensorflow from its source directory;
please exit the tensorflow source tree, and relaunch your python interpreter
from there.`


I have the following system features:

windows 64 bit
python 3.5.0 64 bit
Nvidia computing toolkit/CUDA/v8.0./(the cuDNN version 6.0)
all of them are added to my path location also which is: Python\Python35\Scripts
i have tensorflow in Python\Python35\Lib\site-packages\tensorflow
I even have a _pywrap_tensorflow.so file and pywrap_tensorflow.py",0,,2,2018-01-08T10:46:51Z,2018-01-10T01:16:38Z,NONE,"On running the following command: import tensorflow I get an error:

`C:\Users\Neerav>python
Python 3.5.0 (v3.5.0:374f501f4567, Sep 13 2015, 02:27:37) [MSC v.1900 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow
Traceback (most recent call last):
  File ""C:\Users\Neerav\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 18, in swig_import_helper
    fp, pathname, description = imp.find_module('_pywrap_tensorflow', [dirname(__file__)])
  File ""C:\Users\Neerav\AppData\Local\Programs\Python\Python35\lib\imp.py"", line 296, in find_module
    raise ImportError(_ERR_MSG.format(name), name=name)
ImportError: No module named '_pywrap_tensorflow'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\Neerav\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\__init__.py"", line 54, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Neerav\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 28, in <module>
    _pywrap_tensorflow = swig_import_helper()
  File ""C:\Users\Neerav\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 20, in swig_import_helper
    import _pywrap_tensorflow
ImportError: No module named '_pywrap_tensorflow'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\Neerav\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""C:\Users\Neerav\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\__init__.py"", line 60, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\Neerav\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 18, in swig_import_helper
    fp, pathname, description = imp.find_module('_pywrap_tensorflow', [dirname(__file__)])
  File ""C:\Users\Neerav\AppData\Local\Programs\Python\Python35\lib\imp.py"", line 296, in find_module
    raise ImportError(_ERR_MSG.format(name), name=name)
ImportError: No module named '_pywrap_tensorflow'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\Neerav\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\__init__.py"", line 54, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Neerav\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 28, in <module>
    _pywrap_tensorflow = swig_import_helper()
  File ""C:\Users\Neerav\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 20, in swig_import_helper
    import _pywrap_tensorflow
ImportError: No module named '_pywrap_tensorflow'


Error importing tensorflow.  Unless you are using bazel,
you should not try to import tensorflow from its source directory;
please exit the tensorflow source tree, and relaunch your python interpreter
from there.`


I have the following system features:

windows 64 bit
python 3.5.0 64 bit
Nvidia computing toolkit/CUDA/v8.0./(the cuDNN version 6.0)
all of them are added to my path location also which is: Python\Python35\Scripts
i have tensorflow in Python\Python35\Lib\site-packages\tensorflow
I even have a _pywrap_tensorflow.so file and pywrap_tensorflow.py",2018-01-10T00:20:15Z,2,1,0,2.254334778144042
437,15940,[tensorflow lite] add setUseNNAPI to the Interpreter class,"awaiting review,cla: yes,comp:lite",add setUseNNAPI to the Interpreter class,1,,3,2018-01-08T09:30:18Z,2018-02-16T18:48:59Z,NONE,add setUseNNAPI to the Interpreter class,2018-02-06T14:57:00Z,38,1,4,3.754334778144042
438,15939,Slim VGG losses increase gradually with default training configuration,,"Hi, when I try to train imagenet with slim vgg network with default configuration,
The loss increases gradually from ~0.1 to over 10000. 
I am not even able to debug this issue because, all tensors losses are encapsulated inside slim.
Is there any way to debug this issue? ",0,,3,2018-01-08T09:30:13Z,2018-01-29T23:13:45Z,NONE,"Hi, when I try to train imagenet with slim vgg network with default configuration,
The loss increases gradually from ~0.1 to over 10000. 
I am not even able to debug this issue because, all tensors losses are encapsulated inside slim.
Is there any way to debug this issue? ",2018-01-08T19:02:48Z,21,1,3,2.754334778144042
439,15938,An easy problem about tensorflow tf.reduce_mean op,,"I know... there might not be a suitable palce to ask this question, but I really hope someone cloud help me.

i want to use the ""tf.reduce_mean"" to obtain the mean of an array (ignore the zeros element)
eg:
    data = [[1,2,3],[4,5,6],[0,0,0]]   
    i want to obtain mean= [2.5, 3.5, 4.5]  
    but  tf.reduce_mean op gets the mean=[1.6, 2.3, 3]

Thank you very much!
  ",0,,1,2018-01-08T08:33:27Z,2018-01-08T15:55:30Z,NONE,"I know... there might not be a suitable palce to ask this question, but I really hope someone cloud help me.

i want to use the ""tf.reduce_mean"" to obtain the mean of an array (ignore the zeros element)
eg:
    data = [[1,2,3],[4,5,6],[0,0,0]]   
    i want to obtain mean= [2.5, 3.5, 4.5]  
    but  tf.reduce_mean op gets the mean=[1.6, 2.3, 3]

Thank you very much!
  ",2018-01-08T15:55:30Z,0,1,0,1.7543347781440422
440,15931,Making SQLite better,"awaiting review,cla: yes,stat:awaiting response",,1,,3,2018-01-07T19:35:06Z,2018-02-06T15:36:49Z,CONTRIBUTOR,,2018-02-06T14:57:05Z,29,2,4,4.753084869277317
441,15930,Discontinuity at halfway point in graph output,stat:awaiting response,"- **I have written custom code (as opposed to using a stock example script provided in TensorFlow)**:
to reproduce the error:
1) convert HnH_gate.txt to HnH_gate.py
2) Edit mypath in out() method at end of file for your system.  Save 
3) in python: run HnH_gate.py
4) run out() to create csv files for the good and bad output
            i) out(""101"", new_probka_good)
            ii) out(""102"", new_probka_bad)
5) Plot data from hh_101.csv and hh_102.csv and verify the discontinuity at half way point in hh_102.csv
6) Two additional tests can be run:
          i) Edit parameter timepoints in main() to show error remaps to half way point.
          ii) My temporary correction is to create 2x points and throw half away.  this is done in p_update() setting cut_in_half = True

7) This same error was found running the code in Tensorflow 1.4 on MacOS Sierra.  My system info is:


== cat /etc/issue ===============================================
Linux PAULP-XPS15 4.4.0-43-Microsoft #1-Microsoft Wed Dec 31 14:42:53 PST 2014 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""16.04.3 LTS (Xenial Xerus)""
VERSION_ID=""16.04""
VERSION_CODENAME=xenial

== are we in docker =============================================
No

== compiler =====================================================
c++ (Ubuntu 5.4.0-6ubuntu1~16.04.5) 5.4.0 20160609
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux PAULP-XPS15 4.4.0-43-Microsoft #1-Microsoft Wed Dec 31 14:42:53 PST 2014 x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================
numpy (1.13.3)
numpydoc (0.7.0)
protobuf (3.4.1)
tensorflow (1.3.0)
tensorflow-tensorboard (0.1.5)

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.3.0
tf.GIT_VERSION = b'unknown'
tf.COMPILER_VERSION = b'unknown'
Sanity check: array([1], dtype=int32)

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
./tf_env_collect.sh: line 105: nvidia-smi: command not found

== cuda libs  ===================================================


### Describe the problem
I am running a RNN for a Hodgkin and Huxley type gating of an ion channel protein
called HnH_gate.py.
The program takes a placeholder vmem and produces a timeseries output of the size
timepoints.
At the halfway point in the timeseries there is a discontinuity in the results
This only appears with some arrays fed to my tf.placeholder.  Others produce normal
results.  I can correct for the problem by doubling the number of timepoints requested
and throwing half away.

The array:
vmem_list_good = [[-100.0, -90.0, -80.0, -70.0, -60.0, -50.0, -41.0, -30.0, -20.0, -10.0, 0.0, 10.0, 20.0, 30.0, 40.0, 50.0],
                [-100.0, -90.0, -80.0, -70.0, -60.0, -50.0, -41.0, -30.0, -20.0, -10.0, 0.0, 10.0, 20.0, 30.0, 40.0, 50.0]]
appears to work perfectly
The array:
vmem_list_bad = [[80.0, 60.0, 40.0, 20.0, 00.0, -20.0, -41.0, -60.0, -80.0, -55.0, 0.0, 10.0, 20.0, 30.0, 40.0, 50.0],
            [70.0, 50.0, 30.0, 10.0, -10.0, -30.0, -50.0, -70.0, -90.0, -30.0, -10.0, 0.0, 10.0, 20.0, 30.0, 40.0]]

shows the error.

To see my temporary correction, edit the parameter in the HH.p_update() method
to: cut_in_half = True

I have written a short output routine to export the simulation to a csv file,
just edit the path and provide a string to make a unique filename:

out(""101"", new_probka_good)
out(""102"", new_probka_bad)

### Source code / logs
program file is: HnH_gate.py (provided as HnH_gate.txt)
HnH_gate.txt  (convert to HnH_gate.py)
[HnH_gate.txt](https://github.com/tensorflow/tensorflow/files/1610057/HnH_gate.txt)

System and Error Description: HnH_gate_bug_report.txt
[HnH_gate_bug_report.txt](https://github.com/tensorflow/tensorflow/files/1610056/HnH_gate_bug_report.txt)

Output example demonstrating problem: Artifact plotting new_probka_bad.py
[Artifact plotting new_probka_bad.pdf](https://github.com/tensorflow/tensorflow/files/1610058/Artifact.plotting.new_probka_bad.pdf)

Thanks for your help.
Paul
  
  ",0,,3,2018-01-07T18:17:27Z,2018-01-08T23:12:15Z,NONE,"- **I have written custom code (as opposed to using a stock example script provided in TensorFlow)**:
to reproduce the error:
1) convert HnH_gate.txt to HnH_gate.py
2) Edit mypath in out() method at end of file for your system.  Save 
3) in python: run HnH_gate.py
4) run out() to create csv files for the good and bad output
            i) out(""101"", new_probka_good)
            ii) out(""102"", new_probka_bad)
5) Plot data from hh_101.csv and hh_102.csv and verify the discontinuity at half way point in hh_102.csv
6) Two additional tests can be run:
          i) Edit parameter timepoints in main() to show error remaps to half way point.
          ii) My temporary correction is to create 2x points and throw half away.  this is done in p_update() setting cut_in_half = True

7) This same error was found running the code in Tensorflow 1.4 on MacOS Sierra.  My system info is:


== cat /etc/issue ===============================================
Linux PAULP-XPS15 4.4.0-43-Microsoft #1-Microsoft Wed Dec 31 14:42:53 PST 2014 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""16.04.3 LTS (Xenial Xerus)""
VERSION_ID=""16.04""
VERSION_CODENAME=xenial

== are we in docker =============================================
No

== compiler =====================================================
c++ (Ubuntu 5.4.0-6ubuntu1~16.04.5) 5.4.0 20160609
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux PAULP-XPS15 4.4.0-43-Microsoft #1-Microsoft Wed Dec 31 14:42:53 PST 2014 x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================
numpy (1.13.3)
numpydoc (0.7.0)
protobuf (3.4.1)
tensorflow (1.3.0)
tensorflow-tensorboard (0.1.5)

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.3.0
tf.GIT_VERSION = b'unknown'
tf.COMPILER_VERSION = b'unknown'
Sanity check: array([1], dtype=int32)

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
./tf_env_collect.sh: line 105: nvidia-smi: command not found

== cuda libs  ===================================================


### Describe the problem
I am running a RNN for a Hodgkin and Huxley type gating of an ion channel protein
called HnH_gate.py.
The program takes a placeholder vmem and produces a timeseries output of the size
timepoints.
At the halfway point in the timeseries there is a discontinuity in the results
This only appears with some arrays fed to my tf.placeholder.  Others produce normal
results.  I can correct for the problem by doubling the number of timepoints requested
and throwing half away.

The array:
vmem_list_good = [[-100.0, -90.0, -80.0, -70.0, -60.0, -50.0, -41.0, -30.0, -20.0, -10.0, 0.0, 10.0, 20.0, 30.0, 40.0, 50.0],
                [-100.0, -90.0, -80.0, -70.0, -60.0, -50.0, -41.0, -30.0, -20.0, -10.0, 0.0, 10.0, 20.0, 30.0, 40.0, 50.0]]
appears to work perfectly
The array:
vmem_list_bad = [[80.0, 60.0, 40.0, 20.0, 00.0, -20.0, -41.0, -60.0, -80.0, -55.0, 0.0, 10.0, 20.0, 30.0, 40.0, 50.0],
            [70.0, 50.0, 30.0, 10.0, -10.0, -30.0, -50.0, -70.0, -90.0, -30.0, -10.0, 0.0, 10.0, 20.0, 30.0, 40.0]]

shows the error.

To see my temporary correction, edit the parameter in the HH.p_update() method
to: cut_in_half = True

I have written a short output routine to export the simulation to a csv file,
just edit the path and provide a string to make a unique filename:

out(""101"", new_probka_good)
out(""102"", new_probka_bad)

### Source code / logs
program file is: HnH_gate.py (provided as HnH_gate.txt)
HnH_gate.txt  (convert to HnH_gate.py)
[HnH_gate.txt](https://github.com/tensorflow/tensorflow/files/1610057/HnH_gate.txt)

System and Error Description: HnH_gate_bug_report.txt
[HnH_gate_bug_report.txt](https://github.com/tensorflow/tensorflow/files/1610056/HnH_gate_bug_report.txt)

Output example demonstrating problem: Artifact plotting new_probka_bad.py
[Artifact plotting new_probka_bad.pdf](https://github.com/tensorflow/tensorflow/files/1610058/Artifact.plotting.new_probka_bad.pdf)

Thanks for your help.
Paul
  
  ",2018-01-08T06:54:37Z,1,1,0,2.753084869277317
442,15929,"C API, SIGABRT abort, Non-OK-status: RegisterAlreadyLocked, Invalid name.","stat:awaiting response,type:support","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Working with public C API.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
MacOS 10.13.2 (17C88)
- **TensorFlow installed from (source or binary)**:
binary
- **TensorFlow version (use command below)**:
- libtensorflow 1.4.1 (from brew package)
- **Python version**: 
non
- **Bazel version (if compiling from source)**:
non
- **GCC/Compiler version (if compiling from source)**:
Apple Swift version 4.0.3 (swiftlang-900.0.74.1 clang-900.0.39.2), lldb-900.0.64, Swift-4.0
- **CUDA/cuDNN version**:
non
- **GPU model and memory**:
non
- **Exact command to reproduce**:
Using swift code as example (https://github.com/Octadero/Example).

### Describe the problem
Dear TensorFlow community, 
It is really strange issue, from time to time at the same code, I have SIGABRT crash.
```
2018-01-05 21:03:55.627002: F tensorflow/core/framework/op.cc:165] Non-OK-status: RegisterAlreadyLocked(deferred_[i]) status: Invalid argument: Invalid name: {\242	
(lldb) bt
* thread #1, queue = 'com.apple.main-thread', stop reason = signal SIGABRT
    frame #0: 0x00007fff528f7e3e libsystem_kernel.dylib`__pthread_kill + 10
    frame #1: 0x0000000108cd41b4 libsystem_pthread.dylib`pthread_kill + 333
    frame #2: 0x00007fff52854312 libsystem_c.dylib`abort + 127
    frame #3: 0x0000000108e600c0 libtensorflow_framework.so`tensorflow::internal::LogMessageFatal::~LogMessageFatal() + 32
    frame #4: 0x0000000108e600d0 libtensorflow_framework.so`tensorflow::internal::LogMessageFatal::~LogMessageFatal() + 16
    frame #5: 0x0000000108d28676 libtensorflow_framework.so`tensorflow::OpRegistry::MustCallDeferred() const + 406
    frame #6: 0x0000000108d2819d libtensorflow_framework.so`tensorflow::OpRegistry::LookUp(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, tensorflow::OpRegistrationData const**) const + 61
    frame #7: 0x0000000108d0c875 libtensorflow_framework.so`tensorflow::FunctionLibraryDefinition::LookUp(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, tensorflow::OpRegistrationData const**) const + 117
    frame #8: 0x0000000108d27b0a libtensorflow_framework.so`tensorflow::OpRegistryInterface::LookUpOpDef(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, tensorflow::OpDef const**) const + 42
    frame #9: 0x0000000108d81b65 libtensorflow_framework.so`tensorflow::Graph::AddNode(tensorflow::NodeDef const&, tensorflow::Status*) + 69
    frame #10: 0x0000000108d8177a libtensorflow_framework.so`tensorflow::Graph::Graph(tensorflow::OpRegistryInterface const*) + 458
    frame #11: 0x00000001004f7d73 libtensorflow.so`TF_NewGraph + 51
  * frame #12: 0x00000001066b739d CAPI`newGraph() at Graph.swift:26
    frame #13: 0x00000001063021b3 TensorFlowKit`Graph.init() at Graph.swift:36
    frame #14: 0x000000010630214a TensorFlowKit`Graph.__allocating_init() at Graph.swift:0
    frame #15: 0x0000000106500f2d TensorFlowKit`static SavedModel.load(exportPath=""/Users/Volodymyr/Projects/Examples/03_Reinforcement/Resources/save/"", tags=1 value, options=0x00000001098e35d0, self=TensorFlowKit.SavedModel) at SavedModel.swift:221
    frame #16: 0x00000001000069f4 03_Reinforcement`static Network.loadGraph(self=_3_Reinforcement.Network) at Network.swift:146
    frame #17: 0x0000000100003428 03_Reinforcement`Network.init() at Network.swift:73
    frame #18: 0x0000000100002e4c 03_Reinforcement`Network.__allocating_init() at Network.swift:0
    frame #19: 0x0000000100008656 03_Reinforcement`main at main.swift:32
    frame #20: 0x00007fff527a8115 libdyld.dylib`start + 1
    frame #21: 0x00007fff527a8115 libdyld.dylib`start + 1
```
Sanitizer options can't help to resolve that issue. List of libs loaded in attached file.
[dyld_log.txt](https://github.com/tensorflow/tensorflow/files/1609977/dyld_log.txt)


### Source code / logs
Using C API I am alloc [new Graph by TF_NewGraph()](https://github.com/Octadero/TensorFlow/blob/34addfc80cb7f220a7d9afa310f8a9845dba0d36/Sources/CAPI/Graph.swift#L26)",0,,5,2018-01-07T17:15:42Z,2018-01-09T16:06:20Z,NONE,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Working with public C API.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
MacOS 10.13.2 (17C88)
- **TensorFlow installed from (source or binary)**:
binary
- **TensorFlow version (use command below)**:
- libtensorflow 1.4.1 (from brew package)
- **Python version**: 
non
- **Bazel version (if compiling from source)**:
non
- **GCC/Compiler version (if compiling from source)**:
Apple Swift version 4.0.3 (swiftlang-900.0.74.1 clang-900.0.39.2), lldb-900.0.64, Swift-4.0
- **CUDA/cuDNN version**:
non
- **GPU model and memory**:
non
- **Exact command to reproduce**:
Using swift code as example (https://github.com/Octadero/Example).

### Describe the problem
Dear TensorFlow community, 
It is really strange issue, from time to time at the same code, I have SIGABRT crash.
```
2018-01-05 21:03:55.627002: F tensorflow/core/framework/op.cc:165] Non-OK-status: RegisterAlreadyLocked(deferred_[i]) status: Invalid argument: Invalid name: {\242	
(lldb) bt
* thread #1, queue = 'com.apple.main-thread', stop reason = signal SIGABRT
    frame #0: 0x00007fff528f7e3e libsystem_kernel.dylib`__pthread_kill + 10
    frame #1: 0x0000000108cd41b4 libsystem_pthread.dylib`pthread_kill + 333
    frame #2: 0x00007fff52854312 libsystem_c.dylib`abort + 127
    frame #3: 0x0000000108e600c0 libtensorflow_framework.so`tensorflow::internal::LogMessageFatal::~LogMessageFatal() + 32
    frame #4: 0x0000000108e600d0 libtensorflow_framework.so`tensorflow::internal::LogMessageFatal::~LogMessageFatal() + 16
    frame #5: 0x0000000108d28676 libtensorflow_framework.so`tensorflow::OpRegistry::MustCallDeferred() const + 406
    frame #6: 0x0000000108d2819d libtensorflow_framework.so`tensorflow::OpRegistry::LookUp(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, tensorflow::OpRegistrationData const**) const + 61
    frame #7: 0x0000000108d0c875 libtensorflow_framework.so`tensorflow::FunctionLibraryDefinition::LookUp(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, tensorflow::OpRegistrationData const**) const + 117
    frame #8: 0x0000000108d27b0a libtensorflow_framework.so`tensorflow::OpRegistryInterface::LookUpOpDef(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, tensorflow::OpDef const**) const + 42
    frame #9: 0x0000000108d81b65 libtensorflow_framework.so`tensorflow::Graph::AddNode(tensorflow::NodeDef const&, tensorflow::Status*) + 69
    frame #10: 0x0000000108d8177a libtensorflow_framework.so`tensorflow::Graph::Graph(tensorflow::OpRegistryInterface const*) + 458
    frame #11: 0x00000001004f7d73 libtensorflow.so`TF_NewGraph + 51
  * frame #12: 0x00000001066b739d CAPI`newGraph() at Graph.swift:26
    frame #13: 0x00000001063021b3 TensorFlowKit`Graph.init() at Graph.swift:36
    frame #14: 0x000000010630214a TensorFlowKit`Graph.__allocating_init() at Graph.swift:0
    frame #15: 0x0000000106500f2d TensorFlowKit`static SavedModel.load(exportPath=""/Users/Volodymyr/Projects/Examples/03_Reinforcement/Resources/save/"", tags=1 value, options=0x00000001098e35d0, self=TensorFlowKit.SavedModel) at SavedModel.swift:221
    frame #16: 0x00000001000069f4 03_Reinforcement`static Network.loadGraph(self=_3_Reinforcement.Network) at Network.swift:146
    frame #17: 0x0000000100003428 03_Reinforcement`Network.init() at Network.swift:73
    frame #18: 0x0000000100002e4c 03_Reinforcement`Network.__allocating_init() at Network.swift:0
    frame #19: 0x0000000100008656 03_Reinforcement`main at main.swift:32
    frame #20: 0x00007fff527a8115 libdyld.dylib`start + 1
    frame #21: 0x00007fff527a8115 libdyld.dylib`start + 1
```
Sanitizer options can't help to resolve that issue. List of libs loaded in attached file.
[dyld_log.txt](https://github.com/tensorflow/tensorflow/files/1609977/dyld_log.txt)


### Source code / logs
Using C API I am alloc [new Graph by TF_NewGraph()](https://github.com/Octadero/TensorFlow/blob/34addfc80cb7f220a7d9afa310f8a9845dba0d36/Sources/CAPI/Graph.swift#L26)",2018-01-09T08:26:15Z,2,1,0,3.753084869277317
443,15928,Utility classes for writing Java source code from a C++ process (part 2),"awaiting testing (then merge),cla: yes","Part 2 of pull request #14094 that has been splitted into several commits.

This part features only a language-agnostic writer that outputs generated source code into a file or in memory. Note that this class is being kept apart from the Java-specific code generators since it could be a good candidate to be moved in the core io library in the future (ref #13748).

cc: @asimshankar ",1,,2,2018-01-07T16:51:22Z,2018-01-11T14:52:03Z,CONTRIBUTOR,"Part 2 of pull request #14094 that has been splitted into several commits.

This part features only a language-agnostic writer that outputs generated source code into a file or in memory. Note that this class is being kept apart from the Java-specific code generators since it could be a good candidate to be moved in the core io library in the future (ref #13748).

cc: @asimshankar ",2018-01-11T14:51:37Z,4,2,1,4.253084869277317
444,15927,R1.5/verbs w 0 copies,cla: yes,"## Verbs implementation to use direct tensor writes (0 copies)

### Motivation:

Following HKUST research on the use of GPU direct, and their [GDR implementation](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/gdr/README.md), we wish to adopt the 0 copies approach and apply it to the current verbs implementation, while keeping the current implementation advantages, such as configurability and the use of RDMA for control messages.

### Performance:

Compared with the current GRPC, verbs and GDR implementation, the result implementation gave the best performance for every model, with any number of nodes. For VGG16 on 8 nodes with 4 P100 GPUs each, the prototype beat the second place by over 15%.

### Implementation requirements:

1. Tensor writes need to be done directly from the source Tensor to the destination Tensor, with no memory copies in between. This should be done for all DMAble tensors which are located either on CPU or on a RDMA compatible GPU device (GPU direct). 
2. Non DMAble tensors (CanMemCopy == false) will be serialized to proto on the sender side, RDMA written to a registered buffer on the receiver side, and then deserialized by the receiver.
3. Tensors which are located on a non-RDMA-compatible GPU, will be RDMA written to a registered CPU proxy buffer on the receiver side, and then copied to GPU by the receiver.

### Implementation constrains:

For best stability and proof of correctness, we will divide the implementation to two stages:
1. At first stage we will keep changes to the current implementation to the minimum possible. The expense will be that we may have unused or unnecessary code leftovers, which may also affect performance. 
2. At second stage, we will re-iterate over the code and remove irrelevant code parts.
The design of the solution aims that we will achieve both stages with relative ease. 

### Design guidelines:

1. Since we do not want to do any unnecessary memory copying, we will no longer allocate a fixed CPU buffer as the destination for the RDMA write. Instead we will do the writing directly to the result tensor, or if the result tensor is on a device which does not support RDMA, we will do the writing to a proxy CPU tensor and then copy its content to the result tensor.
2. The address of the destination Tensor needs to be sent to the sender side for writing, meaning that the result/proxy tensor should be pre-allocated on the receiver side, prior to sending the tensor request. In order to do that, we need to know its meta-data, i.e. shape and data-type for DMAble tensors, and proto-size for serialized tensors. Unfortunately, this information is only available on the sender side which complicates manners. In order to avoid sending extra messages for querying the meta-data on each step, we store a local meta-data cache per tensor. Based on the assumption that the meta-data of a tensor rarely changes between steps, we expect that on most times the cache will only be updated once. When the sender receives a request for a tensor, if it is the first time this tensor is requested, or in the rare case that the meta-data did change, the sender will first send a meta-data response, on which the receiver will update the local cache, and reallocate the result/proxy tensors if required. When the receiver sends the tensor request, it will contain also the meta-data currently stored in its local cache, so the sender can compare it to see if there was a change.
3. When the sender writes the tensor content to the result tensor, no additional data is being written with it. That means we need to reside on ibverbs immediate (uint32_t) to indicate which request we are responding to (in order to trigger the receive callback). The easiest and most elegant way is to key the recv callback with a unique request_index (uint32_t), instead of the current key_with_step_id (string). 
4. Since the sender no longer writes the tensor from/to fixed buffers, we no longer need to schedule the writes using the local/remote status. In addition we no longer rely on the RmdaTensorBuffer members as the source/destination addresses and rkey/lkey. Instead, each RdmaTensorBuffer will hold multiple ""Response"" objects (one per step-id), from which we derive destination address and rkey. The source address and lkey are always the ones of the source Tensor.
5. With the addition of tensor pre-allocation, we noticed there is a large code similarity between sending the first tensor request and re-sending the request in case of meta-data changes. After implementing a common method for tensor pre-allocation, it turned out that implementation becomes much simpler by encapsulating the process of request sending/re-sending, meta-data response callback and content response callback, all in a single ""Request"" class. The request class holds all the relevant request information, which reduces excessive parameter passing and lambda capturing. This decision is purely for elegance and code simplicity, and we decided to implement it in first stage because it makes the implementation much easier.

### New types/classes:

* **enum RdmaImmDataType** - Immediate types to distinguish between different RDMA writes on the remote side. Ack writes and control-message writes have a fixed immediate value. The rest of the writes are tensor writes and the immediate value is the relevant request index.
* **enum  RdmaWriteIDType**    - Types to distinguish between different RDMA write-complete events: Ack, control message, tensor DMA write and tensor proto write.
* **class RdmaWriteID**        - Context for RDMA write complete events. Holds the RdmaWriteIDType and additional data.
* **class RemoteAddressContext** - Remote address information (address + mr). Will be passed as write context for tensor proto writes.
* **class RdmaTensorMetaData** - Meta-data for a tensor (type, shape, is_dead, proto_size).
* **class RdmaMemoryMgr**      - Manages the meta-data cache, and the registered memory regions.
* **class RdmaTensorRequest**  - Holds and manages information for a single tensor request throughout the entire receive cycle. API:
	* Start() - Start the request.
	* RecvTensorMetaData() - Receive meta-data from the remote side.
	* RecvTensorContent() - Receive tensor content from the remote side and invoke the done() callback. 
* **class RdmaTensorResponse** - Holds information for a single tensor response, such as destination address and rkey.

### Protocol changes:

The protocol messages themselves will remain mostly unchanged at the first stage, but will be used differently, as described below. The current messages structures already have most of the required fields for the new implementation. The only change is the ""buffer_size"" field which is no longer used since we are no longer sending additional information with the tensor, and thus it is now always equal to the ""tensor_bytes"" field. Instead, we use that field to pass the ""request_index"".

### Message structure:

| type | name_size | name | step_id | request_index | remote_addr | rkey | is_dead | data_type | tensor_shape | tensor_bytes |
|------|---------- |------|---------|---------------|-------------|------|---------|-----------|--------------|--------------|
|  1B  |    2B     | 512  |  8B     |      8B       |         8B  |   4B |      1B |     XB    |    XB        |    8B        |

* **RDMA_MESSAGE_TENSOR_REQUEST**  - (receiver ==> sender) The original tensor request. 
	* type - The message type.
	* name (name_size) - Name of the requested tensor.
	* step_id - Step ID.
	* request_index - Request index.
	* remote_addr/rkey - Address/rkey of the result/proxy tensor. Irrelevant for first-time request.
	* is_dead/data_type/tensor_shape/tensor_bytes - The current meta-data as stored in the receiver local cache. The sender will use that information to know if the receiver's cache requires updating.
* **RDMA_MESSAGE_BUFFER_REQUEST**  - (sender ==> receiver) The meta-data update message in case meta-data had changed (or if it is the first time the tensor is requested).
	* type - The message type.
	* request_index - Request index.
	* is_dead/data_type/tensor_shape/tensor_bytes - The up-to-date meta-data.
* **RDMA_MESSAGE_BUFFER_RESPONSE** - (receiver ==> sender) Tensor re-requset after meta-data update and reallocation of result/proxy tensors.
	* type - The message type.
	* name (name_size) - Name of the requested tensor.
	* step_id - Step ID.
	* request_index - Request index.
	* remote_addr/rkey - Address/rkey of the reallocated result/proxy tensor.
	* is_dead/data_type/tensor_shape/tensor_bytes - The new meta-data. Will be removed in the next phase.
* **RDMA_MESSAGE_TENSOR_WRITE**    - (sender ==> receiver) No longer sent. There is only a direct write of the tensor content to the result/proxy tensor. Request index passed as the immediate value of the write.
* **RDMA_MESSAGE_TENSOR_IDLE**     - (receiver ==> sender) No longer sent.

![alt text](https://raw.githubusercontent.com/Mellanox/tensorflow/eladw_verbs_w_0_copies/tensorflow/contrib/verbs/verbs_with_0_copies_phase1_protocol.jpg ""Phase 1 message protocol"")

### Second stage optimizations:
1. Remove unused code leftovers.
2. Remove the ACK buffer completely, since we can rely completely on its immediate value.

### Future optimizations:
1. Map the tensor names to indexes, to significantly reduce the request message size.
2. Understand the purpose of empty tensors and if we can skip remote fetching for them.
3. Consider concatenating multiple requests and/or using multiple message buffers.
4. Consider a no-request architecture.
  ",0,,9,2018-01-07T13:09:15Z,2018-01-10T12:18:57Z,CONTRIBUTOR,"## Verbs implementation to use direct tensor writes (0 copies)

### Motivation:

Following HKUST research on the use of GPU direct, and their [GDR implementation](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/gdr/README.md), we wish to adopt the 0 copies approach and apply it to the current verbs implementation, while keeping the current implementation advantages, such as configurability and the use of RDMA for control messages.

### Performance:

Compared with the current GRPC, verbs and GDR implementation, the result implementation gave the best performance for every model, with any number of nodes. For VGG16 on 8 nodes with 4 P100 GPUs each, the prototype beat the second place by over 15%.

### Implementation requirements:

1. Tensor writes need to be done directly from the source Tensor to the destination Tensor, with no memory copies in between. This should be done for all DMAble tensors which are located either on CPU or on a RDMA compatible GPU device (GPU direct). 
2. Non DMAble tensors (CanMemCopy == false) will be serialized to proto on the sender side, RDMA written to a registered buffer on the receiver side, and then deserialized by the receiver.
3. Tensors which are located on a non-RDMA-compatible GPU, will be RDMA written to a registered CPU proxy buffer on the receiver side, and then copied to GPU by the receiver.

### Implementation constrains:

For best stability and proof of correctness, we will divide the implementation to two stages:
1. At first stage we will keep changes to the current implementation to the minimum possible. The expense will be that we may have unused or unnecessary code leftovers, which may also affect performance. 
2. At second stage, we will re-iterate over the code and remove irrelevant code parts.
The design of the solution aims that we will achieve both stages with relative ease. 

### Design guidelines:

1. Since we do not want to do any unnecessary memory copying, we will no longer allocate a fixed CPU buffer as the destination for the RDMA write. Instead we will do the writing directly to the result tensor, or if the result tensor is on a device which does not support RDMA, we will do the writing to a proxy CPU tensor and then copy its content to the result tensor.
2. The address of the destination Tensor needs to be sent to the sender side for writing, meaning that the result/proxy tensor should be pre-allocated on the receiver side, prior to sending the tensor request. In order to do that, we need to know its meta-data, i.e. shape and data-type for DMAble tensors, and proto-size for serialized tensors. Unfortunately, this information is only available on the sender side which complicates manners. In order to avoid sending extra messages for querying the meta-data on each step, we store a local meta-data cache per tensor. Based on the assumption that the meta-data of a tensor rarely changes between steps, we expect that on most times the cache will only be updated once. When the sender receives a request for a tensor, if it is the first time this tensor is requested, or in the rare case that the meta-data did change, the sender will first send a meta-data response, on which the receiver will update the local cache, and reallocate the result/proxy tensors if required. When the receiver sends the tensor request, it will contain also the meta-data currently stored in its local cache, so the sender can compare it to see if there was a change.
3. When the sender writes the tensor content to the result tensor, no additional data is being written with it. That means we need to reside on ibverbs immediate (uint32_t) to indicate which request we are responding to (in order to trigger the receive callback). The easiest and most elegant way is to key the recv callback with a unique request_index (uint32_t), instead of the current key_with_step_id (string). 
4. Since the sender no longer writes the tensor from/to fixed buffers, we no longer need to schedule the writes using the local/remote status. In addition we no longer rely on the RmdaTensorBuffer members as the source/destination addresses and rkey/lkey. Instead, each RdmaTensorBuffer will hold multiple ""Response"" objects (one per step-id), from which we derive destination address and rkey. The source address and lkey are always the ones of the source Tensor.
5. With the addition of tensor pre-allocation, we noticed there is a large code similarity between sending the first tensor request and re-sending the request in case of meta-data changes. After implementing a common method for tensor pre-allocation, it turned out that implementation becomes much simpler by encapsulating the process of request sending/re-sending, meta-data response callback and content response callback, all in a single ""Request"" class. The request class holds all the relevant request information, which reduces excessive parameter passing and lambda capturing. This decision is purely for elegance and code simplicity, and we decided to implement it in first stage because it makes the implementation much easier.

### New types/classes:

* **enum RdmaImmDataType** - Immediate types to distinguish between different RDMA writes on the remote side. Ack writes and control-message writes have a fixed immediate value. The rest of the writes are tensor writes and the immediate value is the relevant request index.
* **enum  RdmaWriteIDType**    - Types to distinguish between different RDMA write-complete events: Ack, control message, tensor DMA write and tensor proto write.
* **class RdmaWriteID**        - Context for RDMA write complete events. Holds the RdmaWriteIDType and additional data.
* **class RemoteAddressContext** - Remote address information (address + mr). Will be passed as write context for tensor proto writes.
* **class RdmaTensorMetaData** - Meta-data for a tensor (type, shape, is_dead, proto_size).
* **class RdmaMemoryMgr**      - Manages the meta-data cache, and the registered memory regions.
* **class RdmaTensorRequest**  - Holds and manages information for a single tensor request throughout the entire receive cycle. API:
	* Start() - Start the request.
	* RecvTensorMetaData() - Receive meta-data from the remote side.
	* RecvTensorContent() - Receive tensor content from the remote side and invoke the done() callback. 
* **class RdmaTensorResponse** - Holds information for a single tensor response, such as destination address and rkey.

### Protocol changes:

The protocol messages themselves will remain mostly unchanged at the first stage, but will be used differently, as described below. The current messages structures already have most of the required fields for the new implementation. The only change is the ""buffer_size"" field which is no longer used since we are no longer sending additional information with the tensor, and thus it is now always equal to the ""tensor_bytes"" field. Instead, we use that field to pass the ""request_index"".

### Message structure:

| type | name_size | name | step_id | request_index | remote_addr | rkey | is_dead | data_type | tensor_shape | tensor_bytes |
|------|---------- |------|---------|---------------|-------------|------|---------|-----------|--------------|--------------|
|  1B  |    2B     | 512  |  8B     |      8B       |         8B  |   4B |      1B |     XB    |    XB        |    8B        |

* **RDMA_MESSAGE_TENSOR_REQUEST**  - (receiver ==> sender) The original tensor request. 
	* type - The message type.
	* name (name_size) - Name of the requested tensor.
	* step_id - Step ID.
	* request_index - Request index.
	* remote_addr/rkey - Address/rkey of the result/proxy tensor. Irrelevant for first-time request.
	* is_dead/data_type/tensor_shape/tensor_bytes - The current meta-data as stored in the receiver local cache. The sender will use that information to know if the receiver's cache requires updating.
* **RDMA_MESSAGE_BUFFER_REQUEST**  - (sender ==> receiver) The meta-data update message in case meta-data had changed (or if it is the first time the tensor is requested).
	* type - The message type.
	* request_index - Request index.
	* is_dead/data_type/tensor_shape/tensor_bytes - The up-to-date meta-data.
* **RDMA_MESSAGE_BUFFER_RESPONSE** - (receiver ==> sender) Tensor re-requset after meta-data update and reallocation of result/proxy tensors.
	* type - The message type.
	* name (name_size) - Name of the requested tensor.
	* step_id - Step ID.
	* request_index - Request index.
	* remote_addr/rkey - Address/rkey of the reallocated result/proxy tensor.
	* is_dead/data_type/tensor_shape/tensor_bytes - The new meta-data. Will be removed in the next phase.
* **RDMA_MESSAGE_TENSOR_WRITE**    - (sender ==> receiver) No longer sent. There is only a direct write of the tensor content to the result/proxy tensor. Request index passed as the immediate value of the write.
* **RDMA_MESSAGE_TENSOR_IDLE**     - (receiver ==> sender) No longer sent.

![alt text](https://raw.githubusercontent.com/Mellanox/tensorflow/eladw_verbs_w_0_copies/tensorflow/contrib/verbs/verbs_with_0_copies_phase1_protocol.jpg ""Phase 1 message protocol"")

### Second stage optimizations:
1. Remove unused code leftovers.
2. Remove the ACK buffer completely, since we can rely completely on its immediate value.

### Future optimizations:
1. Map the tensor names to indexes, to significantly reduce the request message size.
2. Understand the purpose of empty tensors and if we can skip remote fetching for them.
3. Consider concatenating multiple requests and/or using multiple message buffers.
4. Consider a no-request architecture.
  ",2018-01-07T14:42:51Z,3,2,1,6.753084869277317
445,15926,_Assert3DImage that adds a control dependency for the shape check,"awaiting testing (then merge),cla: yes","I noticed that the _Check3DImage was always used in exactly the same way and extracted this into its own convenience function.

The only remaining use of _Check3DImage was an import in gen_image_ops.py saying
""# TODO(drpng): remove these once internal use has discontinued."", so maybe it could be removed entirely.",1,,2,2018-01-07T12:32:26Z,2018-01-23T20:03:45Z,CONTRIBUTOR,"I noticed that the _Check3DImage was always used in exactly the same way and extracted this into its own convenience function.

The only remaining use of _Check3DImage was an import in gen_image_ops.py saying
""# TODO(drpng): remove these once internal use has discontinued."", so maybe it could be removed entirely.",2018-01-23T20:03:40Z,16,2,3,4.253084869277317
446,15925,cmake compile error C2678: binary '*': no operator found sparse_column_iterable.cc,,"`cmake .. -A x64 -DCMAKE_BUILD_TYPE=Release -DSWIG_EXECUTABLE=C:/ProgramData/chocolatey/bin/swig.exe -DPYTHON_EXECUTABLE=C:/Python36/python.exe -DPYTHON_LIBRARIES=C:/Python36/libs/python36.lib -Dtensorflow_WIN_CPU_SIMD_OPTIONS=/arch:AVX2 `

```
c:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.12.25827\include\algorithm(2417): error C2678: binary '*': no operator found which takes a left-hand operand of type 'const tensorflow::boosted_trees::utils::`anonymous-namespace'::IndicesRowIterator' (or there is no acceptable conversion) (compiling source file D:\_working_dir\_ml\tensorflow\tensorflow\contrib\boosted_trees\lib\utils\sparse_column_iterable.cc) [D:\_working_dir\_ml\tensorflow\tensorflow\contrib\cmake\build\tf_core_kernels.vcxproj]
  c:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.12.25827\include\algorithm(2417): error C2100: illegal indirection (compiling source file D:\_working_dir\_ml\tensorflow\tensorflow\contrib\boosted_trees\lib\utils\sparse_column_iterable.cc) [D:\_working_dir\_ml\tensorflow\tensorflow\con
trib\cmake\build\tf_core_kernels.vcxproj]
```
Windows 8.1 x64
cmake 3.10.1
swig 3.0.9
Visual Studio 2017 Community

the same problem asked
https://stackoverflow.com/questions/48058113/compiling-tensorflow-1-4-on-windows-10",0,,8,2018-01-07T12:22:32Z,2018-01-30T19:59:57Z,NONE,"`cmake .. -A x64 -DCMAKE_BUILD_TYPE=Release -DSWIG_EXECUTABLE=C:/ProgramData/chocolatey/bin/swig.exe -DPYTHON_EXECUTABLE=C:/Python36/python.exe -DPYTHON_LIBRARIES=C:/Python36/libs/python36.lib -Dtensorflow_WIN_CPU_SIMD_OPTIONS=/arch:AVX2 `

```
c:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.12.25827\include\algorithm(2417): error C2678: binary '*': no operator found which takes a left-hand operand of type 'const tensorflow::boosted_trees::utils::`anonymous-namespace'::IndicesRowIterator' (or there is no acceptable conversion) (compiling source file D:\_working_dir\_ml\tensorflow\tensorflow\contrib\boosted_trees\lib\utils\sparse_column_iterable.cc) [D:\_working_dir\_ml\tensorflow\tensorflow\contrib\cmake\build\tf_core_kernels.vcxproj]
  c:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.12.25827\include\algorithm(2417): error C2100: illegal indirection (compiling source file D:\_working_dir\_ml\tensorflow\tensorflow\contrib\boosted_trees\lib\utils\sparse_column_iterable.cc) [D:\_working_dir\_ml\tensorflow\tensorflow\con
trib\cmake\build\tf_core_kernels.vcxproj]
```
Windows 8.1 x64
cmake 3.10.1
swig 3.0.9
Visual Studio 2017 Community

the same problem asked
https://stackoverflow.com/questions/48058113/compiling-tensorflow-1-4-on-windows-10",2018-01-08T00:58:11Z,23,1,3,5.253084869277317
447,15924,Tensorflow Optimize for Inference KeyError.,stat:awaiting response,"I got the following error when optimizing the graph for inference:
Traceback (most recent call last):
  File ""C:\Python35\lib\runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""C:\Python35\lib\runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""C:\Python35\lib\site-packages\tensorflow\python\tools\optimize_for_
inference.py"", line 146, in <module>
    app.run(main=main, argv=[sys.argv[0]] + unparsed)
  File ""C:\Python35\lib\site-packages\tensorflow\python\platform\app.py"", l
ine 48, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""C:\Python35\lib\site-packages\tensorflow\python\tools\optimize_for_
inference.py"", line 90, in main
    FLAGS.output_names.split("",""), FLAGS.placeholder_type_enum)
  File ""C:\Python35\lib\site-packages\tensorflow\python\tools\optimize_for_
inference_lib.py"", line 109, in optimize_for_inference
    placeholder_type_enum)
  File ""C:\Python35\lib\site-packages\tensorflow\python\tools\strip_unused_
lib.py"", line 83, in strip_unused
    raise KeyError(""The following input nodes were not found: %s\n"" % not_f
ound)
KeyError: ""The following input nodes were not found: {'input'}\n""

Please help me soon!",0,,4,2018-01-07T10:02:14Z,2018-01-08T05:09:43Z,NONE,"I got the following error when optimizing the graph for inference:
Traceback (most recent call last):
  File ""C:\Python35\lib\runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""C:\Python35\lib\runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""C:\Python35\lib\site-packages\tensorflow\python\tools\optimize_for_
inference.py"", line 146, in <module>
    app.run(main=main, argv=[sys.argv[0]] + unparsed)
  File ""C:\Python35\lib\site-packages\tensorflow\python\platform\app.py"", l
ine 48, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""C:\Python35\lib\site-packages\tensorflow\python\tools\optimize_for_
inference.py"", line 90, in main
    FLAGS.output_names.split("",""), FLAGS.placeholder_type_enum)
  File ""C:\Python35\lib\site-packages\tensorflow\python\tools\optimize_for_
inference_lib.py"", line 109, in optimize_for_inference
    placeholder_type_enum)
  File ""C:\Python35\lib\site-packages\tensorflow\python\tools\strip_unused_
lib.py"", line 83, in strip_unused
    raise KeyError(""The following input nodes were not found: %s\n"" % not_f
ound)
KeyError: ""The following input nodes were not found: {'input'}\n""

Please help me soon!",2018-01-07T18:54:30Z,1,1,0,3.253084869277317
448,15923,Add clean_dep to copts macro.,"awaiting testing (then merge),cla: yes","Currently, copts macro has select statement with //tensorflow/...
This resulted in bazel error when any supermodule that uses tensorflow as a submoudle and uses copts macro.",0,,5,2018-01-07T04:38:49Z,2018-01-11T14:50:41Z,CONTRIBUTOR,"Currently, copts macro has select statement with //tensorflow/...
This resulted in bazel error when any supermodule that uses tensorflow as a submoudle and uses copts macro.",2018-01-08T08:58:11Z,4,2,1,4.753084869277317
449,15922,Add clean_dep to a bazel macro.,"awaiting testing (then merge),cla: yes","Currently, copts macro has select statement with //tensorflow/...
This resulted in bazel error when any supermodule that uses tensorflow as a submoudle and uses copts macro.
",0,,5,2018-01-07T04:36:44Z,2018-01-11T14:48:42Z,CONTRIBUTOR,"Currently, copts macro has select statement with //tensorflow/...
This resulted in bazel error when any supermodule that uses tensorflow as a submoudle and uses copts macro.
",2018-01-08T08:57:45Z,4,2,1,4.753084869277317
450,15921,iOS: Op type not registered 'DecodeWav',type:build/install,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: I am trying to run graph model from Simple Audio Recognition example on iOS.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OS X 10.13
- **TensorFlow installed from (source or binary)**: Branch r1.4
- **TensorFlow version (use command below)**: 1.4
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: Build label: 0.9.0-homebrew
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A

### Describe the problem

I am trying to run graph model from Simple Audio Recognition example on iOS. When I am calling `session->Create(tensorflow_graph)` with the graph I get the error: ""Could not create TensorFlow Graph: Not found: Op type not registered 'DecodeWav'..."".

My initial thought is that because I am using TensorFlow-experimental (1.1.1) from pods, it's possible that this Op type is not registered. So I tried building it myself, which builds without errors with command: `tensorflow/contrib/makefile/build_all_ios.sh`. I then remove TensorFlow-experimental (1.1.1) from the project and link my own build of tensorflow, but I get the same error. 

I also found the following PR - [[iOS] Add optional Selective Registration of Ops #14421](https://github.com/tensorflow/tensorflow/pull/14421)

I tried building from master with the above PR merged like so:
 
For iPhone 5:
`tensorflow/contrib/makefile/build_all_ios.sh -a armv7 -g /Users/anton/Development/tensorflow/tensorflow/examples/ios/simple/data/tensorflow_inception_graph_speech.pb`

For iPhone SE:
`tensorflow/contrib/makefile/build_all_ios.sh -a arm64 -g /Users/anton/Development/tensorflow/tensorflow/examples/ios/simple/data/tensorflow_inception_graph_speech.pb`

If I then go and check the file `/tensorflow/tensorflow/core/framework/ops_to_register.h` (auto-generated after above command) I can see that DecodeWav is listed among kernels and operations:

```
// This file was autogenerated by print_selective_registration_header.py
#ifndef OPS_TO_REGISTER
#define OPS_TO_REGISTER

    namespace {
      constexpr const char* skip(const char* x) {
        return (*x) ? (*x == ' ' ? skip(x + 1) : x) : x;
      }

      constexpr bool isequal(const char* x, const char* y) {
        return (*skip(x) && *skip(y))
                   ? (*skip(x) == *skip(y) && isequal(skip(x) + 1, skip(y) + 1))
                   : (!*skip(x) && !*skip(y));
      }

      template<int N>
      struct find_in {
        static constexpr bool f(const char* x, const char* const y[N]) {
          return isequal(x, y[0]) || find_in<N - 1>::f(x, y + 1);
        }
      };

      template<>
      struct find_in<0> {
        static constexpr bool f(const char* x, const char* const y[]) {
          return false;
        }
      };
    }  // end namespace
    constexpr const char* kNecessaryOpKernelClasses[] = {
""BinaryOp< CPUDevice, functor::add<float>>"",
""AudioSpectrogramOp"",
""ConstantOp"",
""Conv2DOp<CPUDevice, float>"",
""DecodeWavOp"",
""IdentityOp"",
""MatMulOp<CPUDevice, float, false >"",
""MaxPoolingOp<CPUDevice, float>"",
""MfccOp"",
""NoOp"",
""PlaceholderOp"",
""ReluOp<CPUDevice, float>"",
""ReshapeOp"",
""SoftmaxOp<CPUDevice, float>"",
""RecvOp"",
""SendOp"",
};
#define SHOULD_REGISTER_OP_KERNEL(clz) (find_in<sizeof(kNecessaryOpKernelClasses) / sizeof(*kNecessaryOpKernelClasses)>::f(clz, kNecessaryOpKernelClasses))

constexpr inline bool ShouldRegisterOp(const char op[]) {
  return false
     || isequal(op, ""Add"")
     || isequal(op, ""AudioSpectrogram"")
     || isequal(op, ""Const"")
     || isequal(op, ""Conv2D"")
     || isequal(op, ""DecodeWav"")
     || isequal(op, ""Identity"")
     || isequal(op, ""MatMul"")
     || isequal(op, ""MaxPool"")
     || isequal(op, ""Mfcc"")
     || isequal(op, ""NoOp"")
     || isequal(op, ""Placeholder"")
     || isequal(op, ""Relu"")
     || isequal(op, ""Reshape"")
     || isequal(op, ""Softmax"")
     || isequal(op, ""_Recv"")
     || isequal(op, ""_Send"")
  ;
}
#define SHOULD_REGISTER_OP(op) ShouldRegisterOp(op)

#define SHOULD_REGISTER_OP_GRADIENT false
#endif

```

But when I try to run the graph model I still get same error message. I have removed the pod version, and I am 100% sure I am running my own build version of tensorflow on iOS.

I can't tell if this is a bug or I am doing something wrong during the build process.

Has anyone tried running any graph that uses DecodeWav on iOS?

Thanks.

### Source code / logs

Error: Could not create TensorFlow Graph: Not found: Op type not registered 'DecodeWav' in binary running on Antons-iPhone. Make sure the Op and Kernel are registered in the binary running in this process.





",1,,10,2018-01-07T02:55:51Z,2018-01-31T19:36:01Z,NONE,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: I am trying to run graph model from Simple Audio Recognition example on iOS.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OS X 10.13
- **TensorFlow installed from (source or binary)**: Branch r1.4
- **TensorFlow version (use command below)**: 1.4
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: Build label: 0.9.0-homebrew
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A

### Describe the problem

I am trying to run graph model from Simple Audio Recognition example on iOS. When I am calling `session->Create(tensorflow_graph)` with the graph I get the error: ""Could not create TensorFlow Graph: Not found: Op type not registered 'DecodeWav'..."".

My initial thought is that because I am using TensorFlow-experimental (1.1.1) from pods, it's possible that this Op type is not registered. So I tried building it myself, which builds without errors with command: `tensorflow/contrib/makefile/build_all_ios.sh`. I then remove TensorFlow-experimental (1.1.1) from the project and link my own build of tensorflow, but I get the same error. 

I also found the following PR - [[iOS] Add optional Selective Registration of Ops #14421](https://github.com/tensorflow/tensorflow/pull/14421)

I tried building from master with the above PR merged like so:
 
For iPhone 5:
`tensorflow/contrib/makefile/build_all_ios.sh -a armv7 -g /Users/anton/Development/tensorflow/tensorflow/examples/ios/simple/data/tensorflow_inception_graph_speech.pb`

For iPhone SE:
`tensorflow/contrib/makefile/build_all_ios.sh -a arm64 -g /Users/anton/Development/tensorflow/tensorflow/examples/ios/simple/data/tensorflow_inception_graph_speech.pb`

If I then go and check the file `/tensorflow/tensorflow/core/framework/ops_to_register.h` (auto-generated after above command) I can see that DecodeWav is listed among kernels and operations:

```
// This file was autogenerated by print_selective_registration_header.py
#ifndef OPS_TO_REGISTER
#define OPS_TO_REGISTER

    namespace {
      constexpr const char* skip(const char* x) {
        return (*x) ? (*x == ' ' ? skip(x + 1) : x) : x;
      }

      constexpr bool isequal(const char* x, const char* y) {
        return (*skip(x) && *skip(y))
                   ? (*skip(x) == *skip(y) && isequal(skip(x) + 1, skip(y) + 1))
                   : (!*skip(x) && !*skip(y));
      }

      template<int N>
      struct find_in {
        static constexpr bool f(const char* x, const char* const y[N]) {
          return isequal(x, y[0]) || find_in<N - 1>::f(x, y + 1);
        }
      };

      template<>
      struct find_in<0> {
        static constexpr bool f(const char* x, const char* const y[]) {
          return false;
        }
      };
    }  // end namespace
    constexpr const char* kNecessaryOpKernelClasses[] = {
""BinaryOp< CPUDevice, functor::add<float>>"",
""AudioSpectrogramOp"",
""ConstantOp"",
""Conv2DOp<CPUDevice, float>"",
""DecodeWavOp"",
""IdentityOp"",
""MatMulOp<CPUDevice, float, false >"",
""MaxPoolingOp<CPUDevice, float>"",
""MfccOp"",
""NoOp"",
""PlaceholderOp"",
""ReluOp<CPUDevice, float>"",
""ReshapeOp"",
""SoftmaxOp<CPUDevice, float>"",
""RecvOp"",
""SendOp"",
};
#define SHOULD_REGISTER_OP_KERNEL(clz) (find_in<sizeof(kNecessaryOpKernelClasses) / sizeof(*kNecessaryOpKernelClasses)>::f(clz, kNecessaryOpKernelClasses))

constexpr inline bool ShouldRegisterOp(const char op[]) {
  return false
     || isequal(op, ""Add"")
     || isequal(op, ""AudioSpectrogram"")
     || isequal(op, ""Const"")
     || isequal(op, ""Conv2D"")
     || isequal(op, ""DecodeWav"")
     || isequal(op, ""Identity"")
     || isequal(op, ""MatMul"")
     || isequal(op, ""MaxPool"")
     || isequal(op, ""Mfcc"")
     || isequal(op, ""NoOp"")
     || isequal(op, ""Placeholder"")
     || isequal(op, ""Relu"")
     || isequal(op, ""Reshape"")
     || isequal(op, ""Softmax"")
     || isequal(op, ""_Recv"")
     || isequal(op, ""_Send"")
  ;
}
#define SHOULD_REGISTER_OP(op) ShouldRegisterOp(op)

#define SHOULD_REGISTER_OP_GRADIENT false
#endif

```

But when I try to run the graph model I still get same error message. I have removed the pod version, and I am 100% sure I am running my own build version of tensorflow on iOS.

I can't tell if this is a bug or I am doing something wrong during the build process.

Has anyone tried running any graph that uses DecodeWav on iOS?

Thanks.

### Source code / logs

Error: Could not create TensorFlow Graph: Not found: Op type not registered 'DecodeWav' in binary running on Antons-iPhone. Make sure the Op and Kernel are registered in the binary running in this process.





",2018-01-10T01:21:37Z,24,1,3,7.253084869277317
451,15918,Add pos_weights practical interpretation,cla: no,"The current  weighted_cross_entropy_with_logits docs don't explain practically the relationship of 
`pos_weights > 1`,  `pos_weights < 1` to precision, recall, and class imbalance.",0,,3,2018-01-06T20:24:18Z,2018-01-17T04:47:00Z,CONTRIBUTOR,"The current  weighted_cross_entropy_with_logits docs don't explain practically the relationship of 
`pos_weights > 1`,  `pos_weights < 1` to precision, recall, and class imbalance.",2018-01-07T05:50:59Z,11,2,2,3.7518706487516997
452,15917,Update docs for `concat` in case `axis < 0`,"awaiting testing (then merge),cla: yes","This fix tries to address the issue raised in #15905 where the documentation does not cover the case of `axis < 0` for `tf.concat`.

This fix fixes #15905.

Signed-off-by: Yong Tang <yong.tang.github@outlook.com>",1,,1,2018-01-06T19:37:24Z,2018-01-07T14:57:16Z,MEMBER,"This fix tries to address the issue raised in #15905 where the documentation does not cover the case of `axis < 0` for `tf.concat`.

This fix fixes #15905.

Signed-off-by: Yong Tang <yong.tang.github@outlook.com>",2018-01-07T14:50:52Z,1,3,0,4.7518706487517
453,15916,Object Tracking Support ,stat:awaiting response,"I have a bug after updating to the latest android studio and building the detection app with it.
For previous versions of android studio I didn't have this issue before

when I ran the tf_detect app it showed an error for few seconds that says ""Object Tracking Support Not Found...""
and when I add the line ""dependencies {
    compile 'org.tensorflow:tensorflow-android:+'
}""
to the gradle build file it shows another error
Error:(42, 0) Could not find method compile() for arguments [org.tensorflow:tensorflow-android:+] on object of type org.gradle.api.internal.artifacts.dsl.dependencies.DefaultDependencyHandler.
<a href=""openFile:C:\Users\mohda\Desktop\tensorflow-master_new\tensorflow-master\tensorflow\examples\android\build.gradle"">Open File</a>

any suggestions or fixes to this issue please?
What am I doing:
I have created a custom trained detector and it was working fine till android studio was updated. I have even tried with a fresh copy of the original demo and I have the same error. Yet when I downloaded the nightly build apk it didn't show any error. so it must be the android studio / tensorflow compatibility / dependency issue here.
Thanks 
  ",0,,3,2018-01-06T17:03:49Z,2018-02-05T19:32:24Z,NONE,"I have a bug after updating to the latest android studio and building the detection app with it.
For previous versions of android studio I didn't have this issue before

when I ran the tf_detect app it showed an error for few seconds that says ""Object Tracking Support Not Found...""
and when I add the line ""dependencies {
    compile 'org.tensorflow:tensorflow-android:+'
}""
to the gradle build file it shows another error
Error:(42, 0) Could not find method compile() for arguments [org.tensorflow:tensorflow-android:+] on object of type org.gradle.api.internal.artifacts.dsl.dependencies.DefaultDependencyHandler.
<a href=""openFile:C:\Users\mohda\Desktop\tensorflow-master_new\tensorflow-master\tensorflow\examples\android\build.gradle"">Open File</a>

any suggestions or fixes to this issue please?
What am I doing:
I have created a custom trained detector and it was working fine till android studio was updated. I have even tried with a fresh copy of the original demo and I have the same error. Yet when I downloaded the nightly build apk it didn't show any error. so it must be the android studio / tensorflow compatibility / dependency issue here.
Thanks 
  ",2018-01-07T01:02:14Z,29,1,4,2.7518706487516997
454,15914,How to know my loss function does not have numerical problems?,stat:awaiting response,"I wrote the following loss function that I post below. How do I know that I don't have any kind of numerical issues with it? (because something tells me that I do)

```
def gather_cols(params, indices, name=None):
    """"""Gather columns of a 2D tensor.

    Args:
        params: A 2D tensor.
        indices: A 1D tensor. Must be one of the following types: ``int32``, ``int64``.
        name: A name for the operation (optional).

    Returns:
        A 2D Tensor. Has the same type as ``params``.
    """"""
    with tf.op_scope([params, indices], name, ""gather_cols"") as scope:
        # Check input
        params = tf.convert_to_tensor(params, name=""params"")
        indices = tf.convert_to_tensor(indices, name=""indices"")
        try:
            params.get_shape().assert_has_rank(2)
        except ValueError:
            raise ValueError('\'params\' must be 2D.')
        try:
            indices.get_shape().assert_has_rank(1)
        except ValueError:
            raise ValueError('\'params\' must be 1D.')

        # Define op
        p_shape = tf.shape(params)
        p_flat = tf.reshape(params, [-1])
        i_flat = tf.reshape(tf.reshape(tf.range(0, p_shape[0]) * p_shape[1],
                                       [-1, 1]) + indices, [-1])
        return tf.reshape(tf.gather(p_flat, i_flat),
                          [p_shape[0], -1])


def custom_binary_crossentropy(y_true, y_pred):
    # Assumes y_pred are probabilities and that y_true has actually 2 labels inside
    # Calculate: gain(y1, y2) * log(p) + gain(y2, y1) * log(1 - p)
    # gain(x1, x2) = (2 ^ x1 - 1) / ((2 ^ x1 - 1) + (2 ^ x2 - 1))

    # Gather y1 and y2 first
    y1 = gather_cols(y_true, [0])
    y2 = gather_cols(y_true, [1])

    # Get 2^y - 1
    y1_g = tf.subtract(tf.pow(tf.fill(tf.shape(y1), 2.0), y1), tf.fill(tf.shape(y1), 1.0))
    y2_g = tf.subtract(tf.pow(tf.fill(tf.shape(y2), 2.0), y1), tf.fill(tf.shape(y2), 1.0))

    # Get gains
    gain1 = tf.div(y1_g, tf.add(y1_g, y2_g))
    gain2 = tf.div(y2_g, tf.add(y1_g, y2_g))

    # Get logs
    log1 = tf.log(y_pred)
    log2 = tf.log(tf.subtract(tf.fill(tf.shape(y_pred), 1.0), y_pred))

    return -K.mean(tf.add(tf.multiply(gain1, log1), tf.multiply(gain2, log2)))
```",0,,1,2018-01-06T16:36:28Z,2018-01-07T01:05:33Z,NONE,"I wrote the following loss function that I post below. How do I know that I don't have any kind of numerical issues with it? (because something tells me that I do)

```
def gather_cols(params, indices, name=None):
    """"""Gather columns of a 2D tensor.

    Args:
        params: A 2D tensor.
        indices: A 1D tensor. Must be one of the following types: ``int32``, ``int64``.
        name: A name for the operation (optional).

    Returns:
        A 2D Tensor. Has the same type as ``params``.
    """"""
    with tf.op_scope([params, indices], name, ""gather_cols"") as scope:
        # Check input
        params = tf.convert_to_tensor(params, name=""params"")
        indices = tf.convert_to_tensor(indices, name=""indices"")
        try:
            params.get_shape().assert_has_rank(2)
        except ValueError:
            raise ValueError('\'params\' must be 2D.')
        try:
            indices.get_shape().assert_has_rank(1)
        except ValueError:
            raise ValueError('\'params\' must be 1D.')

        # Define op
        p_shape = tf.shape(params)
        p_flat = tf.reshape(params, [-1])
        i_flat = tf.reshape(tf.reshape(tf.range(0, p_shape[0]) * p_shape[1],
                                       [-1, 1]) + indices, [-1])
        return tf.reshape(tf.gather(p_flat, i_flat),
                          [p_shape[0], -1])


def custom_binary_crossentropy(y_true, y_pred):
    # Assumes y_pred are probabilities and that y_true has actually 2 labels inside
    # Calculate: gain(y1, y2) * log(p) + gain(y2, y1) * log(1 - p)
    # gain(x1, x2) = (2 ^ x1 - 1) / ((2 ^ x1 - 1) + (2 ^ x2 - 1))

    # Gather y1 and y2 first
    y1 = gather_cols(y_true, [0])
    y2 = gather_cols(y_true, [1])

    # Get 2^y - 1
    y1_g = tf.subtract(tf.pow(tf.fill(tf.shape(y1), 2.0), y1), tf.fill(tf.shape(y1), 1.0))
    y2_g = tf.subtract(tf.pow(tf.fill(tf.shape(y2), 2.0), y1), tf.fill(tf.shape(y2), 1.0))

    # Get gains
    gain1 = tf.div(y1_g, tf.add(y1_g, y2_g))
    gain2 = tf.div(y2_g, tf.add(y1_g, y2_g))

    # Get logs
    log1 = tf.log(y_pred)
    log2 = tf.log(tf.subtract(tf.fill(tf.shape(y_pred), 1.0), y_pred))

    return -K.mean(tf.add(tf.multiply(gain1, log1), tf.multiply(gain2, log2)))
```",2018-01-07T01:02:09Z,1,1,0,1.7518706487516997
455,15909,Python Configuration Error,type:support,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **TensorFlow installed from (source or binary)**:  source 
- **TensorFlow version (use command below)**:the latest master branch
- **Python version**: 3.6.3 in anaconda ,python path is :C:/Users/huo_y/Anaconda3/python.exe
- **Bazel version (if compiling from source)**:0.9.0
- **GCC/Compiler version (if compiling from source)**:msvc 14
- **CUDA/cuDNN version**: no
- **GPU model and memory**: no
- **Exact command to reproduce**:
 bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package

### Describe the problem
when I build tensorflow with bazel on windows by msys2 shell. I got this error
Python Configuration Error : --define PYTHON_BIN_PATH='C:/Users/huo_y/Anaconda3/python.exe' is not executable. Is it the python binary?

### Source code / logs
ERROR: C:/users/huo_y/tensorflow-master/util/python/BUILD:5:1: no such package '@local_config_python//': Traceback (most recent call last):
        File ""C:/users/huo_y/tensorflow-master/third_party/py/python_configure.bzl"", line 291
                _create_local_python_repository(repository_ctx)
        File ""C:/users/huo_y/tensorflow-master/third_party/py/python_configure.bzl"", line 251, in _create_local_python_repository
                _check_python_bin(repository_ctx, python_bin)
        File ""C:/users/huo_y/tensorflow-master/third_party/py/python_configure.bzl"", line 204, in _check_python_bin
                _fail((""--define %s='%s' is not execut...)))
        File ""C:/users/huo_y/tensorflow-master/third_party/py/python_configure.bzl"", line 27, in _fail
                fail((""%sPython Configuration Error:%...)))
Python Configuration Error: --define PYTHON_BIN_PATH='C:/Users/huo_y/Anaconda3/python.exe' is not executable. Is it the python binary?
 and referenced by '//util/python:python_headers'
ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: Loading failed
INFO: Elapsed time: 16.677s
FAILED: Build did NOT complete successfully (63 packages loaded)
    currently loading: tensorflow/core/kernels
    Fetching http://ufpr.dl.sourceforge.net/project/swig/swig/swig-3.0.8/swig-3.0.8.tar.gz; 32,768b 4s

It may cause by python_configure.bzl I think,But I don't know how to correct it.

here is on function about the error  in python_config.bzl 
```
def _check_python_bin(repository_ctx, python_bin):
  """"""Checks the python bin path.""""""
  cmd =  '[[ -x ""%s"" ]] && [[ ! -d ""%s"" ]]' % (python_bin, python_bin)
  result = repository_ctx.execute([""bash"", ""-c"", cmd])
  if result.return_code == 1:
    _fail(""--define %s='%s' is not executable. Is it the python binary?"" % (
        _PYTHON_BIN_PATH, python_bin))
```
I find that when I run configure  the python path is windows format just like C:/Users/huo_y/Anaconda3/python.exe  but in msys2 the path may show /c/Users/huo_y/Anaconda3/python.exe .  I guess that when using bash -c ,it should need the path just like /c/Users/huo_y/Anaconda3/python.exe can get the correct return code, but it seems that the python_bin parameter is the windows path format .

can anyone help check it because I don't be farmiliar with bazel

",0,,1,2018-01-06T10:50:49Z,2018-01-09T22:28:30Z,NONE,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **TensorFlow installed from (source or binary)**:  source 
- **TensorFlow version (use command below)**:the latest master branch
- **Python version**: 3.6.3 in anaconda ,python path is :C:/Users/huo_y/Anaconda3/python.exe
- **Bazel version (if compiling from source)**:0.9.0
- **GCC/Compiler version (if compiling from source)**:msvc 14
- **CUDA/cuDNN version**: no
- **GPU model and memory**: no
- **Exact command to reproduce**:
 bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package

### Describe the problem
when I build tensorflow with bazel on windows by msys2 shell. I got this error
Python Configuration Error : --define PYTHON_BIN_PATH='C:/Users/huo_y/Anaconda3/python.exe' is not executable. Is it the python binary?

### Source code / logs
ERROR: C:/users/huo_y/tensorflow-master/util/python/BUILD:5:1: no such package '@local_config_python//': Traceback (most recent call last):
        File ""C:/users/huo_y/tensorflow-master/third_party/py/python_configure.bzl"", line 291
                _create_local_python_repository(repository_ctx)
        File ""C:/users/huo_y/tensorflow-master/third_party/py/python_configure.bzl"", line 251, in _create_local_python_repository
                _check_python_bin(repository_ctx, python_bin)
        File ""C:/users/huo_y/tensorflow-master/third_party/py/python_configure.bzl"", line 204, in _check_python_bin
                _fail((""--define %s='%s' is not execut...)))
        File ""C:/users/huo_y/tensorflow-master/third_party/py/python_configure.bzl"", line 27, in _fail
                fail((""%sPython Configuration Error:%...)))
Python Configuration Error: --define PYTHON_BIN_PATH='C:/Users/huo_y/Anaconda3/python.exe' is not executable. Is it the python binary?
 and referenced by '//util/python:python_headers'
ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: Loading failed
INFO: Elapsed time: 16.677s
FAILED: Build did NOT complete successfully (63 packages loaded)
    currently loading: tensorflow/core/kernels
    Fetching http://ufpr.dl.sourceforge.net/project/swig/swig/swig-3.0.8/swig-3.0.8.tar.gz; 32,768b 4s

It may cause by python_configure.bzl I think,But I don't know how to correct it.

here is on function about the error  in python_config.bzl 
```
def _check_python_bin(repository_ctx, python_bin):
  """"""Checks the python bin path.""""""
  cmd =  '[[ -x ""%s"" ]] && [[ ! -d ""%s"" ]]' % (python_bin, python_bin)
  result = repository_ctx.execute([""bash"", ""-c"", cmd])
  if result.return_code == 1:
    _fail(""--define %s='%s' is not executable. Is it the python binary?"" % (
        _PYTHON_BIN_PATH, python_bin))
```
I find that when I run configure  the python path is windows format just like C:/Users/huo_y/Anaconda3/python.exe  but in msys2 the path may show /c/Users/huo_y/Anaconda3/python.exe .  I guess that when using bash -c ,it should need the path just like /c/Users/huo_y/Anaconda3/python.exe can get the correct return code, but it seems that the python_bin parameter is the windows path format .

can anyone help check it because I don't be farmiliar with bazel

",2018-01-09T22:28:28Z,3,1,1,1.7518706487516997
456,15907,nightly installed TF is the new 1.5 TF?,stat:awaiting response,"today I heard that there are a new version 1.5 TF which is with good support dynamic graph.

And I also find there is a new nightly installed method.


So this nightly installing method is install the new Version TF?


```
chg0901@ubuntu:~$ python3
Python 3.5.2 (default, Nov 23 2017, 16:37:01) 
[GCC 5.4.0 20160609] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
2018-01-06 16:49:32.242801: I tensorflow/core/platform/s3/aws_logging.cc:53] Initializing Curl library
>>> print(tf.__version__)
1.6.0-dev20180105
>>> x = [[2.]]
>>> m = tf.matmul(x,x)
>>> print(m)
Tensor(""MatMul:0"", shape=(1, 1), dtype=float32)
>>> print(tf.Session().run(m))
2018-01-06 16:51:25.418750: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX FMA
[[ 4.]]

```

  ",0,,2,2018-01-06T07:58:38Z,2018-01-18T16:34:47Z,NONE,"today I heard that there are a new version 1.5 TF which is with good support dynamic graph.

And I also find there is a new nightly installed method.


So this nightly installing method is install the new Version TF?


```
chg0901@ubuntu:~$ python3
Python 3.5.2 (default, Nov 23 2017, 16:37:01) 
[GCC 5.4.0 20160609] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
2018-01-06 16:49:32.242801: I tensorflow/core/platform/s3/aws_logging.cc:53] Initializing Curl library
>>> print(tf.__version__)
1.6.0-dev20180105
>>> x = [[2.]]
>>> m = tf.matmul(x,x)
>>> print(m)
Tensor(""MatMul:0"", shape=(1, 1), dtype=float32)
>>> print(tf.Session().run(m))
2018-01-06 16:51:25.418750: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX FMA
[[ 4.]]

```

  ",2018-01-06T18:54:44Z,12,1,2,2.2518706487516997
457,15906,Add additional argument to freeze_graph,"awaiting testing (then merge),cla: yes,stat:awaiting tensorflower",This PR fixes the failed testFreezeGraphV1 test for PR https://github.com/tensorflow/tensorflow/pull/14341,1,,13,2018-01-06T07:20:01Z,2018-01-25T19:05:09Z,CONTRIBUTOR,This PR fixes the failed testFreezeGraphV1 test for PR https://github.com/tensorflow/tensorflow/pull/14341,2018-01-23T01:18:11Z,19,2,3,9.7518706487517
458,15905,Documentation does not explain the utility of -1 as value for the axis parameter of the tf.concat method,stat:awaiting response,"### System information

- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N/A
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OS X, version 10.13.2
- **TensorFlow installed from (source or binary)**: Binary (pip)
- **TensorFlow version (use command below)**: v1.3.0-rc1-5211-gab0fcac 1.5.0-dev20171126
- **Python version**: Python 3.5.0
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**:  N/A
- **CUDA/cuDNN version**:  N/A
- **GPU model and memory**:  N/A
- **Exact command to reproduce**: Just run a Python script with the code I am sharing with you

### Describe the problem

It apparently concatenates along the last axis. See the following example:  

```
import tensorflow as tf

t1 = [[[1, 2], [2, 3]], [[4, 4], [5, 3]]]
t2 = [[[7, 4], [8, 4]], [[2, 10], [15, 11]]]

with tf.Session() as sess:
    result = sess.run(tf.concat([t1, t2], -1))
    print(result)
```

which produces

```
[[[ 1  2  7  4]
  [ 2  3  8  4]]

 [[ 4  4  2 10]
  [ 5  3 15 11]]]
``` 

The following documention does not seem to explain this use case:

- https://www.tensorflow.org/api_docs/python/tf/concat
- https://www.tensorflow.org/versions/r1.5/api_docs/python/tf/concat
  
",0,,5,2018-01-06T03:41:22Z,2018-01-07T14:57:16Z,NONE,"### System information

- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N/A
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OS X, version 10.13.2
- **TensorFlow installed from (source or binary)**: Binary (pip)
- **TensorFlow version (use command below)**: v1.3.0-rc1-5211-gab0fcac 1.5.0-dev20171126
- **Python version**: Python 3.5.0
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**:  N/A
- **CUDA/cuDNN version**:  N/A
- **GPU model and memory**:  N/A
- **Exact command to reproduce**: Just run a Python script with the code I am sharing with you

### Describe the problem

It apparently concatenates along the last axis. See the following example:  

```
import tensorflow as tf

t1 = [[[1, 2], [2, 3]], [[4, 4], [5, 3]]]
t2 = [[[7, 4], [8, 4]], [[2, 10], [15, 11]]]

with tf.Session() as sess:
    result = sess.run(tf.concat([t1, t2], -1))
    print(result)
```

which produces

```
[[[ 1  2  7  4]
  [ 2  3  8  4]]

 [[ 4  4  2 10]
  [ 5  3 15 11]]]
``` 

The following documention does not seem to explain this use case:

- https://www.tensorflow.org/api_docs/python/tf/concat
- https://www.tensorflow.org/versions/r1.5/api_docs/python/tf/concat
  
",2018-01-06T12:54:19Z,1,1,0,3.7518706487516997
459,15902," W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations. 2018-01-06 09:50:33.745519: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations. 2018-01-06 09:50:33.745553: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.",,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",0,,2,2018-01-06T02:12:58Z,2018-01-06T05:39:30Z,NONE,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",2018-01-06T05:39:30Z,0,1,0,2.2518706487516997
460,15901,Branch 180993147,cla: yes,,0,,4,2018-01-06T01:20:08Z,2018-01-06T03:46:31Z,CONTRIBUTOR,,2018-01-06T01:21:28Z,0,2,0,4.2518706487517
461,15899,Addresses S3 timeout configurability discussed in #15868,"awaiting testing (then merge),cla: yes","This provides the ability to specify S3 timeouts via environment variables, as requested in #15868.
",1,,5,2018-01-06T00:54:44Z,2018-01-17T15:11:54Z,CONTRIBUTOR,"This provides the ability to specify S3 timeouts via environment variables, as requested in #15868.
",2018-01-11T14:52:37Z,11,2,2,5.7518706487517
462,15897,Tensor Core support for NVIDIA Volta architecture,stat:awaiting tensorflower,"

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: NO
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**:
5386775e64aac0bb5020974122645da900bc312a
- **Python version**: 3.5
- **Bazel version (if compiling from source)**:0.8.1
- **GCC/Compiler version (if comp6iling from source)**:5.4.0
- **CUDA/cuDNN version**:9.1 / 7.0.5
- **GPU model and memory**: Titan V
- **Exact command to reproduce**:

### Describe the problem
It is widely reported that using float16 on Nvidia Volta architecture comes only with x2 improvement instead of the expected x4 x8 improvement using Tensor Cores
https://github.com/tensorflow/benchmarks/issues/77
https://devblogs.nvidia.com/parallelforall/programming-tensor-cores-cuda-9/

I checked that Tensorflow master branch used 
cudnnGetConvolutionForwardAlgorithm
to get the best possible algorithm for the given GPU.
However I think either
cudnnGetConvolutionForwardAlgorithm_v7
or 
cudnnFindConvolutionForwardAlgorithmEx
should be used to fully utilize the Volta architecture.
Could you please check this issue with a Volta architecture GPU?
### Source code / logs
",1,,11,2018-01-05T23:48:20Z,2018-01-10T05:24:52Z,NONE,"

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: NO
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**:
5386775e64aac0bb5020974122645da900bc312a
- **Python version**: 3.5
- **Bazel version (if compiling from source)**:0.8.1
- **GCC/Compiler version (if comp6iling from source)**:5.4.0
- **CUDA/cuDNN version**:9.1 / 7.0.5
- **GPU model and memory**: Titan V
- **Exact command to reproduce**:

### Describe the problem
It is widely reported that using float16 on Nvidia Volta architecture comes only with x2 improvement instead of the expected x4 x8 improvement using Tensor Cores
https://github.com/tensorflow/benchmarks/issues/77
https://devblogs.nvidia.com/parallelforall/programming-tensor-cores-cuda-9/

I checked that Tensorflow master branch used 
cudnnGetConvolutionForwardAlgorithm
to get the best possible algorithm for the given GPU.
However I think either
cudnnGetConvolutionForwardAlgorithm_v7
or 
cudnnFindConvolutionForwardAlgorithmEx
should be used to fully utilize the Volta architecture.
Could you please check this issue with a Volta architecture GPU?
### Source code / logs
",2018-01-09T22:25:59Z,5,1,1,7.750690398438997
463,15893,Transpose for high dimensional tensors using eigen,"awaiting testing (then merge),cla: yes","In our [library](https://github.com/Bihaqo/t3f) we rely heavily on fast transposes of high dimensional tensors, so we have added few extra cases to the CPU and GPU transpose functors. Previously it was done up to dimension 5 and we included dimensions 6, 7, 8 (similarly to the TENSORFLOW_USE_SYCL case).",1,,5,2018-01-05T22:41:55Z,2018-01-23T14:19:36Z,CONTRIBUTOR,"In our [library](https://github.com/Bihaqo/t3f) we rely heavily on fast transposes of high dimensional tensors, so we have added few extra cases to the CPU and GPU transpose functors. Previously it was done up to dimension 5 and we included dimensions 6, 7, 8 (similarly to the TENSORFLOW_USE_SYCL case).",2018-01-05T22:43:47Z,18,2,3,5.750690398438997
464,15892,Change GitHub repo URL from http://www.tensorflow.org to https://www.tensorflow.org,,Reasoning: HTTPS all the things,1,,7,2018-01-05T22:24:22Z,2018-01-31T00:52:25Z,CONTRIBUTOR,Reasoning: HTTPS all the things,2018-01-06T07:00:07Z,26,2,3,6.750690398438997
465,15891,Dependencies of tensors created within a tf.while_loop() might not be executed,"stat:awaiting tensorflower,type:bug/performance","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes. See test case below.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS 'Sierra' Version 10.12.6 (16G1114)
- **TensorFlow installed from (source or binary)**: Both. I have compiled TensorFlow at 136697ecdc64b5171522fb7f89cfe51a02f0f1c1 with my small change in PR #15823. I have also tried using the pip package.
- **TensorFlow version (use command below)**: ('v1.4.0-19-ga52c8d9b01', '1.4.1') (pip package)
- **Python version**: 2.7.10
- **Bazel version (if compiling from source)**: 0.9.0-homebrew
- **GCC/Compiler version (if compiling from source)**: Apple LLVM version 8.1.0 (clang-802.0.42)
- **CUDA/cuDNN version**: CUDA 9.0.176_mac, cuDNN 9.0-osx-x64-v7
- **GPU model and memory**: NVIDIA GeForce GT 750M with 2048 MB device memory (CUDA Compute Capability 3.0)
- **Exact command to reproduce**:

`python repro.py`

.. where `repro.py` contains the test case to reproduce, listed below.

### Describe the problem
Here is my test case:

```python
# Part I
from __future__ import division, print_function
import numpy as np
import tensorflow as tf
from tensorflow.python.ops import resource_variable_ops as rr

rs = np.random.RandomState(seed = 2)
A = rs.normal(size = (10, 10,))
print('singular values of A: %s' % (np.linalg.svd(A, compute_uv = False),))
B = rs.normal(size = (10, 10,))
print('singular values of B: %s' % (np.linalg.svd(B, compute_uv = False),))



# Part II
A_var = tf.Variable(B)
init_A_var_op = tf.assign(A_var, A)
A_dep = tf.constant(10, tf.int32)

with tf.control_dependencies([init_A_var_op]):
    A_dep = A_dep + 1

with tf.control_dependencies([A_dep]):
    var_s = tf.svd(A_var, compute_uv = False)
with tf.Session() as session:
    session.run(tf.global_variables_initializer())
    computed_s, computed_A_dep = session.run([var_s, A_dep])
print('computed_s = %s, computed_A_dep = %d' % (computed_s, computed_A_dep,))



# Part III
A_var = tf.Variable(B)
init_A_var_op = tf.assign(A_var, A)
A_dep = tf.constant(9, tf.int32)

def loop_condition(j, A_dep):
    return j < 1
def loop_body(j, A_dep):
    with tf.control_dependencies([init_A_var_op]):
        A_dep = A_dep + 1
    return j + 1, A_dep

_, A_dep = tf.while_loop(loop_condition,
                         loop_body,
                         loop_vars = [tf.constant(0, tf.int32), A_dep],
                         parallel_iterations = 1,
                         back_prop = False)

with tf.control_dependencies([A_dep]):
    var_s = tf.svd(A_var, compute_uv = False)
with tf.Session() as session:
    session.run(tf.global_variables_initializer())
    computed_s, computed_A_dep = session.run([var_s, A_dep])
print('computed_s = %s, computed_A_dep = %d' % (computed_s, computed_A_dep,))



# Part IV
A_var = rr.ResourceVariable(B)
init_A_var_op = A_var.assign(A)
A_dep = tf.constant(8, tf.int32)

def loop_condition(j, A_dep):
    return j < 1
def loop_body(j, A_dep):
    with tf.control_dependencies([init_A_var_op]):
        A_dep = A_dep + 1
    return j + 1, A_dep

_, A_dep = tf.while_loop(loop_condition,
                         loop_body,
                         loop_vars = [tf.constant(0, tf.int32), A_dep],
                         parallel_iterations = 1,
                         back_prop = False)

with tf.control_dependencies([A_dep]):
    var_s = tf.svd(A_var.read_value(), compute_uv = False)
with tf.Session() as session:
    session.run(tf.global_variables_initializer())
    computed_s, computed_A_dep = session.run([var_s, A_dep])
print('computed_s = %s, computed_A_dep = %d' % (computed_s, computed_A_dep,))
```

Part I is basic setup. I create two random 10&times;10 matrices and compute their singular values:
<pre>
singular values of A: [ 5.65906715  4.9420261   4.40626739  3.73506125  2.70703249  2.57429488
  1.73387162  1.16000494  0.58836563  0.39101954]
singular values of B: [ 7.0283055   4.65840063  4.48502098  3.25319445  2.94667168  2.74267484
  1.86004291  1.6626967   0.63884034  0.27131664]
</pre>

Part II shows usage of control_dependencies() to guarantee that `A` has been assigned to `A_var` before the singular values of `A_var` are computed. The output from this part is:
<pre>
computed_s = [ 5.65906715  4.9420261   4.40626739  3.73506125  2.70703249  2.57429488
  1.73387162  1.16000494  0.58836563  0.39101954], computed_A_dep = 11
</pre>

(This is the expected result for Part II.)

In Part III, I have introduced use of a tf.while_loop(). Now, tf.svd() is returning the singular values of `B`:
<pre>
computed_s = [ 7.0283055   4.65840063  4.48502098  3.25319445  2.94667168  2.74267484
  1.86004291  1.6626967   0.63884034  0.27131664], computed_A_dep = 10
</pre>

(This is **not** the expected result for Part III. I expect that the singular values of `A` would be printed.)

In Part IV, based on reading https://github.com/tensorflow/tensorflow/issues/4663#issuecomment-336609536 , I switched to using `ResourceVariable`. However, the output is still the same (the singular values of `B`):
<pre>
computed_s = [ 7.0283055   4.65840063  4.48502098  3.25319445  2.94667168  2.74267484
  1.86004291  1.6626967   0.63884034  0.27131664], computed_A_dep = 9
</pre>

(This is **not** the expected result for Part IV. I expect that the singular values of `A` would be printed.)

It appears the issue is that tf.control_dependencies() on tensors created by tf.while_loop() might not execute the tensors' own dependencies.

This used to work okay (around TensorFlow 1.1, if I recall correctly).

While searching for a previous report of this issue, I found #6087 which appears related, in that the sample code there has a tf.while_loop() that creates tensors with dependencies. When I run the sample code, I consistently get result = 10. This is an unexpected result, in my opinion. What is happening is that `update_x` runs exactly once, so for each of the 5 loop iterations, `x` has the value 2.

I tried rewriting the sample code to use a ResourceVariable, but the output is the same:

```python
from __future__ import division, print_function
import tensorflow as tf
from tensorflow.python.ops import resource_variable_ops as rr

with tf.variable_scope('state'):
    x = rr.ResourceVariable(tf.constant(1, dtype=tf.float32))
    update_x = x.assign(x.read_value() + 1)

def iter_fun(i, y):
    # comment the line below, the program will run without any error
    # but I need control_dependencies, or at least some way to replace it...
    with tf.control_dependencies([update_x]):
        y = y + tf.Print(x.read_value(), ['i = ', i, 'y = ', y, 'x = ', x.read_value()])
    return (i+1, y,)

with tf.variable_scope('iteration'):
    num_iterations = 5
    initial_i = tf.constant(0, dtype=tf.int32)
    initial_y = tf.constant(0, dtype=tf.float32)
    _, result = tf.while_loop(
        cond=lambda i, *_: i < num_iterations,
        body=iter_fun,
        loop_vars=(initial_i, initial_y))

init_op = tf.global_variables_initializer()

with tf.Session() as sess:
    sess.run(init_op)
    print(sess.run(result))
```
  ",1,,12,2018-01-05T21:51:29Z,2018-02-06T18:29:02Z,CONTRIBUTOR,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes. See test case below.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS 'Sierra' Version 10.12.6 (16G1114)
- **TensorFlow installed from (source or binary)**: Both. I have compiled TensorFlow at 136697ecdc64b5171522fb7f89cfe51a02f0f1c1 with my small change in PR #15823. I have also tried using the pip package.
- **TensorFlow version (use command below)**: ('v1.4.0-19-ga52c8d9b01', '1.4.1') (pip package)
- **Python version**: 2.7.10
- **Bazel version (if compiling from source)**: 0.9.0-homebrew
- **GCC/Compiler version (if compiling from source)**: Apple LLVM version 8.1.0 (clang-802.0.42)
- **CUDA/cuDNN version**: CUDA 9.0.176_mac, cuDNN 9.0-osx-x64-v7
- **GPU model and memory**: NVIDIA GeForce GT 750M with 2048 MB device memory (CUDA Compute Capability 3.0)
- **Exact command to reproduce**:

`python repro.py`

.. where `repro.py` contains the test case to reproduce, listed below.

### Describe the problem
Here is my test case:

```python
# Part I
from __future__ import division, print_function
import numpy as np
import tensorflow as tf
from tensorflow.python.ops import resource_variable_ops as rr

rs = np.random.RandomState(seed = 2)
A = rs.normal(size = (10, 10,))
print('singular values of A: %s' % (np.linalg.svd(A, compute_uv = False),))
B = rs.normal(size = (10, 10,))
print('singular values of B: %s' % (np.linalg.svd(B, compute_uv = False),))



# Part II
A_var = tf.Variable(B)
init_A_var_op = tf.assign(A_var, A)
A_dep = tf.constant(10, tf.int32)

with tf.control_dependencies([init_A_var_op]):
    A_dep = A_dep + 1

with tf.control_dependencies([A_dep]):
    var_s = tf.svd(A_var, compute_uv = False)
with tf.Session() as session:
    session.run(tf.global_variables_initializer())
    computed_s, computed_A_dep = session.run([var_s, A_dep])
print('computed_s = %s, computed_A_dep = %d' % (computed_s, computed_A_dep,))



# Part III
A_var = tf.Variable(B)
init_A_var_op = tf.assign(A_var, A)
A_dep = tf.constant(9, tf.int32)

def loop_condition(j, A_dep):
    return j < 1
def loop_body(j, A_dep):
    with tf.control_dependencies([init_A_var_op]):
        A_dep = A_dep + 1
    return j + 1, A_dep

_, A_dep = tf.while_loop(loop_condition,
                         loop_body,
                         loop_vars = [tf.constant(0, tf.int32), A_dep],
                         parallel_iterations = 1,
                         back_prop = False)

with tf.control_dependencies([A_dep]):
    var_s = tf.svd(A_var, compute_uv = False)
with tf.Session() as session:
    session.run(tf.global_variables_initializer())
    computed_s, computed_A_dep = session.run([var_s, A_dep])
print('computed_s = %s, computed_A_dep = %d' % (computed_s, computed_A_dep,))



# Part IV
A_var = rr.ResourceVariable(B)
init_A_var_op = A_var.assign(A)
A_dep = tf.constant(8, tf.int32)

def loop_condition(j, A_dep):
    return j < 1
def loop_body(j, A_dep):
    with tf.control_dependencies([init_A_var_op]):
        A_dep = A_dep + 1
    return j + 1, A_dep

_, A_dep = tf.while_loop(loop_condition,
                         loop_body,
                         loop_vars = [tf.constant(0, tf.int32), A_dep],
                         parallel_iterations = 1,
                         back_prop = False)

with tf.control_dependencies([A_dep]):
    var_s = tf.svd(A_var.read_value(), compute_uv = False)
with tf.Session() as session:
    session.run(tf.global_variables_initializer())
    computed_s, computed_A_dep = session.run([var_s, A_dep])
print('computed_s = %s, computed_A_dep = %d' % (computed_s, computed_A_dep,))
```

Part I is basic setup. I create two random 10&times;10 matrices and compute their singular values:
<pre>
singular values of A: [ 5.65906715  4.9420261   4.40626739  3.73506125  2.70703249  2.57429488
  1.73387162  1.16000494  0.58836563  0.39101954]
singular values of B: [ 7.0283055   4.65840063  4.48502098  3.25319445  2.94667168  2.74267484
  1.86004291  1.6626967   0.63884034  0.27131664]
</pre>

Part II shows usage of control_dependencies() to guarantee that `A` has been assigned to `A_var` before the singular values of `A_var` are computed. The output from this part is:
<pre>
computed_s = [ 5.65906715  4.9420261   4.40626739  3.73506125  2.70703249  2.57429488
  1.73387162  1.16000494  0.58836563  0.39101954], computed_A_dep = 11
</pre>

(This is the expected result for Part II.)

In Part III, I have introduced use of a tf.while_loop(). Now, tf.svd() is returning the singular values of `B`:
<pre>
computed_s = [ 7.0283055   4.65840063  4.48502098  3.25319445  2.94667168  2.74267484
  1.86004291  1.6626967   0.63884034  0.27131664], computed_A_dep = 10
</pre>

(This is **not** the expected result for Part III. I expect that the singular values of `A` would be printed.)

In Part IV, based on reading https://github.com/tensorflow/tensorflow/issues/4663#issuecomment-336609536 , I switched to using `ResourceVariable`. However, the output is still the same (the singular values of `B`):
<pre>
computed_s = [ 7.0283055   4.65840063  4.48502098  3.25319445  2.94667168  2.74267484
  1.86004291  1.6626967   0.63884034  0.27131664], computed_A_dep = 9
</pre>

(This is **not** the expected result for Part IV. I expect that the singular values of `A` would be printed.)

It appears the issue is that tf.control_dependencies() on tensors created by tf.while_loop() might not execute the tensors' own dependencies.

This used to work okay (around TensorFlow 1.1, if I recall correctly).

While searching for a previous report of this issue, I found #6087 which appears related, in that the sample code there has a tf.while_loop() that creates tensors with dependencies. When I run the sample code, I consistently get result = 10. This is an unexpected result, in my opinion. What is happening is that `update_x` runs exactly once, so for each of the 5 loop iterations, `x` has the value 2.

I tried rewriting the sample code to use a ResourceVariable, but the output is the same:

```python
from __future__ import division, print_function
import tensorflow as tf
from tensorflow.python.ops import resource_variable_ops as rr

with tf.variable_scope('state'):
    x = rr.ResourceVariable(tf.constant(1, dtype=tf.float32))
    update_x = x.assign(x.read_value() + 1)

def iter_fun(i, y):
    # comment the line below, the program will run without any error
    # but I need control_dependencies, or at least some way to replace it...
    with tf.control_dependencies([update_x]):
        y = y + tf.Print(x.read_value(), ['i = ', i, 'y = ', y, 'x = ', x.read_value()])
    return (i+1, y,)

with tf.variable_scope('iteration'):
    num_iterations = 5
    initial_i = tf.constant(0, dtype=tf.int32)
    initial_y = tf.constant(0, dtype=tf.float32)
    _, result = tf.while_loop(
        cond=lambda i, *_: i < num_iterations,
        body=iter_fun,
        loop_vars=(initial_i, initial_y))

init_op = tf.global_variables_initializer()

with tf.Session() as sess:
    sess.run(init_op)
    print(sess.run(result))
```
  ",2018-01-11T21:54:17Z,31,2,4,9.250690398438998
466,15889,error while bazel build,stat:awaiting response,"## System information

- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): 
**no**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
**Ubuntu 16.04**
- TensorFlow installed from (source or binary):
 **Source**
- TensorFlow version (use command below):
 **r1.5**
- Python version: 
**3.6.3(Anaconda)**
- GCC/Compiler version (if compiling from source): 
**5.4**
- CUDA/cuDNN version:
 **9.0.176 / 7.0.5**
- GPU model and memory: 
**NVIDIA GTX 1080, Driver 385.111, 8G**
- Bazel version (if compiling from source):
**0.9**
- Exact command to reproduce:
`bazel build  --config=opt --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" --config=cuda //tensorflow/tools/pip_package:build_pip_package --action_env=""LD_LIBRARY_PATH=${LD_LIBRARY_PATH}""`

##  Describe the problem

Use the commend line:
`bazel build  --config=opt --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" --config=cuda //tensorflow/tools/pip_package:build_pip_package --action_env=""LD_LIBRARY_PATH=${LD_LIBRARY_PATH}""`
And got the error:

```
> ERROR: /home/xxh/tensorflow/tensorflow/contrib/boosted_trees/BUILD:559:1: Linking of rule '//tensorflow/contrib/boosted_trees:gen_gen_stats_accumulator_ops_py_wrap_py_wrappers_cc' failed (Exit 1)
> /usr/bin/ld: warning: libcublas.so.9.0, needed by bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sboosted_Utrees_Cgen_Ugen_Ustats_Uaccumulator_Uops_Upy_Uwrap_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so, not found (try using -rpath or -rpath-link)
> /usr/bin/ld: warning: libcudnn.so.7, needed by bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sboosted_Utrees_Cgen_Ugen_Ustats_Uaccumulator_Uops_Upy_Uwrap_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so, not found (try using -rpath or -rpath-link)
> /usr/bin/ld: warning: libcufft.so.9.0, needed by bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sboosted_Utrees_Cgen_Ugen_Ustats_Uaccumulator_Uops_Upy_Uwrap_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so, not found (try using -rpath or -rpath-link)
> /usr/bin/ld: warning: libcurand.so.9.0, needed by bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sboosted_Utrees_Cgen_Ugen_Ustats_Uaccumulator_Uops_Upy_Uwrap_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so, not found (try using -rpath or -rpath-link)
> bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sboosted_Utrees_Cgen_Ugen_Ustats_Uaccumulator_Uops_Upy_Uwrap_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasGemmEx@libcublas.so.9.0'
> bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sboosted_Utrees_Cgen_Ugen_Ustats_Uaccumulator_Uops_Upy_Uwrap_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasZhpmv_v2@libcublas.so.9.0'
> bazel-out/host/
> ......
> ......
> bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sboosted_Utrees_Cgen_Ugen_Ustats_Uaccumulator_Uops_Upy_Uwrap_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasDsymm_v2@libcublas.so.9.0'
> bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sboosted_Utrees_Cgen_Ugen_Ustats_Uaccumulator_Uops_Upy_Uwrap_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnCreateLRNDescriptor@libcudnn.so.7'
> collect2: error: ld returned 1 exit status
> Target //tensorflow/tools/pip_package:build_pip_package failed to build
> Use --verbose_failures to see the command lines of failed build steps.
> INFO: Elapsed time: 894.638s, Critical Path: 24.00s
> FAILED: Build did NOT complete successfully
```
When coppileing,I got a lot of warning like this.
`WARNING: /home/xxh/tensorflow/tensorflow/contrib/learn/BUILD:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': No longer supported. Switch to SavedModel immediately.`

Cuda 9.0 test pass! And copied the cudnn.h and libcudnn* to cuda file.
It's all fine.
  
  ",0,,3,2018-01-05T20:05:06Z,2018-01-09T11:07:40Z,NONE,"## System information

- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): 
**no**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
**Ubuntu 16.04**
- TensorFlow installed from (source or binary):
 **Source**
- TensorFlow version (use command below):
 **r1.5**
- Python version: 
**3.6.3(Anaconda)**
- GCC/Compiler version (if compiling from source): 
**5.4**
- CUDA/cuDNN version:
 **9.0.176 / 7.0.5**
- GPU model and memory: 
**NVIDIA GTX 1080, Driver 385.111, 8G**
- Bazel version (if compiling from source):
**0.9**
- Exact command to reproduce:
`bazel build  --config=opt --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" --config=cuda //tensorflow/tools/pip_package:build_pip_package --action_env=""LD_LIBRARY_PATH=${LD_LIBRARY_PATH}""`

##  Describe the problem

Use the commend line:
`bazel build  --config=opt --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" --config=cuda //tensorflow/tools/pip_package:build_pip_package --action_env=""LD_LIBRARY_PATH=${LD_LIBRARY_PATH}""`
And got the error:

```
> ERROR: /home/xxh/tensorflow/tensorflow/contrib/boosted_trees/BUILD:559:1: Linking of rule '//tensorflow/contrib/boosted_trees:gen_gen_stats_accumulator_ops_py_wrap_py_wrappers_cc' failed (Exit 1)
> /usr/bin/ld: warning: libcublas.so.9.0, needed by bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sboosted_Utrees_Cgen_Ugen_Ustats_Uaccumulator_Uops_Upy_Uwrap_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so, not found (try using -rpath or -rpath-link)
> /usr/bin/ld: warning: libcudnn.so.7, needed by bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sboosted_Utrees_Cgen_Ugen_Ustats_Uaccumulator_Uops_Upy_Uwrap_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so, not found (try using -rpath or -rpath-link)
> /usr/bin/ld: warning: libcufft.so.9.0, needed by bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sboosted_Utrees_Cgen_Ugen_Ustats_Uaccumulator_Uops_Upy_Uwrap_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so, not found (try using -rpath or -rpath-link)
> /usr/bin/ld: warning: libcurand.so.9.0, needed by bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sboosted_Utrees_Cgen_Ugen_Ustats_Uaccumulator_Uops_Upy_Uwrap_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so, not found (try using -rpath or -rpath-link)
> bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sboosted_Utrees_Cgen_Ugen_Ustats_Uaccumulator_Uops_Upy_Uwrap_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasGemmEx@libcublas.so.9.0'
> bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sboosted_Utrees_Cgen_Ugen_Ustats_Uaccumulator_Uops_Upy_Uwrap_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasZhpmv_v2@libcublas.so.9.0'
> bazel-out/host/
> ......
> ......
> bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sboosted_Utrees_Cgen_Ugen_Ustats_Uaccumulator_Uops_Upy_Uwrap_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasDsymm_v2@libcublas.so.9.0'
> bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sboosted_Utrees_Cgen_Ugen_Ustats_Uaccumulator_Uops_Upy_Uwrap_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnCreateLRNDescriptor@libcudnn.so.7'
> collect2: error: ld returned 1 exit status
> Target //tensorflow/tools/pip_package:build_pip_package failed to build
> Use --verbose_failures to see the command lines of failed build steps.
> INFO: Elapsed time: 894.638s, Critical Path: 24.00s
> FAILED: Build did NOT complete successfully
```
When coppileing,I got a lot of warning like this.
`WARNING: /home/xxh/tensorflow/tensorflow/contrib/learn/BUILD:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': No longer supported. Switch to SavedModel immediately.`

Cuda 9.0 test pass! And copied the cudnn.h and libcudnn* to cuda file.
It's all fine.
  
  ",2018-01-06T07:00:03Z,4,1,1,2.750690398438997
467,15888,Cleanup CocoaPods dependency from TFLite iOS examples,"awaiting review,cla: yes",,1,,2,2018-01-05T19:47:50Z,2018-01-18T18:59:20Z,CONTRIBUTOR,,2018-01-18T18:59:20Z,13,2,2,4.250690398438997
468,15886,"Successful Local Build of Tensorflow r1.5 GPU for Python 3.6, CUDA Toolkit 9.0, and CUDNN 7.0 on Windows 7 X64 SP1 using CMake in VS 2015 Update 3","stat:community support,type:build/install","I have spent a week trying to compile Tensorflow from source using Bazel on Windows with no success. In 2 days, I was able to compile it using CMake following the command output from a successful build I saw on Jenkins on 02-Jan-2018.

I wanted to provide details to spare others the pain of development in the future. The whole build took 6 hours to compile on my system.

I have an older system, which is why I was doing this. You will need to path variables in the attached scripts to work with the path variables for your system. For the most part, however, the scripts replicate what is mentioned on github here:

https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/cmake

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:  Yes

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 7 x64 SP1
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: r1.5.0-rc0
- **Python version**: 3.6.4
- **CMake version (if compiling from source)**: CMake 3.10.1
- **GCC/Compiler version (if compiling from source)**: cl.exe (Visual Studio 2015 Update 3)
- **CUDA/cuDNN version**: 9.0.176 / 7.0.4
- **GPU model and memory**: NVIDIA Quadro K4000 P8, Driver 385.54, 3072 MiB
- **SWIG Version**: swigwin-3.0.12
- **Git Version**: Git for Windows 2.15.1 64-bit
- **MSBuild Version**: 14.0.25420.1
- **CPU**: Intel Xeon E5-2620 v2

- **Exact command to reproduce**:

After installing the above, I wrote a batch script to set and clean up system environment variables (please see attached script). Due to the 1024 character limit for PATH on windows, I manually edited the PATH in the registry editor to overcome this limitation.

I then wrote another script that set local variables, cloned tensorflow source and checked out version 1.5, then prepared the source with cmake and compiled with msbuild.

The final output was a python wheel, which I pip installed. I successfully ran the standard hello world script without error, i.e.

```
import tensorflow as tf
hello = tf.constant('Hello')
sess = tf.Session()
sess.run(hello)
```

as well as a small AlexNet network without issues.

I hope this helps future users and further emphasizes that it is possible to build Tensorflow 1.5 for GPU on Windows 7. I have not compiled this with AVX support, but that could be a next improvement (I'm not sure if this is possible. I only know MKL support is limited to Linux at the moment. However, Windows binaries for MKL and MPI can be downloaded from the Intel website).


[Scripts.zip](https://github.com/tensorflow/tensorflow/files/1607554/Scripts.zip)



",0,,8,2018-01-05T18:50:07Z,2018-01-31T00:41:51Z,NONE,"I have spent a week trying to compile Tensorflow from source using Bazel on Windows with no success. In 2 days, I was able to compile it using CMake following the command output from a successful build I saw on Jenkins on 02-Jan-2018.

I wanted to provide details to spare others the pain of development in the future. The whole build took 6 hours to compile on my system.

I have an older system, which is why I was doing this. You will need to path variables in the attached scripts to work with the path variables for your system. For the most part, however, the scripts replicate what is mentioned on github here:

https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/cmake

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:  Yes

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 7 x64 SP1
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: r1.5.0-rc0
- **Python version**: 3.6.4
- **CMake version (if compiling from source)**: CMake 3.10.1
- **GCC/Compiler version (if compiling from source)**: cl.exe (Visual Studio 2015 Update 3)
- **CUDA/cuDNN version**: 9.0.176 / 7.0.4
- **GPU model and memory**: NVIDIA Quadro K4000 P8, Driver 385.54, 3072 MiB
- **SWIG Version**: swigwin-3.0.12
- **Git Version**: Git for Windows 2.15.1 64-bit
- **MSBuild Version**: 14.0.25420.1
- **CPU**: Intel Xeon E5-2620 v2

- **Exact command to reproduce**:

After installing the above, I wrote a batch script to set and clean up system environment variables (please see attached script). Due to the 1024 character limit for PATH on windows, I manually edited the PATH in the registry editor to overcome this limitation.

I then wrote another script that set local variables, cloned tensorflow source and checked out version 1.5, then prepared the source with cmake and compiled with msbuild.

The final output was a python wheel, which I pip installed. I successfully ran the standard hello world script without error, i.e.

```
import tensorflow as tf
hello = tf.constant('Hello')
sess = tf.Session()
sess.run(hello)
```

as well as a small AlexNet network without issues.

I hope this helps future users and further emphasizes that it is possible to build Tensorflow 1.5 for GPU on Windows 7. I have not compiled this with AVX support, but that could be a next improvement (I'm not sure if this is possible. I only know MKL support is limited to Linux at the moment. However, Windows binaries for MKL and MPI can be downloaded from the Intel website).


[Scripts.zip](https://github.com/tensorflow/tensorflow/files/1607554/Scripts.zip)



",2018-01-06T06:59:58Z,26,1,3,5.250690398438997
469,15885,Deletes unnecessary lines of code,"awaiting testing (then merge),cla: yes","The rest of this function changed sufficiently that these lines of code are not doing anything and can cause bugs.

Fixes issue #15852",1,,1,2018-01-05T18:41:14Z,2018-01-05T20:51:47Z,MEMBER,"The rest of this function changed sufficiently that these lines of code are not doing anything and can cause bugs.

Fixes issue #15852",2018-01-05T18:41:30Z,0,3,0,4.750690398438997
470,15882,"tfdbg error ""Dump root directory does not exist"" with empty fetches",type:bug/performance,"### System information
- **Have I written custom code**: yes
- **OS Platform and Distribution**: Linux Ubuntu 16.04
- **TensorFlow installed from**: binary (pip install)
- **TensorFlow version**:
== tensorflow import ============================================
tf.VERSION = 1.4.1
tf.GIT_VERSION = v1.4.0-19-ga52c8d9
tf.COMPILER_VERSION = v1.4.0-19-ga52c8d9
Sanity check: array([1], dtype=int32)
- **Python version**: 2.7.12
- **CUDA/cuDNN version**: 
== cuda libs  ===================================================
/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudart_static.a
/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudart.so.9.0.176
/usr/local/cuda-9.0/doc/man/man7/libcudart.so.7
/usr/local/cuda-9.0/doc/man/man7/libcudart.7
/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart.so.8.0.61
/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart_static.a
/usr/local/cuda-8.0/doc/man/man7/libcudart.so.7
/usr/local/cuda-8.0/doc/man/man7/libcudart.7
- **GPU model and memory**: GeForce GTX 1080, 8114MiB
- **Exact command to reproduce**: see code below

### Describe the problem
`LocalCLIDebugWrapperSession.run()` does not behave like `tf.Session.run()` if there are no fetches. The dump directory will never be created and it crashes with an `IOError`. For me this issue occured in a situation like this:
```
      session.run([var.initializer for var in not_initialized_from_checkpoint])
```
where actually everything was restored from the checkpoint and `not_initialized_from_checkpoint` was empty. This code runs fine with an ordinary tf.Session but crashed with tfdbg. It took me some time to track down the issue. If it's not too hard to fix, it would be nice to keep other users from the same pain (maybe - just speculating - #13604 crashes for the same reason)

### Source code / logs
```
import tensorflow as tf
from tensorflow.python import debug as tf_debug

sess = tf.Session()
dbg_sess = tf_debug.LocalCLIDebugWrapperSession(tf.Session())

print sess.run([tf.constant(1.0)])     # [1.0]
print sess.run([])                     # []
print dbg_sess.run([tf.constant(1.0)]) # [1.0]
print dbg_sess.run([])                 # IOError: Dump root directory /tmp/tfdbg_ai_aWv does not exist
```
",1,,4,2018-01-05T15:34:48Z,2018-01-30T23:34:01Z,NONE,"### System information
- **Have I written custom code**: yes
- **OS Platform and Distribution**: Linux Ubuntu 16.04
- **TensorFlow installed from**: binary (pip install)
- **TensorFlow version**:
== tensorflow import ============================================
tf.VERSION = 1.4.1
tf.GIT_VERSION = v1.4.0-19-ga52c8d9
tf.COMPILER_VERSION = v1.4.0-19-ga52c8d9
Sanity check: array([1], dtype=int32)
- **Python version**: 2.7.12
- **CUDA/cuDNN version**: 
== cuda libs  ===================================================
/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudart_static.a
/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudart.so.9.0.176
/usr/local/cuda-9.0/doc/man/man7/libcudart.so.7
/usr/local/cuda-9.0/doc/man/man7/libcudart.7
/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart.so.8.0.61
/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart_static.a
/usr/local/cuda-8.0/doc/man/man7/libcudart.so.7
/usr/local/cuda-8.0/doc/man/man7/libcudart.7
- **GPU model and memory**: GeForce GTX 1080, 8114MiB
- **Exact command to reproduce**: see code below

### Describe the problem
`LocalCLIDebugWrapperSession.run()` does not behave like `tf.Session.run()` if there are no fetches. The dump directory will never be created and it crashes with an `IOError`. For me this issue occured in a situation like this:
```
      session.run([var.initializer for var in not_initialized_from_checkpoint])
```
where actually everything was restored from the checkpoint and `not_initialized_from_checkpoint` was empty. This code runs fine with an ordinary tf.Session but crashed with tfdbg. It took me some time to track down the issue. If it's not too hard to fix, it would be nice to keep other users from the same pain (maybe - just speculating - #13604 crashes for the same reason)

### Source code / logs
```
import tensorflow as tf
from tensorflow.python import debug as tf_debug

sess = tf.Session()
dbg_sess = tf_debug.LocalCLIDebugWrapperSession(tf.Session())

print sess.run([tf.constant(1.0)])     # [1.0]
print sess.run([])                     # []
print dbg_sess.run([tf.constant(1.0)]) # [1.0]
print dbg_sess.run([])                 # IOError: Dump root directory /tmp/tfdbg_ai_aWv does not exist
```
",2018-01-06T01:26:18Z,25,1,3,4.250690398438997
471,15878,Windows: Release script for C library GPU builds on Windows,"awaiting testing (then merge),cla: yes","Fixed https://github.com/tensorflow/tensorflow/issues/11062

@gunan @asimshankar Can you also setup a job for GPU build using `tensorflow/tools/ci_build/windows/libtensorflow_gpu.sh` like http://ci.tensorflow.org/view/Nightly/job/nightly-libtensorflow-windows/
  ",1,,3,2018-01-05T13:19:12Z,2018-01-11T15:03:42Z,MEMBER,"Fixed https://github.com/tensorflow/tensorflow/issues/11062

@gunan @asimshankar Can you also setup a job for GPU build using `tensorflow/tools/ci_build/windows/libtensorflow_gpu.sh` like http://ci.tensorflow.org/view/Nightly/job/nightly-libtensorflow-windows/
  ",2018-01-09T08:52:05Z,6,3,1,5.750690398438997
472,15877,Documentation fix to contrib.signals,"awaiting testing (then merge),cla: yes","Fixed very confusing little typo.
The input `signals` is segmented into variable number of frames (with frame_length=256).
`frame_step` is the stride, not the size of a frame/window.",1,,2,2018-01-05T12:51:26Z,2018-01-24T21:31:51Z,CONTRIBUTOR,"Fixed very confusing little typo.
The input `signals` is segmented into variable number of frames (with frame_length=256).
`frame_step` is the stride, not the size of a frame/window.",2018-01-23T20:05:10Z,19,2,3,4.250690398438997
473,15876,tfcompile with --config=monolithic and -fvisibility=hidden results in undefined reference __xla_cpu_runtime_EigenMatMulF32,stat:awaiting tensorflower,"Some background first. For DeepSpeech, I have been experimenting simplification of our set of dependencies, trying to do a `--config=monolithic` build. The root cause for that was being able to run a `SYCL`-enabled build on my system (Ubuntu 17.10). Using `OpenCL` on this would trigger dependency load-chain that in the end loads `libmirprotobuf`. This would clash with the `protobuf` symbols already built in our `libtensorflow_framework` / `libtensorflow_cc`. To avoid this, monolithic build and forcing `visibility=hidden` seemed to be the best solution.

This allows us to move from those libraries (non tfcompile build, tfcompile adds `libdeepspeech_model.so` and all the `XLA` dependencies):
 - `libdeepspeech.so`
 - `libdeepspeech_utils.so`
 - `libtensorflow_cc.so`
 - `libtensorflow_framework.so`

To just:
 - `libdeepspeech.so`
 - `libdeepspeech_utils.so`

This way, we have all needed TensorFlow bits within `libdeepspeech.so`, and those symbols are not re-exported thus avoiding any unwanted interaction. I could get `SYCL` build nearly working on Intel GPU.

Adding `tfcompile` in the equation, however, lead to linking issues. Symptom would be that build completes, but when one links binary against the model's `.so`, then it fails with:
```
undefined reference __xla_cpu_runtime_EigenMatMulF32
```

Checking with `objdump -t bazel-bin/tensorflow/compiler/xla/service/cpu/_objs/runtime_matmul/tensorflow/compiler/xla/service/cpu/runtime_matmul*.o | grep EigenMatMul` would show that the symbol is properly built into `runtime_matmul`, but that it is hidden.

I would be able to solve that by exposing `__xla_cpu_runtime_EigenMatMulF32` and `__xla_cpu_runtime_EigenMatMulF64` through `TF_EXPORT`.",0,,17,2018-01-05T11:07:28Z,2018-01-16T18:03:20Z,CONTRIBUTOR,"Some background first. For DeepSpeech, I have been experimenting simplification of our set of dependencies, trying to do a `--config=monolithic` build. The root cause for that was being able to run a `SYCL`-enabled build on my system (Ubuntu 17.10). Using `OpenCL` on this would trigger dependency load-chain that in the end loads `libmirprotobuf`. This would clash with the `protobuf` symbols already built in our `libtensorflow_framework` / `libtensorflow_cc`. To avoid this, monolithic build and forcing `visibility=hidden` seemed to be the best solution.

This allows us to move from those libraries (non tfcompile build, tfcompile adds `libdeepspeech_model.so` and all the `XLA` dependencies):
 - `libdeepspeech.so`
 - `libdeepspeech_utils.so`
 - `libtensorflow_cc.so`
 - `libtensorflow_framework.so`

To just:
 - `libdeepspeech.so`
 - `libdeepspeech_utils.so`

This way, we have all needed TensorFlow bits within `libdeepspeech.so`, and those symbols are not re-exported thus avoiding any unwanted interaction. I could get `SYCL` build nearly working on Intel GPU.

Adding `tfcompile` in the equation, however, lead to linking issues. Symptom would be that build completes, but when one links binary against the model's `.so`, then it fails with:
```
undefined reference __xla_cpu_runtime_EigenMatMulF32
```

Checking with `objdump -t bazel-bin/tensorflow/compiler/xla/service/cpu/_objs/runtime_matmul/tensorflow/compiler/xla/service/cpu/runtime_matmul*.o | grep EigenMatMul` would show that the symbol is properly built into `runtime_matmul`, but that it is hidden.

I would be able to solve that by exposing `__xla_cpu_runtime_EigenMatMulF32` and `__xla_cpu_runtime_EigenMatMulF64` through `TF_EXPORT`.",2018-01-05T11:08:29Z,11,2,2,10.750690398438996
474,15875,fix the comments which mistake x for y in gradient_checker,"awaiting testing (then merge),cla: yes",Fix the comments which mistake x for y in `gradient_checker.py`.,1,,3,2018-01-05T10:12:34Z,2018-01-05T19:12:01Z,CONTRIBUTOR,Fix the comments which mistake x for y in `gradient_checker.py`.,2018-01-05T18:05:00Z,0,2,0,4.750690398438997
475,15871,TFlite toco failed to convert the quantized inception protobuf to tflite format ,"comp:lite,type:feature","Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:

Linux qiuji01 4.4.0-72-generic #93-Ubuntu SMP Fri Mar 31 14:07:41 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""16.04.2 LTS (Xenial Xerus)""
VERSION_ID=""16.04""
VERSION_CODENAME=xenial

- **TensorFlow installed from (source or binary)**:
source
 
- **TensorFlow version (use command below)**:
tensorflow (1.5.0rc0)

git commit id:
commit f99275a6a309699c73e1bbebd89ba9aa32e79aa3
Author: Amit Patankar <amitpatankar@google.com>
Date:   Thu Jan 4 17:35:54 2018 -0800

- **Python version**: 
2.7.12
- **Bazel version (if compiling from source)**:
0.5.4.
- **GCC/Compiler version (if compiling from source)**:
gcc version 5.4.0 20160609
- **CUDA/cuDNN version**:
no
- **GPU model and memory**:
no
- **Exact command to reproduce**:

/root/TF/new/out/toco/execroot/org_tensorflow/bazel-out/local-opt/bin/tensorflow/contrib/lite/toco/toco --input_file=incept_8wn_gt.pb --output_file=incept_8wn_gt.tflite --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE --inference_type=QUANTIZED_UINT8 --input_shape=1,299,299,3 --input_array=""Mul""  --output_array=""softmax""

### Describe the problem
I am trying to follow the transform_graph (~/tensorflow/tools/graph_transforms/) README.md to get a 8 bit quantized model, then feed it into the Tflite toco (~/tensorflow/contrib/lite/) to  transform it from protobuf into tflite format.  My steps are:
1. download the inception model  from http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz and extract it.

2. build the graph_transforms using the following command:
bazel --output_base=../out/transform_graph/ build -s -c opt tensorflow/tools/graph_transforms:transform_graph

3. quantize and optimize the inception model using t he following command:
/root/TF/new/out/transform_graph/execroot/org_tensorflow/bazel-out/local-opt/bin/tensorflow/tools/graph_transforms/transform_graph --in_graph=classify_image_graph_def.pb --out_graph=incept_8wn_gt.pb --inputs='Mul:0' --outputs='softmax:0' --transforms='add_default_attributes strip_unused_nodes(type=float, shape=""1,299,299,3"") remove_nodes(op=Identity, op=CheckNumerics) fold_constants(ignore_errors=true) fold_batch_norms fold_old_batch_norms quantize_weights quantize_nodes strip_unused_nodes sort_by_execution_order'

4. build the toco using the following command:
bazel --output_base=../out/toco build tensorflow/contrib/lite/toco:toco

5. transform the quantized inception model from protobuf format into TFlite format using the following the command:

/root/TF/new/out/toco/execroot/org_tensorflow/bazel-out/local-opt/bin/tensorflow/contrib/lite/toco/toco --input_file=incept_8wn_gt.pb --output_file=incept_8wn_gt.tflite --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE --inference_type=QUANTIZED_UINT8 --input_shape=1,299,299,3 --input_array=""Mul""  --output_array=""softmax""





### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

The error log is as following:

2018-01-05 08:08:37.682405: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.682532: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.682593: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.682651: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.682762: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.682950: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.683126: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.683270: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.683385: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.683442: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.683496: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.683546: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.683645: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.683743: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.683862: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.683960: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.684051: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.684106: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.684156: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.684210: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.684273: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.684364: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.684418: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.684468: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.684538: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.684591: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.684738: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.685056: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.685112: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.685163: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.685213: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.685263: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.685700: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.685757: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.685883: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.686121: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.686378: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.686456: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.688240: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.688313: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.688395: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.688454: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.688508: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.688537: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.688566: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.688589: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.688612: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.688634: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.688654: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.688675: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.688697: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.688722: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.688743: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.688763: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.688781: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.688797: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.688813: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.688856: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.688907: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.688949: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.689010: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.689133: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.689182: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.689298: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.689361: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.689442: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.689709: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.689791: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.689845: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.689940: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.689992: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.690186: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.690258: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.690290: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.690314: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.690338: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.690360: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.690382: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.690403: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.690425: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedMaxPool
2018-01-05 08:08:37.690489: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.690739: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.690833: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.690915: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.691020: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.691122: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.691263: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.691368: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.691476: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.691513: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.691536: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.691727: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.691757: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.691782: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.691807: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.692065: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.692081: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.692093: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.692104: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.692115: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.692125: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.692135: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.692148: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.692158: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.692168: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.692178: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.692189: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.692199: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.692208: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.692219: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedMaxPool
2018-01-05 08:08:37.692229: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedAvgPool
2018-01-05 08:08:37.692241: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.692252: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.692262: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.692273: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.692283: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.692293: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.692302: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.692312: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.692322: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.692331: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.692359: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.692411: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.692439: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.692584: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.692599: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.692609: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.692618: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.692628: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.692639: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.692650: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.692664: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.692675: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.692685: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.692696: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.692706: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.692717: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.692727: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.692773: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.692800: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.692815: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.692827: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.692837: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.692848: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.692858: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.692868: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.692877: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.692901: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.692912: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.692921: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.692932: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.692942: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.692965: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.692996: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.693021: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.693046: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.693070: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.693098: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.693111: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.693121: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.693130: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.693139: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.693148: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.693157: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.693168: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.693179: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.693188: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.693199: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.693209: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.693219: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.693229: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.693255: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.693380: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.693407: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.693431: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.693445: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConcat
2018-01-05 08:08:37.693459: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedAvgPool
2018-01-05 08:08:37.693473: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.693484: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.693495: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.693505: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.693514: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.693522: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.693530: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.693539: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.693549: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.693557: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.693566: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.693574: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.693583: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.693591: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.693615: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.693639: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.693723: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.693736: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.693746: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.693755: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.693764: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.693772: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.693781: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.693790: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.693799: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.693808: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.693817: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.693826: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.693834: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.693842: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.693869: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.693891: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.693957: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.694080: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.694349: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.694394: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.694461: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.694474: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.694483: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.694492: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.694501: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.694510: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.694518: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.694527: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.694536: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.694544: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.694553: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.694561: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.694572: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.694581: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.694636: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.694651: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.694661: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.694669: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.694678: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.694687: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.694696: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.694704: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.694713: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConcat
2018-01-05 08:08:37.694724: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.694734: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.694741: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.694750: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.694759: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.694767: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.694775: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.694783: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.694792: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.694800: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.694808: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.694817: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.694825: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.694833: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.694841: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.694850: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.694857: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.694865: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.694874: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.694881: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.694889: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.694897: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.694906: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.694914: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.694922: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.694930: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.694938: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.694946: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.694954: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.694963: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.694971: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.694979: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.694987: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.694994: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.695002: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.695011: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.695020: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.695028: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.695037: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.695045: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.695052: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.695060: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.695069: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedAvgPool
2018-01-05 08:08:37.695079: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.695088: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.695096: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.695105: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.695113: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.695120: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.695128: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.695137: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConcat
2018-01-05 08:08:37.695147: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.695157: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.695164: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.695172: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.695180: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.695188: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.695197: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.695206: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.695215: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.695224: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.695233: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.695241: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.695249: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.695259: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.695269: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.695278: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.695298: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.695308: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.695317: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.695325: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.695333: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.695342: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedMaxPool
2018-01-05 08:08:37.695352: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.695361: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.695370: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.695378: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.695387: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.695395: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.695403: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.695411: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConcat
2018-01-05 08:08:37.695421: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.695431: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.695439: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.695448: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.695456: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.695464: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.695472: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.695481: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.695490: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.695498: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.695507: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.695516: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.695524: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.695532: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.695541: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.695550: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.695558: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.695567: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.695575: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.695583: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.695591: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.695599: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.695608: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.695616: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.695624: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.695633: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.695641: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.695649: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.695658: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.695667: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.695675: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.695684: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.695692: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.695700: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.695708: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.695717: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.695725: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.695734: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.695743: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.695751: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.695760: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.695768: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.695776: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.695786: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.695794: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.695803: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.695811: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.695819: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.695827: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.695835: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.695844: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.695853: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.695861: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.695869: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.695877: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.695884: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.695892: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.695901: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.695909: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.695918: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.695926: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.695934: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.695941: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.695949: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedAvgPool
2018-01-05 08:08:37.695959: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.695969: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.695977: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.695986: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.695994: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.696002: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.696010: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.696019: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConcat
2018-01-05 08:08:37.696029: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.696040: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.696048: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.696056: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.696064: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.696072: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.696079: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.696088: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.696096: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.696105: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.696114: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.696121: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.696130: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.696138: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.696146: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.696155: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.696163: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.696172: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.696180: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.696188: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.696195: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.696204: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.696212: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.696221: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.696229: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.696237: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.696245: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.696252: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.696261: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.696270: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.696278: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.696287: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.696296: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.696303: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.696311: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.696319: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.696328: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.696337: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.696345: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.696354: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.696362: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.696370: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.696378: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.696387: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.696395: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.696404: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.696412: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.696420: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.696427: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.696436: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.696445: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.696453: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.696462: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.696471: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.696479: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.696487: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.696495: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedAvgPool
2018-01-05 08:08:37.696506: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.696514: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.696523: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.696532: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.696540: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.696548: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.696556: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.696565: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.696574: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.696582: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.696591: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.696599: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.696607: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.696614: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.696624: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConcat
2018-01-05 08:08:37.696635: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedAvgPool
2018-01-05 08:08:37.696645: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.696654: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.696663: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.696671: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.696679: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.696687: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.696695: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.696703: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.696712: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.696720: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.696729: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.696737: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.696745: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.696753: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.696762: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.696771: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.696779: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.696788: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.696797: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.696804: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.696812: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.696821: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.696830: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.696838: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.696847: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.696855: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.696863: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.696870: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.696879: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.696887: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.696896: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.696904: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.696912: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.696921: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.696928: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.696936: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.696945: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.696953: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.696962: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.696970: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.696978: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.696985: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.696994: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.697002: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.697011: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.697019: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.697028: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.697036: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.697045: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.697053: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.697062: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.697070: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.697078: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.697086: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.697094: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.697102: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.697111: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.697120: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.697129: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.697137: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.697145: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.697153: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.697161: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.697169: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.697178: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.697186: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.697195: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.697203: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.697211: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.697218: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.697227: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConcat
2018-01-05 08:08:37.697238: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedAvgPool
2018-01-05 08:08:37.697248: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.697257: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.697266: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.697275: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.697283: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.697292: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.697300: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.697309: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.697318: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.697327: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.697335: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.697344: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.697352: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.697360: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.697370: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.697378: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.697386: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.697396: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.697404: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.697412: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.697420: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.697428: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.697437: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.697445: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.697453: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.697462: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.697470: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.697478: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.697486: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.697495: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.697503: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.697511: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.697519: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.697527: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.697534: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.697543: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.697552: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.697560: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.697569: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.697577: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.697585: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.697593: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.697601: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.697610: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.697618: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.697626: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.697634: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.697642: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.697650: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.697658: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.697667: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.697676: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.697685: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.697693: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.697702: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.697709: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.697718: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.697727: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.697735: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.697744: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.697752: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.697760: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.697768: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.697776: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.697785: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.697793: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.697802: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.697810: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.697818: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.697826: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.697835: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConcat
2018-01-05 08:08:37.697845: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.697854: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.697863: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.697872: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.697881: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.697889: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.697897: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.697905: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.697914: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.697922: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.697931: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.697940: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.697948: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.697956: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.697965: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.697973: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.697981: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.697990: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.697998: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.698006: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.698014: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.698022: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.698031: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.698040: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.698047: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.698056: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.698064: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.698071: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.698080: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.698089: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.698097: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.698106: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.698115: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.698123: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.698130: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.698139: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.698147: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.698155: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.698163: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.698171: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.698179: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.698187: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.698195: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedMaxPool
2018-01-05 08:08:37.698204: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConcat
2018-01-05 08:08:37.698215: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.698225: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.698233: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.698242: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.698250: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.698258: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.698266: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.698275: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.698284: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.698292: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.698301: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.706033: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.706066: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.706077: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.706089: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.706101: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.706111: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.706121: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.706131: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.706139: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.706147: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.706160: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.706179: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.706200: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.706216: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.706229: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.706243: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.706257: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.706276: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.706294: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.706487: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.706530: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.706543: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.706552: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.706560: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.706570: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.706580: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.706589: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.706598: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.706607: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.706615: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.706624: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.706655: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.706669: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.706677: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.706686: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.706696: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.706704: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.706712: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.706725: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.706737: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.706746: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.706756: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.706764: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.706772: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.706781: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.706791: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedAvgPool
2018-01-05 08:08:37.706801: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.706811: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.706819: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.706829: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.706837: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.706846: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.706853: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.706863: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConcat
2018-01-05 08:08:37.706882: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedMaxPool
2018-01-05 08:08:37.706902: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.706917: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.706929: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.706938: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.706948: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.706956: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.706964: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.706973: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.706982: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.706991: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.707001: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.707009: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.707017: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.707025: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.707034: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.707046: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.707058: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.707068: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.707083: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.707104: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.707119: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.707133: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.707148: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.707160: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.707175: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.707188: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.707203: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.707217: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.707232: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.707247: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.707262: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.707277: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.707311: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.707326: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.707339: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.707355: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.707370: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.707384: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.707398: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.707412: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.707425: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.707438: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.707453: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.707468: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.707482: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.707497: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.707512: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.707526: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.707539: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.707556: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.707571: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.707585: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.707600: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.707615: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.707629: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.707643: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.707658: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.707673: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.707687: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.707702: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.707717: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.707731: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.707744: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.707761: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConcat
2018-01-05 08:08:37.707780: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedAvgPool
2018-01-05 08:08:37.707798: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedReshape
2018-01-05 08:08:37.707814: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedMatMul
2018-01-05 08:08:37.707830: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.707845: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.707860: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.707875: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.707890: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.707904: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Dequantize
2018-01-05 08:08:37.763586: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 1080 operators, 3039 arrays (0 quantized)
2018-01-05 08:08:37.849158: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 793 operators, 2752 arrays (1 quantized)
2018-01-05 08:08:37.934553: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 2: 793 operators, 2752 arrays (1 quantized)
2018-01-05 08:08:38.022469: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before pre-quantization graph transformations: 793 operators, 2752 arrays (1 quantized)
2018-01-05 08:08:38.082050: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After pre-quantization graph transformations pass 1: 793 operators, 2752 arrays (1 quantized)
2018-01-05 08:08:38.156372: F tensorflow/contrib/lite/toco/tooling_util.cc:1217] Array conv/Conv2D_eightbit/Mul__port__0/min, which is an input to the (Unsupported TensorFlow op: QuantizeV2) operator producing the output array conv/Conv2D_eightbit/Mul__port__0/quantize, is lacking min/max data, which is necessary for quantization. Either target a non-quantized output format, or change the input graph to contain min/max information, or pass --default_ranges_min= and --default_ranges_max= if you do not care about the accuracy of results.

",0,,11,2018-01-05T08:38:40Z,2018-02-06T23:39:26Z,CONTRIBUTOR,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:

Linux qiuji01 4.4.0-72-generic #93-Ubuntu SMP Fri Mar 31 14:07:41 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""16.04.2 LTS (Xenial Xerus)""
VERSION_ID=""16.04""
VERSION_CODENAME=xenial

- **TensorFlow installed from (source or binary)**:
source
 
- **TensorFlow version (use command below)**:
tensorflow (1.5.0rc0)

git commit id:
commit f99275a6a309699c73e1bbebd89ba9aa32e79aa3
Author: Amit Patankar <amitpatankar@google.com>
Date:   Thu Jan 4 17:35:54 2018 -0800

- **Python version**: 
2.7.12
- **Bazel version (if compiling from source)**:
0.5.4.
- **GCC/Compiler version (if compiling from source)**:
gcc version 5.4.0 20160609
- **CUDA/cuDNN version**:
no
- **GPU model and memory**:
no
- **Exact command to reproduce**:

/root/TF/new/out/toco/execroot/org_tensorflow/bazel-out/local-opt/bin/tensorflow/contrib/lite/toco/toco --input_file=incept_8wn_gt.pb --output_file=incept_8wn_gt.tflite --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE --inference_type=QUANTIZED_UINT8 --input_shape=1,299,299,3 --input_array=""Mul""  --output_array=""softmax""

### Describe the problem
I am trying to follow the transform_graph (~/tensorflow/tools/graph_transforms/) README.md to get a 8 bit quantized model, then feed it into the Tflite toco (~/tensorflow/contrib/lite/) to  transform it from protobuf into tflite format.  My steps are:
1. download the inception model  from http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz and extract it.

2. build the graph_transforms using the following command:
bazel --output_base=../out/transform_graph/ build -s -c opt tensorflow/tools/graph_transforms:transform_graph

3. quantize and optimize the inception model using t he following command:
/root/TF/new/out/transform_graph/execroot/org_tensorflow/bazel-out/local-opt/bin/tensorflow/tools/graph_transforms/transform_graph --in_graph=classify_image_graph_def.pb --out_graph=incept_8wn_gt.pb --inputs='Mul:0' --outputs='softmax:0' --transforms='add_default_attributes strip_unused_nodes(type=float, shape=""1,299,299,3"") remove_nodes(op=Identity, op=CheckNumerics) fold_constants(ignore_errors=true) fold_batch_norms fold_old_batch_norms quantize_weights quantize_nodes strip_unused_nodes sort_by_execution_order'

4. build the toco using the following command:
bazel --output_base=../out/toco build tensorflow/contrib/lite/toco:toco

5. transform the quantized inception model from protobuf format into TFlite format using the following the command:

/root/TF/new/out/toco/execroot/org_tensorflow/bazel-out/local-opt/bin/tensorflow/contrib/lite/toco/toco --input_file=incept_8wn_gt.pb --output_file=incept_8wn_gt.tflite --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE --inference_type=QUANTIZED_UINT8 --input_shape=1,299,299,3 --input_array=""Mul""  --output_array=""softmax""





### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

The error log is as following:

2018-01-05 08:08:37.682405: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.682532: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.682593: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.682651: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.682762: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.682950: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.683126: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.683270: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.683385: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.683442: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.683496: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.683546: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.683645: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.683743: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.683862: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.683960: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.684051: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.684106: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.684156: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.684210: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.684273: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.684364: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.684418: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.684468: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.684538: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.684591: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.684738: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.685056: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.685112: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.685163: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.685213: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.685263: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.685700: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.685757: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.685883: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.686121: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.686378: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.686456: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.688240: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.688313: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.688395: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.688454: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.688508: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.688537: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.688566: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.688589: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.688612: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.688634: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.688654: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.688675: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.688697: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.688722: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.688743: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.688763: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.688781: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.688797: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.688813: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.688856: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.688907: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.688949: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.689010: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.689133: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.689182: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.689298: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.689361: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.689442: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.689709: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.689791: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.689845: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.689940: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.689992: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.690186: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.690258: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.690290: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.690314: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.690338: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.690360: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.690382: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.690403: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.690425: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedMaxPool
2018-01-05 08:08:37.690489: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.690739: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.690833: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.690915: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.691020: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.691122: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.691263: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.691368: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.691476: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.691513: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.691536: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.691727: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.691757: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.691782: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.691807: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.692065: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.692081: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.692093: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.692104: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.692115: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.692125: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.692135: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.692148: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.692158: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.692168: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.692178: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.692189: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.692199: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.692208: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.692219: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedMaxPool
2018-01-05 08:08:37.692229: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedAvgPool
2018-01-05 08:08:37.692241: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.692252: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.692262: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.692273: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.692283: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.692293: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.692302: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.692312: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.692322: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.692331: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.692359: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.692411: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.692439: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.692584: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.692599: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.692609: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.692618: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.692628: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.692639: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.692650: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.692664: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.692675: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.692685: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.692696: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.692706: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.692717: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.692727: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.692773: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.692800: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.692815: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.692827: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.692837: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.692848: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.692858: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.692868: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.692877: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.692901: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.692912: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.692921: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.692932: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.692942: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.692965: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.692996: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.693021: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.693046: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.693070: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.693098: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.693111: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.693121: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.693130: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.693139: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.693148: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.693157: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.693168: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.693179: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.693188: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.693199: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.693209: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.693219: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.693229: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.693255: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.693380: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.693407: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.693431: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.693445: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConcat
2018-01-05 08:08:37.693459: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedAvgPool
2018-01-05 08:08:37.693473: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.693484: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.693495: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.693505: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.693514: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.693522: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.693530: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.693539: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.693549: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.693557: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.693566: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.693574: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.693583: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.693591: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.693615: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.693639: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.693723: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.693736: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.693746: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.693755: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.693764: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.693772: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.693781: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.693790: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.693799: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.693808: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.693817: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.693826: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.693834: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.693842: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.693869: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.693891: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.693957: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.694080: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.694349: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.694394: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.694461: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.694474: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.694483: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.694492: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.694501: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.694510: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.694518: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.694527: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.694536: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.694544: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.694553: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.694561: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.694572: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.694581: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.694636: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.694651: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.694661: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.694669: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.694678: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.694687: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.694696: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.694704: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.694713: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConcat
2018-01-05 08:08:37.694724: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.694734: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.694741: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.694750: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.694759: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.694767: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.694775: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.694783: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.694792: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.694800: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.694808: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.694817: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.694825: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.694833: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.694841: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.694850: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.694857: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.694865: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.694874: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.694881: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.694889: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.694897: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.694906: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.694914: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.694922: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.694930: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.694938: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.694946: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.694954: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.694963: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.694971: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.694979: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.694987: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.694994: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.695002: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.695011: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.695020: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.695028: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.695037: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.695045: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.695052: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.695060: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.695069: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedAvgPool
2018-01-05 08:08:37.695079: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.695088: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.695096: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.695105: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.695113: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.695120: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.695128: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.695137: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConcat
2018-01-05 08:08:37.695147: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.695157: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.695164: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.695172: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.695180: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.695188: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.695197: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.695206: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.695215: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.695224: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.695233: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.695241: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.695249: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.695259: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.695269: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.695278: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.695298: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.695308: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.695317: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.695325: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.695333: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.695342: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedMaxPool
2018-01-05 08:08:37.695352: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.695361: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.695370: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.695378: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.695387: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.695395: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.695403: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.695411: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConcat
2018-01-05 08:08:37.695421: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.695431: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.695439: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.695448: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.695456: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.695464: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.695472: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.695481: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.695490: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.695498: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.695507: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.695516: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.695524: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.695532: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.695541: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.695550: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.695558: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.695567: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.695575: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.695583: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.695591: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.695599: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.695608: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.695616: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.695624: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.695633: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.695641: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.695649: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.695658: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.695667: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.695675: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.695684: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.695692: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.695700: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.695708: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.695717: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.695725: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.695734: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.695743: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.695751: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.695760: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.695768: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.695776: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.695786: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.695794: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.695803: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.695811: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.695819: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.695827: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.695835: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.695844: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.695853: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.695861: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.695869: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.695877: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.695884: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.695892: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.695901: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.695909: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.695918: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.695926: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.695934: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.695941: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.695949: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedAvgPool
2018-01-05 08:08:37.695959: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.695969: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.695977: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.695986: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.695994: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.696002: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.696010: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.696019: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConcat
2018-01-05 08:08:37.696029: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.696040: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.696048: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.696056: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.696064: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.696072: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.696079: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.696088: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.696096: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.696105: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.696114: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.696121: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.696130: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.696138: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.696146: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.696155: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.696163: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.696172: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.696180: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.696188: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.696195: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.696204: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.696212: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.696221: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.696229: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.696237: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.696245: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.696252: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.696261: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.696270: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.696278: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.696287: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.696296: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.696303: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.696311: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.696319: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.696328: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.696337: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.696345: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.696354: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.696362: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.696370: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.696378: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.696387: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.696395: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.696404: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.696412: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.696420: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.696427: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.696436: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.696445: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.696453: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.696462: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.696471: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.696479: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.696487: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.696495: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedAvgPool
2018-01-05 08:08:37.696506: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.696514: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.696523: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.696532: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.696540: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.696548: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.696556: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.696565: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.696574: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.696582: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.696591: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.696599: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.696607: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.696614: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.696624: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConcat
2018-01-05 08:08:37.696635: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedAvgPool
2018-01-05 08:08:37.696645: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.696654: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.696663: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.696671: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.696679: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.696687: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.696695: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.696703: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.696712: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.696720: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.696729: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.696737: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.696745: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.696753: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.696762: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.696771: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.696779: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.696788: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.696797: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.696804: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.696812: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.696821: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.696830: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.696838: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.696847: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.696855: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.696863: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.696870: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.696879: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.696887: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.696896: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.696904: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.696912: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.696921: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.696928: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.696936: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.696945: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.696953: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.696962: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.696970: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.696978: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.696985: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.696994: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.697002: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.697011: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.697019: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.697028: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.697036: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.697045: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.697053: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.697062: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.697070: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.697078: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.697086: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.697094: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.697102: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.697111: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.697120: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.697129: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.697137: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.697145: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.697153: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.697161: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.697169: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.697178: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.697186: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.697195: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.697203: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.697211: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.697218: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.697227: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConcat
2018-01-05 08:08:37.697238: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedAvgPool
2018-01-05 08:08:37.697248: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.697257: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.697266: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.697275: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.697283: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.697292: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.697300: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.697309: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.697318: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.697327: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.697335: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.697344: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.697352: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.697360: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.697370: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.697378: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.697386: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.697396: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.697404: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.697412: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.697420: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.697428: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.697437: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.697445: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.697453: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.697462: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.697470: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.697478: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.697486: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.697495: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.697503: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.697511: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.697519: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.697527: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.697534: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.697543: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.697552: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.697560: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.697569: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.697577: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.697585: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.697593: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.697601: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.697610: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.697618: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.697626: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.697634: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.697642: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.697650: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.697658: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.697667: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.697676: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.697685: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.697693: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.697702: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.697709: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.697718: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.697727: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.697735: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.697744: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.697752: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.697760: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.697768: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.697776: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.697785: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.697793: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.697802: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.697810: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.697818: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.697826: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.697835: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConcat
2018-01-05 08:08:37.697845: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.697854: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.697863: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.697872: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.697881: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.697889: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.697897: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.697905: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.697914: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.697922: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.697931: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.697940: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.697948: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.697956: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.697965: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.697973: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.697981: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.697990: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.697998: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.698006: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.698014: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.698022: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.698031: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.698040: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.698047: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.698056: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.698064: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.698071: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.698080: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.698089: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.698097: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.698106: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.698115: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.698123: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.698130: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.698139: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.698147: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.698155: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.698163: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.698171: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.698179: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.698187: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.698195: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedMaxPool
2018-01-05 08:08:37.698204: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConcat
2018-01-05 08:08:37.698215: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.698225: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.698233: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.698242: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.698250: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.698258: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.698266: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.698275: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.698284: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.698292: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.698301: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.706033: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.706066: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.706077: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.706089: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.706101: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.706111: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.706121: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.706131: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.706139: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.706147: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.706160: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.706179: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.706200: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.706216: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.706229: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.706243: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.706257: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.706276: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.706294: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.706487: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.706530: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.706543: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.706552: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.706560: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.706570: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.706580: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.706589: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.706598: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.706607: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.706615: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.706624: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.706655: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.706669: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.706677: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.706686: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.706696: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.706704: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.706712: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.706725: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.706737: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.706746: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.706756: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.706764: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.706772: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.706781: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.706791: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedAvgPool
2018-01-05 08:08:37.706801: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.706811: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.706819: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.706829: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.706837: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.706846: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.706853: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.706863: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConcat
2018-01-05 08:08:37.706882: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedMaxPool
2018-01-05 08:08:37.706902: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.706917: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.706929: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.706938: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.706948: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.706956: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.706964: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.706973: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.706982: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.706991: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.707001: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.707009: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.707017: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.707025: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.707034: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.707046: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.707058: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.707068: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.707083: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.707104: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.707119: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.707133: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.707148: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.707160: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.707175: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.707188: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.707203: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.707217: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.707232: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.707247: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.707262: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.707277: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.707311: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.707326: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.707339: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.707355: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.707370: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.707384: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.707398: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.707412: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.707425: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.707438: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.707453: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.707468: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.707482: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.707497: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.707512: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.707526: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.707539: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.707556: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.707571: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.707585: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.707600: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.707615: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.707629: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.707643: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.707658: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.707673: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.707687: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.707702: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.707717: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.707731: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.707744: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.707761: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConcat
2018-01-05 08:08:37.707780: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedAvgPool
2018-01-05 08:08:37.707798: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedReshape
2018-01-05 08:08:37.707814: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedMatMul
2018-01-05 08:08:37.707830: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.707845: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.707860: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.707875: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.707890: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.707904: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Dequantize
2018-01-05 08:08:37.763586: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 1080 operators, 3039 arrays (0 quantized)
2018-01-05 08:08:37.849158: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 793 operators, 2752 arrays (1 quantized)
2018-01-05 08:08:37.934553: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 2: 793 operators, 2752 arrays (1 quantized)
2018-01-05 08:08:38.022469: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before pre-quantization graph transformations: 793 operators, 2752 arrays (1 quantized)
2018-01-05 08:08:38.082050: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After pre-quantization graph transformations pass 1: 793 operators, 2752 arrays (1 quantized)
2018-01-05 08:08:38.156372: F tensorflow/contrib/lite/toco/tooling_util.cc:1217] Array conv/Conv2D_eightbit/Mul__port__0/min, which is an input to the (Unsupported TensorFlow op: QuantizeV2) operator producing the output array conv/Conv2D_eightbit/Mul__port__0/quantize, is lacking min/max data, which is necessary for quantization. Either target a non-quantized output format, or change the input graph to contain min/max information, or pass --default_ranges_min= and --default_ranges_max= if you do not care about the accuracy of results.

",2018-01-05T08:50:10Z,31,2,4,7.750690398438997
476,15867,Branch 180856860,"awaiting testing (then merge),cla: yes",,0,,7,2018-01-05T00:24:19Z,2018-01-05T14:04:12Z,CONTRIBUTOR,,2018-01-05T00:25:02Z,0,2,0,5.750690398438997
477,15866,TensorFlow fails to build with MPI,stat:awaiting tensorflower,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: CentOS 7.4
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: r1.5
- **Python version**: 2.7.5
- **Bazel version (if compiling from source)**: 0.9.0 & 0.7.0
- **GCC/Compiler version (if compiling from source)**: 6.3.0
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**:
bazel build --copt -mfma --copt -mavx2 --copt -O3 --verbose_failures -s -c opt //tensorflow/tools/pip_package:build_pip_package

### Describe the problem
TensorFlow fails to build with MPI with the following error:
```
ERROR: /disk/public_tf/tensorflow/tensorflow/contrib/mpi_collectives/BUILD:40:1: undeclared inclusion(s) in rule '//tensorflow/contrib/mpi_collectives:python/ops/_mpi_ops.so':
this rule is missing dependency declarations for the following files included by 'tensorflow/contrib/mpi_collectives/kernels/mpi_ops.cc':
  '/disk/public_tf/tensorflow/tensorflow/stream_executor/lib/statusor.h'
  '/disk/public_tf/tensorflow/tensorflow/stream_executor/platform/port.h'
  '/disk/public_tf/tensorflow/tensorflow/stream_executor/lib/error.h'
  '/disk/public_tf/tensorflow/tensorflow/stream_executor/lib/status.h'
  '/disk/public_tf/tensorflow/tensorflow/stream_executor/lib/stringpiece.h'
  '/disk/public_tf/tensorflow/tensorflow/stream_executor/platform/logging.h'
tensorflow/contrib/mpi_collectives/kernels/mpi_ops.cc:128:6: warning: 'bool tensorflow::contrib::mpi_collectives::{anonymous}::IsGPUDevice() [with T = Eigen::GpuDevice]' defined but not used [-Wunused-function]
 bool IsGPUDevice<GPUDevice>() {
      ^~~~~~~~~~~~~~~~~~~~~~
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 379.332s, Critical Path: 76.80s
FAILED: Build did NOT complete successfully
```
This error persists even when Bazel 0.7.0 is used to build TensorFlow.

  
  ",1,,7,2018-01-04T22:51:23Z,2018-01-17T22:32:45Z,CONTRIBUTOR,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: CentOS 7.4
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: r1.5
- **Python version**: 2.7.5
- **Bazel version (if compiling from source)**: 0.9.0 & 0.7.0
- **GCC/Compiler version (if compiling from source)**: 6.3.0
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**:
bazel build --copt -mfma --copt -mavx2 --copt -O3 --verbose_failures -s -c opt //tensorflow/tools/pip_package:build_pip_package

### Describe the problem
TensorFlow fails to build with MPI with the following error:
```
ERROR: /disk/public_tf/tensorflow/tensorflow/contrib/mpi_collectives/BUILD:40:1: undeclared inclusion(s) in rule '//tensorflow/contrib/mpi_collectives:python/ops/_mpi_ops.so':
this rule is missing dependency declarations for the following files included by 'tensorflow/contrib/mpi_collectives/kernels/mpi_ops.cc':
  '/disk/public_tf/tensorflow/tensorflow/stream_executor/lib/statusor.h'
  '/disk/public_tf/tensorflow/tensorflow/stream_executor/platform/port.h'
  '/disk/public_tf/tensorflow/tensorflow/stream_executor/lib/error.h'
  '/disk/public_tf/tensorflow/tensorflow/stream_executor/lib/status.h'
  '/disk/public_tf/tensorflow/tensorflow/stream_executor/lib/stringpiece.h'
  '/disk/public_tf/tensorflow/tensorflow/stream_executor/platform/logging.h'
tensorflow/contrib/mpi_collectives/kernels/mpi_ops.cc:128:6: warning: 'bool tensorflow::contrib::mpi_collectives::{anonymous}::IsGPUDevice() [with T = Eigen::GpuDevice]' defined but not used [-Wunused-function]
 bool IsGPUDevice<GPUDevice>() {
      ^~~~~~~~~~~~~~~~~~~~~~
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 379.332s, Critical Path: 76.80s
FAILED: Build did NOT complete successfully
```
This error persists even when Bazel 0.7.0 is used to build TensorFlow.

  
  ",2018-01-08T15:47:41Z,13,2,2,6.749542514629212
478,15864,Fix docs,cla: yes,,0,,2,2018-01-04T22:24:25Z,2018-01-05T01:35:54Z,MEMBER,,2018-01-05T00:46:43Z,1,3,0,4.249542514629212
479,15860,Branch 180746153,cla: no,,0,,5,2018-01-04T21:06:41Z,2018-01-05T01:03:06Z,CONTRIBUTOR,,2018-01-04T21:08:26Z,1,2,0,4.749542514629212
480,15858,Add unsortedsegment(prod/min/max/sqrt_n/mean).,"awaiting review,cla: yes","
This pull request
- adds CPU/GPU implementations of
  - `tf.unsorted_segment_min`,
  - `tf.unsorted_segment_prod` and a
  - GPU implementation for `tf.unsorted_segment_max`.
- adds python implementations of:
  - `tf.unsorted_segment_mean`
  - `tf.unsorted_segment_sqrt_n`
- fixes the gradient calculation for unsorted_segment_sum/max.
  #13055 introduced silent dropping of negative values on the cpu.
  However, the gradient of e.g. unsorted_segment_sum used tf.gather
  and therefore failed for negative indices on cpu.
  
I tried to simplify the code and to remove code duplication,
addressing this [todo](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/segment_reduction_ops.cc#L362).
(Also I once filed an  [issue](https://github.com/tensorflow/tensorflow/issues/7389)  to add these ops but only now had time to finish it ;).)


Some notes on this pull request:

- tf.gather returns zero on GPU for negative indices and raises an exception on
  CPU. To overcome this, the current implementation masks negative indices and
  sets them to zero later. This is of course not as efficient as the original gather.
  Would it make sense to use something like `if ""gpu"" in op.device.lower():` to run
  different functions, depending on the device?
- Instead of having a native op for mean/sqrt_n I added them in python.
  Making them native would require two additional template arguments
  (a functor to process the counter), resulting in more complicated code with
  probably only minor performance improvement, if at all. In my quick
  benchmarks, the python ops are nearly as fast as unsorted_segment_sum. However,
  they use more memory than a native op would, due to creating a tensor of ones.
  (bincount doesn't work here as it doesn't support negative indices)
- I simply copy-pasted some atomicOp code in `cuda_kernel_helper.h` instead of
  writing this more nicely as for a short period there was a completely different
  version of this file online and I wanted to check back with you before
  doing something more sophisticated. What's the status of this?

  Additionally, I guess the function `AccumulateInto` in `segment_reduction_ops_gpu.cu.cc`
  could be removed? CudaAtomicAdd has specializations for complex types in
  cuda_kernel_helper.h

Cheers,
Phil",1,,21,2018-01-04T20:05:30Z,2018-02-07T19:00:00Z,CONTRIBUTOR,"
This pull request
- adds CPU/GPU implementations of
  - `tf.unsorted_segment_min`,
  - `tf.unsorted_segment_prod` and a
  - GPU implementation for `tf.unsorted_segment_max`.
- adds python implementations of:
  - `tf.unsorted_segment_mean`
  - `tf.unsorted_segment_sqrt_n`
- fixes the gradient calculation for unsorted_segment_sum/max.
  #13055 introduced silent dropping of negative values on the cpu.
  However, the gradient of e.g. unsorted_segment_sum used tf.gather
  and therefore failed for negative indices on cpu.
  
I tried to simplify the code and to remove code duplication,
addressing this [todo](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/segment_reduction_ops.cc#L362).
(Also I once filed an  [issue](https://github.com/tensorflow/tensorflow/issues/7389)  to add these ops but only now had time to finish it ;).)


Some notes on this pull request:

- tf.gather returns zero on GPU for negative indices and raises an exception on
  CPU. To overcome this, the current implementation masks negative indices and
  sets them to zero later. This is of course not as efficient as the original gather.
  Would it make sense to use something like `if ""gpu"" in op.device.lower():` to run
  different functions, depending on the device?
- Instead of having a native op for mean/sqrt_n I added them in python.
  Making them native would require two additional template arguments
  (a functor to process the counter), resulting in more complicated code with
  probably only minor performance improvement, if at all. In my quick
  benchmarks, the python ops are nearly as fast as unsorted_segment_sum. However,
  they use more memory than a native op would, due to creating a tensor of ones.
  (bincount doesn't work here as it doesn't support negative indices)
- I simply copy-pasted some atomicOp code in `cuda_kernel_helper.h` instead of
  writing this more nicely as for a short period there was a completely different
  version of this file online and I wanted to check back with you before
  doing something more sophisticated. What's the status of this?

  Additionally, I guess the function `AccumulateInto` in `segment_reduction_ops_gpu.cu.cc`
  could be removed? CudaAtomicAdd has specializations for complex types in
  cuda_kernel_helper.h

Cheers,
Phil",2018-01-23T14:16:15Z,33,2,4,13.749542514629212
481,15857,Update documentation for gather_nd/gather to specify behaviors for out-of-bound indices,"awaiting testing (then merge),cla: yes","This fix updates documentation for gather_nd/gather/scatter_nd to specify behaviors for out-of-bound indices. Basically, on CPU an error will be returned and on GPU 0 value will be filled to the expected positions of the output.

This fix closes #13687. This fix closes #12608.

Signed-off-by: Yong Tang <yong.tang.github@outlook.com>",0,,2,2018-01-04T20:02:37Z,2018-01-05T20:53:32Z,MEMBER,"This fix updates documentation for gather_nd/gather/scatter_nd to specify behaviors for out-of-bound indices. Basically, on CPU an error will be returned and on GPU 0 value will be filled to the expected positions of the output.

This fix closes #13687. This fix closes #12608.

Signed-off-by: Yong Tang <yong.tang.github@outlook.com>",2018-01-05T06:06:07Z,1,3,0,4.249542514629212
482,15856,MKL DNN: Implementing MKL DNN version of Softmax,"awaiting testing (then merge),cla: yes",New MKL DNN implementation of Softmax is added. ,0,,2,2018-01-04T18:45:43Z,2018-01-17T05:47:51Z,CONTRIBUTOR,New MKL DNN implementation of Softmax is added. ,2018-01-16T16:51:55Z,13,2,2,3.249542514629212
483,15855,Export inception model after retrain,"awaiting testing (then merge),cla: yes","The are many issues and Stackoverflow posts asking how to export a retrained Inception model: https://github.com/tensorflow/serving/issues/449, https://github.com/tensorflow/serving/issues/33, https://github.com/tensorflow/serving/issues/7. It would be nice if retrain.py did this so that it's easier for newcomers to use Tensorflow Serving.

This PR exports the model after retrain is finished. I've also added a comment on how to serve the retrained model.

Confirmed works for tensorflow version 1.4.1.
  
  ",1,,12,2018-01-04T18:14:39Z,2018-02-16T01:20:20Z,CONTRIBUTOR,"The are many issues and Stackoverflow posts asking how to export a retrained Inception model: https://github.com/tensorflow/serving/issues/449, https://github.com/tensorflow/serving/issues/33, https://github.com/tensorflow/serving/issues/7. It would be nice if retrain.py did this so that it's easier for newcomers to use Tensorflow Serving.

This PR exports the model after retrain is finished. I've also added a comment on how to serve the retrained model.

Confirmed works for tensorflow version 1.4.1.
  
  ",2018-01-04T18:18:25Z,42,2,4,9.249542514629212
484,15854,Enable tilde expansion in debug wrappers,"awaiting testing (then merge),cla: yes","This commit allows paths beginning with '~' to be used when specifying where
debug files should be dumped. The tilde will now be expanded to the user's
home directory.",1,,3,2018-01-04T17:50:11Z,2018-01-04T19:10:26Z,CONTRIBUTOR,"This commit allows paths beginning with '~' to be used when specifying where
debug files should be dumped. The tilde will now be expanded to the user's
home directory.",2018-01-04T17:56:24Z,0,2,0,4.749542514629212
485,15853,Fixes and formatting to configure.py,"awaiting testing (then merge),cla: yes","* Fix a bug in error generation regarding true/false string parsing
* Some minor style fixes",1,,1,2018-01-04T16:05:37Z,2018-01-04T20:54:55Z,CONTRIBUTOR,"* Fix a bug in error generation regarding true/false string parsing
* Some minor style fixes",2018-01-04T19:24:30Z,0,2,0,3.749542514629212
486,15852,Eager: crashed when using embedding_lookup in tfe.defun in tfe.GradientTape,"comp:eager,stat:awaiting tensorflower","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Win10
- **TensorFlow installed from (source or binary)**:binary
- **TensorFlow version (use command below)**:1.5.0dev20171230
- **Python version**: 3.6
- **Bazel version (if compiling from source)**:N/A
- **GCC/Compiler version (if compiling from source)**:N/A
- **CUDA/cuDNN version**:9.0/7.0
- **GPU model and memory**:pascal
- **Exact command to reproduce**:N/A
### Describe the problem
When I train a seq2seq model in eager, backward will raise a error:
```Python
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
c:\users\yanyan\anaconda3\lib\site-packages\tensorflow\python\framework\ops.py in get_attr(self, name)
   2162           with errors.raise_exception_on_not_ok_status() as status:
-> 2163             c_api.TF_OperationGetAttrValueProto(self._c_op, name, buf, status)
   2164           data = c_api.TF_GetBuffer(buf)

c:\users\yanyan\anaconda3\lib\site-packages\tensorflow\python\framework\errors_impl.py in __exit__(self, type_arg, value_arg, traceback_arg)
    472             compat.as_text(c_api.TF_Message(self.status.status)),
--> 473             c_api.TF_GetCode(self.status.status))
    474     # Delete the underlying status object from memory otherwise it stays alive

InvalidArgumentError: Operation 'embedding_lookup' has no attr named '_XlaCompile'.

During handling of the above exception, another exception occurred:

ValueError                                Traceback (most recent call last)
c:\users\yanyan\anaconda3\lib\site-packages\tensorflow\python\ops\gradients_impl.py in _MaybeCompile(scope, op, func, grad_fn)
    369     try:
--> 370       xla_compile = op.get_attr(""_XlaCompile"")
    371       xla_separate_compiled_gradients = op.get_attr(

c:\users\yanyan\anaconda3\lib\site-packages\tensorflow\python\framework\ops.py in get_attr(self, name)
   2166         # Convert to ValueError for backwards compatibility.
-> 2167         raise ValueError(str(e))
   2168       x = attr_value_pb2.AttrValue()

ValueError: Operation 'embedding_lookup' has no attr named '_XlaCompile'.

During handling of the above exception, another exception occurred:

IndexError                                Traceback (most recent call last)
<ipython-input-1-ed6ea9045e3f> in <module>()
     12         embed = tfe.Variable(np.ones((10, 100)).astype(np.float32))
     13         toy_data = np.ones((1, 10)).astype(np.int64)
---> 14         embedding_crash(toy_data, embed)

c:\users\yanyan\anaconda3\lib\site-packages\tensorflow\python\eager\function.py in decorated(*args, **kwds)
    639       arguments_to_functions[cache_key] = _defun_internal(
    640           name, func, args, kwds)
--> 641     return arguments_to_functions[cache_key](*args)
    642 
    643   return decorated

c:\users\yanyan\anaconda3\lib\site-packages\tensorflow\python\eager\function.py in __call__(self, *args)
    461         self._extra_inputs):
    462       if not self._has_backprop:
--> 463         self._compute_backprop()
    464       return self._backprop_call(tensor_inputs)
    465 

c:\users\yanyan\anaconda3\lib\site-packages\tensorflow\python\eager\function.py in _compute_backprop(self)
    359             filtered_outputs,
    360             self._input_placeholders,
--> 361             grad_ys=self._out_grad_placeholders)
    362         shapes = tuple(x.shape for x in in_gradients if x is not None)
    363     captures = list(sorted(c.captured_tensors, key=lambda x: x.name))

c:\users\yanyan\anaconda3\lib\site-packages\tensorflow\python\ops\gradients_impl.py in gradients(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients)
    607                 # functions.
    608                 in_grads = _MaybeCompile(
--> 609                     grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))
    610               else:
    611                 # For function call ops, we add a 'SymbolicGradient'

c:\users\yanyan\anaconda3\lib\site-packages\tensorflow\python\ops\gradients_impl.py in _MaybeCompile(scope, op, func, grad_fn)
    373       xla_scope = op.get_attr(""_XlaScope"").decode()
    374     except ValueError:
--> 375       return grad_fn()  # Exit early
    376 
    377   if not xla_compile:

c:\users\yanyan\anaconda3\lib\site-packages\tensorflow\python\ops\gradients_impl.py in <lambda>()
    607                 # functions.
    608                 in_grads = _MaybeCompile(
--> 609                     grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))
    610               else:
    611                 # For function call ops, we add a 'SymbolicGradient'

c:\users\yanyan\anaconda3\lib\site-packages\tensorflow\python\ops\resource_variable_ops.py in _GatherGrad(op, grad)
    895     # TODO(apassos): implement this for EAGER mode.
    896     while handle.op.type != ""VarHandleOp"":
--> 897       handle = handle.op.inputs[0]
    898   params_shape = gen_resource_variable_ops.variable_shape(handle)
    899   size = array_ops.expand_dims(array_ops.size(indices), 0)

c:\users\yanyan\anaconda3\lib\site-packages\tensorflow\python\framework\ops.py in __getitem__(self, i)
   1992 
   1993     def __getitem__(self, i):
-> 1994       return self._inputs[i]
   1995 
   1996 # pylint: enable=protected-access

IndexError: list index out of range
```
No problem when remove @tfe.defun, but eager is very slow without defun so I need to compile model with tfe.defun.
code to reproduce error:
```Python
import tensorflow as tf
import tensorflow.contrib.eager as tfe
import numpy as np
tfe.enable_eager_execution()
@tfe.defun
def embedding_crash(x, embedding):
    return tf.nn.embedding_lookup(embedding, x)
with tf.device(""gpu:0""):
    with tfe.GradientTape() as g:
        embed = tfe.Variable(np.ones((10, 100)).astype(np.float32))
        toy_data = np.ones((1, 10)).astype(np.int64)
        embedding_crash(toy_data, embed)
```
  ",1,,3,2018-01-04T16:01:07Z,2018-01-24T16:50:20Z,NONE,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Win10
- **TensorFlow installed from (source or binary)**:binary
- **TensorFlow version (use command below)**:1.5.0dev20171230
- **Python version**: 3.6
- **Bazel version (if compiling from source)**:N/A
- **GCC/Compiler version (if compiling from source)**:N/A
- **CUDA/cuDNN version**:9.0/7.0
- **GPU model and memory**:pascal
- **Exact command to reproduce**:N/A
### Describe the problem
When I train a seq2seq model in eager, backward will raise a error:
```Python
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
c:\users\yanyan\anaconda3\lib\site-packages\tensorflow\python\framework\ops.py in get_attr(self, name)
   2162           with errors.raise_exception_on_not_ok_status() as status:
-> 2163             c_api.TF_OperationGetAttrValueProto(self._c_op, name, buf, status)
   2164           data = c_api.TF_GetBuffer(buf)

c:\users\yanyan\anaconda3\lib\site-packages\tensorflow\python\framework\errors_impl.py in __exit__(self, type_arg, value_arg, traceback_arg)
    472             compat.as_text(c_api.TF_Message(self.status.status)),
--> 473             c_api.TF_GetCode(self.status.status))
    474     # Delete the underlying status object from memory otherwise it stays alive

InvalidArgumentError: Operation 'embedding_lookup' has no attr named '_XlaCompile'.

During handling of the above exception, another exception occurred:

ValueError                                Traceback (most recent call last)
c:\users\yanyan\anaconda3\lib\site-packages\tensorflow\python\ops\gradients_impl.py in _MaybeCompile(scope, op, func, grad_fn)
    369     try:
--> 370       xla_compile = op.get_attr(""_XlaCompile"")
    371       xla_separate_compiled_gradients = op.get_attr(

c:\users\yanyan\anaconda3\lib\site-packages\tensorflow\python\framework\ops.py in get_attr(self, name)
   2166         # Convert to ValueError for backwards compatibility.
-> 2167         raise ValueError(str(e))
   2168       x = attr_value_pb2.AttrValue()

ValueError: Operation 'embedding_lookup' has no attr named '_XlaCompile'.

During handling of the above exception, another exception occurred:

IndexError                                Traceback (most recent call last)
<ipython-input-1-ed6ea9045e3f> in <module>()
     12         embed = tfe.Variable(np.ones((10, 100)).astype(np.float32))
     13         toy_data = np.ones((1, 10)).astype(np.int64)
---> 14         embedding_crash(toy_data, embed)

c:\users\yanyan\anaconda3\lib\site-packages\tensorflow\python\eager\function.py in decorated(*args, **kwds)
    639       arguments_to_functions[cache_key] = _defun_internal(
    640           name, func, args, kwds)
--> 641     return arguments_to_functions[cache_key](*args)
    642 
    643   return decorated

c:\users\yanyan\anaconda3\lib\site-packages\tensorflow\python\eager\function.py in __call__(self, *args)
    461         self._extra_inputs):
    462       if not self._has_backprop:
--> 463         self._compute_backprop()
    464       return self._backprop_call(tensor_inputs)
    465 

c:\users\yanyan\anaconda3\lib\site-packages\tensorflow\python\eager\function.py in _compute_backprop(self)
    359             filtered_outputs,
    360             self._input_placeholders,
--> 361             grad_ys=self._out_grad_placeholders)
    362         shapes = tuple(x.shape for x in in_gradients if x is not None)
    363     captures = list(sorted(c.captured_tensors, key=lambda x: x.name))

c:\users\yanyan\anaconda3\lib\site-packages\tensorflow\python\ops\gradients_impl.py in gradients(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients)
    607                 # functions.
    608                 in_grads = _MaybeCompile(
--> 609                     grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))
    610               else:
    611                 # For function call ops, we add a 'SymbolicGradient'

c:\users\yanyan\anaconda3\lib\site-packages\tensorflow\python\ops\gradients_impl.py in _MaybeCompile(scope, op, func, grad_fn)
    373       xla_scope = op.get_attr(""_XlaScope"").decode()
    374     except ValueError:
--> 375       return grad_fn()  # Exit early
    376 
    377   if not xla_compile:

c:\users\yanyan\anaconda3\lib\site-packages\tensorflow\python\ops\gradients_impl.py in <lambda>()
    607                 # functions.
    608                 in_grads = _MaybeCompile(
--> 609                     grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))
    610               else:
    611                 # For function call ops, we add a 'SymbolicGradient'

c:\users\yanyan\anaconda3\lib\site-packages\tensorflow\python\ops\resource_variable_ops.py in _GatherGrad(op, grad)
    895     # TODO(apassos): implement this for EAGER mode.
    896     while handle.op.type != ""VarHandleOp"":
--> 897       handle = handle.op.inputs[0]
    898   params_shape = gen_resource_variable_ops.variable_shape(handle)
    899   size = array_ops.expand_dims(array_ops.size(indices), 0)

c:\users\yanyan\anaconda3\lib\site-packages\tensorflow\python\framework\ops.py in __getitem__(self, i)
   1992 
   1993     def __getitem__(self, i):
-> 1994       return self._inputs[i]
   1995 
   1996 # pylint: enable=protected-access

IndexError: list index out of range
```
No problem when remove @tfe.defun, but eager is very slow without defun so I need to compile model with tfe.defun.
code to reproduce error:
```Python
import tensorflow as tf
import tensorflow.contrib.eager as tfe
import numpy as np
tfe.enable_eager_execution()
@tfe.defun
def embedding_crash(x, embedding):
    return tf.nn.embedding_lookup(embedding, x)
with tf.device(""gpu:0""):
    with tfe.GradientTape() as g:
        embed = tfe.Variable(np.ones((10, 100)).astype(np.float32))
        toy_data = np.ones((1, 10)).astype(np.int64)
        embedding_crash(toy_data, embed)
```
  ",2018-01-05T00:05:43Z,20,1,3,3.749542514629212
487,15851,Tensorflow: Non-deterministic behaviour with large model using while_loop,stat:awaiting response,"Hello!
I believe to have found a bug in Tensorflow when running the code below. I am currently trying to build a neural transducer, and have stumbled across TF sometimes not returning any values for a function. I have not had the chance yet to test this out on another machine (no GPU, TF 1.4.1, Ubuntu 17.10). I am not sure whether this is indeed a bug or not, so I'm first posting it here. The code is redacted a bit to highlight only the parts that fail. [I've also posted to StackOverflow](https://stackoverflow.com/questions/48081063/tensorflow-non-deterministic-behaviour-with-large-model-using-while-loop) considering it might be an error in my code, but haven't got any response yet.

Notes:

- I believe the bug occurs around line 160, in the body of the while loop in the function run_full_transducer
- The session is returning [encoder_outputs, transducer_outputs]
- I do not use random functions
- As far as I can tell, if I remove the Print OP in line 164, the output is always 0

Example of a correct return value (more or less):
```
array([[[ 0.00811536, -0.00200322, -0.01177037,  0.03676344, -0.01909475,
             -0.03157664,  0.026092  ,  0.02367685, -0.01894805,  0.02832799,
              0.0377345 , -0.02583589, -0.02908566,  0.0299024 ,  0.00518877,
             -0.00064737,  0.01431572, -0.01053502, -0.01783628, -0.00382657,
              0.00076749, -0.02705991,  0.00112415, -0.0193013 ,  0.02346764,
              0.03014467,  0.02663364,  0.02503882,  0.03362656, -0.01877708,
              0.01859642,  0.02460729, -0.01395229, -0.03033791,  0.01177907,
             -0.03049169, -0.00389978,  0.02221515, -0.00073605,  0.01248251,
              0.00424051,  0.01070387,  0.02818898,  0.0321721 , -0.02462685,
              0.03495178, -0.02408989, -0.02742486,  0.00331823, -0.02311424,
             -0.01327039,  0.01095297,  0.02584363,  0.02083527, -0.01588045,
              0.02837921,  0.02100117,  0.00918638,  0.00109535, -0.02965789,
              0.01040822, -0.03240473,  0.00453057, -0.00603903]],
    
           [[ 0.01053647, -0.00457577, -0.01939731,  0.06317309, -0.03113565,
             -0.05525927,  0.04647589,  0.04213476, -0.03498235,  0.04962765,
              0.05989208, -0.04340284, -0.04777668,  0.05346756,  0.00395604,
             -0.0005207 ,  0.02079381, -0.01424338, -0.02584206, -0.00530154,
             -0.00031365, -0.04966826, -0.00091683, -0.03025239,  0.04526306,
              0.0595435 ,  0.0463665 ,  0.04578522,  0.05916505, -0.031725  ,
              0.03164144,  0.04257958, -0.02865831, -0.04795898,  0.01856991,
             -0.05512668, -0.00730711,  0.03953242,  0.00017992,  0.01710426,
              0.00754557,  0.01975578,  0.0469296 ,  0.05237873, -0.04435374,
              0.05924731, -0.04474678, -0.04605344,  0.00947831, -0.04284734,
             -0.01979787,  0.02003288,  0.04196753,  0.03900779, -0.02887472,
              0.05130195,  0.03419674,  0.0105699 ,  0.001114  , -0.0524303 ,
              0.01738651, -0.06084244,  0.01364262, -0.01153531]]], dtype=float32), array([], shape=(0, 1, 3), dtype=float32)]
```
Incorrect:
```
 [array([[[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
              0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
              0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
              0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
              0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]],
    
           [[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
              0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
              0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
              0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
              0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]]], dtype=float32), array([], shape=(0, 1, 3), dtype=float32)]
```

Code:
``` python
 import tensorflow as tf
    from tensorflow.contrib.rnn import LSTMCell, LSTMStateTuple
    from tensorflow.python.layers import core as layers_core
    import numpy as np
    # NOTE: Time major
    
    # Constants
    input_dimensions = 1
    vocab_size = 3
    input_embedding_size = 20
    encoder_hidden_units = 64
    inputs_embedded = True
    transducer_hidden_units = 64
    batch_size = 1
    GO_SYMBOL = vocab_size - 1  # TODO: Make these constants correct
    END_SYMBOL = vocab_size
    input_block_size = 2
    log_prob_init_value = 0
    
    
    # ---------------- Helper classes -----------------------
    
    
    # ----------------- Model -------------------------------
    embeddings = tf.Variable(tf.random_uniform([vocab_size, input_embedding_size], -1.0, 1.0), dtype=tf.float32)
    
    
    class Model(object):
        def __init__(self):
            self.encoder_inputs, self.encoder_inputs_length, self.encoder_hidden_state, \
            self.encoder_outputs, self.encoder_hidden_state_new = self.build_encoder_model()
            self.encoder_raw_outputs, self.trans_hidden_state, self.transducer_amount_outputs, \
            self.transducer_hidden_state_new, self.logits, self.decoder_prediction = self.build_transducer_model()
    
        def build_encoder_model(self):
            encoder_inputs = tf.Variable(tf.zeros(shape=(input_block_size, batch_size, input_dimensions)),
                                         dtype=tf.float32, name='encoder_inputs', trainable=False)
            encoder_inputs_length = tf.Variable([tf.shape(encoder_inputs)[0]], dtype=tf.int32,
                                                name='encoder_inputs_length', trainable=False)
            encoder_hidden_state = tf.Variable(tf.zeros(shape=(2, 1, encoder_hidden_units)), dtype=tf.float32,
                                               name='encoder_hidden_state')  # Save the state as one tensor
    
            if inputs_embedded is True:
                encoder_inputs_embedded = encoder_inputs
            else:
                encoder_inputs_embedded = tf.nn.embedding_lookup(embeddings, encoder_inputs)
    
            # Build model
            encoder_cell = tf.contrib.rnn.LSTMCell(encoder_hidden_units)
    
            # Build previous state
            encoder_hidden_c, encoder_hidden_h = tf.split(encoder_hidden_state, num_or_size_splits=2, axis=0)
            encoder_hidden_c = tf.reshape(encoder_hidden_c, shape=[-1, encoder_hidden_units])
            encoder_hidden_h = tf.reshape(encoder_hidden_h, shape=[-1, encoder_hidden_units])
            encoder_hidden_state_t = LSTMStateTuple(encoder_hidden_c, encoder_hidden_h)
    
            #   encoder_outputs: [max_time, batch_size, num_units]
            encoder_outputs, encoder_hidden_state_new = tf.nn.dynamic_rnn(
                encoder_cell, encoder_inputs_embedded,
                sequence_length=encoder_inputs_length, time_major=True,
                dtype=tf.float32, initial_state=encoder_hidden_state_t)
    
            # Modify output of encoder_hidden_state_new so that it can be fed back in again without problems.
            encoder_hidden_state_new = tf.concat([encoder_hidden_state_new.c, encoder_hidden_state_new.h], axis=0)
            encoder_hidden_state_new = tf.reshape(encoder_hidden_state_new, shape=[2, -1, encoder_hidden_units])
    
            return encoder_inputs, encoder_inputs_length, encoder_hidden_state, encoder_outputs, encoder_hidden_state_new
    
        def build_transducer_model(self):
            encoder_raw_outputs = tf.Variable(tf.zeros(shape=(input_block_size, 1, encoder_hidden_units)),
                                              dtype=tf.float32,
                                              name='encoder_raw_outputs')
            trans_hidden_state = tf.Variable(tf.zeros(shape=(2, 1, transducer_hidden_units)),
                                             dtype=tf.float32,
                                             name='trans_hidden_state')  # Save the state as one tensor
            transducer_amount_outputs = tf.Variable(0, dtype=tf.int32, name='transducer_amount_outputs',
                                                    trainable=False)
    
            # Model building
            helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(
                embedding=embeddings,
                start_tokens=tf.tile([GO_SYMBOL], [batch_size]),
                end_token=END_SYMBOL)
    
            attention_states = tf.transpose(encoder_raw_outputs,
                                            [1, 0, 2])  # attention_states: [batch_size, max_time, num_units]
    
            attention_mechanism = tf.contrib.seq2seq.LuongAttention(
                encoder_hidden_units, attention_states)
    
            decoder_cell = tf.contrib.seq2seq.AttentionWrapper(
                tf.contrib.rnn.LSTMCell(transducer_hidden_units),
                attention_mechanism,
                attention_layer_size=transducer_hidden_units)
    
            projection_layer = layers_core.Dense(vocab_size, use_bias=False)
    
            # Build previous state
            trans_hidden_c, trans_hidden_h = tf.split(trans_hidden_state, num_or_size_splits=2, axis=0)
            trans_hidden_c = tf.reshape(trans_hidden_c, shape=[-1, transducer_hidden_units])
            trans_hidden_h = tf.reshape(trans_hidden_h, shape=[-1, transducer_hidden_units])
            trans_hidden_state_t = LSTMStateTuple(trans_hidden_c, trans_hidden_h)
    
            decoder = tf.contrib.seq2seq.BasicDecoder(
                decoder_cell, helper,
                decoder_cell.zero_state(1, tf.float32).clone(cell_state=trans_hidden_state_t),
                output_layer=projection_layer)
    
            outputs, transducer_hidden_state_new, _ = tf.contrib.seq2seq.dynamic_decode(decoder,
                                                                                        output_time_major=True,
                                                                                        maximum_iterations=transducer_amount_outputs)
            logits = outputs.rnn_output  # logits of shape [max_time,batch_size,vocab_size]
            decoder_prediction = outputs.sample_id  # For debugging
    
            # Modify output of transducer_hidden_state_new so that it can be fed back in again without problems.
            transducer_hidden_state_new = tf.concat(
                [transducer_hidden_state_new[0].c, transducer_hidden_state_new[0].h],
                axis=0)
            transducer_hidden_state_new = tf.reshape(transducer_hidden_state_new,
                                                     shape=[2, -1, transducer_hidden_units])
    
            return encoder_raw_outputs, trans_hidden_state, transducer_amount_outputs, transducer_hidden_state_new, \
                   logits, decoder_prediction
    
    
    model = Model()
    
    
    # ----------------- Alignment -------------------------
    
    # ----------------- Training --------------------------
    
    def run_full_transducer():
        # Inputs
        max_blocks = tf.placeholder(dtype=tf.int32, name='max_blocks')
        inputs_full_raw = tf.placeholder(shape=(None, batch_size, input_dimensions), dtype=tf.float32,
                                         name='inputs_full_raw')
        transducer_list_outputs = tf.placeholder(shape=(None,), dtype=tf.int32,
                                                 name='transducer_list_outputs')  # amount to output per block
    
        # Turn inputs into tensor which is easily readable
        inputs_full = tf.reshape(inputs_full_raw, shape=[max_blocks, input_block_size, batch_size, input_dimensions])
    
        # Outputs
        outputs_ta = tf.TensorArray(dtype=tf.float32, size=max_blocks)
    
        # Hidden states
        # TODO: make these correct
        encoder_hidden_init = tf.ones(shape=(2, 1, encoder_hidden_units))
        trans_hidden_init = tf.ones(shape=(2, 1, transducer_hidden_units))
    
        init_state = (0, outputs_ta, encoder_hidden_init, trans_hidden_init)
    
        def cond(current_block, outputs_int, encoder_hidden, trans_hidden):
            return current_block < max_blocks
    
        def body(current_block, outputs_int, encoder_hidden, trans_hidden):
            # Process encoder
            model.encoder_inputs = model.encoder_inputs.assign(inputs_full[current_block])
            model.encoder_inputs_length = model.encoder_inputs_length.assign([tf.shape(model.encoder_inputs)[0]])
            model.encoder_hidden_state = model.encoder_hidden_state.assign(encoder_hidden)
    
            # TODO: Error is SOMETIMES gone when using tf.Print
            current_block = tf.Print(current_block, [model.encoder_inputs], message='Enc in: ')
            #current_block = tf.Print(current_block, [model.encoder_outputs], message='Enc out: ')
    
            # Flow data from encoder to transducer
            model.encoder_raw_outputs = model.encoder_raw_outputs.assign(model.encoder_outputs)
            model.trans_hidden_state = model.trans_hidden_state.assign(trans_hidden)
            model.transducer_amount_outputs = model.transducer_amount_outputs.assign(transducer_list_outputs[current_block])
    
            # Note the outputs
            outputs_int = outputs_int.write(current_block, model.logits)
    
            return current_block + 1, outputs_int, model.encoder_hidden_state_new, model.transducer_hidden_state_new
    
        _, outputs_final, _, _ = tf.while_loop(cond, body, init_state)
    
        # Process outputs
        outputs = outputs_final.stack()  # Now the outputs are of shape [block, amount_of_trans_out, batch_size, vocab]
        outputs = tf.reshape(outputs, shape=(-1, 1, vocab_size))  # And now its [amount_outputs, batch_size, vocab]
    
        model.encoder_outputs = tf.Print(model.encoder_outputs, [model.encoder_outputs], message='Current block enc out: ')
    
        return max_blocks, inputs_full_raw, transducer_list_outputs, outputs, model.encoder_outputs
    
    # ---------------------- Testing -----------------------------
    
    
    # ---------------------- Management -----------------------------
    
    init = tf.global_variables_initializer()
    
    with tf.Session() as sess:
        sess.run(init)
    
        inp_max_blocks, inp_inputs_full_raw, inp_trans_list_out, out_outputs, enc_out = run_full_transducer()
    
        print sess.run([enc_out, out_outputs], feed_dict={
            inp_max_blocks: 3,
            inp_inputs_full_raw: np.ones(shape=(3 * input_block_size, 1, input_dimensions)),
            inp_trans_list_out: [1, 3, 2]
        })
```

Info about machine:
```

== cat /etc/issue ===============================================
Linux nikita-coolboi 4.13.0-21-generic #24-Ubuntu SMP Mon Dec 18 17:29:16 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""17.10 (Artful Aardvark)""
VERSION_ID=""17.10""
VERSION_CODENAME=artful

== are we in docker =============================================
No

== compiler =====================================================
c++ (Ubuntu 7.2.0-8ubuntu3) 7.2.0
Copyright (C) 2017 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux nikita-coolboi 4.13.0-21-generic #24-Ubuntu SMP Mon Dec 18 17:29:16 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================
numpy (1.13.3)
protobuf (3.5.1)
tensorflow (1.4.1)
tensorflow-tensorboard (0.4.0rc3)

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.4.1
tf.GIT_VERSION = v1.4.0-19-ga52c8d9
tf.COMPILER_VERSION = v1.4.0-19-ga52c8d9
Sanity check: array([1], dtype=int32)

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
tf.sh: line 105: nvidia-smi: command not found

== cuda libs  ===================================================
```
Have I written custom code Yes
OS Platform and Distribution Ubuntu 17.10 (Artful Aardvark)
TensorFlow installed from binary
TensorFlow version 1.4.1
Bazel version N/A
CUDA/cuDNN version N/A
GPU model and memory N/A
Exact command to reproduce Execute the code block as a python file a few times.

Thanks!
Nikita
  ",0,,2,2018-01-04T15:20:55Z,2018-01-09T19:53:38Z,NONE,"Hello!
I believe to have found a bug in Tensorflow when running the code below. I am currently trying to build a neural transducer, and have stumbled across TF sometimes not returning any values for a function. I have not had the chance yet to test this out on another machine (no GPU, TF 1.4.1, Ubuntu 17.10). I am not sure whether this is indeed a bug or not, so I'm first posting it here. The code is redacted a bit to highlight only the parts that fail. [I've also posted to StackOverflow](https://stackoverflow.com/questions/48081063/tensorflow-non-deterministic-behaviour-with-large-model-using-while-loop) considering it might be an error in my code, but haven't got any response yet.

Notes:

- I believe the bug occurs around line 160, in the body of the while loop in the function run_full_transducer
- The session is returning [encoder_outputs, transducer_outputs]
- I do not use random functions
- As far as I can tell, if I remove the Print OP in line 164, the output is always 0

Example of a correct return value (more or less):
```
array([[[ 0.00811536, -0.00200322, -0.01177037,  0.03676344, -0.01909475,
             -0.03157664,  0.026092  ,  0.02367685, -0.01894805,  0.02832799,
              0.0377345 , -0.02583589, -0.02908566,  0.0299024 ,  0.00518877,
             -0.00064737,  0.01431572, -0.01053502, -0.01783628, -0.00382657,
              0.00076749, -0.02705991,  0.00112415, -0.0193013 ,  0.02346764,
              0.03014467,  0.02663364,  0.02503882,  0.03362656, -0.01877708,
              0.01859642,  0.02460729, -0.01395229, -0.03033791,  0.01177907,
             -0.03049169, -0.00389978,  0.02221515, -0.00073605,  0.01248251,
              0.00424051,  0.01070387,  0.02818898,  0.0321721 , -0.02462685,
              0.03495178, -0.02408989, -0.02742486,  0.00331823, -0.02311424,
             -0.01327039,  0.01095297,  0.02584363,  0.02083527, -0.01588045,
              0.02837921,  0.02100117,  0.00918638,  0.00109535, -0.02965789,
              0.01040822, -0.03240473,  0.00453057, -0.00603903]],
    
           [[ 0.01053647, -0.00457577, -0.01939731,  0.06317309, -0.03113565,
             -0.05525927,  0.04647589,  0.04213476, -0.03498235,  0.04962765,
              0.05989208, -0.04340284, -0.04777668,  0.05346756,  0.00395604,
             -0.0005207 ,  0.02079381, -0.01424338, -0.02584206, -0.00530154,
             -0.00031365, -0.04966826, -0.00091683, -0.03025239,  0.04526306,
              0.0595435 ,  0.0463665 ,  0.04578522,  0.05916505, -0.031725  ,
              0.03164144,  0.04257958, -0.02865831, -0.04795898,  0.01856991,
             -0.05512668, -0.00730711,  0.03953242,  0.00017992,  0.01710426,
              0.00754557,  0.01975578,  0.0469296 ,  0.05237873, -0.04435374,
              0.05924731, -0.04474678, -0.04605344,  0.00947831, -0.04284734,
             -0.01979787,  0.02003288,  0.04196753,  0.03900779, -0.02887472,
              0.05130195,  0.03419674,  0.0105699 ,  0.001114  , -0.0524303 ,
              0.01738651, -0.06084244,  0.01364262, -0.01153531]]], dtype=float32), array([], shape=(0, 1, 3), dtype=float32)]
```
Incorrect:
```
 [array([[[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
              0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
              0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
              0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
              0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]],
    
           [[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
              0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
              0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
              0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
              0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]]], dtype=float32), array([], shape=(0, 1, 3), dtype=float32)]
```

Code:
``` python
 import tensorflow as tf
    from tensorflow.contrib.rnn import LSTMCell, LSTMStateTuple
    from tensorflow.python.layers import core as layers_core
    import numpy as np
    # NOTE: Time major
    
    # Constants
    input_dimensions = 1
    vocab_size = 3
    input_embedding_size = 20
    encoder_hidden_units = 64
    inputs_embedded = True
    transducer_hidden_units = 64
    batch_size = 1
    GO_SYMBOL = vocab_size - 1  # TODO: Make these constants correct
    END_SYMBOL = vocab_size
    input_block_size = 2
    log_prob_init_value = 0
    
    
    # ---------------- Helper classes -----------------------
    
    
    # ----------------- Model -------------------------------
    embeddings = tf.Variable(tf.random_uniform([vocab_size, input_embedding_size], -1.0, 1.0), dtype=tf.float32)
    
    
    class Model(object):
        def __init__(self):
            self.encoder_inputs, self.encoder_inputs_length, self.encoder_hidden_state, \
            self.encoder_outputs, self.encoder_hidden_state_new = self.build_encoder_model()
            self.encoder_raw_outputs, self.trans_hidden_state, self.transducer_amount_outputs, \
            self.transducer_hidden_state_new, self.logits, self.decoder_prediction = self.build_transducer_model()
    
        def build_encoder_model(self):
            encoder_inputs = tf.Variable(tf.zeros(shape=(input_block_size, batch_size, input_dimensions)),
                                         dtype=tf.float32, name='encoder_inputs', trainable=False)
            encoder_inputs_length = tf.Variable([tf.shape(encoder_inputs)[0]], dtype=tf.int32,
                                                name='encoder_inputs_length', trainable=False)
            encoder_hidden_state = tf.Variable(tf.zeros(shape=(2, 1, encoder_hidden_units)), dtype=tf.float32,
                                               name='encoder_hidden_state')  # Save the state as one tensor
    
            if inputs_embedded is True:
                encoder_inputs_embedded = encoder_inputs
            else:
                encoder_inputs_embedded = tf.nn.embedding_lookup(embeddings, encoder_inputs)
    
            # Build model
            encoder_cell = tf.contrib.rnn.LSTMCell(encoder_hidden_units)
    
            # Build previous state
            encoder_hidden_c, encoder_hidden_h = tf.split(encoder_hidden_state, num_or_size_splits=2, axis=0)
            encoder_hidden_c = tf.reshape(encoder_hidden_c, shape=[-1, encoder_hidden_units])
            encoder_hidden_h = tf.reshape(encoder_hidden_h, shape=[-1, encoder_hidden_units])
            encoder_hidden_state_t = LSTMStateTuple(encoder_hidden_c, encoder_hidden_h)
    
            #   encoder_outputs: [max_time, batch_size, num_units]
            encoder_outputs, encoder_hidden_state_new = tf.nn.dynamic_rnn(
                encoder_cell, encoder_inputs_embedded,
                sequence_length=encoder_inputs_length, time_major=True,
                dtype=tf.float32, initial_state=encoder_hidden_state_t)
    
            # Modify output of encoder_hidden_state_new so that it can be fed back in again without problems.
            encoder_hidden_state_new = tf.concat([encoder_hidden_state_new.c, encoder_hidden_state_new.h], axis=0)
            encoder_hidden_state_new = tf.reshape(encoder_hidden_state_new, shape=[2, -1, encoder_hidden_units])
    
            return encoder_inputs, encoder_inputs_length, encoder_hidden_state, encoder_outputs, encoder_hidden_state_new
    
        def build_transducer_model(self):
            encoder_raw_outputs = tf.Variable(tf.zeros(shape=(input_block_size, 1, encoder_hidden_units)),
                                              dtype=tf.float32,
                                              name='encoder_raw_outputs')
            trans_hidden_state = tf.Variable(tf.zeros(shape=(2, 1, transducer_hidden_units)),
                                             dtype=tf.float32,
                                             name='trans_hidden_state')  # Save the state as one tensor
            transducer_amount_outputs = tf.Variable(0, dtype=tf.int32, name='transducer_amount_outputs',
                                                    trainable=False)
    
            # Model building
            helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(
                embedding=embeddings,
                start_tokens=tf.tile([GO_SYMBOL], [batch_size]),
                end_token=END_SYMBOL)
    
            attention_states = tf.transpose(encoder_raw_outputs,
                                            [1, 0, 2])  # attention_states: [batch_size, max_time, num_units]
    
            attention_mechanism = tf.contrib.seq2seq.LuongAttention(
                encoder_hidden_units, attention_states)
    
            decoder_cell = tf.contrib.seq2seq.AttentionWrapper(
                tf.contrib.rnn.LSTMCell(transducer_hidden_units),
                attention_mechanism,
                attention_layer_size=transducer_hidden_units)
    
            projection_layer = layers_core.Dense(vocab_size, use_bias=False)
    
            # Build previous state
            trans_hidden_c, trans_hidden_h = tf.split(trans_hidden_state, num_or_size_splits=2, axis=0)
            trans_hidden_c = tf.reshape(trans_hidden_c, shape=[-1, transducer_hidden_units])
            trans_hidden_h = tf.reshape(trans_hidden_h, shape=[-1, transducer_hidden_units])
            trans_hidden_state_t = LSTMStateTuple(trans_hidden_c, trans_hidden_h)
    
            decoder = tf.contrib.seq2seq.BasicDecoder(
                decoder_cell, helper,
                decoder_cell.zero_state(1, tf.float32).clone(cell_state=trans_hidden_state_t),
                output_layer=projection_layer)
    
            outputs, transducer_hidden_state_new, _ = tf.contrib.seq2seq.dynamic_decode(decoder,
                                                                                        output_time_major=True,
                                                                                        maximum_iterations=transducer_amount_outputs)
            logits = outputs.rnn_output  # logits of shape [max_time,batch_size,vocab_size]
            decoder_prediction = outputs.sample_id  # For debugging
    
            # Modify output of transducer_hidden_state_new so that it can be fed back in again without problems.
            transducer_hidden_state_new = tf.concat(
                [transducer_hidden_state_new[0].c, transducer_hidden_state_new[0].h],
                axis=0)
            transducer_hidden_state_new = tf.reshape(transducer_hidden_state_new,
                                                     shape=[2, -1, transducer_hidden_units])
    
            return encoder_raw_outputs, trans_hidden_state, transducer_amount_outputs, transducer_hidden_state_new, \
                   logits, decoder_prediction
    
    
    model = Model()
    
    
    # ----------------- Alignment -------------------------
    
    # ----------------- Training --------------------------
    
    def run_full_transducer():
        # Inputs
        max_blocks = tf.placeholder(dtype=tf.int32, name='max_blocks')
        inputs_full_raw = tf.placeholder(shape=(None, batch_size, input_dimensions), dtype=tf.float32,
                                         name='inputs_full_raw')
        transducer_list_outputs = tf.placeholder(shape=(None,), dtype=tf.int32,
                                                 name='transducer_list_outputs')  # amount to output per block
    
        # Turn inputs into tensor which is easily readable
        inputs_full = tf.reshape(inputs_full_raw, shape=[max_blocks, input_block_size, batch_size, input_dimensions])
    
        # Outputs
        outputs_ta = tf.TensorArray(dtype=tf.float32, size=max_blocks)
    
        # Hidden states
        # TODO: make these correct
        encoder_hidden_init = tf.ones(shape=(2, 1, encoder_hidden_units))
        trans_hidden_init = tf.ones(shape=(2, 1, transducer_hidden_units))
    
        init_state = (0, outputs_ta, encoder_hidden_init, trans_hidden_init)
    
        def cond(current_block, outputs_int, encoder_hidden, trans_hidden):
            return current_block < max_blocks
    
        def body(current_block, outputs_int, encoder_hidden, trans_hidden):
            # Process encoder
            model.encoder_inputs = model.encoder_inputs.assign(inputs_full[current_block])
            model.encoder_inputs_length = model.encoder_inputs_length.assign([tf.shape(model.encoder_inputs)[0]])
            model.encoder_hidden_state = model.encoder_hidden_state.assign(encoder_hidden)
    
            # TODO: Error is SOMETIMES gone when using tf.Print
            current_block = tf.Print(current_block, [model.encoder_inputs], message='Enc in: ')
            #current_block = tf.Print(current_block, [model.encoder_outputs], message='Enc out: ')
    
            # Flow data from encoder to transducer
            model.encoder_raw_outputs = model.encoder_raw_outputs.assign(model.encoder_outputs)
            model.trans_hidden_state = model.trans_hidden_state.assign(trans_hidden)
            model.transducer_amount_outputs = model.transducer_amount_outputs.assign(transducer_list_outputs[current_block])
    
            # Note the outputs
            outputs_int = outputs_int.write(current_block, model.logits)
    
            return current_block + 1, outputs_int, model.encoder_hidden_state_new, model.transducer_hidden_state_new
    
        _, outputs_final, _, _ = tf.while_loop(cond, body, init_state)
    
        # Process outputs
        outputs = outputs_final.stack()  # Now the outputs are of shape [block, amount_of_trans_out, batch_size, vocab]
        outputs = tf.reshape(outputs, shape=(-1, 1, vocab_size))  # And now its [amount_outputs, batch_size, vocab]
    
        model.encoder_outputs = tf.Print(model.encoder_outputs, [model.encoder_outputs], message='Current block enc out: ')
    
        return max_blocks, inputs_full_raw, transducer_list_outputs, outputs, model.encoder_outputs
    
    # ---------------------- Testing -----------------------------
    
    
    # ---------------------- Management -----------------------------
    
    init = tf.global_variables_initializer()
    
    with tf.Session() as sess:
        sess.run(init)
    
        inp_max_blocks, inp_inputs_full_raw, inp_trans_list_out, out_outputs, enc_out = run_full_transducer()
    
        print sess.run([enc_out, out_outputs], feed_dict={
            inp_max_blocks: 3,
            inp_inputs_full_raw: np.ones(shape=(3 * input_block_size, 1, input_dimensions)),
            inp_trans_list_out: [1, 3, 2]
        })
```

Info about machine:
```

== cat /etc/issue ===============================================
Linux nikita-coolboi 4.13.0-21-generic #24-Ubuntu SMP Mon Dec 18 17:29:16 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""17.10 (Artful Aardvark)""
VERSION_ID=""17.10""
VERSION_CODENAME=artful

== are we in docker =============================================
No

== compiler =====================================================
c++ (Ubuntu 7.2.0-8ubuntu3) 7.2.0
Copyright (C) 2017 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux nikita-coolboi 4.13.0-21-generic #24-Ubuntu SMP Mon Dec 18 17:29:16 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================
numpy (1.13.3)
protobuf (3.5.1)
tensorflow (1.4.1)
tensorflow-tensorboard (0.4.0rc3)

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.4.1
tf.GIT_VERSION = v1.4.0-19-ga52c8d9
tf.COMPILER_VERSION = v1.4.0-19-ga52c8d9
Sanity check: array([1], dtype=int32)

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
tf.sh: line 105: nvidia-smi: command not found

== cuda libs  ===================================================
```
Have I written custom code Yes
OS Platform and Distribution Ubuntu 17.10 (Artful Aardvark)
TensorFlow installed from binary
TensorFlow version 1.4.1
Bazel version N/A
CUDA/cuDNN version N/A
GPU model and memory N/A
Exact command to reproduce Execute the code block as a python file a few times.

Thanks!
Nikita
  ",2018-01-05T01:40:58Z,5,1,1,2.249542514629212
488,15850,unable to install from source with undefined external dependency target error,stat:awaiting response,"
------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: r1.4.0
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: 0.9.0
- **GCC/Compiler version (if compiling from source)**: 5.4.0
- **CUDA/cuDNN version**: no
- **GPU model and memory**: no
- **Exact command to reproduce**:
`sudo bazel build --config=opt --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" //tensorflow/tools/pip_package:build_pip_package`
### Describe the problem
 I'm trying to install r1.4.0 from source with CPU version and follow the install guidelines on the official website.  But some errors occur. (The errors are shown in ""Source code / logs"" )
(1) It seems that the undefined external dependency target ""@local_config_sycl"" is referred in some files (eg. //tensorflow-1.4.0/ third_party/eigen3/build ).
(2) I am sure the openCL suppurt is disabled when configure.
(3) I tried to install version r1.0.1 from source, but the same errors occur.
(4) I tried to install version r1.4.0 from binary, and success.

Why this happens and how can I fix it? Thank you!

### Source code / logs
`ERROR: /home/tangdehong/.cache/bazel/_bazel_root/4bf03e1269a0e4905c62fa69a980acb4/external/local_config_sycl/sycl/BUILD:4:1: First argument of 'load' must be a label and start with either '//', ':', or '@'. Use --incompatible_load_argument_is_label=false to temporarily disable this check.
ERROR: /home/tangdehong/.cache/bazel/_bazel_root/4bf03e1269a0e4905c62fa69a980acb4/external/local_config_sycl/sycl/BUILD:6:1: First argument of 'load' must be a label and start with either '//', ':', or '@'. Use --incompatible_load_argument_is_label=false to temporarily disable this check.
ERROR: /home/tangdehong/.cache/bazel/_bazel_root/4bf03e1269a0e4905c62fa69a980acb4/external/local_config_sycl/sycl/BUILD:30:9: Traceback (most recent call last):
	File ""/home/tangdehong/.cache/bazel/_bazel_root/4bf03e1269a0e4905c62fa69a980acb4/external/local_config_sycl/sycl/BUILD"", line 27
		cc_library(name = ""syclrt"", srcs = [sycl_libr..."")], <3 more arguments>)
	File ""/home/tangdehong/.cache/bazel/_bazel_root/4bf03e1269a0e4905c62fa69a980acb4/external/local_config_sycl/sycl/BUILD"", line 30, in cc_library
		sycl_library_path
name 'sycl_library_path' is not defined
ERROR: /home/tangdehong/.cache/bazel/_bazel_root/4bf03e1269a0e4905c62fa69a980acb4/external/local_config_sycl/sycl/BUILD:39:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '@local_config_sycl//sycl:sycl'
ERROR: /home/tangdehong/OpenSourceCode/tensorflow-1.4.0/third_party/eigen3/BUILD:20:1: Target '@local_config_sycl//sycl:sycl' contains an error and its package is in error and referenced by '//third_party/eigen3:eigen3'
ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: Loading failed
INFO: Elapsed time: 0.745s
FAILED: Build did NOT complete successfully (0 packages loaded)
    currently loading: tensorflow/contrib/losses ... (17 packages)
`
Thank you!",0,,6,2018-01-04T15:16:57Z,2018-01-10T20:43:22Z,NONE,"
------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: r1.4.0
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: 0.9.0
- **GCC/Compiler version (if compiling from source)**: 5.4.0
- **CUDA/cuDNN version**: no
- **GPU model and memory**: no
- **Exact command to reproduce**:
`sudo bazel build --config=opt --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" //tensorflow/tools/pip_package:build_pip_package`
### Describe the problem
 I'm trying to install r1.4.0 from source with CPU version and follow the install guidelines on the official website.  But some errors occur. (The errors are shown in ""Source code / logs"" )
(1) It seems that the undefined external dependency target ""@local_config_sycl"" is referred in some files (eg. //tensorflow-1.4.0/ third_party/eigen3/build ).
(2) I am sure the openCL suppurt is disabled when configure.
(3) I tried to install version r1.0.1 from source, but the same errors occur.
(4) I tried to install version r1.4.0 from binary, and success.

Why this happens and how can I fix it? Thank you!

### Source code / logs
`ERROR: /home/tangdehong/.cache/bazel/_bazel_root/4bf03e1269a0e4905c62fa69a980acb4/external/local_config_sycl/sycl/BUILD:4:1: First argument of 'load' must be a label and start with either '//', ':', or '@'. Use --incompatible_load_argument_is_label=false to temporarily disable this check.
ERROR: /home/tangdehong/.cache/bazel/_bazel_root/4bf03e1269a0e4905c62fa69a980acb4/external/local_config_sycl/sycl/BUILD:6:1: First argument of 'load' must be a label and start with either '//', ':', or '@'. Use --incompatible_load_argument_is_label=false to temporarily disable this check.
ERROR: /home/tangdehong/.cache/bazel/_bazel_root/4bf03e1269a0e4905c62fa69a980acb4/external/local_config_sycl/sycl/BUILD:30:9: Traceback (most recent call last):
	File ""/home/tangdehong/.cache/bazel/_bazel_root/4bf03e1269a0e4905c62fa69a980acb4/external/local_config_sycl/sycl/BUILD"", line 27
		cc_library(name = ""syclrt"", srcs = [sycl_libr..."")], <3 more arguments>)
	File ""/home/tangdehong/.cache/bazel/_bazel_root/4bf03e1269a0e4905c62fa69a980acb4/external/local_config_sycl/sycl/BUILD"", line 30, in cc_library
		sycl_library_path
name 'sycl_library_path' is not defined
ERROR: /home/tangdehong/.cache/bazel/_bazel_root/4bf03e1269a0e4905c62fa69a980acb4/external/local_config_sycl/sycl/BUILD:39:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '@local_config_sycl//sycl:sycl'
ERROR: /home/tangdehong/OpenSourceCode/tensorflow-1.4.0/third_party/eigen3/BUILD:20:1: Target '@local_config_sycl//sycl:sycl' contains an error and its package is in error and referenced by '//third_party/eigen3:eigen3'
ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: Loading failed
INFO: Elapsed time: 0.745s
FAILED: Build did NOT complete successfully (0 packages loaded)
    currently loading: tensorflow/contrib/losses ... (17 packages)
`
Thank you!",2018-01-08T15:46:49Z,6,1,1,4.249542514629212
489,15848,*** Error in `python': double free or corruption (out): 0x00007fc5d674d9b0 ***,type:build/install,"I've built the latest version of TensorFlow from github repository with the following commands.

bazel build -s --config=mkl -c opt --copt=-msse4.1 --copt=-msse4.2 //tensorflow/tools/pip_package:build_pip_package

bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg

pip install /tmp/tensorflow_pkg/tensorflow-1.4.0-cp36-cp36m-linux_x86_64.whl

And get the following error after the import tensorflow command in python

Python 3.6.3 |Anaconda custom (64-bit)| (default, Oct 16 2017, 15:28:36) 
[GCC 4.8.2 20140120 (Red Hat 4.8.2-15)] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
Intel(R) Distribution for Python is brought to you by Intel Corporation.
Please check out: https://software.intel.com/en-us/python-distribution
>>> import tensorflow as tf
*** Error in `python': double free or corruption (out): 0x00007fc5d674d9b0 ***

Any suggestions on how to fix this issue?",1,,7,2018-01-04T12:56:02Z,2018-02-14T17:17:23Z,NONE,"I've built the latest version of TensorFlow from github repository with the following commands.

bazel build -s --config=mkl -c opt --copt=-msse4.1 --copt=-msse4.2 //tensorflow/tools/pip_package:build_pip_package

bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg

pip install /tmp/tensorflow_pkg/tensorflow-1.4.0-cp36-cp36m-linux_x86_64.whl

And get the following error after the import tensorflow command in python

Python 3.6.3 |Anaconda custom (64-bit)| (default, Oct 16 2017, 15:28:36) 
[GCC 4.8.2 20140120 (Red Hat 4.8.2-15)] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
Intel(R) Distribution for Python is brought to you by Intel Corporation.
Please check out: https://software.intel.com/en-us/python-distribution
>>> import tensorflow as tf
*** Error in `python': double free or corruption (out): 0x00007fc5d674d9b0 ***

Any suggestions on how to fix this issue?",2018-01-05T01:40:49Z,40,1,4,5.749542514629212
490,15844,MonitoredTrainingSession aborts without error or exeption,,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No 
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 16.04.3
- **TensorFlow installed from (source or binary)**:
binary, pip install
- **TensorFlow version (use command below)**:
v1.4.0-19-ga52c8d9 1.4.1
- **Python version**: 
Python 3.6.3 :: Anaconda, Inc.
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
cuda_8.0.61.2, libcudnn.so.6.0.21
- **GPU model and memory**:
Titan-X, 32GB
- **Exact command to reproduce**:
During some training runs, this script just ends after few epochs with printing 'eval done'. It doesn't print any error nor an exception. In other runs, with the same setup in runs through. How could this happen that the for-loop stops even the epochs is smaller than 100?

Edit: I tried it also without the try-except block around the `MonitoredTrainingSession`, but it was the same: no exception, no error, epoch ~ 15 and printing ""eval done""


```python
    tf.logging.set_verbosity(tf.logging.INFO)
    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '0'

    listener_eval = ExampleCheckpointSaverListener(fun_after_save=eval_model)
    listener_generate = ExampleCheckpointSaverListener(fun_after_save=generate)
    hooks_train = [
      tf.train.SummarySaverHook(output_dir=model_path, save_secs=60, scaffold=scaffold_train),
      tf.train.LoggingTensorHook(train_log_tensors, every_n_secs=30),
      tf.train.CheckpointSaverHook(checkpoint_dir=model_path, save_secs=600, scaffold=scaffold_train,
                                   listeners=[listener_eval, listener_generate])
    ]

    try:
      with tf.train.MonitoredTrainingSession(is_chief=True, checkpoint_dir=model_path, save_checkpoint_secs=None,
                                             hooks=hooks_train, save_summaries_secs=None, save_summaries_steps=None,
                                             config=sess_config, scaffold=scaffold_train, log_step_count_steps=1000, stop_grace_period_secs=20) as sess:

        for epochs in range(0, 100):
          print(epochs)
          try:
            while True:
              sess.run([train_op])
          except tf.errors.OutOfRangeError as e:
            print(""Epoch %d end."" % epochs)
            sess.run(train_reader.iterator.initializer)
    except:
      print(""MonitoredTrainingSession"")
      print(sys.exc_info())

    print(""eval done"")
```

[tf_env.txt](https://github.com/tensorflow/tensorflow/files/1603159/tf_env.txt)


  ",0,,4,2018-01-04T11:29:10Z,2018-01-05T21:07:25Z,CONTRIBUTOR,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No 
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 16.04.3
- **TensorFlow installed from (source or binary)**:
binary, pip install
- **TensorFlow version (use command below)**:
v1.4.0-19-ga52c8d9 1.4.1
- **Python version**: 
Python 3.6.3 :: Anaconda, Inc.
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
cuda_8.0.61.2, libcudnn.so.6.0.21
- **GPU model and memory**:
Titan-X, 32GB
- **Exact command to reproduce**:
During some training runs, this script just ends after few epochs with printing 'eval done'. It doesn't print any error nor an exception. In other runs, with the same setup in runs through. How could this happen that the for-loop stops even the epochs is smaller than 100?

Edit: I tried it also without the try-except block around the `MonitoredTrainingSession`, but it was the same: no exception, no error, epoch ~ 15 and printing ""eval done""


```python
    tf.logging.set_verbosity(tf.logging.INFO)
    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '0'

    listener_eval = ExampleCheckpointSaverListener(fun_after_save=eval_model)
    listener_generate = ExampleCheckpointSaverListener(fun_after_save=generate)
    hooks_train = [
      tf.train.SummarySaverHook(output_dir=model_path, save_secs=60, scaffold=scaffold_train),
      tf.train.LoggingTensorHook(train_log_tensors, every_n_secs=30),
      tf.train.CheckpointSaverHook(checkpoint_dir=model_path, save_secs=600, scaffold=scaffold_train,
                                   listeners=[listener_eval, listener_generate])
    ]

    try:
      with tf.train.MonitoredTrainingSession(is_chief=True, checkpoint_dir=model_path, save_checkpoint_secs=None,
                                             hooks=hooks_train, save_summaries_secs=None, save_summaries_steps=None,
                                             config=sess_config, scaffold=scaffold_train, log_step_count_steps=1000, stop_grace_period_secs=20) as sess:

        for epochs in range(0, 100):
          print(epochs)
          try:
            while True:
              sess.run([train_op])
          except tf.errors.OutOfRangeError as e:
            print(""Epoch %d end."" % epochs)
            sess.run(train_reader.iterator.initializer)
    except:
      print(""MonitoredTrainingSession"")
      print(sys.exc_info())

    print(""eval done"")
```

[tf_env.txt](https://github.com/tensorflow/tensorflow/files/1603159/tf_env.txt)


  ",2018-01-04T22:31:35Z,1,2,0,4.249542514629212
491,15843,WIP: LSTMBlockFusedCell supports Dropout,cla: yes,"Fix #13649.

The work has not been done yet. However I'm not sure whether the solution is approved, I mean, using DropoutWrapper for inner implementation. So I open the PR early to collect feedback. cc @ebrevdo.

I'll add test case later.
  ",0,,2,2018-01-04T11:08:15Z,2018-01-09T04:03:07Z,CONTRIBUTOR,"Fix #13649.

The work has not been done yet. However I'm not sure whether the solution is approved, I mean, using DropoutWrapper for inner implementation. So I open the PR early to collect feedback. cc @ebrevdo.

I'll add test case later.
  ",2018-01-09T04:03:07Z,5,2,1,3.249542514629212
492,15842,Windows: Disable bfloat16_test and framework_dtypes_test,"awaiting review,cla: yes","Reenable after fixing
https://github.com/tensorflow/tensorflow/issues/15297

@gunan ",1,,3,2018-01-04T11:01:19Z,2018-01-05T19:13:04Z,MEMBER,"Reenable after fixing
https://github.com/tensorflow/tensorflow/issues/15297

@gunan ",2018-01-04T11:52:40Z,1,3,0,5.749542514629212
493,15838,Gif can't be decoded. InvalidArgumentError: Invalid GIF data,stat:awaiting response,"## System information
**Have I written custom code : yes
OS Platform and Distribution : Mac OS 10.12.3
TensorFlow installed from : pip3
TensorFlow version : 1.4
Bazel version : N/A
CUDA/cuDNN version : N/A
GPU model and memory : N/A**

## Describe the problem
![0071qvrrgy1fn3h6v55gag308w0adx6p](https://user-images.githubusercontent.com/8256827/34550343-04b28b42-f14b-11e7-9b8a-729a82ba4742.gif)

The gif can be decoded by PIL,but the error occurred when I used tf.image.decode_gif to decode.

```
def load_gif(image_path, sess):
    image = tf.read_file(image_path)
    image = tf.image.decode_gif(image)
    return sess.run(image)

load_gif('0071Qvrrgy1fn3h6v55gag308w0adx6p',sess))
```

The error is:
```
NotFoundError                             Traceback (most recent call last)
/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)
   1322     try:
-> 1323       return fn(*args)
   1324     except errors.OpError as e:

/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py in _run_fn(session, feed_dict, fetch_list, target_list, options, run_metadata)
   1301                                    feed_dict, fetch_list, target_list,
-> 1302                                    status, run_metadata)
   1303 

/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py in __exit__(self, type_arg, value_arg, traceback_arg)
    472             compat.as_text(c_api.TF_Message(self.status.status)),
--> 473             c_api.TF_GetCode(self.status.status))
    474     # Delete the underlying status object from memory otherwise it stays alive

NotFoundError: /Users/chunchang/Downloads/0071Qvrrgy1fn3h6v55gag308w0adx6p; No such file or directory
	 [[Node: ReadFile_21 = ReadFile[_device=""/job:localhost/replica:0/task:0/device:CPU:0""](ReadFile_21/filename)]]

During handling of the above exception, another exception occurred:

NotFoundError                             Traceback (most recent call last)
<ipython-input-70-7b1d3fa2f712> in <module>()
      1 print(load_gif('/Users/chunchang/Downloads/0071Qvrrgy1fn3h6v55gag308w0adx6p',
----> 2                sess))

<ipython-input-55-fd8c43263043> in load_gif(image_path, sess)
      3     # image = tf.image.decode_png(image, channels=3)
      4     image = tf.image.decode_gif(image)
----> 5     return sess.run(image)

/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py in run(self, fetches, feed_dict, options, run_metadata)
    887     try:
    888       result = self._run(None, fetches, feed_dict, options_ptr,
--> 889                          run_metadata_ptr)
    890       if run_metadata:
    891         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)
   1118     if final_fetches or final_targets or (handle and feed_dict_tensor):
   1119       results = self._do_run(handle, final_targets, final_fetches,
-> 1120                              feed_dict_tensor, options, run_metadata)
   1121     else:
   1122       results = []

/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)
   1315     if handle is None:
   1316       return self._do_call(_run_fn, self._session, feeds, fetches, targets,
-> 1317                            options, run_metadata)
   1318     else:
   1319       return self._do_call(_prun_fn, self._session, handle, feeds, fetches)

/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)
   1334         except KeyError:
   1335           pass
-> 1336       raise type(e)(node_def, op, message)
   1337 
   1338   def _extend_graph(self):

NotFoundError: /Users/chunchang/Downloads/0071Qvrrgy1fn3h6v55gag308w0adx6p; No such file or directory
	 [[Node: ReadFile_21 = ReadFile[_device=""/job:localhost/replica:0/task:0/device:CPU:0""](ReadFile_21/filename)]]

Caused by op 'ReadFile_21', defined at:
  File ""/usr/local/Cellar/python3/3.6.2/Frameworks/Python.framework/Versions/3.6/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/usr/local/Cellar/python3/3.6.2/Frameworks/Python.framework/Versions/3.6/lib/python3.6/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py"", line 16, in <module>
    app.launch_new_instance()
  File ""/usr/local/lib/python3.6/site-packages/traitlets/config/application.py"", line 658, in launch_instance
    app.start()
  File ""/usr/local/lib/python3.6/site-packages/ipykernel/kernelapp.py"", line 477, in start
    ioloop.IOLoop.instance().start()
  File ""/usr/local/lib/python3.6/site-packages/zmq/eventloop/ioloop.py"", line 177, in start
    super(ZMQIOLoop, self).start()
  File ""/usr/local/lib/python3.6/site-packages/tornado/ioloop.py"", line 888, in start
    handler_func(fd_obj, events)
  File ""/usr/local/lib/python3.6/site-packages/tornado/stack_context.py"", line 277, in null_wrapper
    return fn(*args, **kwargs)
  File ""/usr/local/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py"", line 440, in _handle_events
    self._handle_recv()
  File ""/usr/local/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py"", line 472, in _handle_recv
    self._run_callback(callback, msg)
  File ""/usr/local/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py"", line 414, in _run_callback
    callback(*args, **kwargs)
  File ""/usr/local/lib/python3.6/site-packages/tornado/stack_context.py"", line 277, in null_wrapper
    return fn(*args, **kwargs)
  File ""/usr/local/lib/python3.6/site-packages/ipykernel/kernelbase.py"", line 283, in dispatcher
    return self.dispatch_shell(stream, msg)
  File ""/usr/local/lib/python3.6/site-packages/ipykernel/kernelbase.py"", line 235, in dispatch_shell
    handler(stream, idents, msg)
  File ""/usr/local/lib/python3.6/site-packages/ipykernel/kernelbase.py"", line 399, in execute_request
    user_expressions, allow_stdin)
  File ""/usr/local/lib/python3.6/site-packages/ipykernel/ipkernel.py"", line 196, in do_execute
    res = shell.run_cell(code, store_history=store_history, silent=silent)
  File ""/usr/local/lib/python3.6/site-packages/ipykernel/zmqshell.py"", line 533, in run_cell
    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)
  File ""/usr/local/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2698, in run_cell
    interactivity=interactivity, compiler=compiler, result=result)
  File ""/usr/local/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2808, in run_ast_nodes
    if self.run_code(code, result):
  File ""/usr/local/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2862, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-70-7b1d3fa2f712>"", line 2, in <module>
    sess))
  File ""<ipython-input-55-fd8c43263043>"", line 2, in load_gif
    image = tf.read_file(image_path)
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/gen_io_ops.py"", line 376, in read_file
    ""ReadFile"", filename=filename, name=name)
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 2956, in create_op
    op_def=op_def)
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1470, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

NotFoundError (see above for traceback): /Users/chunchang/Downloads/0071Qvrrgy1fn3h6v55gag308w0adx6p; No such file or directory
	 [[Node: ReadFile_21 = ReadFile[_device=""/job:localhost/replica:0/task:0/device:CPU:0""](ReadFile_21/filename)]]```
  
  
  
  ",0,,5,2018-01-04T07:27:19Z,2018-01-23T18:01:19Z,NONE,"## System information
**Have I written custom code : yes
OS Platform and Distribution : Mac OS 10.12.3
TensorFlow installed from : pip3
TensorFlow version : 1.4
Bazel version : N/A
CUDA/cuDNN version : N/A
GPU model and memory : N/A**

## Describe the problem
![0071qvrrgy1fn3h6v55gag308w0adx6p](https://user-images.githubusercontent.com/8256827/34550343-04b28b42-f14b-11e7-9b8a-729a82ba4742.gif)

The gif can be decoded by PIL,but the error occurred when I used tf.image.decode_gif to decode.

```
def load_gif(image_path, sess):
    image = tf.read_file(image_path)
    image = tf.image.decode_gif(image)
    return sess.run(image)

load_gif('0071Qvrrgy1fn3h6v55gag308w0adx6p',sess))
```

The error is:
```
NotFoundError                             Traceback (most recent call last)
/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)
   1322     try:
-> 1323       return fn(*args)
   1324     except errors.OpError as e:

/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py in _run_fn(session, feed_dict, fetch_list, target_list, options, run_metadata)
   1301                                    feed_dict, fetch_list, target_list,
-> 1302                                    status, run_metadata)
   1303 

/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py in __exit__(self, type_arg, value_arg, traceback_arg)
    472             compat.as_text(c_api.TF_Message(self.status.status)),
--> 473             c_api.TF_GetCode(self.status.status))
    474     # Delete the underlying status object from memory otherwise it stays alive

NotFoundError: /Users/chunchang/Downloads/0071Qvrrgy1fn3h6v55gag308w0adx6p; No such file or directory
	 [[Node: ReadFile_21 = ReadFile[_device=""/job:localhost/replica:0/task:0/device:CPU:0""](ReadFile_21/filename)]]

During handling of the above exception, another exception occurred:

NotFoundError                             Traceback (most recent call last)
<ipython-input-70-7b1d3fa2f712> in <module>()
      1 print(load_gif('/Users/chunchang/Downloads/0071Qvrrgy1fn3h6v55gag308w0adx6p',
----> 2                sess))

<ipython-input-55-fd8c43263043> in load_gif(image_path, sess)
      3     # image = tf.image.decode_png(image, channels=3)
      4     image = tf.image.decode_gif(image)
----> 5     return sess.run(image)

/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py in run(self, fetches, feed_dict, options, run_metadata)
    887     try:
    888       result = self._run(None, fetches, feed_dict, options_ptr,
--> 889                          run_metadata_ptr)
    890       if run_metadata:
    891         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)
   1118     if final_fetches or final_targets or (handle and feed_dict_tensor):
   1119       results = self._do_run(handle, final_targets, final_fetches,
-> 1120                              feed_dict_tensor, options, run_metadata)
   1121     else:
   1122       results = []

/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)
   1315     if handle is None:
   1316       return self._do_call(_run_fn, self._session, feeds, fetches, targets,
-> 1317                            options, run_metadata)
   1318     else:
   1319       return self._do_call(_prun_fn, self._session, handle, feeds, fetches)

/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)
   1334         except KeyError:
   1335           pass
-> 1336       raise type(e)(node_def, op, message)
   1337 
   1338   def _extend_graph(self):

NotFoundError: /Users/chunchang/Downloads/0071Qvrrgy1fn3h6v55gag308w0adx6p; No such file or directory
	 [[Node: ReadFile_21 = ReadFile[_device=""/job:localhost/replica:0/task:0/device:CPU:0""](ReadFile_21/filename)]]

Caused by op 'ReadFile_21', defined at:
  File ""/usr/local/Cellar/python3/3.6.2/Frameworks/Python.framework/Versions/3.6/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/usr/local/Cellar/python3/3.6.2/Frameworks/Python.framework/Versions/3.6/lib/python3.6/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py"", line 16, in <module>
    app.launch_new_instance()
  File ""/usr/local/lib/python3.6/site-packages/traitlets/config/application.py"", line 658, in launch_instance
    app.start()
  File ""/usr/local/lib/python3.6/site-packages/ipykernel/kernelapp.py"", line 477, in start
    ioloop.IOLoop.instance().start()
  File ""/usr/local/lib/python3.6/site-packages/zmq/eventloop/ioloop.py"", line 177, in start
    super(ZMQIOLoop, self).start()
  File ""/usr/local/lib/python3.6/site-packages/tornado/ioloop.py"", line 888, in start
    handler_func(fd_obj, events)
  File ""/usr/local/lib/python3.6/site-packages/tornado/stack_context.py"", line 277, in null_wrapper
    return fn(*args, **kwargs)
  File ""/usr/local/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py"", line 440, in _handle_events
    self._handle_recv()
  File ""/usr/local/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py"", line 472, in _handle_recv
    self._run_callback(callback, msg)
  File ""/usr/local/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py"", line 414, in _run_callback
    callback(*args, **kwargs)
  File ""/usr/local/lib/python3.6/site-packages/tornado/stack_context.py"", line 277, in null_wrapper
    return fn(*args, **kwargs)
  File ""/usr/local/lib/python3.6/site-packages/ipykernel/kernelbase.py"", line 283, in dispatcher
    return self.dispatch_shell(stream, msg)
  File ""/usr/local/lib/python3.6/site-packages/ipykernel/kernelbase.py"", line 235, in dispatch_shell
    handler(stream, idents, msg)
  File ""/usr/local/lib/python3.6/site-packages/ipykernel/kernelbase.py"", line 399, in execute_request
    user_expressions, allow_stdin)
  File ""/usr/local/lib/python3.6/site-packages/ipykernel/ipkernel.py"", line 196, in do_execute
    res = shell.run_cell(code, store_history=store_history, silent=silent)
  File ""/usr/local/lib/python3.6/site-packages/ipykernel/zmqshell.py"", line 533, in run_cell
    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)
  File ""/usr/local/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2698, in run_cell
    interactivity=interactivity, compiler=compiler, result=result)
  File ""/usr/local/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2808, in run_ast_nodes
    if self.run_code(code, result):
  File ""/usr/local/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2862, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-70-7b1d3fa2f712>"", line 2, in <module>
    sess))
  File ""<ipython-input-55-fd8c43263043>"", line 2, in load_gif
    image = tf.read_file(image_path)
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/gen_io_ops.py"", line 376, in read_file
    ""ReadFile"", filename=filename, name=name)
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 2956, in create_op
    op_def=op_def)
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1470, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

NotFoundError (see above for traceback): /Users/chunchang/Downloads/0071Qvrrgy1fn3h6v55gag308w0adx6p; No such file or directory
	 [[Node: ReadFile_21 = ReadFile[_device=""/job:localhost/replica:0/task:0/device:CPU:0""](ReadFile_21/filename)]]```
  
  
  
  ",2018-01-04T19:05:38Z,19,1,3,3.749542514629212
494,15836,Add axis support for `tf.nn.crelu`,"awaiting testing (then merge),cla: yes","This fix tries to address the issue raised in #15619 where it was not possible to specify an `axis` for
`tf.nn.crelu`. By default, `axis=-1` was used for concatenation implicitly.

This fix adds the support of `axis` for `tf.nn.crelu`, and adds test cases for it.

This fix fixes #15619.

Signed-off-by: Yong Tang <yong.tang.github@outlook.com>",1,,7,2018-01-04T06:21:58Z,2018-01-11T17:33:24Z,MEMBER,"This fix tries to address the issue raised in #15619 where it was not possible to specify an `axis` for
`tf.nn.crelu`. By default, `axis=-1` was used for concatenation implicitly.

This fix adds the support of `axis` for `tf.nn.crelu`, and adds test cases for it.

This fix fixes #15619.

Signed-off-by: Yong Tang <yong.tang.github@outlook.com>",2018-01-05T15:42:22Z,7,3,1,7.749542514629212
495,15834,I think some bug in  tf.contrib.layers.l2_regularizer,,"
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:win10
- **TensorFlow installed from (source or binary)**:install
- **TensorFlow version (use command below)**:1.5
- **Python version**: 3.5
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

### Describe the problem
when I call the  tf.contrib.layers.l2_regularizer(0.5)(w), I was toll that ""Expected int64, got 0.5 of type 'float' instead."" But the doc says that it need a float clearly and as a weight it can't be an integer.

### Source code / logs
` l2 = 0
for w in tf.global_variables():
    l2 += tf.contrib.layers.l2_regularizer(0.5)(w)
loss = tf.losses.softmax_cross_entropy(onehot_labels=one_hot_labels,logits=result_vector)+l2`

  ",0,,1,2018-01-04T05:41:11Z,2018-01-08T15:45:58Z,NONE,"
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:win10
- **TensorFlow installed from (source or binary)**:install
- **TensorFlow version (use command below)**:1.5
- **Python version**: 3.5
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

### Describe the problem
when I call the  tf.contrib.layers.l2_regularizer(0.5)(w), I was toll that ""Expected int64, got 0.5 of type 'float' instead."" But the doc says that it need a float clearly and as a weight it can't be an integer.

### Source code / logs
` l2 = 0
for w in tf.global_variables():
    l2 += tf.contrib.layers.l2_regularizer(0.5)(w)
loss = tf.losses.softmax_cross_entropy(onehot_labels=one_hot_labels,logits=result_vector)+l2`

  ",2018-01-04T06:52:26Z,4,1,1,1.7495425146292118
496,15832,Feature Request: saver.save mkdir if directory not exist,,"### Describe the problem
`saver.save(sess, 'my-model') `
returns error if directory not exist.
It would be nice if saver can automatically create the missing directory.
`Traceback (most recent call last):
  File ""tf_voice_recognition.py"", line 783, in <module>
    saver.save(sess, 'my-model')
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1594, in save
    raise exc
ValueError: Parent directory of my-model doesn't exist, can't save.
`",0,,4,2018-01-04T04:06:02Z,2018-01-04T09:01:05Z,CONTRIBUTOR,"### Describe the problem
`saver.save(sess, 'my-model') `
returns error if directory not exist.
It would be nice if saver can automatically create the missing directory.
`Traceback (most recent call last):
  File ""tf_voice_recognition.py"", line 783, in <module>
    saver.save(sess, 'my-model')
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1594, in save
    raise exc
ValueError: Parent directory of my-model doesn't exist, can't save.
`",2018-01-04T05:58:27Z,0,2,0,4.249542514629212
497,15830,tensorflow/contrib/lite/kernels/resize_bilinear.cc:42 NumInputs(node) != 1 (2 != 1),"comp:lite,stat:awaiting response","###System information
Have I written custom code: Yes, 
OS Platform and Distribution: Ubuntu14.04
TensorFlow installed from: source build w/ Bazel
TensorFlow version: 1.4
Python version: Anaconda 3.5.2
Bazel version: 0.9.0
GCC/Compiler version (if compiling from source): gcc version 4.8.4
CUDA/cuDNN version: Not relevant
GPU model and memory: Not  relevant
Exact command to reproduce: Not relevant


### Describe the problem

I construct a network with only bilinear_resize operation (I use the tf.image.resize_bilinear) and add operation, and  convert it to a tflite model successfully. However,  when I run the tflite mode, it comes to the errors as follows:

java.lang.NullPointerException: Can not allocate memory for the given inputs: tensorflow/contrib/lite/kernels/resize_bilinear.cc:42 NumInputs(node) != 1 (2 != 1)

I find the code line in resize_bilinear.cc:42 as follows:
TF_LITE_ENSURE_EQ(context, NumInputs(node), 1);

Is it right to modify the code line to : 
TF_LITE_ENSURE(context, NumInputs(node) == 1 || NumInputs(node) == 2); 
   


### Source code / logs
def network():
      img = tf.placeholder(name='img', dtype=tf.float32, shape=(1,100,100,3))
      img = tf.layers.conv2d(img, 3, (3,3), padding='same', name='conv1')
      img = tf.image.resize_bilinear(img, [200,200])
      var = tf.get_variable('weights', dtype=tf.float32, shape=(1,200,200,3))
      val = img + var
      out = tf.identity(val, name='out')


  ",0,,3,2018-01-04T03:42:09Z,2018-01-30T22:09:50Z,NONE,"###System information
Have I written custom code: Yes, 
OS Platform and Distribution: Ubuntu14.04
TensorFlow installed from: source build w/ Bazel
TensorFlow version: 1.4
Python version: Anaconda 3.5.2
Bazel version: 0.9.0
GCC/Compiler version (if compiling from source): gcc version 4.8.4
CUDA/cuDNN version: Not relevant
GPU model and memory: Not  relevant
Exact command to reproduce: Not relevant


### Describe the problem

I construct a network with only bilinear_resize operation (I use the tf.image.resize_bilinear) and add operation, and  convert it to a tflite model successfully. However,  when I run the tflite mode, it comes to the errors as follows:

java.lang.NullPointerException: Can not allocate memory for the given inputs: tensorflow/contrib/lite/kernels/resize_bilinear.cc:42 NumInputs(node) != 1 (2 != 1)

I find the code line in resize_bilinear.cc:42 as follows:
TF_LITE_ENSURE_EQ(context, NumInputs(node), 1);

Is it right to modify the code line to : 
TF_LITE_ENSURE(context, NumInputs(node) == 1 || NumInputs(node) == 2); 
   


### Source code / logs
def network():
      img = tf.placeholder(name='img', dtype=tf.float32, shape=(1,100,100,3))
      img = tf.layers.conv2d(img, 3, (3,3), padding='same', name='conv1')
      img = tf.image.resize_bilinear(img, [200,200])
      var = tf.get_variable('weights', dtype=tf.float32, shape=(1,200,200,3))
      val = img + var
      out = tf.identity(val, name='out')


  ",2018-01-04T12:59:48Z,26,1,3,2.749542514629212
498,15829,[Bazel/Windows] Wrap rm -rf in Bash for Windows (and some refactoring),"awaiting review,cla: yes","`rm -rf` is not available in Windows command prompt. Run it in Bash like `patch -pl`.

Refactor the wrapping logic into a new macro `_wrap_bash_cmd`.",1,,3,2018-01-04T03:10:47Z,2018-01-05T01:33:43Z,CONTRIBUTOR,"`rm -rf` is not available in Windows command prompt. Run it in Bash like `patch -pl`.

Refactor the wrapping logic into a new macro `_wrap_bash_cmd`.",2018-01-04T15:56:59Z,1,2,0,4.749542514629212
499,15827,ci.tensorflow.org lacks a security certificate,type:support,"### The Problem
https://ci.tensorflow.org/ now lacks a security certificate (or it expired).
To repro, visit https://ci.tensorflow.org/. Chrome will note that the connection is not private.

This is breaking TensorBoard's tests that run on travis. The tests `pip install` nightly versions of TensorFlow from these URLs:

- https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-cpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=cpu-slave/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-1.head-cp27-none-linux_x86_64.whl 
- https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-cpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3,label=cpu-slave/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-1.head-cp34-cp34m-linux_x86_64.whl

### Error Logs from a Failed Test Run

> Collecting tensorflow==1.head from https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-cpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=cpu-slave/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-1.head-cp27-none-linux_x86_64.whl
> Exception:
> Traceback (most recent call last):
>   File ""/home/travis/virtualenv/python2.7.14/lib/python2.7/site-packages/pip/basecommand.py"", line 215, in main
>     status = self.run(options, args)
>   File ""/home/travis/virtualenv/python2.7.14/lib/python2.7/site-packages/pip/commands/install.py"", line 335, in run
>     wb.build(autobuilding=True)
>   File ""/home/travis/virtualenv/python2.7.14/lib/python2.7/site-packages/pip/wheel.py"", line 749, in build
>     self.requirement_set.prepare_files(self.finder)
>   File ""/home/travis/virtualenv/python2.7.14/lib/python2.7/site-packages/pip/req/req_set.py"", line 380, in prepare_files
>     ignore_dependencies=self.ignore_dependencies))
>   File ""/home/travis/virtualenv/python2.7.14/lib/python2.7/site-packages/pip/req/req_set.py"", line 620, in _prepare_file
>     session=self.session, hashes=hashes)
>   File ""/home/travis/virtualenv/python2.7.14/lib/python2.7/site-packages/pip/download.py"", line 821, in unpack_url
>     hashes=hashes
>   File ""/home/travis/virtualenv/python2.7.14/lib/python2.7/site-packages/pip/download.py"", line 659, in unpack_http_url
>     hashes)
>   File ""/home/travis/virtualenv/python2.7.14/lib/python2.7/site-packages/pip/download.py"", line 853, in _download_http_url
>     stream=True,
>   File ""/home/travis/virtualenv/python2.7.14/lib/python2.7/site-packages/pip/_vendor/requests/sessions.py"", line 488, in get
>     return self.request('GET', url, **kwargs)
>   File ""/home/travis/virtualenv/python2.7.14/lib/python2.7/site-packages/pip/download.py"", line 386, in request
>     return super(PipSession, self).request(method, url, *args, **kwargs)
>   File ""/home/travis/virtualenv/python2.7.14/lib/python2.7/site-packages/pip/_vendor/requests/sessions.py"", line 475, in request
>     resp = self.send(prep, **send_kwargs)
>   File ""/home/travis/virtualenv/python2.7.14/lib/python2.7/site-packages/pip/_vendor/requests/sessions.py"", line 596, in send
>     r = adapter.send(request, **kwargs)
>   File ""/home/travis/virtualenv/python2.7.14/lib/python2.7/site-packages/pip/_vendor/cachecontrol/adapter.py"", line 47, in send
>     resp = super(CacheControlAdapter, self).send(request, **kw)
>   File ""/home/travis/virtualenv/python2.7.14/lib/python2.7/site-packages/pip/_vendor/requests/adapters.py"", line 497, in send
>     raise SSLError(e, request=request)
> SSLError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:661)

  ",1,,6,2018-01-04T00:12:28Z,2018-01-04T08:37:37Z,MEMBER,"### The Problem
https://ci.tensorflow.org/ now lacks a security certificate (or it expired).
To repro, visit https://ci.tensorflow.org/. Chrome will note that the connection is not private.

This is breaking TensorBoard's tests that run on travis. The tests `pip install` nightly versions of TensorFlow from these URLs:

- https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-cpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=cpu-slave/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-1.head-cp27-none-linux_x86_64.whl 
- https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-cpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3,label=cpu-slave/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-1.head-cp34-cp34m-linux_x86_64.whl

### Error Logs from a Failed Test Run

> Collecting tensorflow==1.head from https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-cpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=cpu-slave/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-1.head-cp27-none-linux_x86_64.whl
> Exception:
> Traceback (most recent call last):
>   File ""/home/travis/virtualenv/python2.7.14/lib/python2.7/site-packages/pip/basecommand.py"", line 215, in main
>     status = self.run(options, args)
>   File ""/home/travis/virtualenv/python2.7.14/lib/python2.7/site-packages/pip/commands/install.py"", line 335, in run
>     wb.build(autobuilding=True)
>   File ""/home/travis/virtualenv/python2.7.14/lib/python2.7/site-packages/pip/wheel.py"", line 749, in build
>     self.requirement_set.prepare_files(self.finder)
>   File ""/home/travis/virtualenv/python2.7.14/lib/python2.7/site-packages/pip/req/req_set.py"", line 380, in prepare_files
>     ignore_dependencies=self.ignore_dependencies))
>   File ""/home/travis/virtualenv/python2.7.14/lib/python2.7/site-packages/pip/req/req_set.py"", line 620, in _prepare_file
>     session=self.session, hashes=hashes)
>   File ""/home/travis/virtualenv/python2.7.14/lib/python2.7/site-packages/pip/download.py"", line 821, in unpack_url
>     hashes=hashes
>   File ""/home/travis/virtualenv/python2.7.14/lib/python2.7/site-packages/pip/download.py"", line 659, in unpack_http_url
>     hashes)
>   File ""/home/travis/virtualenv/python2.7.14/lib/python2.7/site-packages/pip/download.py"", line 853, in _download_http_url
>     stream=True,
>   File ""/home/travis/virtualenv/python2.7.14/lib/python2.7/site-packages/pip/_vendor/requests/sessions.py"", line 488, in get
>     return self.request('GET', url, **kwargs)
>   File ""/home/travis/virtualenv/python2.7.14/lib/python2.7/site-packages/pip/download.py"", line 386, in request
>     return super(PipSession, self).request(method, url, *args, **kwargs)
>   File ""/home/travis/virtualenv/python2.7.14/lib/python2.7/site-packages/pip/_vendor/requests/sessions.py"", line 475, in request
>     resp = self.send(prep, **send_kwargs)
>   File ""/home/travis/virtualenv/python2.7.14/lib/python2.7/site-packages/pip/_vendor/requests/sessions.py"", line 596, in send
>     r = adapter.send(request, **kwargs)
>   File ""/home/travis/virtualenv/python2.7.14/lib/python2.7/site-packages/pip/_vendor/cachecontrol/adapter.py"", line 47, in send
>     resp = super(CacheControlAdapter, self).send(request, **kw)
>   File ""/home/travis/virtualenv/python2.7.14/lib/python2.7/site-packages/pip/_vendor/requests/adapters.py"", line 497, in send
>     raise SSLError(e, request=request)
> SSLError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:661)

  ",2018-01-04T00:17:14Z,0,3,0,7.249542514629212
500,15826,Remove :0 in final result argument,"awaiting review,cla: no","Small change to line 393 - in my recent testing of retrain and the label_image workflow as of today (TF master @  136697e) I had to remove :0 from the label_image output_layer argument.

Thank you.",0,,4,2018-01-04T00:00:28Z,2018-01-04T15:17:07Z,CONTRIBUTOR,"Small change to line 393 - in my recent testing of retrain and the label_image workflow as of today (TF master @  136697e) I had to remove :0 from the label_image output_layer argument.

Thank you.",2018-01-04T00:00:52Z,0,2,0,4.249542514629212
501,15825,Update advise.md,"awaiting testing (then merge),cla: yes",,0,,4,2018-01-03T21:42:09Z,2018-01-04T01:35:51Z,CONTRIBUTOR,,2018-01-03T21:43:33Z,1,2,0,4.248425498398469
502,15824,Move SpeechActivity animation to XML.,"awaiting testing (then merge),cla: yes","Moved SpeechActivity animation to `res/animator/color_animation.xml`.

Specifying the animation in code is tougher to read and detracts from what is supposed to be a simple example of how to use TF in Android.",1,,2,2018-01-03T21:14:54Z,2018-02-16T01:20:36Z,CONTRIBUTOR,"Moved SpeechActivity animation to `res/animator/color_animation.xml`.

Specifying the animation in code is tougher to read and detracts from what is supposed to be a simple example of how to use TF in Android.",2018-02-06T07:39:19Z,43,2,4,4.248425498398469
503,15823,Fix a pessimizing-move warning in GetDeviceLapackInfo(),"awaiting testing (then merge),cla: yes","clang reports:
<pre>
./tensorflow/core/kernels/cuda_solvers.h:430:10: warning: moving a local object in a return statement prevents copy elision [-Wpessimizing-move]
  return std::move(new_dev_info);
         ^
./tensorflow/core/kernels/cuda_solvers.h:430:10: note: remove std::move call here
  return std::move(new_dev_info);
         ^~~~~~~~~~            ~
</pre>",1,,3,2018-01-03T20:39:18Z,2018-01-23T14:25:15Z,CONTRIBUTOR,"clang reports:
<pre>
./tensorflow/core/kernels/cuda_solvers.h:430:10: warning: moving a local object in a return statement prevents copy elision [-Wpessimizing-move]
  return std::move(new_dev_info);
         ^
./tensorflow/core/kernels/cuda_solvers.h:430:10: note: remove std::move call here
  return std::move(new_dev_info);
         ^~~~~~~~~~            ~
</pre>",2018-01-03T20:49:20Z,20,2,3,4.748425498398469
504,15822,Updating error handling in normalize_tuple,"awaiting testing (then merge),cla: yes","In normalize_tuple we test to see if all values are an int (or able to be cast to an int) using `int()`

`ValueError` is thrown if `int()` is called with an input like 'asdf' - this is caught and gives a helpful error, using the 'name' param to provide more context.

**However**, when given an input other than a string or int, a `TypeError` is thrown. This is **not** caught - making error messages much more esoteric than the helpful one written out here, especially when coming from a very long stack trace.

For example, before this change I was getting an error:
```
TypeError: int() argument must be a string or a number, not 'tuple'
```

With this change, I get the more useful:
```
ValueError: The `kernel_size` argument must be a tuple of 2 integers. 
Received: ((0, 3), 50) including element (0, 3) of type <type 'tuple'>
```
  
  ",1,,2,2018-01-03T16:41:25Z,2018-01-26T05:31:40Z,CONTRIBUTOR,"In normalize_tuple we test to see if all values are an int (or able to be cast to an int) using `int()`

`ValueError` is thrown if `int()` is called with an input like 'asdf' - this is caught and gives a helpful error, using the 'name' param to provide more context.

**However**, when given an input other than a string or int, a `TypeError` is thrown. This is **not** caught - making error messages much more esoteric than the helpful one written out here, especially when coming from a very long stack trace.

For example, before this change I was getting an error:
```
TypeError: int() argument must be a string or a number, not 'tuple'
```

With this change, I get the more useful:
```
ValueError: The `kernel_size` argument must be a tuple of 2 integers. 
Received: ((0, 3), 50) including element (0, 3) of type <type 'tuple'>
```
  
  ",2018-01-24T13:14:27Z,23,2,3,4.248425498398469
505,15821,TFGAN not compatible with eager execution mode,comp:eager,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Colaboratory Google Compute Engine backend (not sure about OS here)
- **TensorFlow installed from (source or binary)**: binary (non-GPU version)
- **TensorFlow version (use command below)**: 1.5.0-dev20180102
- **Python version**: 3

### Describe the problem
When enabling eager execution mode
```python
import tensorflow.contrib.eager as tfe
tfe.enable_eager_execution()
```
and running 
```python
noise_dims = 64
gan_model = tfgan.gan_model(
    generator_fn,
    discriminator_fn,
    real_data=images,
    generator_inputs=tf.random_normal([batch_size, noise_dims]))
```
from the [TFGAN tutorial](https://github.com/tensorflow/models/blob/master/research/gan/tutorial.ipynb) by @joel-shor, I get the following error:
```bash
ValueError: When Eager Execution is enabled, variable collections are not supported.
```
because of the following lines:
https://github.com/tensorflow/tensorflow/blob/8c2d6fc2b0202304885d5d6c3cba57eb2a1b3262/tensorflow/contrib/gan/python/train.py#L119-L121
.

Are there any plans to make TFGAN compatible with eager in the short term? Is there any help wanted in this regard? I'd be happy to contribute.",1,,12,2018-01-03T16:08:13Z,2018-01-13T00:53:50Z,NONE,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Colaboratory Google Compute Engine backend (not sure about OS here)
- **TensorFlow installed from (source or binary)**: binary (non-GPU version)
- **TensorFlow version (use command below)**: 1.5.0-dev20180102
- **Python version**: 3

### Describe the problem
When enabling eager execution mode
```python
import tensorflow.contrib.eager as tfe
tfe.enable_eager_execution()
```
and running 
```python
noise_dims = 64
gan_model = tfgan.gan_model(
    generator_fn,
    discriminator_fn,
    real_data=images,
    generator_inputs=tf.random_normal([batch_size, noise_dims]))
```
from the [TFGAN tutorial](https://github.com/tensorflow/models/blob/master/research/gan/tutorial.ipynb) by @joel-shor, I get the following error:
```bash
ValueError: When Eager Execution is enabled, variable collections are not supported.
```
because of the following lines:
https://github.com/tensorflow/tensorflow/blob/8c2d6fc2b0202304885d5d6c3cba57eb2a1b3262/tensorflow/contrib/gan/python/train.py#L119-L121
.

Are there any plans to make TFGAN compatible with eager in the short term? Is there any help wanted in this regard? I'd be happy to contribute.",2018-01-03T23:37:49Z,10,1,2,8.248425498398468
506,15820,"Revert ""Remove unneeded branch check (#13495)""","awaiting testing (then merge),cla: yes","This reverts commit 3aee5f1df97f44d9c14995505895f1877d7de8ae.

Fix build with Python3",0,,1,2018-01-03T15:36:42Z,2018-01-03T16:36:32Z,MEMBER,"This reverts commit 3aee5f1df97f44d9c14995505895f1877d7de8ae.

Fix build with Python3",2018-01-03T15:46:57Z,0,3,0,3.7484254983984693
507,15815,"When data become large,parition variables can not initialized successfully",,"#15216 
i have a issue, but nobody help me to solve it ",0,,6,2018-01-03T12:42:54Z,2018-01-30T00:17:14Z,NONE,"#15216 
i have a issue, but nobody help me to solve it ",2018-01-04T01:39:21Z,27,1,3,4.248425498398469
508,15812,"Closing input stream, runner session in TensorFlowInferenceInterface.java and fixing bit changes","cla: yes,stat:awaiting response",,1,,3,2018-01-03T11:19:08Z,2018-01-04T23:28:59Z,CONTRIBUTOR,,2018-01-03T18:43:27Z,1,2,0,4.748425498398469
509,15807,Fix unstable test case for Select op,"awaiting testing (then merge),cla: yes","Fix #14862. CF: #15764

In the test case for Select op, the condition might switch to another value when `x1` is close to `x2`.  The PR is opened to resolve the unstable condition.

Test passed for 100 times.
```bash
[facai@h1077922 tensorflow]$ bazel test --runs_per_test=100 -c opt //tensorflow/cc:gradients_math_grad_test
HEAD is now at c642574... TST: modify unstable test case
INFO: Found 1 test target...
Target //tensorflow/cc:gradients_math_grad_test up-to-date:
  bazel-bin/tensorflow/cc/gradients_math_grad_test
INFO: Elapsed time: 49.959s, Critical Path: 21.29s
//tensorflow/cc:gradients_math_grad_test                                 PASSED in 21.3s
  Stats over 100 runs: max = 21.3s, min = 0.7s, avg = 11.2s, dev = 4.4s

Executed 1 out of 1 test: 1 test passes.
```

cc @gunan @drpngx 
  ",1,,4,2018-01-03T06:12:08Z,2018-01-04T02:13:18Z,CONTRIBUTOR,"Fix #14862. CF: #15764

In the test case for Select op, the condition might switch to another value when `x1` is close to `x2`.  The PR is opened to resolve the unstable condition.

Test passed for 100 times.
```bash
[facai@h1077922 tensorflow]$ bazel test --runs_per_test=100 -c opt //tensorflow/cc:gradients_math_grad_test
HEAD is now at c642574... TST: modify unstable test case
INFO: Found 1 test target...
Target //tensorflow/cc:gradients_math_grad_test up-to-date:
  bazel-bin/tensorflow/cc/gradients_math_grad_test
INFO: Elapsed time: 49.959s, Critical Path: 21.29s
//tensorflow/cc:gradients_math_grad_test                                 PASSED in 21.3s
  Stats over 100 runs: max = 21.3s, min = 0.7s, avg = 11.2s, dev = 4.4s

Executed 1 out of 1 test: 1 test passes.
```

cc @gunan @drpngx 
  ",2018-01-03T20:57:59Z,1,2,0,5.248425498398469
510,15806,Can we install tensorflow with a package manager in Linux distro?,,"As we all know, Tensorflow is a major opensource deeplearning framework to the developers. 
BTW, How can we install tensorflow with a package manager such as apt-get (for *.deb), yum/zypper/dnf (for *.rpm)  in Linux distro?

* Reference - https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/ci_build/builds

I could not find related scripts from the above web address. Does tensorflow support 1)  manual compilation with ./tools/ci_build/build/*.sh and 2) pre-built docker-based compilation only?

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: CentOS 7.0 and Ubuntu 16.04.3
- **TensorFlow installed from (source or binary)**: Latest version of Tensorflow (form github)
- **TensorFlow version (use command below)**: Latest version of Tensorflow (form github)
- **Python version**:  2.7
- **Bazel version (if compiling from source)**: with Cmake (w/o Bazel)
- **GCC/Compiler version (if compiling from source)**: GCC 5.0
- **CUDA/cuDNN version**:  None (w/ CPU only)
- **GPU model and memory**: None , DRAM 16GB
- **Exact command to reproduce**:   Nonthing.



### Describe the problem
No support

### Source code / logs
Omission. 

  ",0,,1,2018-01-03T06:03:26Z,2018-01-03T20:55:34Z,NONE,"As we all know, Tensorflow is a major opensource deeplearning framework to the developers. 
BTW, How can we install tensorflow with a package manager such as apt-get (for *.deb), yum/zypper/dnf (for *.rpm)  in Linux distro?

* Reference - https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/ci_build/builds

I could not find related scripts from the above web address. Does tensorflow support 1)  manual compilation with ./tools/ci_build/build/*.sh and 2) pre-built docker-based compilation only?

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: CentOS 7.0 and Ubuntu 16.04.3
- **TensorFlow installed from (source or binary)**: Latest version of Tensorflow (form github)
- **TensorFlow version (use command below)**: Latest version of Tensorflow (form github)
- **Python version**:  2.7
- **Bazel version (if compiling from source)**: with Cmake (w/o Bazel)
- **GCC/Compiler version (if compiling from source)**: GCC 5.0
- **CUDA/cuDNN version**:  None (w/ CPU only)
- **GPU model and memory**: None , DRAM 16GB
- **Exact command to reproduce**:   Nonthing.



### Describe the problem
No support

### Source code / logs
Omission. 

  ",2018-01-03T20:55:34Z,0,1,0,1.7484254983984693
511,15803,Memory allocation improvement for `decode_libsvm`,"awaiting testing (then merge),cla: yes","This fix is an improvement to #14330. Previously, string split was handled through `str_util::Split`, which may incur unnecessary memory allocations. This fix uses StringPiece instead.

See comment https://github.com/tensorflow/tensorflow/pull/14330#pullrequestreview-79877956 for reference.

Signed-off-by: Yong Tang <yong.tang.github@outlook.com>",1,,1,2018-01-03T02:34:18Z,2018-01-03T20:26:52Z,MEMBER,"This fix is an improvement to #14330. Previously, string split was handled through `str_util::Split`, which may incur unnecessary memory allocations. This fix uses StringPiece instead.

See comment https://github.com/tensorflow/tensorflow/pull/14330#pullrequestreview-79877956 for reference.

Signed-off-by: Yong Tang <yong.tang.github@outlook.com>",2018-01-03T20:26:39Z,0,3,0,4.748425498398469
512,15802,tf.stack eats memory over time,,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.4.1
- **Python version**:  3.5
- **CUDA/cuDNN version**: 8
- **GPU model and memory**: 1060 + 6GB
 

### Describe the problem
I use tf.stack to stack 2 images. But the memory used by this program increase over time. I use `memory_profiler` check it. it is caused by tf.stack, here is the minimal re-produce code:

```
import tensorflow as tf
import glob
import gc
from memory_profiler import profile


@profile
def function_mark():
  pass

@profile
def stack_images():
  image_file_list = glob.glob(""car_images/*.jpg"")
  sess = tf.Session()

  for _ in range(300):
    # read image
    image1 = tf.gfile.FastGFile(image_file_list[0], 'rb').read()
    image2 = tf.gfile.FastGFile(image_file_list[1], 'rb').read()
    # decode image
    image1_decode = tf.image.decode_image(image1, channels=3)
    image2_decode = tf.image.decode_image(image2, channels=3)
    # stack image
    image_stack = tf.stack([image1_decode, image2_decode])
    # run session
    r_image_stack = sess.run(image_stack)
    # mark function. so I can check the memory-usage of every loop.
    function_mark()
    # force garbage collection, so all the un-reference variable will be freed.
    del r_image_stack
    gc.collect()
```

 First, I profile it line by line, here is the result:
**you can see it very clearly that line 26 take a lot of memory. My image is 800*600 and I only stack 2 image each time, so 1.3G memory consumption is not normal.**

> 
> Line #    Mem usage    Increment   Line Contents
> ================================================
>     11    190.0 MiB    190.0 MiB   @profile
>     12                             def stack_images():
>     13    190.0 MiB      0.0 MiB     image_file_list = glob.glob(""car_images/*.jpg"")
>     14    421.6 MiB    231.7 MiB     sess = tf.Session()
>     15
>     16   1998.4 MiB      0.0 MiB     for _ in range(300):
> 
>     17                                 # read image
>     18   1992.5 MiB      1.5 MiB       image1 = tf.gfile.FastGFile(image_file_list[0], 'rb').read()
>     19   1992.5 MiB      0.0 MiB       image2 = tf.gfile.FastGFile(image_file_list[1], 'rb').read()
>     20                                 # decode image
>     21   1992.8 MiB     77.6 MiB       image1_decode = tf.image.decode_image(image1, channels=3)
>     22   1993.3 MiB    108.6 MiB       image2_decode = tf.image.decode_image(image2, channels=3)
>     23                                 # stack image
>     24   1993.3 MiB      0.7 MiB       image_stack = tf.stack([image1_decode, image2_decode])
>     25                                 # run session
>     26   1998.4 MiB   1350.4 MiB       r_image_stack = sess.run(image_stack)
>     27                                 # mark function. so I can check the memory-usage of every loop.
>     28   1998.4 MiB     29.0 MiB       function_mark()
>     29                                 # force garbage collection, so all the un-reference variable will be freed.
>     30   1998.4 MiB      0.0 MiB       del r_image_stack
>     31   1998.4 MiB      8.9 MiB       gc.collect()

Then **I profile it over time.** I use function `function_mark` to distinguish each loop. So we can see it very clearly that the memory usage of this program is increase over time.
![figure_1](https://user-images.githubusercontent.com/5325686/34505710-d25c80b0-f061-11e7-99c6-5db2e990d9aa.png)

My question is: How should I avoid this problem. because it cause a serious performance regression.
",0,,2,2018-01-03T02:15:52Z,2018-01-03T06:27:07Z,CONTRIBUTOR,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.4.1
- **Python version**:  3.5
- **CUDA/cuDNN version**: 8
- **GPU model and memory**: 1060 + 6GB
 

### Describe the problem
I use tf.stack to stack 2 images. But the memory used by this program increase over time. I use `memory_profiler` check it. it is caused by tf.stack, here is the minimal re-produce code:

```
import tensorflow as tf
import glob
import gc
from memory_profiler import profile


@profile
def function_mark():
  pass

@profile
def stack_images():
  image_file_list = glob.glob(""car_images/*.jpg"")
  sess = tf.Session()

  for _ in range(300):
    # read image
    image1 = tf.gfile.FastGFile(image_file_list[0], 'rb').read()
    image2 = tf.gfile.FastGFile(image_file_list[1], 'rb').read()
    # decode image
    image1_decode = tf.image.decode_image(image1, channels=3)
    image2_decode = tf.image.decode_image(image2, channels=3)
    # stack image
    image_stack = tf.stack([image1_decode, image2_decode])
    # run session
    r_image_stack = sess.run(image_stack)
    # mark function. so I can check the memory-usage of every loop.
    function_mark()
    # force garbage collection, so all the un-reference variable will be freed.
    del r_image_stack
    gc.collect()
```

 First, I profile it line by line, here is the result:
**you can see it very clearly that line 26 take a lot of memory. My image is 800*600 and I only stack 2 image each time, so 1.3G memory consumption is not normal.**

> 
> Line #    Mem usage    Increment   Line Contents
> ================================================
>     11    190.0 MiB    190.0 MiB   @profile
>     12                             def stack_images():
>     13    190.0 MiB      0.0 MiB     image_file_list = glob.glob(""car_images/*.jpg"")
>     14    421.6 MiB    231.7 MiB     sess = tf.Session()
>     15
>     16   1998.4 MiB      0.0 MiB     for _ in range(300):
> 
>     17                                 # read image
>     18   1992.5 MiB      1.5 MiB       image1 = tf.gfile.FastGFile(image_file_list[0], 'rb').read()
>     19   1992.5 MiB      0.0 MiB       image2 = tf.gfile.FastGFile(image_file_list[1], 'rb').read()
>     20                                 # decode image
>     21   1992.8 MiB     77.6 MiB       image1_decode = tf.image.decode_image(image1, channels=3)
>     22   1993.3 MiB    108.6 MiB       image2_decode = tf.image.decode_image(image2, channels=3)
>     23                                 # stack image
>     24   1993.3 MiB      0.7 MiB       image_stack = tf.stack([image1_decode, image2_decode])
>     25                                 # run session
>     26   1998.4 MiB   1350.4 MiB       r_image_stack = sess.run(image_stack)
>     27                                 # mark function. so I can check the memory-usage of every loop.
>     28   1998.4 MiB     29.0 MiB       function_mark()
>     29                                 # force garbage collection, so all the un-reference variable will be freed.
>     30   1998.4 MiB      0.0 MiB       del r_image_stack
>     31   1998.4 MiB      8.9 MiB       gc.collect()

Then **I profile it over time.** I use function `function_mark` to distinguish each loop. So we can see it very clearly that the memory usage of this program is increase over time.
![figure_1](https://user-images.githubusercontent.com/5325686/34505710-d25c80b0-f061-11e7-99c6-5db2e990d9aa.png)

My question is: How should I avoid this problem. because it cause a serious performance regression.
",2018-01-03T06:27:07Z,0,2,0,3.2484254983984693
513,15801,Remove calculation of unnecessary matrix columns in SVD gradient,"awaiting testing (then merge),cla: yes","The SVD gradient calculation when `compute_uv=False` currently uses the orthogonal ""U"" and ""V"" matrices returned by the SVD operation with `full_matrices=True`, but it really requires only the `full_matrices=False` versions.  This pull request makes the calculation use the `full_matrices=False` versions.

@rmlarsen pointed out that this change could be made in https://github.com/tensorflow/tensorflow/pull/14259#discussion_r157067512.",1,,8,2018-01-03T01:41:42Z,2018-01-25T21:11:40Z,CONTRIBUTOR,"The SVD gradient calculation when `compute_uv=False` currently uses the orthogonal ""U"" and ""V"" matrices returned by the SVD operation with `full_matrices=True`, but it really requires only the `full_matrices=False` versions.  This pull request makes the calculation use the `full_matrices=False` versions.

@rmlarsen pointed out that this change could be made in https://github.com/tensorflow/tensorflow/pull/14259#discussion_r157067512.",2018-01-03T01:49:21Z,22,2,3,7.248425498398469
514,15800,[bug] tf.estimator.DNNClassifier setting n_classes has no effect,,"### Describe the problem
I was following the examples for a [tensorflow estimator](https://developers.googleblog.com/2017/09/introducing-tensorflow-datasets.html). I am setting n_classes but the label check in (_check_labels tensorflow/python/estimator/canned/head.py"", line 222) keeps kicking back the following error:

**ValueError: Mismatched label shape. Classifier configured with n_classes=1.  Received 4. Suggested Fix: check your n_classes argument to the estimator and/or the shape of your label.**

### Source code / logs
``` python
import tensorflow as tf
import numpy as np

trainX = np.array([1,0,2,3])
num_classes = 4
feature_names = ['f1']
feature_columns = [tf.feature_column.numeric_column(k) for k in feature_names]
classifier = tf.estimator.DNNClassifier(feature_columns=feature_columns, 
                                            n_classes=num_classes, #setting number of classes here
                                            hidden_units=[10])

def input_fn():
    my_int_variable = tf.get_variable(""my_int_variable"", [1], dtype=tf.int32, initializer=tf.zeros_initializer)
    label = tf.one_hot(my_int_variable, num_classes) #using same number of classes
    return {'f1':trainX},label

classifier.train(input_fn=lambda: input_fn())
```

``` bash
Traceback (most recent call last):
  File ""test.py"", line 17, in <module>
    classifier.train(input_fn=lambda: input_fn())
  File ""/home/user/tensorflow/tf/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 302, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/home/user/tensorflow/tf/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 711, in _train_model
    features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)
  File ""/home/user/tensorflow/tf/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 694, in _call_model_fn
    model_fn_results = self._model_fn(features=features, **kwargs)
  File ""/home/user/tensorflow/tf/lib/python2.7/site-packages/tensorflow/python/estimator/canned/dnn.py"", line 334, in _model_fn
    config=config)
  File ""/home/user/tensorflow/tf/lib/python2.7/site-packages/tensorflow/python/estimator/canned/dnn.py"", line 203, in _dnn_model_fn
    logits=logits)
  File ""/home/user/tensorflow/tf/lib/python2.7/site-packages/tensorflow/python/estimator/canned/head.py"", line 493, in create_estimator_spec
    features=features, mode=mode, logits=logits, labels=labels)
  File ""/home/user/tensorflow/tf/lib/python2.7/site-packages/tensorflow/python/estimator/canned/head.py"", line 433, in create_loss
    label_ids = self._label_ids(_check_labels(_maybe_expand_dim(labels), 1))
  File ""/home/user/tensorflow/tf/lib/python2.7/site-packages/tensorflow/python/estimator/canned/head.py"", line 222, in _check_labels
    (expected_labels_dimension, dim1))
ValueError: Mismatched label shape. Classifier configured with n_classes=1.  Received 4. Suggested Fix: check your n_classes argument to the estimator and/or the shape of your label.
```

------------------------

### System information
Mac OSX 10.12.6
Python 2.7
Tensorflow ('v1.4.0-19-ga52c8d9b01', '1.4.1')



",0,,4,2018-01-03T00:18:44Z,2018-01-03T15:52:39Z,NONE,"### Describe the problem
I was following the examples for a [tensorflow estimator](https://developers.googleblog.com/2017/09/introducing-tensorflow-datasets.html). I am setting n_classes but the label check in (_check_labels tensorflow/python/estimator/canned/head.py"", line 222) keeps kicking back the following error:

**ValueError: Mismatched label shape. Classifier configured with n_classes=1.  Received 4. Suggested Fix: check your n_classes argument to the estimator and/or the shape of your label.**

### Source code / logs
``` python
import tensorflow as tf
import numpy as np

trainX = np.array([1,0,2,3])
num_classes = 4
feature_names = ['f1']
feature_columns = [tf.feature_column.numeric_column(k) for k in feature_names]
classifier = tf.estimator.DNNClassifier(feature_columns=feature_columns, 
                                            n_classes=num_classes, #setting number of classes here
                                            hidden_units=[10])

def input_fn():
    my_int_variable = tf.get_variable(""my_int_variable"", [1], dtype=tf.int32, initializer=tf.zeros_initializer)
    label = tf.one_hot(my_int_variable, num_classes) #using same number of classes
    return {'f1':trainX},label

classifier.train(input_fn=lambda: input_fn())
```

``` bash
Traceback (most recent call last):
  File ""test.py"", line 17, in <module>
    classifier.train(input_fn=lambda: input_fn())
  File ""/home/user/tensorflow/tf/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 302, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/home/user/tensorflow/tf/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 711, in _train_model
    features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)
  File ""/home/user/tensorflow/tf/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 694, in _call_model_fn
    model_fn_results = self._model_fn(features=features, **kwargs)
  File ""/home/user/tensorflow/tf/lib/python2.7/site-packages/tensorflow/python/estimator/canned/dnn.py"", line 334, in _model_fn
    config=config)
  File ""/home/user/tensorflow/tf/lib/python2.7/site-packages/tensorflow/python/estimator/canned/dnn.py"", line 203, in _dnn_model_fn
    logits=logits)
  File ""/home/user/tensorflow/tf/lib/python2.7/site-packages/tensorflow/python/estimator/canned/head.py"", line 493, in create_estimator_spec
    features=features, mode=mode, logits=logits, labels=labels)
  File ""/home/user/tensorflow/tf/lib/python2.7/site-packages/tensorflow/python/estimator/canned/head.py"", line 433, in create_loss
    label_ids = self._label_ids(_check_labels(_maybe_expand_dim(labels), 1))
  File ""/home/user/tensorflow/tf/lib/python2.7/site-packages/tensorflow/python/estimator/canned/head.py"", line 222, in _check_labels
    (expected_labels_dimension, dim1))
ValueError: Mismatched label shape. Classifier configured with n_classes=1.  Received 4. Suggested Fix: check your n_classes argument to the estimator and/or the shape of your label.
```

------------------------

### System information
Mac OSX 10.12.6
Python 2.7
Tensorflow ('v1.4.0-19-ga52c8d9b01', '1.4.1')



",2018-01-03T10:05:09Z,0,1,0,3.2484254983984693
515,15797,Change dso_loader to look for libcupti.so instead of libcupti.so.8.0,"awaiting review,cla: yes","On Android, it is hard to package libcupti.so.8.0 with bazel to generate CUDA-enabled apk
The cc_library macro in bazel only looks for *.so, not *.so.*",1,,5,2018-01-02T22:58:58Z,2018-01-05T06:20:35Z,CONTRIBUTOR,"On Android, it is hard to package libcupti.so.8.0 with bazel to generate CUDA-enabled apk
The cc_library macro in bazel only looks for *.so, not *.so.*",2018-01-04T17:29:49Z,3,2,1,5.747337946950052
516,15796,Fix build issues with cuda 9.1 through updating eigen.,cla: yes,,0,,2,2018-01-02T21:55:13Z,2018-01-04T18:32:04Z,OWNER,,2018-01-04T18:26:17Z,2,4,0,5.247337946950052
517,15795,configure.py environment variables,stat:awaiting response,"Have I written custom code: No
OS Platform and Distribution: Linux (Any?)
TensorFlow installed from: Source
TensorFlow version: 1.4 (Master Branch)
Bazel version: 0.6
CUDA/cuDNN version: N/A
GPU model and memory: N/A
Exact command to reproduce: N/A


### System information
- Building from source
- Branch Master (currently d87a9fbbc5f49ec5ae8eb52c62628f0b1a0bf67f)



### Describe the problem
Line 1353 of configure.py needs to have int( ) wrapped around the function get_var, otherwise the string that is returned always flags positive and setting TF_SET_ANDROID_WORKSPACE=0 environment variable to avoid user interaction in building from source will never work.

  ",0,,2,2018-01-02T21:50:40Z,2018-01-03T16:35:40Z,NONE,"Have I written custom code: No
OS Platform and Distribution: Linux (Any?)
TensorFlow installed from: Source
TensorFlow version: 1.4 (Master Branch)
Bazel version: 0.6
CUDA/cuDNN version: N/A
GPU model and memory: N/A
Exact command to reproduce: N/A


### System information
- Building from source
- Branch Master (currently d87a9fbbc5f49ec5ae8eb52c62628f0b1a0bf67f)



### Describe the problem
Line 1353 of configure.py needs to have int( ) wrapped around the function get_var, otherwise the string that is returned always flags positive and setting TF_SET_ANDROID_WORKSPACE=0 environment variable to avoid user interaction in building from source will never work.

  ",2018-01-03T07:34:35Z,1,1,0,2.2473379469500516
518,15790,order quantized table by value for ease of reading,cla: yes,,0,,2,2018-01-02T15:18:42Z,2018-01-02T17:40:27Z,CONTRIBUTOR,,2018-01-02T17:40:24Z,0,2,0,3.2473379469500516
519,15789,order quantized table by value for ease of reading,cla: yes,,0,,2,2018-01-02T15:18:10Z,2018-01-02T17:40:42Z,CONTRIBUTOR,,2018-01-02T17:40:42Z,0,2,0,3.2473379469500516
520,15787,order quantized table by value for ease of reading,cla: yes,,0,,2,2018-01-02T15:11:26Z,2018-01-02T19:29:58Z,CONTRIBUTOR,,2018-01-02T17:40:52Z,0,2,0,3.2473379469500516
521,15786,order quantized table by value for ease of reading,"awaiting review,cla: yes",,1,,4,2018-01-02T15:11:05Z,2018-02-15T23:37:15Z,CONTRIBUTOR,,2018-01-17T19:13:54Z,43,2,4,5.247337946950052
522,15780,[configure] eagerly determine the truthfulness of environment variables,"awaiting testing (then merge),cla: yes","Eagerly determine the truthfulness of environment variables in `get_var` function.

This way we can skip checking, e.g. Android workspace setup if the user sets `TF_SET_ANDROID_WORKSPACE=0`

Tested manually. 
",1,,7,2018-01-02T05:59:18Z,2018-01-03T01:13:02Z,CONTRIBUTOR,"Eagerly determine the truthfulness of environment variables in `get_var` function.

This way we can skip checking, e.g. Android workspace setup if the user sets `TF_SET_ANDROID_WORKSPACE=0`

Tested manually. 
",2018-01-02T06:01:07Z,1,2,0,6.747337946950052
523,15777,libstdc++.so.6: version `CXXABI_1.3.8' not found,,"All of my `tf-nightly` Travis CI pipelines started failing today with following error

``` ImportError: /usr/lib/x86_64-linux-gnu/libstdc++.so.6: version `CXXABI_1.3.8' not found (required by /home/travis/virtualenv/python3.5.4/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)```

Example:
https://travis-ci.org/yaroslavvb/chain_constant_memory/builds/323851093

Any ideas how to fix?
",1,,24,2018-01-01T21:56:33Z,2018-02-06T16:05:33Z,CONTRIBUTOR,"All of my `tf-nightly` Travis CI pipelines started failing today with following error

``` ImportError: /usr/lib/x86_64-linux-gnu/libstdc++.so.6: version `CXXABI_1.3.8' not found (required by /home/travis/virtualenv/python3.5.4/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)```

Example:
https://travis-ci.org/yaroslavvb/chain_constant_memory/builds/323851093

Any ideas how to fix?
",2018-01-01T22:01:25Z,35,2,4,15.246278545814494
524,15776,Compiler Errors Installing Tensorflow from Source,stat:awaiting response,"### System information

- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 7 SP1
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: r1.5
- **Python version**: 3.6.4
- **Bazel version (if compiling from source)**: 0.9.0
- **GCC/Compiler version (if compiling from source)**: 6.4.0
- **CUDA/cuDNN version**: 7.0
- **GPU model and memory**: NVIDIA Quadro K4000

- **Exact command to reproduce**:

bazel build -c opt %BUILD_OPTS% //tensorflow/tools/pip_package:build_pip_package

### Describe the problem

I have tried compiling with MSYS2 and VS2015. I am trying to get VS2015 to work.

###Using VS2015 and `--cpu=x64_windows_msvc --host_cpu=x64_windows_msvc` (among other options), I get the following error:

```
c:\Users\adam.hendry\Downloads\tensorflow>bazel build -c opt %BUILD_OPTS% //tens
orflow/tools/pip_package:build_pip_package
.......................
Loading:
Loading: 0 packages loaded
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (1 packages l
oaded)
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (6 packages l
oaded)
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (48 packages
loaded)
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (81 packages
loaded)
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (93 packages
loaded)
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (94 packages
loaded)
WARNING: C:/users/adam.hendry/downloads/tensorflow/tensorflow/core/BUILD:1807:1:
 in includes attribute of cc_library rule //tensorflow/core:framework_headers_li
b: '../../external/nsync/public' resolves to 'external/nsync/public' not below t
he relative path of its package 'tensorflow/core'. This will be an error in the
future. Since this rule was created by the macro 'cc_header_only_library', the e
rror might have been caused by the macro implementation in C:/users/adam.hendry/
downloads/tensorflow/tensorflow/tensorflow.bzl:1138:30
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (116 packages
 loaded)
INFO: Analysed target //tensorflow/tools/pip_package:build_pip_package (127 pack
ages loaded).
INFO: Found 1 target...
Building: no action
[0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt
INFO: From Compiling external/zlib_archive/uncompr.c [for host]:
cl : Command line warning D9002 : ignoring unknown option '-march=native'
INFO: From Compiling external/zlib_archive/gzlib.c [for host]:
cl : Command line warning D9002 : ignoring unknown option '-march=native'
INFO: From Compiling external/zlib_archive/adler32.c [for host]:
cl : Command line warning D9002 : ignoring unknown option '-march=native'
INFO: From Compiling external/zlib_archive/gzclose.c [for host]:
cl : Command line warning D9002 : ignoring unknown option '-march=native'
INFO: From Compiling external/zlib_archive/compress.c [for host]:
cl : Command line warning D9002 : ignoring unknown option '-march=native'
[134 / 1,014] Compiling external/zlib_archive/deflate.c [for host]; 1s local ...
 (16 actions, 14 running)
INFO: From Compiling external/zlib_archive/infback.c [for host]:
cl : Command line warning D9002 : ignoring unknown option '-march=native'
INFO: From Compiling tensorflow/core/lib/hash/crc32c_accelerate.cc [for host]:
cl : Command line warning D9002 : ignoring unknown option '-march=native'
INFO: From Compiling external/highwayhash/highwayhash/arch_specific.cc [for host
]:
cl : Command line warning D9002 : ignoring unknown option '-march=native'
INFO: From Compiling external/zlib_archive/crc32.c [for host]:
cl : Command line warning D9002 : ignoring unknown option '-march=native'
INFO: From Compiling external/zlib_archive/inflate.c [for host]:
cl : Command line warning D9002 : ignoring unknown option '-march=native'
INFO: From Compiling external/zlib_archive/inftrees.c [for host]:
cl : Command line warning D9002 : ignoring unknown option '-march=native'
INFO: From Compiling external/zlib_archive/gzread.c [for host]:
cl : Command line warning D9002 : ignoring unknown option '-march=native'
INFO: From Compiling external/zlib_archive/gzwrite.c [for host]:
cl : Command line warning D9002 : ignoring unknown option '-march=native'
INFO: From Compiling external/zlib_archive/zutil.c [for host]:
cl : Command line warning D9002 : ignoring unknown option '-march=native'
INFO: From Compiling external/zlib_archive/inffast.c [for host]:
cl : Command line warning D9002 : ignoring unknown option '-march=native'
INFO: From Compiling external/zlib_archive/trees.c [for host]:
cl : Command line warning D9002 : ignoring unknown option '-march=native'
INFO: From Compiling external/zlib_archive/deflate.c [for host]:
cl : Command line warning D9002 : ignoring unknown option '-march=native'
INFO: From Compiling external/fft2d/fft/fftsg.c [for host]:
cl : Command line warning D9002 : ignoring unknown option '-march=native'
INFO: From Compiling external/farmhash_archive/src/farmhash.cc [for host]:
cl : Command line warning D9002 : ignoring unknown option '-march=native'
INFO: From Compiling tensorflow/compiler/xla/executable_run_options.cc:
cl : Command line warning D9002 : ignoring unknown option '-march=native'
[349 / 1,966] Compiling external/protobuf_archive/src/google/protobuf/compiler/j
s/embed.cc [for host]; 1s local ... (5 actions, 3 running)
INFO: From Compiling external/farmhash_archive/src/farmhash.cc:
cl : Command line warning D9002 : ignoring unknown option '-march=native'
INFO: From Compiling external/protobuf_archive/src/google/protobuf/compiler/js/e
mbed.cc [for host]:
cl : Command line warning D9002 : ignoring unknown option '-march=native'
INFO: From Compiling tensorflow/core/grappler/costs/robust_stats.cc:
cl : Command line warning D9002 : ignoring unknown option '-march=native'
INFO: From Compiling external/lmdb/midl.c:
cl : Command line warning D9002 : ignoring unknown option '-march=native'
[643 / 3,343] Compiling external/lmdb/mdb.c; 0s local ... (23 actions, 22 runnin
g)
INFO: From Compiling external/zlib_archive/gzread.c:
cl : Command line warning D9002 : ignoring unknown option '-march=native'
INFO: From Compiling external/zlib_archive/trees.c:
cl : Command line warning D9002 : ignoring unknown option '-march=native'
INFO: From Compiling external/zlib_archive/gzclose.c:
cl : Command line warning D9002 : ignoring unknown option '-march=native'
INFO: From Compiling external/zlib_archive/gzwrite.c:
cl : Command line warning D9002 : ignoring unknown option '-march=native'
INFO: From Compiling external/zlib_archive/crc32.c:
cl : Command line warning D9002 : ignoring unknown option '-march=native'
INFO: From Compiling external/zlib_archive/inffast.c:
cl : Command line warning D9002 : ignoring unknown option '-march=native'
INFO: From Compiling external/zlib_archive/inftrees.c:
cl : Command line warning D9002 : ignoring unknown option '-march=native'
INFO: From Compiling external/zlib_archive/compress.c:
cl : Command line warning D9002 : ignoring unknown option '-march=native'
INFO: From Compiling external/zlib_archive/uncompr.c:
cl : Command line warning D9002 : ignoring unknown option '-march=native'
INFO: From Compiling external/zlib_archive/zutil.c:
cl : Command line warning D9002 : ignoring unknown option '-march=native'
INFO: From Compiling external/zlib_archive/gzlib.c:
cl : Command line warning D9002 : ignoring unknown option '-march=native'
INFO: From Compiling external/zlib_archive/infback.c:
cl : Command line warning D9002 : ignoring unknown option '-march=native'
INFO: From Compiling external/zlib_archive/deflate.c:
cl : Command line warning D9002 : ignoring unknown option '-march=native'
INFO: From Compiling external/zlib_archive/inflate.c:
cl : Command line warning D9002 : ignoring unknown option '-march=native'
INFO: From Compiling external/zlib_archive/adler32.c:
cl : Command line warning D9002 : ignoring unknown option '-march=native'
INFO: From Compiling external/png_archive/pngtrans.c [for host]:
cl : Command line warning D9002 : ignoring unknown option '-march=native'
INFO: From Compiling external/highwayhash/highwayhash/sip_hash.cc [for host]:
cl : Command line warning D9002 : ignoring unknown option '-march=native'
INFO: From Compiling external/png_archive/pngget.c [for host]:
cl : Command line warning D9002 : ignoring unknown option '-march=native'
INFO: From Compiling external/png_archive/pngread.c [for host]:
cl : Command line warning D9002 : ignoring unknown option '-march=native'
ERROR: C:/users/adam.hendry/downloads/tensorflow/tensorflow/compiler/xla/service
/cpu/BUILD:772:1: C++ compilation of rule '//tensorflow/compiler/xla/service/cpu
:custom_call_target_registry' failed (Exit 1): cl.exe failed: error executing co
mmand
  cd C:/users/adam.hendry/appdata/local/temp/_bazel_adam.hendry/qoyar4dt/execroo
t/org_tensorflow
  SET CUDA_COMPUTE_CAPABILITIE=None
    SET CUDA_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.1
    SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.
1
    SET CUDNN_INSTALL_PATH=C:/cuda
    SET INCLUDE=C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE;C
:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\ATLMFC\INCLUDE;C:\Program
Files (x86)\Windows Kits\10\include\10.0.16299.0\ucrt;C:\Program Files (x86)\Win
dows Kits\NETFXSDK\4.6.1\include\um;C:\Program Files (x86)\Windows Kits\10\inclu
de\10.0.16299.0\shared;C:\Program Files (x86)\Windows Kits\10\include\10.0.16299
.0\um;C:\Program Files (x86)\Windows Kits\10\include\10.0.16299.0\winrt;
    SET LIB=C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\LIB\amd64;C:\
Program Files (x86)\Microsoft Visual Studio 14.0\VC\ATLMFC\LIB\amd64;C:\Program
Files (x86)\Windows Kits\10\lib\10.0.16299.0\ucrt\x64;C:\Program Files (x86)\Win
dows Kits\NETFXSDK\4.6.1\lib\um\x64;C:\Program Files (x86)\Windows Kits\10\lib\1
0.0.16299.0\um\x64;
    SET NO_WHOLE_ARCHIVE_OPTION=1
    SET PATH=C:\Program Files (x86)\Microsoft Visual Studio 14.0\Common7\IDE\Com
monExtensions\Microsoft\TestWindow;C:\Program Files (x86)\Microsoft Visual Studi
o 14.0\VC\BIN\amd64;C:\Windows\Microsoft.NET\Framework64\v4.0.30319;C:\Program F
iles (x86)\Microsoft Visual Studio 14.0\VC\VCPackages;C:\Program Files (x86)\Mic
rosoft Visual Studio 14.0\Common7\IDE;C:\Program Files (x86)\Microsoft Visual St
udio 14.0\Common7\Tools;C:\Program Files (x86)\Microsoft Visual Studio 14.0\Team
 Tools\Performance Tools\x64;C:\Program Files (x86)\Microsoft Visual Studio 14.0
\Team Tools\Performance Tools;C:\Program Files (x86)\Windows Kits\10\bin\x64;C:\
Program Files (x86)\Windows Kits\10\bin\x86;C:\Program Files (x86)\Microsoft SDK
s\Windows\v10.0A\bin\NETFX 4.6.1 Tools\x64\;;C:\Windows\system32
    SET PWD=/proc/self/cwd
    SET PYTHON_BIN_PATH=C:/Python/Python36/python.exe
    SET PYTHON_LIB_PATH=C:/Python/Python36/lib/site-packages
    SET TEMP=C:\Users\ADAM~1.HEN\AppData\Local\Temp
    SET TF_CUDA_CLANG=0
    SET TF_CUDA_COMPUTE_CAPABILITIES=3.0
    SET TF_CUDA_VERSION=9.1
    SET TF_CUDNN_VERSION=7
    SET TF_NEED_CUDA=1
    SET TF_NEED_OPENCL_SYCL=0
    SET TMP=C:\Users\ADAM~1.HEN\AppData\Local\Temp
  C:/Program Files (x86)/Microsoft Visual Studio 14.0/VC/bin/amd64/cl.exe /c ten
sorflow/compiler/xla/service/cpu/custom_call_target_registry.cc /Fobazel-out/x64
_windows_msvc-py3-opt/bin/tensorflow/compiler/xla/service/cpu/_objs/custom_call_
target_registry/tensorflow/compiler/xla/service/cpu/custom_call_target_registry.
o /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0600 /D_CRT_SECURE_NO_DEPR
ECATE /D_CRT_SECURE_NO_WARNINGS /D_SILENCE_STDEXT_HASH_DEPRECATION_WARNINGS /big
obj /Zm500 /J /Gy /GF /EHsc /wd4351 /wd4291 /wd4250 /wd4996 -DGEMMLOWP_ALLOW_SLO
W_SCALAR_FALLBACK -w -march=native -D_GLIBCXX_USE_CXX11_ABI=0 /I. /Ibazel-out/x6
4_windows_msvc-py3-opt/genfiles /Iexternal/bazel_tools /Ibazel-out/x64_windows_m
svc-py3-opt/genfiles/external/bazel_tools /Iexternal/bazel_tools/tools/cpp/gcc3
/showIncludes /MD /O2 /DNDEBUG
C:\users\adam.hendry\appdata\local\temp\_bazel_adam.hendry\qoyar4dt\execroot\org
_tensorflow\tensorflow\compiler\xla\service\cpu\custom_call_target_registry.cc :
 fatal error C1083: Cannot open compiler generated file: '': Invalid argument
cl : Command line warning D9002 : ignoring unknown option '-march=native'
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 25.271s, Critical Path: 3.48s
FAILED: Build did NOT complete successfully
```

###Using VS2015 and `--cpu=x64_windows_msys --host_cpu=x64_windows_msys` (among other options), I get this error:

```
c:\Users\adam.hendry\Downloads\tensorflow>bazel build -c opt %BUILD_OPTS% //tens
orflow/tools/pip_package:build_pip_package
.......................
Loading:
Loading: 0 packages loaded
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (1 packages l
oaded)
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (6 packages l
oaded)
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (73 packages
loaded)
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (131 packages
 loaded)
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (141 packages
 loaded)
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (142 packages
 loaded)
WARNING: C:/users/adam.hendry/downloads/tensorflow/tensorflow/core/BUILD:1807:1:
 in includes attribute of cc_library rule //tensorflow/core:framework_headers_li
b: '../../external/nsync/public' resolves to 'external/nsync/public' not below t
he relative path of its package 'tensorflow/core'. This will be an error in the
future. Since this rule was created by the macro 'cc_header_only_library', the e
rror might have been caused by the macro implementation in C:/users/adam.hendry/
downloads/tensorflow/tensorflow/tensorflow.bzl:1138:30
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (157 packages
 loaded)
WARNING: C:/users/adam.hendry/downloads/tensorflow/tensorflow/contrib/learn/BUIL
D:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflo
w/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/sessio
n_bundle:exporter': No longer supported. Switch to SavedModel immediately.
WARNING: C:/users/adam.hendry/downloads/tensorflow/tensorflow/contrib/learn/BUIL
D:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflo
w/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/sessio
n_bundle:gc': No longer supported. Switch to SavedModel immediately.
INFO: Analysed target //tensorflow/tools/pip_package:build_pip_package (254 pack
ages loaded).
INFO: Found 1 target...
Building: no action
[0 / 17] [-----] BazelWorkspaceStatusAction stable-status.txt
[105 / 747] Executing genrule @local_config_cuda//cuda:cuda-lib; 2s local ... (2
4 actions running)
[106 / 747] Executing genrule @local_config_cuda//cuda:cuda-lib; 5s local ... (2
3 actions running)
[111 / 747] Executing genrule @local_config_cuda//cuda:cuda-lib; 10s local ... (
24 actions running)
[113 / 747] Executing genrule //tensorflow/core:version_info_gen [for host]; 13s
 local ... (23 actions running)
[129 / 968] Executing genrule //tensorflow/core:version_info_gen [for host]; 20s
 local ... (24 actions running)
[148 / 1,274] Executing genrule //tensorflow/core:version_info_gen [for host]; 3
1s local ... (24 actions running)
[149 / 1,275] Executing genrule //tensorflow/core:version_info_gen [for host]; 3
7s local ... (24 actions running)
[149 / 1,275] Executing genrule //tensorflow/core:version_info_gen [for host]; 4
6s local ... (24 actions running)
[155 / 1,279] Executing genrule @local_config_cuda//cuda:cuda-include [for host]
; 57s local ... (23 actions running)
[228 / 1,543] Executing genrule @local_config_cuda//cuda:cuda-include [for host]
; 68s local ... (24 actions, 23 running)
[417 / 2,009] Executing genrule @local_config_cuda//cuda:cuda-include [for host]
; 83s local ... (24 actions, 23 running)
[437 / 2,009] Executing genrule @local_config_cuda//cuda:cuda-include [for host]
; 98s local ... (23 actions running)
[458 / 2,009] Executing genrule @local_config_cuda//cuda:cuda-include [for host]
; 115s local ... (24 actions, 23 running)
[523 / 2,096] Executing genrule @local_config_cuda//cuda:cuda-include [for host]
; 135s local ... (24 actions, 23 running)
ERROR: C:/users/adam.hendry/appdata/local/temp/_bazel_adam.hendry/qoyar4dt/exter
nal/com_google_absl/absl/base/BUILD.bazel:30:1: C++ compilation of rule '@com_go
ogle_absl//absl/base:spinlock_wait' failed (Exit 1): gcc failed: error executing
 command
  cd C:/users/adam.hendry/appdata/local/temp/_bazel_adam.hendry/qoyar4dt/execroo
t/org_tensorflow
  SET CUDA_COMPUTE_CAPABILITIE=None
    SET CUDA_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.1
    SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.
1
    SET CUDNN_INSTALL_PATH=C:/cuda
    SET NO_WHOLE_ARCHIVE_OPTION=1
    SET PATH=C:\msys64\usr\bin;C:\msys64\bin;C:\Program Files (x86)\Microsoft Vi
sual Studio 14.0\Common7\IDE\CommonExtensions\Microsoft\TestWindow;C:\Program Fi
les (x86)\MSBuild\14.0\bin;C:\Program Files (x86)\Microsoft Visual Studio 14.0\C
ommon7\IDE\;C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\BIN;C:\Progra
m Files (x86)\Microsoft Visual Studio 14.0\Common7\Tools;C:\Windows\Microsoft.NE
T\Framework\v4.0.30319;C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\VC
Packages;C:\Program Files (x86)\HTML Help Workshop;C:\Program Files (x86)\Micros
oft Visual Studio 14.0\Team Tools\Performance Tools;C:\Program Files (x86)\Windo
ws Kits\10\bin\x86;C:\Program Files (x86)\Microsoft SDKs\Windows\v10.0A\bin\NETF
X 4.6.1 Tools\;C:\Program Files (x86)\Microsoft Visual Studio 14.0\;C:\Program F
iles (x86)\Microsoft Visual Studio 14.0\VC\;C:\Program Files (x86)\Microsoft Vis
ual Studio 14.0\VC\bin\;C:\Program Files (x86)\Windows Kits\10\;C:\Python\Python
36\;C:\Python\Python36\Scripts\;C:\ProgramData\chocolatey\bin;C:\Program Files\G
it LFS;C:\Program Files\CMake\bin;C:\Program Files\Java\jdk1.8.0_152\;C:\Program
Data\Oracle\Java\javapath;C:\Windows;C:\Windows\System32;C:\Windows\System32Wbem
;C:\Windows\System32WindowsPowerShell\v1.0\;C:\bazel\output;C:\apache-maven-3.3.
9\;C:\apache-maven-3.3.9\bin;C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\
v9.1\;%CUDNN_PATH%;C:\Program Files\ImageMagick-7.0.5-Q16\;C:\Program Files (x86
)\QuickTime\QTSystem\;C:\Program Files (x86)\gs\gs9.21\bin\;C:\cppan\cppan.exe;C
:\Program Files (x86)\Tesseract-OCR\;C:\Program Files\MiKTeX 2.9\miktex\bin\x64\
;C:\ffmpeg\bin;C:\Users\adam.hendry\.dnx\bin;C:\Program Files\Microsoft DNX\Dnvm
\;C:\Program Files\HDF_Group\HDF5\1.10.1\bin\;C:\Program Files (x86)\GtkSharp\2.
12\bin;C:\Program Files (x86)\gettext-iconv\lib\gettext;C:\Program Files (x86)\C
ommon Files\Intel\Shared Libraries\redist\ia32\mpirt;C:\Program Files (x86)\Comm
on Files\Intel\Shared Libraries\redist\ia32\compiler;C:\Program Files\Common Fil
es\Microsoft Shared\Windows Live;C:\Program Files (x86)\Common Files\Microsoft S
hared\Windows Live;C:\Program Files (x86)\NVIDIA Corporation\PhysX\Common;C:\Pro
gram Files (x86)\Intel\iCLS Client\;C:\Program Files\Intel\iCLS Client\;C:\Progr
am Files (x86)\Windows Live\Shared;C:\Program Files\Microsoft SQL Server\130\Too
ls\Binn\;C:\Intel\OpenCL\sdk\bin\x64;C:\Intel\OpenCL\sdk\bin\x86\;C:\Intel\OpenC
L\sdk\bin\Pin;C:\Intel\OpenCL\sdk\bin\GTPin;C:\Program Files\Git\cmd;
    SET PWD=/proc/self/cwd
    SET PYTHON_BIN_PATH=C:/Python/Python36/python.exe
    SET PYTHON_LIB_PATH=C:/Python/Python36/lib/site-packages
    SET TF_CUDA_CLANG=0
    SET TF_CUDA_COMPUTE_CAPABILITIES=3.0
    SET TF_CUDA_VERSION=9.1
    SET TF_CUDNN_VERSION=7
    SET TF_NEED_CUDA=1
    SET TF_NEED_OPENCL_SYCL=0
  C:/msys64/usr/bin/gcc -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -w -march=native -
std=gnu++0x -D_GLIBCXX_USE_CXX11_ABI=0 -MD -MF bazel-out/x64_windows_msys-py3-op
t/bin/external/com_google_absl/absl/base/_objs/spinlock_wait/external/com_google
_absl/absl/base/internal/spinlock_wait.d -frandom-seed=bazel-out/x64_windows_msy
s-py3-opt/bin/external/com_google_absl/absl/base/_objs/spinlock_wait/external/co
m_google_absl/absl/base/internal/spinlock_wait.o -D__CLANG_SUPPORT_DYN_ANNOTATIO
N__ -iquote external/com_google_absl -iquote bazel-out/x64_windows_msys-py3-opt/
genfiles/external/com_google_absl -iquote external/bazel_tools -iquote bazel-out
/x64_windows_msys-py3-opt/genfiles/external/bazel_tools -isystem external/bazel_
tools/tools/cpp/gcc3 -Wall -Wextra -Wcast-qual -Wconversion-null -Wmissing-decla
rations -Woverlength-strings -Wpointer-arith -Wunused-local-typedefs -Wunused-re
sult -Wvarargs -Wvla -Wwrite-strings -c external/com_google_absl/absl/base/inter
nal/spinlock_wait.cc -o bazel-out/x64_windows_msys-py3-opt/bin/external/com_goog
le_absl/absl/base/_objs/spinlock_wait/external/com_google_absl/absl/base/interna
l/spinlock_wait.o
In file included from external/com_google_absl/absl/base/config.h:66:0,
                 from external/com_google_absl/absl/base/port.h:23,
                 from external/com_google_absl/absl/base/internal/spinlock_posix
.inc:23,
                 from external/com_google_absl/absl/base/internal/spinlock_wait.
cc:27:
external/com_google_absl/absl/base/policy_checks.h:40:2: error: #error ""Cygwin i
s not supported.""
 #error ""Cygwin is not supported.""
  ^~~~~
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 172.251s, Critical Path: 153.84s
FAILED: Build did NOT complete successfully
```

###Any help would be appreciated. I can give you more details as well.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",0,,24,2018-01-01T20:06:46Z,2018-02-08T21:41:17Z,NONE,"### System information

- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 7 SP1
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: r1.5
- **Python version**: 3.6.4
- **Bazel version (if compiling from source)**: 0.9.0
- **GCC/Compiler version (if compiling from source)**: 6.4.0
- **CUDA/cuDNN version**: 7.0
- **GPU model and memory**: NVIDIA Quadro K4000

- **Exact command to reproduce**:

bazel build -c opt %BUILD_OPTS% //tensorflow/tools/pip_package:build_pip_package

### Describe the problem

I have tried compiling with MSYS2 and VS2015. I am trying to get VS2015 to work.

###Using VS2015 and `--cpu=x64_windows_msvc --host_cpu=x64_windows_msvc` (among other options), I get the following error:

```
c:\Users\adam.hendry\Downloads\tensorflow>bazel build -c opt %BUILD_OPTS% //tens
orflow/tools/pip_package:build_pip_package
.......................
Loading:
Loading: 0 packages loaded
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (1 packages l
oaded)
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (6 packages l
oaded)
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (48 packages
loaded)
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (81 packages
loaded)
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (93 packages
loaded)
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (94 packages
loaded)
WARNING: C:/users/adam.hendry/downloads/tensorflow/tensorflow/core/BUILD:1807:1:
 in includes attribute of cc_library rule //tensorflow/core:framework_headers_li
b: '../../external/nsync/public' resolves to 'external/nsync/public' not below t
he relative path of its package 'tensorflow/core'. This will be an error in the
future. Since this rule was created by the macro 'cc_header_only_library', the e
rror might have been caused by the macro implementation in C:/users/adam.hendry/
downloads/tensorflow/tensorflow/tensorflow.bzl:1138:30
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (116 packages
 loaded)
INFO: Analysed target //tensorflow/tools/pip_package:build_pip_package (127 pack
ages loaded).
INFO: Found 1 target...
Building: no action
[0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt
INFO: From Compiling external/zlib_archive/uncompr.c [for host]:
cl : Command line warning D9002 : ignoring unknown option '-march=native'
INFO: From Compiling external/zlib_archive/gzlib.c [for host]:
cl : Command line warning D9002 : ignoring unknown option '-march=native'
INFO: From Compiling external/zlib_archive/adler32.c [for host]:
cl : Command line warning D9002 : ignoring unknown option '-march=native'
INFO: From Compiling external/zlib_archive/gzclose.c [for host]:
cl : Command line warning D9002 : ignoring unknown option '-march=native'
INFO: From Compiling external/zlib_archive/compress.c [for host]:
cl : Command line warning D9002 : ignoring unknown option '-march=native'
[134 / 1,014] Compiling external/zlib_archive/deflate.c [for host]; 1s local ...
 (16 actions, 14 running)
INFO: From Compiling external/zlib_archive/infback.c [for host]:
cl : Command line warning D9002 : ignoring unknown option '-march=native'
INFO: From Compiling tensorflow/core/lib/hash/crc32c_accelerate.cc [for host]:
cl : Command line warning D9002 : ignoring unknown option '-march=native'
INFO: From Compiling external/highwayhash/highwayhash/arch_specific.cc [for host
]:
cl : Command line warning D9002 : ignoring unknown option '-march=native'
INFO: From Compiling external/zlib_archive/crc32.c [for host]:
cl : Command line warning D9002 : ignoring unknown option '-march=native'
INFO: From Compiling external/zlib_archive/inflate.c [for host]:
cl : Command line warning D9002 : ignoring unknown option '-march=native'
INFO: From Compiling external/zlib_archive/inftrees.c [for host]:
cl : Command line warning D9002 : ignoring unknown option '-march=native'
INFO: From Compiling external/zlib_archive/gzread.c [for host]:
cl : Command line warning D9002 : ignoring unknown option '-march=native'
INFO: From Compiling external/zlib_archive/gzwrite.c [for host]:
cl : Command line warning D9002 : ignoring unknown option '-march=native'
INFO: From Compiling external/zlib_archive/zutil.c [for host]:
cl : Command line warning D9002 : ignoring unknown option '-march=native'
INFO: From Compiling external/zlib_archive/inffast.c [for host]:
cl : Command line warning D9002 : ignoring unknown option '-march=native'
INFO: From Compiling external/zlib_archive/trees.c [for host]:
cl : Command line warning D9002 : ignoring unknown option '-march=native'
INFO: From Compiling external/zlib_archive/deflate.c [for host]:
cl : Command line warning D9002 : ignoring unknown option '-march=native'
INFO: From Compiling external/fft2d/fft/fftsg.c [for host]:
cl : Command line warning D9002 : ignoring unknown option '-march=native'
INFO: From Compiling external/farmhash_archive/src/farmhash.cc [for host]:
cl : Command line warning D9002 : ignoring unknown option '-march=native'
INFO: From Compiling tensorflow/compiler/xla/executable_run_options.cc:
cl : Command line warning D9002 : ignoring unknown option '-march=native'
[349 / 1,966] Compiling external/protobuf_archive/src/google/protobuf/compiler/j
s/embed.cc [for host]; 1s local ... (5 actions, 3 running)
INFO: From Compiling external/farmhash_archive/src/farmhash.cc:
cl : Command line warning D9002 : ignoring unknown option '-march=native'
INFO: From Compiling external/protobuf_archive/src/google/protobuf/compiler/js/e
mbed.cc [for host]:
cl : Command line warning D9002 : ignoring unknown option '-march=native'
INFO: From Compiling tensorflow/core/grappler/costs/robust_stats.cc:
cl : Command line warning D9002 : ignoring unknown option '-march=native'
INFO: From Compiling external/lmdb/midl.c:
cl : Command line warning D9002 : ignoring unknown option '-march=native'
[643 / 3,343] Compiling external/lmdb/mdb.c; 0s local ... (23 actions, 22 runnin
g)
INFO: From Compiling external/zlib_archive/gzread.c:
cl : Command line warning D9002 : ignoring unknown option '-march=native'
INFO: From Compiling external/zlib_archive/trees.c:
cl : Command line warning D9002 : ignoring unknown option '-march=native'
INFO: From Compiling external/zlib_archive/gzclose.c:
cl : Command line warning D9002 : ignoring unknown option '-march=native'
INFO: From Compiling external/zlib_archive/gzwrite.c:
cl : Command line warning D9002 : ignoring unknown option '-march=native'
INFO: From Compiling external/zlib_archive/crc32.c:
cl : Command line warning D9002 : ignoring unknown option '-march=native'
INFO: From Compiling external/zlib_archive/inffast.c:
cl : Command line warning D9002 : ignoring unknown option '-march=native'
INFO: From Compiling external/zlib_archive/inftrees.c:
cl : Command line warning D9002 : ignoring unknown option '-march=native'
INFO: From Compiling external/zlib_archive/compress.c:
cl : Command line warning D9002 : ignoring unknown option '-march=native'
INFO: From Compiling external/zlib_archive/uncompr.c:
cl : Command line warning D9002 : ignoring unknown option '-march=native'
INFO: From Compiling external/zlib_archive/zutil.c:
cl : Command line warning D9002 : ignoring unknown option '-march=native'
INFO: From Compiling external/zlib_archive/gzlib.c:
cl : Command line warning D9002 : ignoring unknown option '-march=native'
INFO: From Compiling external/zlib_archive/infback.c:
cl : Command line warning D9002 : ignoring unknown option '-march=native'
INFO: From Compiling external/zlib_archive/deflate.c:
cl : Command line warning D9002 : ignoring unknown option '-march=native'
INFO: From Compiling external/zlib_archive/inflate.c:
cl : Command line warning D9002 : ignoring unknown option '-march=native'
INFO: From Compiling external/zlib_archive/adler32.c:
cl : Command line warning D9002 : ignoring unknown option '-march=native'
INFO: From Compiling external/png_archive/pngtrans.c [for host]:
cl : Command line warning D9002 : ignoring unknown option '-march=native'
INFO: From Compiling external/highwayhash/highwayhash/sip_hash.cc [for host]:
cl : Command line warning D9002 : ignoring unknown option '-march=native'
INFO: From Compiling external/png_archive/pngget.c [for host]:
cl : Command line warning D9002 : ignoring unknown option '-march=native'
INFO: From Compiling external/png_archive/pngread.c [for host]:
cl : Command line warning D9002 : ignoring unknown option '-march=native'
ERROR: C:/users/adam.hendry/downloads/tensorflow/tensorflow/compiler/xla/service
/cpu/BUILD:772:1: C++ compilation of rule '//tensorflow/compiler/xla/service/cpu
:custom_call_target_registry' failed (Exit 1): cl.exe failed: error executing co
mmand
  cd C:/users/adam.hendry/appdata/local/temp/_bazel_adam.hendry/qoyar4dt/execroo
t/org_tensorflow
  SET CUDA_COMPUTE_CAPABILITIE=None
    SET CUDA_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.1
    SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.
1
    SET CUDNN_INSTALL_PATH=C:/cuda
    SET INCLUDE=C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE;C
:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\ATLMFC\INCLUDE;C:\Program
Files (x86)\Windows Kits\10\include\10.0.16299.0\ucrt;C:\Program Files (x86)\Win
dows Kits\NETFXSDK\4.6.1\include\um;C:\Program Files (x86)\Windows Kits\10\inclu
de\10.0.16299.0\shared;C:\Program Files (x86)\Windows Kits\10\include\10.0.16299
.0\um;C:\Program Files (x86)\Windows Kits\10\include\10.0.16299.0\winrt;
    SET LIB=C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\LIB\amd64;C:\
Program Files (x86)\Microsoft Visual Studio 14.0\VC\ATLMFC\LIB\amd64;C:\Program
Files (x86)\Windows Kits\10\lib\10.0.16299.0\ucrt\x64;C:\Program Files (x86)\Win
dows Kits\NETFXSDK\4.6.1\lib\um\x64;C:\Program Files (x86)\Windows Kits\10\lib\1
0.0.16299.0\um\x64;
    SET NO_WHOLE_ARCHIVE_OPTION=1
    SET PATH=C:\Program Files (x86)\Microsoft Visual Studio 14.0\Common7\IDE\Com
monExtensions\Microsoft\TestWindow;C:\Program Files (x86)\Microsoft Visual Studi
o 14.0\VC\BIN\amd64;C:\Windows\Microsoft.NET\Framework64\v4.0.30319;C:\Program F
iles (x86)\Microsoft Visual Studio 14.0\VC\VCPackages;C:\Program Files (x86)\Mic
rosoft Visual Studio 14.0\Common7\IDE;C:\Program Files (x86)\Microsoft Visual St
udio 14.0\Common7\Tools;C:\Program Files (x86)\Microsoft Visual Studio 14.0\Team
 Tools\Performance Tools\x64;C:\Program Files (x86)\Microsoft Visual Studio 14.0
\Team Tools\Performance Tools;C:\Program Files (x86)\Windows Kits\10\bin\x64;C:\
Program Files (x86)\Windows Kits\10\bin\x86;C:\Program Files (x86)\Microsoft SDK
s\Windows\v10.0A\bin\NETFX 4.6.1 Tools\x64\;;C:\Windows\system32
    SET PWD=/proc/self/cwd
    SET PYTHON_BIN_PATH=C:/Python/Python36/python.exe
    SET PYTHON_LIB_PATH=C:/Python/Python36/lib/site-packages
    SET TEMP=C:\Users\ADAM~1.HEN\AppData\Local\Temp
    SET TF_CUDA_CLANG=0
    SET TF_CUDA_COMPUTE_CAPABILITIES=3.0
    SET TF_CUDA_VERSION=9.1
    SET TF_CUDNN_VERSION=7
    SET TF_NEED_CUDA=1
    SET TF_NEED_OPENCL_SYCL=0
    SET TMP=C:\Users\ADAM~1.HEN\AppData\Local\Temp
  C:/Program Files (x86)/Microsoft Visual Studio 14.0/VC/bin/amd64/cl.exe /c ten
sorflow/compiler/xla/service/cpu/custom_call_target_registry.cc /Fobazel-out/x64
_windows_msvc-py3-opt/bin/tensorflow/compiler/xla/service/cpu/_objs/custom_call_
target_registry/tensorflow/compiler/xla/service/cpu/custom_call_target_registry.
o /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0600 /D_CRT_SECURE_NO_DEPR
ECATE /D_CRT_SECURE_NO_WARNINGS /D_SILENCE_STDEXT_HASH_DEPRECATION_WARNINGS /big
obj /Zm500 /J /Gy /GF /EHsc /wd4351 /wd4291 /wd4250 /wd4996 -DGEMMLOWP_ALLOW_SLO
W_SCALAR_FALLBACK -w -march=native -D_GLIBCXX_USE_CXX11_ABI=0 /I. /Ibazel-out/x6
4_windows_msvc-py3-opt/genfiles /Iexternal/bazel_tools /Ibazel-out/x64_windows_m
svc-py3-opt/genfiles/external/bazel_tools /Iexternal/bazel_tools/tools/cpp/gcc3
/showIncludes /MD /O2 /DNDEBUG
C:\users\adam.hendry\appdata\local\temp\_bazel_adam.hendry\qoyar4dt\execroot\org
_tensorflow\tensorflow\compiler\xla\service\cpu\custom_call_target_registry.cc :
 fatal error C1083: Cannot open compiler generated file: '': Invalid argument
cl : Command line warning D9002 : ignoring unknown option '-march=native'
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 25.271s, Critical Path: 3.48s
FAILED: Build did NOT complete successfully
```

###Using VS2015 and `--cpu=x64_windows_msys --host_cpu=x64_windows_msys` (among other options), I get this error:

```
c:\Users\adam.hendry\Downloads\tensorflow>bazel build -c opt %BUILD_OPTS% //tens
orflow/tools/pip_package:build_pip_package
.......................
Loading:
Loading: 0 packages loaded
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (1 packages l
oaded)
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (6 packages l
oaded)
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (73 packages
loaded)
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (131 packages
 loaded)
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (141 packages
 loaded)
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (142 packages
 loaded)
WARNING: C:/users/adam.hendry/downloads/tensorflow/tensorflow/core/BUILD:1807:1:
 in includes attribute of cc_library rule //tensorflow/core:framework_headers_li
b: '../../external/nsync/public' resolves to 'external/nsync/public' not below t
he relative path of its package 'tensorflow/core'. This will be an error in the
future. Since this rule was created by the macro 'cc_header_only_library', the e
rror might have been caused by the macro implementation in C:/users/adam.hendry/
downloads/tensorflow/tensorflow/tensorflow.bzl:1138:30
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (157 packages
 loaded)
WARNING: C:/users/adam.hendry/downloads/tensorflow/tensorflow/contrib/learn/BUIL
D:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflo
w/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/sessio
n_bundle:exporter': No longer supported. Switch to SavedModel immediately.
WARNING: C:/users/adam.hendry/downloads/tensorflow/tensorflow/contrib/learn/BUIL
D:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflo
w/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/sessio
n_bundle:gc': No longer supported. Switch to SavedModel immediately.
INFO: Analysed target //tensorflow/tools/pip_package:build_pip_package (254 pack
ages loaded).
INFO: Found 1 target...
Building: no action
[0 / 17] [-----] BazelWorkspaceStatusAction stable-status.txt
[105 / 747] Executing genrule @local_config_cuda//cuda:cuda-lib; 2s local ... (2
4 actions running)
[106 / 747] Executing genrule @local_config_cuda//cuda:cuda-lib; 5s local ... (2
3 actions running)
[111 / 747] Executing genrule @local_config_cuda//cuda:cuda-lib; 10s local ... (
24 actions running)
[113 / 747] Executing genrule //tensorflow/core:version_info_gen [for host]; 13s
 local ... (23 actions running)
[129 / 968] Executing genrule //tensorflow/core:version_info_gen [for host]; 20s
 local ... (24 actions running)
[148 / 1,274] Executing genrule //tensorflow/core:version_info_gen [for host]; 3
1s local ... (24 actions running)
[149 / 1,275] Executing genrule //tensorflow/core:version_info_gen [for host]; 3
7s local ... (24 actions running)
[149 / 1,275] Executing genrule //tensorflow/core:version_info_gen [for host]; 4
6s local ... (24 actions running)
[155 / 1,279] Executing genrule @local_config_cuda//cuda:cuda-include [for host]
; 57s local ... (23 actions running)
[228 / 1,543] Executing genrule @local_config_cuda//cuda:cuda-include [for host]
; 68s local ... (24 actions, 23 running)
[417 / 2,009] Executing genrule @local_config_cuda//cuda:cuda-include [for host]
; 83s local ... (24 actions, 23 running)
[437 / 2,009] Executing genrule @local_config_cuda//cuda:cuda-include [for host]
; 98s local ... (23 actions running)
[458 / 2,009] Executing genrule @local_config_cuda//cuda:cuda-include [for host]
; 115s local ... (24 actions, 23 running)
[523 / 2,096] Executing genrule @local_config_cuda//cuda:cuda-include [for host]
; 135s local ... (24 actions, 23 running)
ERROR: C:/users/adam.hendry/appdata/local/temp/_bazel_adam.hendry/qoyar4dt/exter
nal/com_google_absl/absl/base/BUILD.bazel:30:1: C++ compilation of rule '@com_go
ogle_absl//absl/base:spinlock_wait' failed (Exit 1): gcc failed: error executing
 command
  cd C:/users/adam.hendry/appdata/local/temp/_bazel_adam.hendry/qoyar4dt/execroo
t/org_tensorflow
  SET CUDA_COMPUTE_CAPABILITIE=None
    SET CUDA_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.1
    SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.
1
    SET CUDNN_INSTALL_PATH=C:/cuda
    SET NO_WHOLE_ARCHIVE_OPTION=1
    SET PATH=C:\msys64\usr\bin;C:\msys64\bin;C:\Program Files (x86)\Microsoft Vi
sual Studio 14.0\Common7\IDE\CommonExtensions\Microsoft\TestWindow;C:\Program Fi
les (x86)\MSBuild\14.0\bin;C:\Program Files (x86)\Microsoft Visual Studio 14.0\C
ommon7\IDE\;C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\BIN;C:\Progra
m Files (x86)\Microsoft Visual Studio 14.0\Common7\Tools;C:\Windows\Microsoft.NE
T\Framework\v4.0.30319;C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\VC
Packages;C:\Program Files (x86)\HTML Help Workshop;C:\Program Files (x86)\Micros
oft Visual Studio 14.0\Team Tools\Performance Tools;C:\Program Files (x86)\Windo
ws Kits\10\bin\x86;C:\Program Files (x86)\Microsoft SDKs\Windows\v10.0A\bin\NETF
X 4.6.1 Tools\;C:\Program Files (x86)\Microsoft Visual Studio 14.0\;C:\Program F
iles (x86)\Microsoft Visual Studio 14.0\VC\;C:\Program Files (x86)\Microsoft Vis
ual Studio 14.0\VC\bin\;C:\Program Files (x86)\Windows Kits\10\;C:\Python\Python
36\;C:\Python\Python36\Scripts\;C:\ProgramData\chocolatey\bin;C:\Program Files\G
it LFS;C:\Program Files\CMake\bin;C:\Program Files\Java\jdk1.8.0_152\;C:\Program
Data\Oracle\Java\javapath;C:\Windows;C:\Windows\System32;C:\Windows\System32Wbem
;C:\Windows\System32WindowsPowerShell\v1.0\;C:\bazel\output;C:\apache-maven-3.3.
9\;C:\apache-maven-3.3.9\bin;C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\
v9.1\;%CUDNN_PATH%;C:\Program Files\ImageMagick-7.0.5-Q16\;C:\Program Files (x86
)\QuickTime\QTSystem\;C:\Program Files (x86)\gs\gs9.21\bin\;C:\cppan\cppan.exe;C
:\Program Files (x86)\Tesseract-OCR\;C:\Program Files\MiKTeX 2.9\miktex\bin\x64\
;C:\ffmpeg\bin;C:\Users\adam.hendry\.dnx\bin;C:\Program Files\Microsoft DNX\Dnvm
\;C:\Program Files\HDF_Group\HDF5\1.10.1\bin\;C:\Program Files (x86)\GtkSharp\2.
12\bin;C:\Program Files (x86)\gettext-iconv\lib\gettext;C:\Program Files (x86)\C
ommon Files\Intel\Shared Libraries\redist\ia32\mpirt;C:\Program Files (x86)\Comm
on Files\Intel\Shared Libraries\redist\ia32\compiler;C:\Program Files\Common Fil
es\Microsoft Shared\Windows Live;C:\Program Files (x86)\Common Files\Microsoft S
hared\Windows Live;C:\Program Files (x86)\NVIDIA Corporation\PhysX\Common;C:\Pro
gram Files (x86)\Intel\iCLS Client\;C:\Program Files\Intel\iCLS Client\;C:\Progr
am Files (x86)\Windows Live\Shared;C:\Program Files\Microsoft SQL Server\130\Too
ls\Binn\;C:\Intel\OpenCL\sdk\bin\x64;C:\Intel\OpenCL\sdk\bin\x86\;C:\Intel\OpenC
L\sdk\bin\Pin;C:\Intel\OpenCL\sdk\bin\GTPin;C:\Program Files\Git\cmd;
    SET PWD=/proc/self/cwd
    SET PYTHON_BIN_PATH=C:/Python/Python36/python.exe
    SET PYTHON_LIB_PATH=C:/Python/Python36/lib/site-packages
    SET TF_CUDA_CLANG=0
    SET TF_CUDA_COMPUTE_CAPABILITIES=3.0
    SET TF_CUDA_VERSION=9.1
    SET TF_CUDNN_VERSION=7
    SET TF_NEED_CUDA=1
    SET TF_NEED_OPENCL_SYCL=0
  C:/msys64/usr/bin/gcc -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -w -march=native -
std=gnu++0x -D_GLIBCXX_USE_CXX11_ABI=0 -MD -MF bazel-out/x64_windows_msys-py3-op
t/bin/external/com_google_absl/absl/base/_objs/spinlock_wait/external/com_google
_absl/absl/base/internal/spinlock_wait.d -frandom-seed=bazel-out/x64_windows_msy
s-py3-opt/bin/external/com_google_absl/absl/base/_objs/spinlock_wait/external/co
m_google_absl/absl/base/internal/spinlock_wait.o -D__CLANG_SUPPORT_DYN_ANNOTATIO
N__ -iquote external/com_google_absl -iquote bazel-out/x64_windows_msys-py3-opt/
genfiles/external/com_google_absl -iquote external/bazel_tools -iquote bazel-out
/x64_windows_msys-py3-opt/genfiles/external/bazel_tools -isystem external/bazel_
tools/tools/cpp/gcc3 -Wall -Wextra -Wcast-qual -Wconversion-null -Wmissing-decla
rations -Woverlength-strings -Wpointer-arith -Wunused-local-typedefs -Wunused-re
sult -Wvarargs -Wvla -Wwrite-strings -c external/com_google_absl/absl/base/inter
nal/spinlock_wait.cc -o bazel-out/x64_windows_msys-py3-opt/bin/external/com_goog
le_absl/absl/base/_objs/spinlock_wait/external/com_google_absl/absl/base/interna
l/spinlock_wait.o
In file included from external/com_google_absl/absl/base/config.h:66:0,
                 from external/com_google_absl/absl/base/port.h:23,
                 from external/com_google_absl/absl/base/internal/spinlock_posix
.inc:23,
                 from external/com_google_absl/absl/base/internal/spinlock_wait.
cc:27:
external/com_google_absl/absl/base/policy_checks.h:40:2: error: #error ""Cygwin i
s not supported.""
 #error ""Cygwin is not supported.""
  ^~~~~
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 172.251s, Critical Path: 153.84s
FAILED: Build did NOT complete successfully
```

###Any help would be appreciated. I can give you more details as well.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",2018-01-02T07:21:03Z,37,1,4,13.246278545814494
525,15775,R1.2,cla: no,,0,,3,2018-01-01T18:44:44Z,2018-01-02T01:20:34Z,NONE,,2018-01-02T01:20:34Z,1,1,0,2.7462785458144943
526,15774,Fix invalid Markdown in docstring,"awaiting testing (then merge),cla: yes","There is currently invalid markdown in the docstring, which is causing the docs site to render incorrectly.

https://www.tensorflow.org/api_docs/python/tf/contrib/gan/estimator/GANEstimator

<img width=""902"" alt=""screen shot 2018-01-01 at 10 17 59 am"" src=""https://user-images.githubusercontent.com/279498/34469987-1d2fbce6-eedd-11e7-896b-a43f65326fd0.png"">

This patch fixes the indentation on the MD code block, which should fix the issue.",0,,5,2018-01-01T18:19:03Z,2018-01-01T21:20:03Z,CONTRIBUTOR,"There is currently invalid markdown in the docstring, which is causing the docs site to render incorrectly.

https://www.tensorflow.org/api_docs/python/tf/contrib/gan/estimator/GANEstimator

<img width=""902"" alt=""screen shot 2018-01-01 at 10 17 59 am"" src=""https://user-images.githubusercontent.com/279498/34469987-1d2fbce6-eedd-11e7-896b-a43f65326fd0.png"">

This patch fixes the indentation on the MD code block, which should fix the issue.",2018-01-01T18:19:44Z,0,2,0,4.746278545814494
527,15773,How to define multiple loss function and train_op  in tf.estimator.EstimatorSpec,stat:awaiting tensorflower,"

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: v1.4.0
- **Python version**: 2.7.12
- **CUDA/cuDNN version**: 6.0
- **GPU model and memory**: 1080 Ti + 1080

### Describe the problem
I'm currently implementing a pose estimation system and I defined my network with 3 loss and train_op in each of degree, yaw, pitch and roll. And I'm current using your tf.estimator API which I think is pretty convenient to monitor the system, however I found that I may only be able to define one loss and train_op using this set of API. I would like to know if there is any solution to train and monitor the system with multiple loss and train_op at the same time. Thanks.

### Source code / logs
```
    return tf.estimator.EstimatorSpec(
        mode=mode,
        predictions=predictions,
        loss=[yaw_total_loss, pitch_total_loss, roll_total_loss],  # error
        train_op=[yaw_train_op, pitch_train_op, roll_train_op],  # error
        eval_metric_ops=None)
```
",1,,3,2018-01-01T18:14:02Z,2018-01-09T23:48:24Z,NONE,"

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: v1.4.0
- **Python version**: 2.7.12
- **CUDA/cuDNN version**: 6.0
- **GPU model and memory**: 1080 Ti + 1080

### Describe the problem
I'm currently implementing a pose estimation system and I defined my network with 3 loss and train_op in each of degree, yaw, pitch and roll. And I'm current using your tf.estimator API which I think is pretty convenient to monitor the system, however I found that I may only be able to define one loss and train_op using this set of API. I would like to know if there is any solution to train and monitor the system with multiple loss and train_op at the same time. Thanks.

### Source code / logs
```
    return tf.estimator.EstimatorSpec(
        mode=mode,
        predictions=predictions,
        loss=[yaw_total_loss, pitch_total_loss, roll_total_loss],  # error
        train_op=[yaw_train_op, pitch_train_op, roll_train_op],  # error
        eval_metric_ops=None)
```
",2018-01-09T23:13:30Z,8,1,2,3.7462785458144943
528,15772,remove trailing semicolon at the end of line,"awaiting testing (then merge),cla: yes","removed trailing semicolon(;) in the statement

according to [Google Python Style Guide](https://google.github.io/styleguide/pyguide.html?showone=Semicolons#Semicolons)
 _""Do not terminate your lines with semi-colons and do not use semi-colons to put two commands on the same line.""_ ",0,,5,2018-01-01T16:17:46Z,2018-01-01T19:03:28Z,CONTRIBUTOR,"removed trailing semicolon(;) in the statement

according to [Google Python Style Guide](https://google.github.io/styleguide/pyguide.html?showone=Semicolons#Semicolons)
 _""Do not terminate your lines with semi-colons and do not use semi-colons to put two commands on the same line.""_ ",2018-01-01T16:28:55Z,0,2,0,4.746278545814494
529,15768,Training broke with ResourceExausted error,,"------------------------

### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 14.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.4.0
- **Python version**: 3.5
- **CUDA/cuDNN version**: 8
- **GPU model and memory**: TITAN X, 12207MiB

----------------------

Most Probably everyone will be asking about this is a question for StackOverflow. Here is the link to the StackOverflow question,  But please take a look at the problem. There could be some bug in tensorflow as the error occurs after 32 epoch.
https://stackoverflow.com/questions/48007984/training-broke-with-resourceexausted-error

Here is the code of the model, https://paste.ubuntu.com/26298336/
A short description of the model would be,

- Character level Embedding Vector -> Embedding lookup -> LSTM1
- Word level Embedding Vector->Embedding lookup -> LSTM2
- [LSTM1+LSTM2] -> single layer MLP-> softmax layer/CRF layer
- [LSTM1+LSTM2] -> Single layer MLP-> WGAN discriminator

While running the code it produces the following error output at the epoch 32,

`ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[24760,100] [[Node: chars/bidirectional_rnn/bw/bw/while/bw/lstm_cell/split = Split[T=DT_FLOAT, num_split=4, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](gradients_2/Add_3/y, chars/bidirectional_rnn/bw/bw/while/bw/lstm_cell/BiasAdd)]] [[Node: bi-lstm/bidirectional_rnn/bw/bw/stack/_167 = _Recvclient_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_636_bi-lstm/bidirectional_rnn/bw/bw/stack"", tensor_type=DT_INT32, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]]`

My question is if there is any error then it should occur in the first epoch why at 32 epoch?

I am using embedding_lookup in following way,

```
_word_embeddings = tf.Variable(
                        embeddings,
                        name=""_word_embeddings"",
                        dtype=tf.float32,
                        trainable=False)
            word_embeddings = tf.nn.embedding_lookup(_word_embeddings, self.word_ids, name=""word_embeddings"")

```

Where `embeddings` is a `(61698, 100)` sized vector. which is only 24 MB. However in the error message, it showed the error with, `(24760, 100)` sized vector which is only 10MB. It also produces warning while declaring optimizers for the model. it was suggested as below,

> gradients_impl.py:96: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.

",0,,1,2018-01-01T11:31:01Z,2018-01-03T02:13:59Z,NONE,"------------------------

### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 14.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.4.0
- **Python version**: 3.5
- **CUDA/cuDNN version**: 8
- **GPU model and memory**: TITAN X, 12207MiB

----------------------

Most Probably everyone will be asking about this is a question for StackOverflow. Here is the link to the StackOverflow question,  But please take a look at the problem. There could be some bug in tensorflow as the error occurs after 32 epoch.
https://stackoverflow.com/questions/48007984/training-broke-with-resourceexausted-error

Here is the code of the model, https://paste.ubuntu.com/26298336/
A short description of the model would be,

- Character level Embedding Vector -> Embedding lookup -> LSTM1
- Word level Embedding Vector->Embedding lookup -> LSTM2
- [LSTM1+LSTM2] -> single layer MLP-> softmax layer/CRF layer
- [LSTM1+LSTM2] -> Single layer MLP-> WGAN discriminator

While running the code it produces the following error output at the epoch 32,

`ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[24760,100] [[Node: chars/bidirectional_rnn/bw/bw/while/bw/lstm_cell/split = Split[T=DT_FLOAT, num_split=4, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](gradients_2/Add_3/y, chars/bidirectional_rnn/bw/bw/while/bw/lstm_cell/BiasAdd)]] [[Node: bi-lstm/bidirectional_rnn/bw/bw/stack/_167 = _Recvclient_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_636_bi-lstm/bidirectional_rnn/bw/bw/stack"", tensor_type=DT_INT32, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]]`

My question is if there is any error then it should occur in the first epoch why at 32 epoch?

I am using embedding_lookup in following way,

```
_word_embeddings = tf.Variable(
                        embeddings,
                        name=""_word_embeddings"",
                        dtype=tf.float32,
                        trainable=False)
            word_embeddings = tf.nn.embedding_lookup(_word_embeddings, self.word_ids, name=""word_embeddings"")

```

Where `embeddings` is a `(61698, 100)` sized vector. which is only 24 MB. However in the error message, it showed the error with, `(24760, 100)` sized vector which is only 10MB. It also produces warning while declaring optimizers for the model. it was suggested as below,

> gradients_impl.py:96: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.

",2018-01-03T02:13:59Z,2,1,0,1.7462785458144945
530,15766,tf.assert_equal raises incorrect traceback in Eager mode,"comp:eager,type:bug/performance","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:  No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04.1 LTS
- **TensorFlow installed from (source or binary)**: pip binary
- **TensorFlow version (use command below)**: 1.5.0-dev20171227
- **Python version**: 3.5.0
- **Bazel version (if compiling from source)**: 
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: None
- **GPU model and memory**: None
- **Exact command to reproduce**: python main.py


### Describe the problem

In eager mode, tf.assert_equal only shows `[]` in traceback message when two inputs are different. However, in graph mode, it does show different values in the message. 

### Source code / logs

```python
import tensorflow as tf
import tensorflow.contrib.eager as tfe

tfe.enable_eager_execution()

x = tf.constant([1,2,3])
y = tf.constant([3,2,1])

with tf.control_dependencies([tf.assert_equal(x, y)]):
    output = tf.reduce_sum(x)

```


Eager Mode Traceback:

```
Traceback (most recent call last):
  File ""/Users/matt/PycharmProjects/scratch/main.py"", line 9, in <module>
    with tf.control_dependencies([tf.assert_equal(x, y)]):
  File ""/usr/local/var/pyenv/versions/anaconda3-4.1.1/lib/python3.6/site-packages/tensorflow/python/ops/check_ops.py"", line 376, in assert_equal
    summary_msg)))
tensorflow.python.framework.errors_impl.InvalidArgumentError: 
Condition x == y did not hold.
Indices of first 0 different values:
[]
Corresponding x values:
[]
Corresponding y values:
[]
```


Graph Mode Traceback:
```
Traceback (most recent call last):
  File ""/Users/matt/PycharmProjects/scratch/main.py"", line 9, in <module>
    with tf.control_dependencies([tf.assert_equal(x, y)]):
  File ""/usr/local/var/pyenv/versions/anaconda3-4.1.1/lib/python3.6/site-packages/tensorflow/python/ops/check_ops.py"", line 391, in assert_equal
    _assert_static(condition_static, data)
  File ""/usr/local/var/pyenv/versions/anaconda3-4.1.1/lib/python3.6/site-packages/tensorflow/python/ops/check_ops.py"", line 104, in _assert_static
    message='\n'.join(data_static))
tensorflow.python.framework.errors_impl.InvalidArgumentError: 
Condition x == y did not hold element-wise:
x (Const:0) = 
[1 2 3]
y (Const_1:0) = 
[3 2 1]
```
",1,,3,2018-01-01T09:43:43Z,2018-01-05T14:05:19Z,NONE,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:  No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04.1 LTS
- **TensorFlow installed from (source or binary)**: pip binary
- **TensorFlow version (use command below)**: 1.5.0-dev20171227
- **Python version**: 3.5.0
- **Bazel version (if compiling from source)**: 
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: None
- **GPU model and memory**: None
- **Exact command to reproduce**: python main.py


### Describe the problem

In eager mode, tf.assert_equal only shows `[]` in traceback message when two inputs are different. However, in graph mode, it does show different values in the message. 

### Source code / logs

```python
import tensorflow as tf
import tensorflow.contrib.eager as tfe

tfe.enable_eager_execution()

x = tf.constant([1,2,3])
y = tf.constant([3,2,1])

with tf.control_dependencies([tf.assert_equal(x, y)]):
    output = tf.reduce_sum(x)

```


Eager Mode Traceback:

```
Traceback (most recent call last):
  File ""/Users/matt/PycharmProjects/scratch/main.py"", line 9, in <module>
    with tf.control_dependencies([tf.assert_equal(x, y)]):
  File ""/usr/local/var/pyenv/versions/anaconda3-4.1.1/lib/python3.6/site-packages/tensorflow/python/ops/check_ops.py"", line 376, in assert_equal
    summary_msg)))
tensorflow.python.framework.errors_impl.InvalidArgumentError: 
Condition x == y did not hold.
Indices of first 0 different values:
[]
Corresponding x values:
[]
Corresponding y values:
[]
```


Graph Mode Traceback:
```
Traceback (most recent call last):
  File ""/Users/matt/PycharmProjects/scratch/main.py"", line 9, in <module>
    with tf.control_dependencies([tf.assert_equal(x, y)]):
  File ""/usr/local/var/pyenv/versions/anaconda3-4.1.1/lib/python3.6/site-packages/tensorflow/python/ops/check_ops.py"", line 391, in assert_equal
    _assert_static(condition_static, data)
  File ""/usr/local/var/pyenv/versions/anaconda3-4.1.1/lib/python3.6/site-packages/tensorflow/python/ops/check_ops.py"", line 104, in _assert_static
    message='\n'.join(data_static))
tensorflow.python.framework.errors_impl.InvalidArgumentError: 
Condition x == y did not hold element-wise:
x (Const:0) = 
[1 2 3]
y (Const_1:0) = 
[3 2 1]
```
",2018-01-02T18:51:31Z,4,1,1,3.7462785458144943
531,15763,Branch 180441903,cla: yes,,0,,1,2018-01-01T00:05:18Z,2018-01-01T07:18:16Z,MEMBER,,2018-01-01T06:30:37Z,0,3,0,3.7462785458144943
532,15762,Make unused variable warning an error during TF builds.,"awaiting testing (then merge),cla: yes",We will need to eyeball build logs and see if this is really working as intended,1,,10,2017-12-31T23:44:09Z,2018-01-23T07:00:00Z,OWNER,We will need to eyeball build logs and see if this is really working as intended,2017-12-31T23:56:06Z,27,4,3,10.241363138309275
533,15761,"Revert ""add c++ gradient for op: Pow (#15245)""","awaiting testing (then merge),cla: yes","This reverts commit e1ded7fa7cfacaeea43a903e738dd3fe2baabc57.

CC @facaiy ",0,,3,2017-12-31T23:20:10Z,2018-01-01T05:37:03Z,OWNER,"This reverts commit e1ded7fa7cfacaeea43a903e738dd3fe2baabc57.

CC @facaiy ",2018-01-01T00:19:35Z,5,4,1,5.741363138309275
534,15760,Custom gradient aggregation methods,,"I would like a way to apply some custom gradient aggregation ops. Probably the simplest thing to do is just allow `tf.gradients` (and `Optimizer.compute_gradients`) to return un-aggregated gradients so I could work with those?
Anyway, seems like an easy fix? I will do this myself in a month or so (cant now as on holiday), but if someone else wants to do it/has some thoughts, I am interested.
",0,,6,2017-12-31T20:41:42Z,2018-01-03T05:13:03Z,NONE,"I would like a way to apply some custom gradient aggregation ops. Probably the simplest thing to do is just allow `tf.gradients` (and `Optimizer.compute_gradients`) to return un-aggregated gradients so I could work with those?
Anyway, seems like an easy fix? I will do this myself in a month or so (cant now as on holiday), but if someone else wants to do it/has some thoughts, I am interested.
",2018-01-01T22:04:44Z,7,1,1,4.241363138309275
535,15759,Update license year,"awaiting testing (then merge),cla: yes","TO DO:

- [x] Wait to the next year (100% done, depends on timezone)
- [ ] Merge!

:octocat:",0,,7,2017-12-31T20:26:04Z,2018-01-01T01:46:02Z,CONTRIBUTOR,"TO DO:

- [x] Wait to the next year (100% done, depends on timezone)
- [ ] Merge!

:octocat:",2017-12-31T20:36:00Z,5,2,1,5.741363138309275
536,15757,Fixes #15736,cla: yes,"This will allow users to import keras submodules without typing `from tensorflow.python.keras...` but just directly from `tensorflow.keras`.
This change also does not create a duplicate of the module object but just assign it two names, one with `tensorflow.python.keras` keeping current functionality and `tensorflow.keras` allowing a much more consistent API use.

With this change the user is able to import keras objects directly, for example
```python
from tensorflow.keras.layers import Dense
```",0,,2,2017-12-31T14:05:33Z,2017-12-31T22:34:57Z,NONE,"This will allow users to import keras submodules without typing `from tensorflow.python.keras...` but just directly from `tensorflow.keras`.
This change also does not create a duplicate of the module object but just assign it two names, one with `tensorflow.python.keras` keeping current functionality and `tensorflow.keras` allowing a much more consistent API use.

With this change the user is able to import keras objects directly, for example
```python
from tensorflow.keras.layers import Dense
```",2017-12-31T22:34:57Z,0,1,0,2.241363138309275
537,15755,Tensorflow Dataset.from_generator blocks input?,stat:awaiting response,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: both win7 and CentOS 7.2.1511
- **TensorFlow installed from (source or binary)**: pip3
- **TensorFlow version (use command below)**: 'v1.4.0-rc1-11-g130a514 1.4.0' and '1.5.0-dev20180102'
- **Python version**: 3.5.3
- **Bazel version (if compiling from source)**: NA
- **GCC/Compiler version (if compiling from source)**: NA
- **CUDA/cuDNN version**: NA
- **GPU model and memory**: NA
- **Exact command to reproduce**: NA

### Describe the problem

I submitted a problem on stackoverflow but nobody solved it. So I open it here. Does anybody can help to solve it?

Here is the problem:
[https://stackoverflow.com/questions/47917288/tensorflow-dataset-from-generator-blocks-input](https://stackoverflow.com/questions/47917288/tensorflow-dataset-from-generator-blocks-input)

I installed tensorflow 1.4.0 via pip, without gpu support. My python version is 3.5.3
  ",0,,7,2017-12-31T12:14:53Z,2018-01-03T06:41:38Z,NONE,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: both win7 and CentOS 7.2.1511
- **TensorFlow installed from (source or binary)**: pip3
- **TensorFlow version (use command below)**: 'v1.4.0-rc1-11-g130a514 1.4.0' and '1.5.0-dev20180102'
- **Python version**: 3.5.3
- **Bazel version (if compiling from source)**: NA
- **GCC/Compiler version (if compiling from source)**: NA
- **CUDA/cuDNN version**: NA
- **GPU model and memory**: NA
- **Exact command to reproduce**: NA

### Describe the problem

I submitted a problem on stackoverflow but nobody solved it. So I open it here. Does anybody can help to solve it?

Here is the problem:
[https://stackoverflow.com/questions/47917288/tensorflow-dataset-from-generator-blocks-input](https://stackoverflow.com/questions/47917288/tensorflow-dataset-from-generator-blocks-input)

I installed tensorflow 1.4.0 via pip, without gpu support. My python version is 3.5.3
  ",2018-01-03T06:41:38Z,7,1,1,4.741363138309275
538,15753, ImportError: cannot import name 'checkpoint_ops',stat:awaiting response,"
keras version =2.0.8
tensorflow version =1.2.1
anconda and windows 10
install a binary 

> `C:\Windows\System32\YOLO_Object_Detection-master>python flow --h
Traceback (most recent call last):
  File ""flow"", line 4, in <module>
    from darkflow.cli import cliHandler
  File ""C:\Windows\System32\YOLO_Object_Detection-master\darkflow\cli.py"", line 3, in <module>
    from .net.build import TFNet
  File ""C:\Windows\System32\YOLO_Object_Detection-master\darkflow\net\build.py"", line 5, in <module>
    from .ops import op_create, identity
  File ""C:\Windows\System32\YOLO_Object_Detection-master\darkflow\net\ops\__init__.py"", line 1, in <module>
    from .simple import *
  File ""C:\Windows\System32\YOLO_Object_Detection-master\darkflow\net\ops\simple.py"", line 1, in <module>
    import tensorflow.contrib.slim as slim
  File ""C:\Users\hp\Anaconda3\lib\site-packages\tensorflow\contrib\__init__.py"", line 22, in <module>
    from tensorflow.contrib import bayesflow
  File ""C:\Users\hp\Anaconda3\lib\site-packages\tensorflow\contrib\bayesflow\__init__.py"", line 24, in <module>
    from tensorflow.contrib.bayesflow.python.ops import csiszar_divergence
  File ""C:\Users\hp\Anaconda3\lib\site-packages\tensorflow\contrib\bayesflow\python\ops\csiszar_divergence.py"", line 26, in <module>
    from tensorflow.contrib.bayesflow.python.ops.csiszar_divergence_impl import *
  File ""C:\Users\hp\Anaconda3\lib\site-packages\tensorflow\contrib\bayesflow\python\ops\csiszar_divergence_impl.py"", line 43, in <module>
    from tensorflow.contrib import framework as contrib_framework
  File ""C:\Users\hp\Anaconda3\lib\site-packages\tensorflow\contrib\framework\__init__.py"", line 89, in <module>
    from tensorflow.contrib.framework.python.ops import *
  File ""C:\Users\hp\Anaconda3\lib\site-packages\tensorflow\contrib\framework\python\ops\__init__.py"", line 24, in <module>
    from tensorflow.contrib.framework.python.ops.checkpoint_ops import *
  File ""C:\Users\hp\Anaconda3\lib\site-packages\tensorflow\contrib\framework\python\ops\checkpoint_ops.py"", line 22, in <module>
    from tensorflow.python.training import checkpoint_ops
ImportError: cannot import name 'checkpoint_ops'`
  ",0,,3,2017-12-31T11:13:22Z,2018-02-07T14:19:42Z,NONE,"
keras version =2.0.8
tensorflow version =1.2.1
anconda and windows 10
install a binary 

> `C:\Windows\System32\YOLO_Object_Detection-master>python flow --h
Traceback (most recent call last):
  File ""flow"", line 4, in <module>
    from darkflow.cli import cliHandler
  File ""C:\Windows\System32\YOLO_Object_Detection-master\darkflow\cli.py"", line 3, in <module>
    from .net.build import TFNet
  File ""C:\Windows\System32\YOLO_Object_Detection-master\darkflow\net\build.py"", line 5, in <module>
    from .ops import op_create, identity
  File ""C:\Windows\System32\YOLO_Object_Detection-master\darkflow\net\ops\__init__.py"", line 1, in <module>
    from .simple import *
  File ""C:\Windows\System32\YOLO_Object_Detection-master\darkflow\net\ops\simple.py"", line 1, in <module>
    import tensorflow.contrib.slim as slim
  File ""C:\Users\hp\Anaconda3\lib\site-packages\tensorflow\contrib\__init__.py"", line 22, in <module>
    from tensorflow.contrib import bayesflow
  File ""C:\Users\hp\Anaconda3\lib\site-packages\tensorflow\contrib\bayesflow\__init__.py"", line 24, in <module>
    from tensorflow.contrib.bayesflow.python.ops import csiszar_divergence
  File ""C:\Users\hp\Anaconda3\lib\site-packages\tensorflow\contrib\bayesflow\python\ops\csiszar_divergence.py"", line 26, in <module>
    from tensorflow.contrib.bayesflow.python.ops.csiszar_divergence_impl import *
  File ""C:\Users\hp\Anaconda3\lib\site-packages\tensorflow\contrib\bayesflow\python\ops\csiszar_divergence_impl.py"", line 43, in <module>
    from tensorflow.contrib import framework as contrib_framework
  File ""C:\Users\hp\Anaconda3\lib\site-packages\tensorflow\contrib\framework\__init__.py"", line 89, in <module>
    from tensorflow.contrib.framework.python.ops import *
  File ""C:\Users\hp\Anaconda3\lib\site-packages\tensorflow\contrib\framework\python\ops\__init__.py"", line 24, in <module>
    from tensorflow.contrib.framework.python.ops.checkpoint_ops import *
  File ""C:\Users\hp\Anaconda3\lib\site-packages\tensorflow\contrib\framework\python\ops\checkpoint_ops.py"", line 22, in <module>
    from tensorflow.python.training import checkpoint_ops
ImportError: cannot import name 'checkpoint_ops'`
  ",2018-01-01T10:01:11Z,41,1,4,2.741363138309275
